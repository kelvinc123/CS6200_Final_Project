{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da51574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelvinchristian/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7728baed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '162c28880534', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'AJWeDHeyTwyzIfu9yXRzhQ', 'version': {'number': '8.5.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'c94b4700cda13820dad5aa74fae6db185ca5c304', 'build_date': '2022-10-24T16:54:16.433628434Z', 'build_snapshot': False, 'lucene_version': '9.4.1', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Password for the 'elastic' user generated by Elasticsearch\n",
    "ELASTIC_USERNAME = \"elastic\"\n",
    "ELASTIC_PASSWORD = \"d=AEspitO-_p+9dRwZeF\"\n",
    "HTTP_CA = \"http_ca.crt\"\n",
    "\n",
    "# os.system(f'export ES_ENDPOINT=\"http://{ELASTIC_USERNAME}:{ELASTIC_PASSWORD}@localhost:9200\"')\n",
    "\n",
    "# Create the client instance\n",
    "client = Elasticsearch(\n",
    "    f\"https://localhost:9200\", \n",
    "    ca_certs=HTTP_CA,\n",
    "    http_auth=(ELASTIC_USERNAME, ELASTIC_PASSWORD)\n",
    ")\n",
    "\n",
    "# Successful response!\n",
    "print(client.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = set(string.punctuation)\n",
    "punctuations.add(\"’\")\n",
    "punctuations.add(\"“\")\n",
    "punctuations.add(\"—\")\n",
    "punctuations.add(\"·\")\n",
    "punctuations.add(\"”\")\n",
    "punctuations.add(\"…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e076169",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(\"dataset\", \"medium_data.csv\")\n",
    "df = pd.read_csv(DATA_PATH).fillna(\"--\")\n",
    "df = df.drop([\"image\"], axis=1)\n",
    "df[\"reading_time\"] = pd.to_numeric(df[\"reading_time\"])\n",
    "df[\"responses\"] = pd.to_numeric(df[\"responses\"], errors=\"coerce\").fillna(-1)\n",
    "df[\"claps\"] = pd.to_numeric(df[\"claps\"])\n",
    "df[\"clap_prop\"] = df[\"claps\"] / df[\"claps\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ea59f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "      <th>clap_prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>--</td>\n",
       "      <td>850</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>--</td>\n",
       "      <td>1100</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>0.000543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>A Grammar of Graphics for Python</td>\n",
       "      <td>767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>0.000379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://towardsdatascience.com/databricks-how-...</td>\n",
       "      <td>Databricks: How to Save Files in CSV on Your L...</td>\n",
       "      <td>When I work on Python projects dealing…</td>\n",
       "      <td>354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>One example of building neural…</td>\n",
       "      <td>211</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                url  \\\n",
       "0   1  https://towardsdatascience.com/a-beginners-gui...   \n",
       "1   2  https://towardsdatascience.com/hands-on-graph-...   \n",
       "2   3  https://towardsdatascience.com/how-to-use-ggpl...   \n",
       "3   4  https://towardsdatascience.com/databricks-how-...   \n",
       "4   5  https://towardsdatascience.com/a-step-by-step-...   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                       How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Files in CSV on Your L...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                  subtitle  claps  responses  reading_time  \\\n",
       "0                                       --    850        8.0             8   \n",
       "1                                       --   1100       11.0             9   \n",
       "2         A Grammar of Graphics for Python    767        1.0             5   \n",
       "3  When I work on Python projects dealing…    354        0.0             4   \n",
       "4          One example of building neural…    211        3.0             4   \n",
       "\n",
       "            publication        date  clap_prop  \n",
       "0  Towards Data Science  2019-05-30   0.000420  \n",
       "1  Towards Data Science  2019-05-30   0.000543  \n",
       "2  Towards Data Science  2019-05-30   0.000379  \n",
       "3  Towards Data Science  2019-05-30   0.000175  \n",
       "4  Towards Data Science  2019-05-30   0.000104  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a72d9642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6508, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b5eeb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>--</td>\n",
       "      <td>850</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>--</td>\n",
       "      <td>1100</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>A Grammar of Graphics for Python</td>\n",
       "      <td>767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://towardsdatascience.com/databricks-how-...</td>\n",
       "      <td>Databricks: How to Save Files in CSV on Your L...</td>\n",
       "      <td>When I work on Python projects dealing…</td>\n",
       "      <td>354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>One example of building neural…</td>\n",
       "      <td>211</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503</th>\n",
       "      <td>6504</td>\n",
       "      <td>https://medium.com/better-marketing/we-vs-i-ho...</td>\n",
       "      <td>“We” vs “I” — How Should You Talk About Yourse...</td>\n",
       "      <td>Basic copywriting choices with a big…</td>\n",
       "      <td>661</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6504</th>\n",
       "      <td>6505</td>\n",
       "      <td>https://medium.com/better-marketing/how-donald...</td>\n",
       "      <td>How Donald Trump Markets Himself</td>\n",
       "      <td>Lessons from who might be the most popular bra...</td>\n",
       "      <td>189</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>6506</td>\n",
       "      <td>https://medium.com/better-marketing/content-an...</td>\n",
       "      <td>Content and Marketing Beyond Mass Consumption</td>\n",
       "      <td>How to acquire customers without wasting money...</td>\n",
       "      <td>207</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>6507</td>\n",
       "      <td>https://medium.com/better-marketing/5-question...</td>\n",
       "      <td>5 Questions All Copywriters Should Ask Clients...</td>\n",
       "      <td>Save time and effort by…</td>\n",
       "      <td>253</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6507</th>\n",
       "      <td>6508</td>\n",
       "      <td>https://medium.com/better-marketing/how-to-wri...</td>\n",
       "      <td>How To Write a Good Business Blog Post</td>\n",
       "      <td>An A-to-Z guide for non-writers</td>\n",
       "      <td>147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6508 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                url  \\\n",
       "0        1  https://towardsdatascience.com/a-beginners-gui...   \n",
       "1        2  https://towardsdatascience.com/hands-on-graph-...   \n",
       "2        3  https://towardsdatascience.com/how-to-use-ggpl...   \n",
       "3        4  https://towardsdatascience.com/databricks-how-...   \n",
       "4        5  https://towardsdatascience.com/a-step-by-step-...   \n",
       "...    ...                                                ...   \n",
       "6503  6504  https://medium.com/better-marketing/we-vs-i-ho...   \n",
       "6504  6505  https://medium.com/better-marketing/how-donald...   \n",
       "6505  6506  https://medium.com/better-marketing/content-an...   \n",
       "6506  6507  https://medium.com/better-marketing/5-question...   \n",
       "6507  6508  https://medium.com/better-marketing/how-to-wri...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1     Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                          How to Use ggplot2 in Python   \n",
       "3     Databricks: How to Save Files in CSV on Your L...   \n",
       "4     A Step-by-Step Implementation of Gradient Desc...   \n",
       "...                                                 ...   \n",
       "6503  “We” vs “I” — How Should You Talk About Yourse...   \n",
       "6504                   How Donald Trump Markets Himself   \n",
       "6505      Content and Marketing Beyond Mass Consumption   \n",
       "6506  5 Questions All Copywriters Should Ask Clients...   \n",
       "6507             How To Write a Good Business Blog Post   \n",
       "\n",
       "                                               subtitle  claps  responses  \\\n",
       "0                                                    --    850        8.0   \n",
       "1                                                    --   1100       11.0   \n",
       "2                      A Grammar of Graphics for Python    767        1.0   \n",
       "3               When I work on Python projects dealing…    354        0.0   \n",
       "4                       One example of building neural…    211        3.0   \n",
       "...                                                 ...    ...        ...   \n",
       "6503              Basic copywriting choices with a big…    661        6.0   \n",
       "6504  Lessons from who might be the most popular bra...    189        1.0   \n",
       "6505  How to acquire customers without wasting money...    207        1.0   \n",
       "6506                           Save time and effort by…    253        2.0   \n",
       "6507                    An A-to-Z guide for non-writers    147        0.0   \n",
       "\n",
       "      reading_time           publication        date  \n",
       "0                8  Towards Data Science  2019-05-30  \n",
       "1                9  Towards Data Science  2019-05-30  \n",
       "2                5  Towards Data Science  2019-05-30  \n",
       "3                4  Towards Data Science  2019-05-30  \n",
       "4                4  Towards Data Science  2019-05-30  \n",
       "...            ...                   ...         ...  \n",
       "6503             6      Better Marketing  2019-12-05  \n",
       "6504             5      Better Marketing  2019-12-05  \n",
       "6505             8      Better Marketing  2019-12-05  \n",
       "6506             5      Better Marketing  2019-12-05  \n",
       "6507             9      Better Marketing  2019-12-05  \n",
       "\n",
       "[6508 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63195a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df[df[\"id\"] == idx].iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5187ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_PATH = os.path.join(\"dataset\", \"html\")\n",
    "with open(os.path.join(HTML_PATH, f\"{idx}.html\"), \"r\") as f:\n",
    "    raw_data = \" \".join(f.read().replace(\"Kelvin Christian\", \"\").strip().split())\n",
    "    \n",
    "data = BeautifulSoup(raw_data, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9acc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join([node.text.strip() for node in data.findChildren([\"p\", \"h1\", \"h2\", \"h3\", \"h4\"]) if not node.text.strip() == \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokens = [s.strip().replace(\"  \", \" \") for s in sent_tokenize(text)]\n",
    "full_text = \" \".join(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ecd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_token = [lemmatizer.lemmatize(w) for w in word_tokenize(text)]\n",
    "filtered_token = [w for w in lemmatized_token if w.lower() not in stop_words]\n",
    "filtered_token = [w for w in filtered_token if w.lower() not in punctuations]\n",
    "clean_text = \" \".join(filtered_token)\n",
    "doc[\"text\"] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfc40fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/09/2022 15:55:47 - INFO - elasticsearch -   POST https://localhost:9200/medium_index/_search?size=50 [status:200 request:0.265s]\n"
     ]
    }
   ],
   "source": [
    "query = \"python tutorial\"\n",
    "column = \"text\"\n",
    "results = client.search(\n",
    "    index=\"medium_index\",\n",
    "    body={\"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\":\n",
    "            {\n",
    "                \"match\": {\n",
    "                    column: {\"query\": query}\n",
    "                }\n",
    "            },\n",
    "            \"should\":\n",
    "            {\n",
    "                \"match_phrase\": {\n",
    "                    column: {\"query\": query}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }},\n",
    "    size=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b88362b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "claps = np.array([result[\"_source\"][\"claps\"] for result in results[\"hits\"][\"hits\"]])\n",
    "claps = claps / np.sum(claps)\n",
    "scores = np.array([result[\"_score\"] for result in results[\"hits\"][\"hits\"] ])\n",
    "scores = scores / np.sum(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61e70278",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [result[\"_source\"] for result in results[\"hits\"][\"hits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8cd05771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 2835,\n",
       "  'url': 'https://towardsdatascience.com/how-to-write-python-command-line-interfaces-like-a-pro-f782450caf0d',\n",
       "  'title': 'How to Write Python Command-Line Interfaces like a\\xa0Pro',\n",
       "  'subtitle': '-',\n",
       "  'claps': 907,\n",
       "  'responses': 6.0,\n",
       "  'reading_time': 8,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-11-08',\n",
       "  'clap_prop': 0.000448015616826617,\n",
       "  'text': 'Towards Data Science Nov 8 2019 Member-only Listen Save Write Python Command-Line Interfaces like Pro Data Scientists face many repetitive similar task includes creating weekly report executing extract transform load ETL job training model using different parameter set Often end bunch Python script change parameter code every time run hate Thats got habit transforming script reusable command-line interface CLI tool increased efficiency made productive daily life started using Argparse wa enjoyable produce lot ugly code thought cant achieve without write lot code even enjoy writing CLI tool Click friend Click webpage Click aim make process writing command-line tool quick fun also preventing frustration caused inability implement intended CLI API sound great doesnt article give hands-on guide build Python CLIs using Click build example step step show basic feature benefit Click offer tutorial able write next CLI tool joy blink eye let get hand dirty Tutorial tutorial build Python CLI using Click evolves step step start basic step introduce new concept offered Click Apart Click use Poetry manage dependency package Preparation First let install Poetry various way see article use pip Next use Poetry create project named cli-tutorial add click funcy dependency create file cli.py later fill code added funcy make use later see module good refer interested reader article ready go implement first CLI side note example code available GitHub account First Click CLI initial CLI read CSV file disk process important tutorial store result Excel file path input- output file configurable user user must specify input file path Specifying output file path optional default output.xlsx Using Click code doe read invoke CLI various way Cool created first CLI using Click Note implemented read_csv process_csv write_excel assume exist supposed One issue CLIs pas parameter generic string issue string must parsed actual type fail due badly formatted user input Look example use path try load CSV file user provide string doe represent path even string formatted correctly corresponding file might exist dont right permission access Wouldnt desirable automatically validate input parse possible fail early helpful error message Ideally without write lot code Click support u specifying type argument Type Specification example CLI want user pas valid path existing file read permission requirement fulfilled load input file Additionally user specifies output file path valid path enforce passing click.Path object type argument click.option decorator click.Path one various type offered Click box also implement custom type scope tutorial detail refer interested reader Click documentation Boolean Flags Another helpful feature offered Click boolean flag Probably famous boolean flag verbose flag set true tool print lot information terminal set false thing printed Click implement add another click.option decorate set is_flag=True get verbose output need call CLI Feature Switch Assume want store result process_csv locally also want upload server Additionally one target server development- testing- production instance access three instance via different URLs One option user select server pas full URL argument ha type error-prone also tedious job situation like use feature switch simplify user life best explained code added three click.option decorator three possible server URLs important bit three option target variable server_url Depending option choose value server_url equal corresponding value defined flag_value chose one adding -- dev -- test -- prod argument execute server_url equal http //test.server.com/api/v2/upload dont specify three flag Click take value -- prod set default=True Username Password Prompts Unfortunately rather luckily server password protected upload file need username password sure provide standard click.option argument However password end command history plain text become security issue like prompt user type password without echoing terminal without storing command history username also want simple prompt echoing Nothing easier know Click code add prompt argument set prompt=True add prompt whenever user doe specify -- user argument still specify like default value used hit enter prompt default determined using function another handy feature offered Click Prompting password without echoing terminal asking confirmation common Click offer dedicated decorator called password_option important note prevent user pas password via -- password MYSECRETPASSWORD enables thats Weve build full CLI call day like give final hint next section Poetry Scripts final tip want give related Click perfectly match CLI topic creating Poetry script Poetry script create executables invoke Python function command line Setuptools script doe look like First need add following pyproject.toml file your-wanted-name alias function process defined module cli_tutorial.cli invoke allows example add multiple CLI function file define alias dont add __name__ == __main__ block Wrap article showed use Click Poetry build CLI tool joy become productive wa small subset feature offered Click many others like callback nested command choice option mention refer interested reader Click documentation might write follow-up post covering advanced topic Stay tuned thank following along post Feel free contact question comment suggestion 1.1K 8 1.1K 1.1K 8 Get email whenever Simon Hawe publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Hari Santanam Nov 8 2019 Member-only Cloud Risk Assessment Data- log analysis AWS Using EMR Cluster HiveQL S3 analyze cloud data log Cloud audit become necessity organization business governmental educational entity increasingly migrate technology infrastructure process 8 min read 8 min read Share idea million reader Bartosz Telenczuk Nov 8 2019 Managing virtual environment pyenv Python developer data scientist already heard virtual environment However managing ten environment created different project daunting pyenv help streamline creation management activating virtual environment old day virtualenv became popular would keep 5 min read 5 min read Rahil Vijay Nov 8 2019 Member-only Alternative Batch Normalization development Batch Normalization BN normalization technique wa turning point development deep learning model enabled various network train converge Despite great success BN exhibit drawback caused distinct behavior normalizing along batch dimension One 7 min read 7 min read Lj Flores Nov 7 2019 Member-only Typhoon Ruby 2014 sentiment analysis Classifying Facebook comment sentiment using Latent Dirichlet Analysis high school short internship Dr. Gerry Bagtasa university professor Institute Environmental Science Meteorology University Philippines Diliman 6 min read 6 min read Ryan P. Dalton Nov 7 2019 Member-only City Homeless Humanitarian Crisis Streets Los Angeles City Within City City Angels anyone tell many city one Traveling Westwood West Adams Silverlake Skid Row youll find nothing le microcosm United States one hundred forty nation worth 6 min read 6 min read Simon Hawe Tech programming enthusiast working Joyn mainly focusing data sciene machine learning data engineering python coding Medium Liu Zheng Better Programming Build Python Script Command-line Tool Frank Andrade Goodbye Low-Quality Udemy Courses Hello Subscription Courses Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Ryan Michael Kay Didact Publication Clear Code Write Code Easy Read Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Nov 8, 2019 Member-only Listen Save How to Write Python Command-Line Interfaces like a Pro We as Data Scientists face doing many repetitive and similar tasks. That includes creating weekly reports, executing extract, transform, load ( ETL) jobs, or training models using different parameter sets. Often, we end up having a bunch of Python scripts, where we change parameters in code every time we run them. I hate doing this! Thats why I got into the habit of transforming my scripts into reusable command-line interface (CLI) tools. This increased my efficiency and made me more productive in my daily life. I started doing this using Argparse but this was not enjoyable as I had to produce a lot of ugly code. So I thought, cant I achieve that without having to write a lot of code over and over again? Can I even enjoy writing CLI tools? Click is your friend! So what is Click? From the webpage: It (Click) aims to make the process of writing command-line tools quick and fun while also preventing any frustration caused by the inability to implement an intended CLI API. To me, that sounds great, doesnt it? In this article, I give you a hands-on guide on how to build Python CLIs using Click. I build up an example step by step that shows you the basic features and benefits Click offers. After this tutorial, you should be able to write your next CLI tool with joy and in a blink of an eye :) So lets get our hands dirty! The Tutorial In this tutorial, we build up a Python CLI using Click that evolves step by step. I start with the basics, and with each step, I introduce a new concept offered by Click. Apart from Click, I use Poetry to manage dependencies and packages. Preparation First, lets install Poetry. There are various ways of doing that, see my article , but here we use pip Next, we use Poetry to create a project named cli-tutorial, add click and funcy as a dependency, and create a file cli.py that we later fill with code I have added funcy, as I will make use of it later. To see what that module is good for, I refer the interested reader to this article . Now, we are ready to go and implement our first CLI. As a side note, all example code is available on my GitHub account. Our First Click CLI Our initial CLI reads a CSV file from disk, processes it (how is not important for this tutorial), and stores the result in an Excel file. Both, the path to the input-, and the output file should be configurable by the user. The user must specify the input file path. Specifying the output file path is optional and defaults to output.xlsx . Using Click, the code that does that reads as What do we do here? Now, you can invoke this CLI in various ways Cool, we have created our first CLI using Click! Note that, I have not implemented read_csv , process_csv , and write_excel but assume they exist and do what they are supposed to do. One issue with CLIs is that we pass parameters as generic strings. Why is that an issue? Because these strings must be parsed to the actual types, which can fail due to badly formatted user input. Look at our example where we use paths and try to load a CSV file. A user can provide a string that does not represent a path at all. And even if the string is formatted correctly, the corresponding file might not exist or you dont have the right permission to access it. Wouldnt it be desirable to automatically validate the input, parse it if possible or fail early with helpful error messages? Ideally, all that without having to write a lot of code? Click supports us with this by specifying types for our arguments. Type Specification In our example CLI, we want the user to pass in a valid path to an existing file for which we have read permissions . If these requirements are fulfilled, we can load the input file. Additionally, if the user specifies an output file path, this should be a valid path. We can enforce all that by passing a click.Path object to the type argument of the click.option decorator click.Path is one of the various types offered by Click out of the box. You can also implement custom types, but this is out of scope for this tutorial. For more details, I refer the interested readers to the Click documentation . Boolean Flags Another helpful feature offered by Click is boolean flags. Probably, the most famous boolean flag is the verbose flag. If set to true, your tool will print out a lot of information to the terminal. If set to false, only a few things are printed. With Click, we can implement that as All you have to do is add another click.option decorate and set is_flag=True . Now, to get a verbose output, you need to call the CLI as Feature Switch Assume we do not only want to store the result of process_csv locally, but we also want to upload it to a server. Additionally, there is not only one target server but a development-, a testing-, and a production instance. You can access these three instances via different URLs. One option for a user to select the server is to pass the full URL as an argument, which she has to type in. But, this is not only error-prone but also a tedious job. In situations like that, I use feature switches to simplify the users life. What they do is best explained through code Here, I have added three click.option decorators for the three possible server URLs. The important bit is, that all three options have the same target variable server_url . Depending on which option you choose,  the value of server_url is equal to the corresponding value defined in flag_value . You chose one of those by adding -- dev , -- test , or -- prod as an argument. So when you execute server_url is equal to https://test.server.com/api/v2/upload. If we dont specify any of the three flags, Click takes the value of --prod, as I set default=True . Username & Password Prompts Unfortunately, or rather luckily :), our servers are password protected. So to upload our file, we need a username and a password. For sure, you can provide those as a standard click.option arguments. However, your password ends up in your command history in plain text. This can become a security issue. We like a prompt for the user to type his password without echoing it to the terminal and without storing it in the command history. For the username, we also want a simple prompt with echoing. Nothing easier than that when you know Click. Here is the code. To add a prompt for an argument, you have to set prompt=True . This will add a prompt whenever a user does not specify the -- user argument, but she can still specify it like that. The default value is used, when you just hit enter on the prompt. The default is determined using a function, which is another handy feature offered by Click. Prompting passwords without echoing it to the terminal and asking for a confirmation is so common, that Click offers a dedicated decorator called password_option . An important note; this will not prevent the user to pass the password via --password MYSECRETPASSWORD. It just enables her not doing that. And thats it. Weve build the full CLI. Before we call it a day, I like to give you a final hint in the next section. Poetry Scripts A final tip that I want to give you, that is not related to Click but perfectly matches the CLI topic, is creating Poetry scripts . With Poetry scripts, you can create executables to invoke your Python functions from the command line, as you can do with Setuptools scripts . So how does that look like? First, you need to add the following to your pyproject.toml file The your-wanted-name is an alias for the function process defined in the module cli_tutorial.cli . Now, you can invoke that through This allows you, for example, to add multiple CLI functions to the same file, to define aliases, and you dont have to add an if __name__ == __main__ block. Wrap Up In this article, I showed you how to use Click and Poetry to build CLI tools with joy and become more productive. This was just a small subset of the features offered by Click. There are many others like callbacks , nested commands , or choice options just to mention a few. For now, I refer the interested reader to the Click documentation but I might write a follow-up post covering these advanced topics. Stay tuned and thank you for following along this post. Feel free to contact me for questions, comments, or suggestions. 1.1K 8 1.1K 1.1K 8 Get an email whenever Simon Hawe publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Hari Santanam Nov 8, 2019 Member-only Cloud Risk Assessment through Data- log analysis in AWS Using EMR Cluster, HiveQL and S3 to analyze cloud data logs Cloud and IT audits have become a necessity in most organizations as business, governmental and educational entities increasingly migrate their technology infrastructures and processes. 8 min read 8 min read Share your ideas with millions of readers. Bartosz Telenczuk Nov 8, 2019 Managing virtual environments with pyenv Most Python developers and data scientist have already heard of virtual environments. However, managing tens of environments created for different projects can be daunting. pyenv will help you to streamline the creation, management and activating virtual environments. In the old days, before the virtualenv became popular, I would keep a 5 min read 5 min read Rahil Vijay Nov 8, 2019 Member-only An Alternative To Batch Normalization The development of Batch Normalization(BN) as a normalization technique was a turning point in the development of deep learning models, it enabled various networks to train and converge. Despite its great success, BN exhibits drawbacks that are caused by its distinct behavior of normalizing along the batch dimension. One of 7 min read 7 min read Lj Flores Nov 7, 2019 Member-only Typhoon Ruby, 2014: A sentiment analysis Classifying Facebook comments sentiments using Latent Dirichlet Analysis In high school, I did a short internship with Dr. Gerry Bagtasa a university professor at the Institute of Environmental Science and Meteorology, University of the Philippines Diliman. 6 min read 6 min read Ryan P. Dalton Nov 7, 2019 Member-only The City of the Homeless: Humanitarian Crisis on the Streets of Los Angeles A City Within a City The City of Angels, as most anyone here will tell you, is many cities in one. Traveling from Westwood to West Adams or from Silverlake to Skid Row, youll find nothing less than a microcosm of the United States: one hundred and forty nations worth 6 min read 6 min read Simon Hawe Tech and programming enthusiast working at Joyn mainly focusing on data sciene, machine learning, data engineering, and python coding. More from Medium Liu Zheng in Better Programming Build Your Python Script Into a Command-line Tool Frank Andrade Goodbye Low-Quality Udemy Courses. Hello, Subscription Courses! Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Ryan Michael Kay in Didact Publication Clear Code: How To Write Code That Is Easy To Read Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4085,\n",
       "  'url': 'https://towardsdatascience.com/back-to-the-metal-top-3-programming-language-to-develop-big-data-frameworks-in-2019-69a44a36a842',\n",
       "  'title': 'Back to the metal: Top 3 Programming language to develop Big Data frameworks',\n",
       "  'subtitle': 'C++, Rust, Go over\\xa0Java…',\n",
       "  'claps': 904,\n",
       "  'responses': 4.0,\n",
       "  'reading_time': 12,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-01-26',\n",
       "  'clap_prop': 0.000446533757013519,\n",
       "  'text': 'Towards Data Science Jan 25 2019 Member-only Listen Save Back metal Top 3 Programming language develop Big Data framework C++ Rust Go Java Data Intensive framework previous blog post Programming language rule Data Intensive Big Data Fast Data framework briefly discussed popular Big Data framework showed Java de-facto programming language Data Intensive framework Java significant advantage e.g Platform Independence Productivity JVM language timeframe 20042014 dominant Big Data framework developed last 10 year lot change happened programming language landscape classic language gone major overhaul modernization Also promising modern programming language appeared elegant feature Computer Hardware ha gone major change rise Multi-Core processor GPU TPU well Containerization Docker Kubernetes came existence became mainstream someone company want develop next disruptive Big Data framework 2019 e.g next Hadoop Kafka Spark programming language best fit Big Data domain vintage language Java language First discus limitation Java propose better alternative context Data Intensive framework development point also valid develop Cloud Native IoT Machine Learning framework Limitations Java Every programming language ha limitation Also Java dominant Programming Language Data Intensive domain ha fair share limitation discus main limitation Java context Data Intensive framework development JVM JVM play huge role Java widely adopted becoming one popular programming language like many thing life sometimes biggest strength also biggest weakness main limitation JVM listed Developer Productivity Java first appeared 1995 wa productive language time lean size simplicity time Java ha added lot feature increasing language specification size/complexity considered among productive language fact Java often criticized verbose nature needing lot boilerplate code last decade Concurrency Although Java wa released pre Multi-Core era Java offer excellent Shared Memory based Concurrency support via Thread Lock deterministic Memory Model high-level abstraction Shared Memory based Concurrency difficult program prone Data Race Java doe offer language level Message Passing based Concurrency easier program correctly Asynchronous event loop based Concurrency better choice I/O heavy task Akka high-performance library offer Message Passing Asynchronous Concurrency Java without in-built support JVM performant language native support e.g Go Erlang Node.js today world Multi-Core processor huge drawback Java Serialization Javas default serialization slow ha security vulnerability result Java serialization another thorny issue Data Intensive landscape Oracle ha labeled horrible mistake plan drop future Java version Solution Back Metal declared obsolete destined demise heydeys Java Close-to-the-Metal language gaining lot interest recent year good reason C programming language wa developed Dennis Ritchie Bell Labs time 19691973 every cycle CPU every Byte memory wa expensive reason C later C++ wa designed churn maximum performance hardware expense language complexity misconception Big Data domain one doe need care much CPU/Memory someone need performance need handle data needed add Machines Big Data Custer adding Machines/Nodes also increase Cloud provider bill Also rise Machine learning/Deep learning hardware architecture change rapidly coming year programming language give full control hardware important coming day Near Metal language another drawback used Data Intensive framework Platform dependency Currently Web Server Operating System overwhelmingly dominated Linux around 97 market share public Cloud dominated Linux well 90 market share meteoric rise Containerization Docker Kubernetes give freedom develop platform e.g Windows targeting platform e.g Linux Thus Platform dependency critical factor choose Programming Language Data Intensive framework development Dont get wrong Java still formidable language develop Data Intensive framework Javas new Virtual Machine GraalVM new Garbage Collector ZGC Java even attractive language almost domain convinced Close-to-the-Metal language dominant Java/Scala coming year develop Data Intensive framework pick three Close-to-the-Metal language potential candidate develop Data Intensive framework 2019 Java/Scala C++ Like pioneer near-Metal language C C++ also ha root Bell Lab time Bell Labs Bjarne Stroustrup ha initially implemented C++ Object Oriented C first commercial release 1985 C++ general-purpose statically typed compiled programming language support multiple programming paradigm functional imperative object-oriented Like C also near Metal language give full control hardware without Memory safety Concurrency safety Similar C C++ also belief following Moto i.e C++ give developer powerful language responsibility developer make program Memory safe Data Race free C++ also ha lot feature functionality Feature Hell probably one difficult programming language master Since 2000 C++ ha added many feature Memory Model Shared Memory based Concurrency lambda make language simpler safer Concurrency friendly change come price C++ language specification ha become bigger even complex Another issue C++ long build time remember building CORBA library taking 30 minute However modern C++ e.g C++17 using principle like Resource Acquisition Initialization RAII comparatively easier develop Memory safe Data Race free programming C++ comparison older version C++ e.g C++98 C++ still lack language-level support Message Passing Concurrency come C++20 Asynchronous event loop based Concurrency Although many C++ library support Message Passing Asynchronous event loop based Concurrency legendary Node.js Asynchronous event loop based Concurrency wa developed C++ Learning C++ difficult Mastering C++ even difficult group niche experienced C++ developer build unbeatable framework domain including Data Intensive domain example 4 node ScyllaDB written C++ outperforms 40 node Cassandra written Java Pros Cons Notable Big Data Projects Rust wa always search dream Programming Language give Performance/Control near-Metal language C C++ safety Runtime language Haskell/Python Finally Rust look like Language Promised i.e give Performance/Control like C/C++ Safety Haskell Python Inspired research programming language Cyclone safer C Graydon Hoare first developed Rust personal project wa later sponsored Mozilla active contribution David Herman Brendan Eich creator JavaScript many others Rust statically typed compiled System Programming language support Functional Imperative programming paradigm First announced 2010 first stable version released 2015 concept Ownership Borrowing offer RAII language level support enables memory thread-safe programming speed C++ without Garbage Collector Virtual Machine really set apart RUST near Metal language e.g C/C++ Go give compile time safety i.e Code compiles run thread safe memory safe discussed Fearless Concurrency Rust also offer language level concurrency support Shared Memory Concurrency Message Passing Concurrency via Channel although still lack Asynchronous event-loop based Concurrency development excellent talk Alex Crichton Mozilla explaining Rust Concurrency Rust also ha expressive type numeric type like ML languages/Haskell ha immutable data structure default result offer excellent functional Concurrency data Concurrency like ML languages/Haskell Rust Web Assembly next big thing Browser developed Mozilla high performant fast Rust code directly converted Web Assembly run Browser Another interesting feature Rust ha self-hosted Compiler i.e Compiler Rust written Rust 23 year Java yet ha self-hosted Compiler Rust also great language Data Intensive domain due memory safe data race free zero cost abstraction concurrency feature Service Mesh platform Linkered migrated Scala+Netty+Finagle Stack Rust achieved much better performance resource utilization Data Intensive runtime Weld written Rust give 30x performance gain Data Intensive framework e.g Spark Pros Cons Notable Big Data Projects Go Go second language list ha root Bell Labs Two three co-creators language Rob Pike Plan 9 UTF-8 Ken Thompson creator Unix worked Bell lab time Unix C C++ wa originated middle 2000 Google huge problem Scalability Developer Scalability 1000 developer work codebase efficiently Application Scalability Application deployed Scalable way 1000 machine Google also issue integrating fresh graduate existing multi-million line complex C++ codebase high compile time C++ codebase issue discussed detail Finding existing language C++ Java sufficient tackle issue Google ha employed two best person software industry Rob Pike Ken Thompson create new language Go wa first announced 2010 first official version released 2012 Go designer taken C basis created simple productive yet powerful statically typed compiled garbage collected System Programming language Another key feature Go compile time fast creates single executable binary file also contains Go Runtime Garbage Collector MB requires separate VM Go also offer CSP based Message Passing Concurrency Communicating Sequential Processes originated Tony Hoare paper almost like way Erlang Although instead using Actor Channel used Erlang Go us goroutine lightweight green thread channel Message Passing Another difference Erlang us point-to-point communication Actors whereas Go us flexible indirect communication goroutines result Go offer simple yet extremely scalable Concurrency Model take advantage modern Multi-Core processor excellent talk Gos Concurrency Model Rob Pike keep language simple productive Go lack lot feature like Shared Memory based Concurrency although Go offer sharing memory channel Moto communicate sharing memory instead share memory communicating many high-level abstraction e.g Generics Backed Google Go ha well accepted community/industry ha excellent toolings/libraries best Infrastructure framework Docker Kubernetes well Data Intensive framework developed using Go Pros Cons Notable Big Data Projects found helpful please share favorite forum Twitter Facebook LinkedIn Comments constructive criticism highly appreciated Thanks reading interested Programming language also read following article Top 10 In-Demand programming language learn 2020 In-depth analysis ranking top programming language job seeker new developer towardsdatascience.com Top 10 Databases Use 2021 MySQL Oracle PostgreSQL Microsoft SQL Server MongoDB Redis Elasticsearch Cassandra MariaDB IBM Db2 md-kamaruzzaman.medium.com Programming language rule Data Intensive Big Data+Fast Data framework brief overview Big Data framework towardsdatascience.com 946 5 946 946 5 Enjoy read Reward writer Beta tip go Md Kamaruzzaman third-party platform choice letting know appreciate story Get email whenever Md Kamaruzzaman publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Brayden Gerrard Jan 25 2019 Member-only Evolution US Electric Grid past 15 year US electric grid ha undergone massive shift mostly better Renewable energy natural gas coal GHG emission see US grid ha evolved recent year 4 min read 4 min read Share idea million reader zgr Gen Jan 25 2019 Member-only Notes Artificial Intelligence Machine Learning Deep Learning curious people AI ha intriguing topic 2018 according McKinsey mentioned key enabler 1 3 spot Gartner Top 10 Strategic Technology Trends 2019 AI became catch-all term refers computer program automatically doe something 22 min read 22 min read Felix Kuestahler Jan 25 2019 Member-only Python Tutorial Twitter Account Retrieval Swiss Government Members Start journey social medium analysis politician Click link Title list tutorial article first article tutorial show extract list tweeting Swiss Government Members via Twitter API extracted data put Panda Dataframe 7 min read 7 min read Tobias Hill Jan 25 2019 got 1 better accuracy data augmentation hard let go thing put love yes really thought wa done 5 th article series accuracy/error metric MNIST example started haunting wa quite sure could improve 4 min read 4 min read Partha Deka Jan 25 2019 Empowering citizen data scientist hardware design manufacturing Improving productivity hardware design manufacturing professional advanced AI tool Authors Partha Deka Rohit Mittal citizen data scientist Expert data scientist rely custom coding make sense data use case could data cleansing data imputation creating segment finding pattern data building 4 min read 4 min read Md Kamaruzzaman Enterprise Architect Certified AWS/AZURE/GCP Architect Full-stack Cloud Big Data Follow Twitter http //twitter.com/KamaruzzMd Medium Yash Prakash Code 17 Golang Packages Know Matt Welsh Using Rust startup cautionary tale Fredy Sandoval Programming language faster Golang Company Comparison Golang JavaBackend Battle 2022 Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Jan 25, 2019 Member-only Listen Save Back to the metal: Top 3 Programming language to develop Big Data frameworks C++, Rust, Go over Java for Data Intensive frameworks In a previous blog post:  Programming language that rules the Data Intensive (Big Data, Fast Data) frameworks  , I have briefly discussed some of the most popular Big Data frameworks and showed that Java is the de-facto programming language in Data Intensive frameworks. Java had significant advantages (e.g. Platform Independence, Productivity, JVM) over other languages during the timeframe 20042014 when most of the dominant Big Data frameworks were developed. In the last 10 years, lots of changes happened in the programming language landscape. Some classic languages have gone through major overhauls and modernizations. Also some very promising, modern programming languages appeared with elegant features. Computer Hardware has gone through major changes ( rise of Multi-Core processors, GPU, TPU ) as well. Containerization with Docker, Kubernetes came to existence and became mainstream. If someone or some company wants to develop the next disruptive Big Data framework in 2019 (e.g. next Hadoop, Kafka, Spark), what programming language will be the best fit? The Big Data domain vintage language Java or any other language? First I will discuss the limitations of Java and then I will propose better alternatives in the context of Data Intensive framework development . Most of the points are also valid to develop Cloud Native , IoT and Machine Learning frameworks. Limitations of Java Every programming language has its limitations. Also Java, the most dominant Programming Language in the Data Intensive domain, has its fair share of limitations. Here I will discuss the main limitations of Java in the context of Data Intensive framework development. JVM: JVM plays a huge role for Java being widely adopted and becoming one the most popular programming language. But like many things in life, sometimes the biggest strength is also the biggest weakness. The main limitations of JVM are listed below: Developer Productivity: When Java first appeared in 1995, it was a very productive language at that time with its lean size and simplicity. With time, Java has added lots of features, increasing language specification size/complexity and can no more be considered among the most productive languages. In fact, Java is often criticized for its verbose nature needing lots of boilerplate code in the last decade. Concurrency: Although Java was released in the pre Multi-Core era, Java offers excellent Shared Memory based Concurrency support via Thread, Lock, deterministic Memory Model and other high-level abstractions. Shared Memory based Concurrency is difficult to program and prone to Data Race. Java does not offer any language level Message Passing based Concurrency (easier to program correctly) or Asynchronous event loop based Concurrency (better choice for I/O heavy tasks). Akka  or other high-performance libraries can offer Message Passing or Asynchronous Concurrency in Java. But without the in-built support from JVM, they will not be as performant as languages which have native support (e.g. Go , Erlang , Node.js ). In todays world of Multi-Core processors, this is a huge drawback of Java. Serialization : Javas default serialization is very slow and has security vulnerabilities. As a result, Java serialization is another thorny issue in the Data Intensive landscape which Oracle has labeled as a  horrible mistake   and plans to drop in future Java versions. Solution: Back to the Metal Once declared obsolete and destined to demise during the heydeys of Java, the Close-to-the-Metal languages are gaining lots of interest in recent years and for good reasons. The C programming language was developed by  Dennis Ritchie  in Bell Labs during a time (19691973) when every cycle of CPU and every Byte of memory was very expensive. For this reason, C (and later C++) was designed to churn out the maximum performance from the hardware with the expense of language complexity. There is a misconception that in Big Data domain, one does not need to care too much about CPU/Memory. If someone needs more performance or need to handle more data, all is needed to add more Machines in Big Data Custer. But adding more Machines/Nodes will also increase Cloud provider bill. Also, with the rise of Machine learning/Deep learning, hardware architecture will change rapidly in the coming years. So, programming languages that give full control over hardware will only be more and more important in coming days. Near Metal languages had another drawback to be used in Data Intensive frameworks: Platform dependency. Currently, Web Server Operating System is overwhelmingly dominated by  Linux with around 97% market share   : The public Cloud is dominated by Linux as well with more than 90% market share: The meteoric rise of Containerization with Docker, Kubernetes gives freedom to develop in any platform (e.g. Windows) targeting any other platform (e.g. Linux). Thus, Platform dependency is no more a critical factor to choose Programming Language for Data Intensive framework development. Dont get me wrong, Java is still a formidable language to develop Data Intensive frameworks. With Javas new  Virtual Machine GraalVM    and new  Garbage Collector ZGC  , Java will be even more attractive language in almost any domain  . But I am convinced that Close-to-the-Metal languages will be more dominant than Java/Scala in coming years to develop Data Intensive frameworks. Here I will pick three Close-to-the-Metal languages as a potential candidate to develop Data Intensive frameworks in 2019 over Java/Scala. C++ Like the pioneer near-Metal language C, C++ also has its root in Bell Lab. During his time in Bell Labs,  Bjarne Stroustrup  has initially implemented C++ as Object Oriented C with first commercial release in 1985. C++ is a general-purpose, statically typed, compiled programming language which supports multiple programming paradigm (functional, imperative, object-oriented). Like C, it is also a near Metal language which gives full control over hardware without Memory safety or Concurrency safety. Similar to C, C++ also believes in the following Moto: i.e. C++ will give the developers a very powerful language but it the responsibility of the developers to make the program Memory safe or Data Race free. C++ also has lots of features and functionality (Feature Hell) and probably one of the most difficult programming languages to master. Since 2000, C++ has added many features (Memory Model, Shared Memory based Concurrency, lambda) to make the language simpler, safer and Concurrency friendly. But these changes have come with a price, C++ language specification has become bigger and even more complex. Another issue of C++ is its long build time (I remember building a CORBA library taking 30 minutes). However, with modern C++ (e.g. C++17  ) and using principles like  Resource Acquisition Is Initialization (RAII)  , it is comparatively easier to develop Memory safe, Data Race free programming in C++ in comparison to the older version of C++ (e.g. C++98). C++ still lacks language-level support for Message Passing Concurrency (will come in  C++20  ) and Asynchronous event loop based Concurrency. Although there are many C++ libraries which supports Message Passing and Asynchronous event loop based Concurrency (legendary Node.js Asynchronous event loop based Concurrency was developed in C++) . Learning C++ is difficult. Mastering C++ is even more difficult. But if there is a group of niche, experienced C++ developer, they can build unbeatable frameworks (in any domain including Data Intensive domain). There is the example of a  4 node ScyllaDB (written in C++) which outperforms the 40 node Cassandra (written in Java)  . Pros: Cons: Notable Big Data Projects: Rust There was always a search for a dream Programming Language which will give the Performance/Control of near-Metal languages (C, C++) and safety of Runtime languages (Haskell/Python). Finally, Rust looks like The Language that Promised i.e. it gives the Performance/Control like C/C++ with the Safety of  Haskell  /  Python  . Inspired by the research programming language  Cyclone (safer C)  ,  Graydon Hoare  first developed Rust as a personal project which was later sponsored by Mozilla with active contribution from David Herman ,  Brendan Eich    (creator of JavaScript)  and many others. Rust is a statically typed, compiled System Programming language which supports Functional and Imperative programming paradigm. First announced in 2010, its first stable version is released in 2015. With the concept of  Ownership  and  Borrowing  , it offers the  RAII  from language level support and enables memory, thread-safe programming with the speed of C++ without any Garbage Collector or Virtual Machine. What really sets apart RUST from other near Metal languages (e.g. C/C++, Go) is that it gives the compile time safety i.e. if a Code compiles, it will run thread safe and memory safe as discussed in  Fearless Concurrency in Rust  . It also offers language level concurrency support for both Shared Memory Concurrency and  Message Passing Concurrency (via Channel)  although it still lacks Asynchronous event-loop based Concurrency (in development). Here is an excellent talk by Alex Crichton from Mozilla explaining Rust Concurrency: Rust also has expressive types and numeric types like ML languages/Haskell and has immutable data structure by default. As a result, it offers excellent functional Concurrency and data Concurrency like ML languages/Haskell. As both Rust and  Web Assembly (the next big thing in Browser)  are developed by Mozilla, high performant and fast Rust code can directly be converted to Web Assembly to run on Browser. Another very interesting feature is that Rust has  self-hosted Compiler  i.e. Compiler of Rust is written in Rust (After 23 years, Java not yet has self-hosted Compiler). Rust is also a great language in the Data Intensive domain due to its memory safe, data race free, zero cost abstraction, concurrency features. The Service Mesh platform  Linkered  is migrated from Scala+Netty+Finagle Stack to Rust and achieved much better performance and resource utilization. The Data Intensive runtime  Weld  which is written in Rust can give up to 30x performance gain for Data Intensive frameworks (e.g. Spark). Pros: Cons: Notable Big Data Projects: Go Go is the second language in this list which has its roots in Bell Labs. Two of the three co-creators of the language:  Rob Pike  (  Plan 9  ,  UTF-8  ) and  Ken Thompson  (creator of Unix) worked in Bell labs during the time when Unix, C, C++ was originated there. In the middle of 2000, Google had a huge problem of Scalability: Developer Scalability (1000 of developers can not work on the same codebase efficiently) and Application Scalability (Application cannot be deployed in a Scalable way on 1000 machines). Google also had the issue of integrating fresh graduates with existing multi-million lines complex C++ codebase, high compile time of C++ codebase and some other issues discussed in detail  here  . Finding existing languages (C++, Java) not sufficient to tackle those issues, Google has employed two of the best person in the software industry: Rob Pike and Ken Thompson to create a new language. Go was first announced in 2010 with the first official version released in 2012. Go designers have taken C as their basis and created a simple, productive yet powerful statically typed, compiled, garbage collected System Programming language. Another key feature of Go is that its compile time is very fast and it creates a single executable binary file which also contains Go Runtime and Garbage Collector (few MB) and requires no separate VM. Go also offers CSP based Message Passing Concurrency (Communicating Sequential Processes, originated from Tony Hoare  paper  ) almost like the same way as Erlang. Although instead of using Actor and Channel (used by Erlang), Go uses  goroutine (lightweight green threads) and channel  for Message Passing. Another difference is Erlang uses point-to-point communication between Actors whereas Go uses flexible, indirect communication between goroutines. As a result, Go offers very simple yet extremely scalable Concurrency Model to take advantage of modern Multi-Core processors. Here is an excellent talk about Gos Concurrency Model by Rob Pike : To keep the language simple and productive, Go lacks lots of features like Shared Memory based Concurrency (although Go offers sharing memory between channel with the Moto:  Do not communicate by sharing memory; instead, share memory by communicating   ) and many high-level abstractions (e.g. Generics). Backed by Google, Go has been well accepted by the community/industry and has excellent toolings/libraries. Some of the best Infrastructure frameworks (Docker, Kubernetes), as well as Data Intensive frameworks, are developed using Go. Pros: Cons: Notable Big Data Projects: If you found this helpful, please share it on your favorite forums ( Twitter, Facebook, LinkedIn ). Comments and constructive criticisms are highly appreciated. Thanks for reading! If you are interested in Programming languages, you can also read my following articles: Top 10 In-Demand programming languages to learn in 2020 In-depth analysis and ranking of the top programming languages for job seekers and new developers towardsdatascience.com Top 10 Databases to Use in 2021 MySQL, Oracle, PostgreSQL, Microsoft SQL Server, MongoDB, Redis, Elasticsearch, Cassandra, MariaDB, IBM Db2 md-kamaruzzaman.medium.com Programming language that rules the Data Intensive (Big Data+Fast Data) frameworks. A brief overview on Big Data frameworks towardsdatascience.com 946 5 946 946 5 Enjoy the read? Reward the writer. Beta Your tip will go to Md Kamaruzzaman through a third-party platform of their choice, letting them know you appreciate their story. Get an email whenever Md Kamaruzzaman publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Brayden Gerrard Jan 25, 2019 Member-only The Evolution Of The US Electric Grid Over the past 15 years, the US electric grid has undergone some massive shifts (mostly for the better). Renewable energy and natural gas are up, while coal and GHG emissions are down. Here we can see how the US grid has evolved in recent years: 4 min read 4 min read Share your ideas with millions of readers. zgr Gen Jan 25, 2019 Member-only Notes on Artificial Intelligence, Machine Learning and Deep Learning for curious people AI has been the most intriguing topic of 2018 according to McKinsey. It is mentioned as the key enabler now at the #1 and #3 spot of Gartner Top 10 Strategic Technology Trends for 2019. AI became a catch-all term that refers to any computer program that automatically does something 22 min read 22 min read Felix Kuestahler Jan 25, 2019 Member-only Python Tutorial: Twitter Account Retrieval of Swiss Government Members Start your journey into social media analysis of politicians Click on the link above the Title for a list of all tutorial articles. This is the first article. The tutorial will show you how to extract a list of tweeting Swiss Government Members via the Twitter API. The extracted data will be put into a Panda Dataframe and then 7 min read 7 min read Tobias Hill Jan 25, 2019 How I got 1% better accuracy by data augmentation It is hard to let go of things you put love into. And yes, I really thought I was done after the 5:th article in this series but then the accuracy/error metrics from the MNIST example started haunting me and I was quite sure that I could improve on 4 min read 4 min read Partha Deka Jan 25, 2019 Empowering a citizen data scientist for hardware design & manufacturing Improving productivity of a hardware design and manufacturing professional with an advanced AI tool Authors: Partha Deka and Rohit Mittal What is a citizen data scientist? Expert data scientists rely on custom coding to make sense out of data. The use case could be data cleansing, data imputation, creating segments, finding patterns in the data, building 4 min read 4 min read Md Kamaruzzaman Enterprise Architect | Certified AWS/AZURE/GCP Architect | Full-stack | Cloud | Big Data | Follow Me On Twitter: https://twitter.com/KamaruzzMd More from Medium Yash Prakash in This Code 17 Golang Packages You Should Know Matt Welsh Using Rust at a startup: A cautionary tale Fredy Sandoval Which Programming language is faster? Golang Company Comparison between Golang and JavaBackend Battle of 2022 Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 359,\n",
       "  'url': 'https://towardsdatascience.com/identifying-the-sources-of-winter-air-pollution-in-bangkok-part-ii-72539f9b767a',\n",
       "  'title': 'Identifying the Sources of Winter Air Pollution in Bangkok Part\\xa0II',\n",
       "  'subtitle': '-',\n",
       "  'claps': 738,\n",
       "  'responses': 10.0,\n",
       "  'reading_time': 9,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 0.0003645375140220985,\n",
       "  'text': \"Towards Data Science Mar 30 2019 Listen Save Identifying Sources Winter Air Pollution Bangkok Part II previous blog looked winter air pollution Bangkok main source pollution come particle smaller 2.5 micrometer PM 2.5 particle particle smaller width human hair easily enter body even making way blood Last week March 17 2019 many province northern part Thailand worst Air Quality Index AQI world due particle pollution far long term solution ha proposed source PM 2.5 particle pollution ha clearly pinpointed notebook identify source high PM 2.5 particle Bangkok machine learning model code found GitHub page High PM2.5 Culprits three major theory regarding source air pollution Bangkok 1 temperature inversion effect cold air along pollution trapped close surface Earth theory wa proposed government beginning 2019 winter season government blamed emission old diesel engine pollution 2 Agricultural burning either locally surrounding province winter lot open agricultural burning occurs throughout country official tried tackle air pollution problem reducing open agricultural burning 3 Pollution province country NGOs blamed pollution near power plant analysis procedure follows Build machine learning model ML predict air pollution level Bangkok using environmental factor weather traffic index fire map Include date-time feature local hour weekday versus weekend model capture effect human activity Identify dominant source pollution using feature importance provided ML model source pollution local AQI depend factor weather pattern wind speed humidity average temperature local traffic hour day pollution agricultural burning AQI depend active fire time lag account geographical separation Fire activity included based distance Bangkok hand pollution correlated fire map model put weight weather pattern wind direction wind speed list feature considered data source Let first walk feature included model Agricultural Burning Major Problem Farmers Southeast Asia pick January March burning season north northeastern province Thailand burning activity large enough make province among polluted place world time Bangkok one might argue region heavily industrial rather agricultural may affected much agricultural burning case tiny size PM 2.5 particle remain suspended atmosphere prolonged period travel long distance weather data average wind speed 10 km/hour reported PM 2.5 level rolling average 24 hour rough estimate current PM 2.5 reading may source far 240 km away picture show fire map measured NASAs satellite indicative agricultural burning Jan 8 2018 Feb 8 2018 yellow circle indicates area within 240 km Bangkok number fire Jan 8 ha acceptable level pollution much lower number fire Feb 8 ha unhealthy level pollution fact fire pattern closely aligns PM 2.5 pattern Weather Patterns temperature inversion effect often occurs winter temperature cooler near ground hotter air top trap cool air flowing stagnant atmospheric condition allows PM 2.5 particle remain suspended air longer hand higher humidity rain help remove particle atmosphere one reason past air pollution wa high government ha sprayed water air Unfortunately mitigation doe appear effective since volume water minuscule compared actual rain much influence doe weather pattern air pollution Lets compare weather winter versus season Temperature wind speed humidity lower winter large amount let look relationship PM 2.5 level Higher temperature disrupts temperature inversion effect wind speed humidity negative correlation pollution level windy day pollution clearly better median distribution PM 2.5 level lower windy day compared day without wind fact pollution level also depends wind direction seen plot selected four major wind direction simplicity day wind come south pollution level lower likely Thai gulf south Bangkok clean ocean wind improves air quality Wind three direction pas overland However wind better stagnant atmospheric condition calm day shift median PM 2.5 level smaller rainy day day rain fewer rainy day winter season data somewhat noisy difference observed cumulative density function Traffic Index One source PM 2.5 particle car engine exhaust campaigning public transportation usage general good environment effectiveness toward reducing PM 2.5 pollution unclear seen PM 2.5 level related time day pollution lower around 3 pm remains high night time plotted traffic data relationship pollution level noisy doe seem strong correlation Including time day weekday versus weekend information model might make relationship clear Autoregression process current PM 2.5 value also depend previous value partial autocorrelation plot show strong correlation 1 hr time lag mean PM 2.5 level autoregression process Thus include 24 hour average value model restriction model allowed see previous value future prediction importance feature directly related long particle stay atmosphere Machine Learning Model picture show dedrogram input feature calculated Spearman correlation dendrogram help identify redundant feature removed model number fire within various distance level PM 2.5 closely related feature away ended using feature model identify major contribution pollution used random forest regression fit model simplicity ease interpretation hyper-parameter tuning 25 data wa allocated validation set model wa retrained using entire dataset model achieves 0.99 R-squared training set Since purpose study understand source air pollution past focused training set plot rank importance contributing factor importance calculated decrease R-squared value upon permuting column re-normalizing sum column expected previous pollution level important predictor followed number fire closest furthest number fire far away 720 km ha influence air quality local humidity traffic even rain hour day important predictor traffic index Among weather feature humidity important feature influence feature illustrated using tree interpreter data Jan 13 2019 8 96 PM 2.5 level start average value 26 PM 2.5 level previous hour wa 62 thus model add value 20 150 fire within 240 km radius thus model add 10 pollution level value 56 1649 fire 240-480 km 896 fire 480-720 km model add value 9 8 respectively low wind speed morning rush hour 8 add 8 model six top factor account 81 total 96 predicted PM 2.5 level remaining feature right le important thus increase predicted pollution value le good day Feb 2 2019 7 pm PM 2.5 level wa 10 pollution level hour wa low thus model subtracts value 10 still lot fire area model add value 2 wind speed wa high reducing value 2 weather traffic good combination many factor result low predicted PM 2.5 level 10 Conclusions PM 2.5 level ha complex relationship various factor number fire weather pattern traffic analysis confirms suspicion many people agricultural burning root cause PM 2.5 pollution Thailand Burning activity far 720 km away Bangkok area extends Myanmar Laos Cambodia cause air problem Bangkok Solving problem easy require collaborative international effort among Southeast Asian country leave fire map March 17 2019 one worst day ever 772 15 772 772 15 Towards Data Science home data science Medium publication sharing concept idea code Sean McClure Mar 30 2019 Step-by-Step Guide Creating R Python Libraries JupyterLab R Python bread butter today machine learning language R provides powerful statistic quick visualization Python offer intuitive syntax abundant support choice interface today major AI framework article well look step involved creating library 32 min read 32 min read Share idea million reader Charlene Chambliss Mar 30 2019 Cleaning Analyzing Visualizing Survey Data Python tutorial using panda matplotlib seaborn produce digestible insight dirty data work data D2C startup good chance asked look survey data least since SurveyMonkey one popular survey platform good chance itll SurveyMonkey data way SurveyMonkey export 10 min read 10 min read Matthew Stewart Mar 30 2019 Handling Discriminatory Biases Data Machine Learning data tell racist course algorithm racist Theyre made people Stephen Bush New Statesman America Ethics Machine Learning time machine learning doe touch particularly sensitive social moral ethical issue Someone give u data set asks u predict house price based 13 min read 13 min read Abraham Kang Mar 30 2019 Member-only Applied AI Going Concept ML Components Opening mind different way applying machine learning real world Abraham Kang special thanks Kunal Patel Jae Duk Seo sounding board providing input article 6 min read 6 min read Michel Kana Ph.D Mar 30 2019 Wild Wide AI responsible data science shoot first new race human Data Science good thing u improves life make thing efficient effective lead better experience however miss-steps data-driven analysis ha already exhibited example data science tool intentionally unintentionally misused 2012 9 min read 9 min read Worasom Kundhikanjana machine learning deep learning computer vision http //github.com/worasom/ Medium Luay Rahil ILLUMINATION Elon Musk Eliminated Remote Work Working Home `` n't Work '' Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Akshad Singi Better Humans 4 Unsexy One-Minute Habits Save 30+ Hours Every Week Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Mar 30, 2019 Listen Save Identifying the Sources of Winter Air Pollution in Bangkok Part II In the previous blog , I looked at the winter air pollution in Bangkok. The main source of pollution comes from particles smaller than 2.5 micrometer (PM 2.5 particles). These particles are smaller than the width of a human hair and can easily enter our bodies, even making their way into our blood. Last week (March 17, 2019), many provinces in the northern part of Thailand had the worst Air Quality Index (AQI) in the world due to particle pollution. So far, no long term solution has been proposed because the source of the PM 2.5 particle pollution has not been clearly pinpointed. In this notebook, I identify the sources of high PM 2.5 particles in Bangkok through a machine learning model. The code can be found in my GitHub page. High PM2.5, Who Are the Culprits ? There are three major theories regarding the source of air pollution in Bangkok: (1) The temperature inversion effect where cold air along with pollution is trapped close to the surface of the Earth. This theory was proposed by the government at the beginning of the 2019 winter season. The government blamed emission from old diesel engines for the pollution. (2) Agricultural burning, either locally or from surrounding provinces. During winter, a lot of open agricultural burning occurs throughout the country. Some officials have tried to tackle the air pollution problem by reducing open agricultural burning. (3) Pollution from other provinces or countries. Some NGOs blamed the pollution on near by power plants. My analysis procedure is as follows: Build a machine learning model(ML) to predict the air pollution level in Bangkok using environmental factor such as weather, traffic index, and fire maps. Include date-time features such as local hour, and weekday versus weekend in the model to capture other effects from human activities. Identify dominant sources of pollution using the feature of importance provided by the ML model. If the source of the pollution is local, then the AQI will depend on factors such as weather patterns (wind speed, humidity, average temperature), local traffic, and hour of day. If the pollution is from agricultural burning, the AQI will depend on active fires with some time lag to account for geographical separation. Fire activities are included based on the distance from Bangkok. On the other hand, if the pollution not correlated with the fire map, then the model should put more weight on weather patterns, such as wind direction and wind speed. Here are a list of features I considered and their data sources: Let me first walk through all the features included in the model. Agricultural Burning is a Major Problem ! Farmers in Southeast Asia pick January March as their burning season. For the north and northeastern provinces in Thailand, these burning activities are large enough to make these provinces among the most polluted places in the world during this time. For Bangkok, one might argue that because the region is heavily industrial rather than agricultural, it may not be affected as much by agricultural burning. But this is not the case. Because of the tiny size of PM 2.5 particles, they remain suspended in the atmosphere for prolonged periods and can travel over very long distances. From the weather data, the average wind speed is 10 km/hour. The reported PM 2.5 level is a rolling average over 24 hours. A rough estimate is that the current PM 2.5 reading may be from sources as far as 240 km away. The picture below shows the fire map measured by NASAs satellites, indicative of agricultural burning, on Jan 8, 2018 and on Feb 8, 2018. The yellow circle indicates the area within 240 km of Bangkok. The number of fires on Jan 8, which has an acceptable level of pollution, is much lower than the number of fires on Feb 8, which has an unhealthy level of pollution. In fact, the fire pattern closely aligns with the PM 2.5 pattern. Weather Patterns The temperature inversion effect often occurs during winter because the temperature is cooler near the ground. The hotter air on top traps the cool air from flowing. This stagnant atmospheric condition allows the PM 2.5 particles to remain suspended in the air for longer. On the other hand, higher humidity or rain will help remove particles from the atmosphere. This is one reason why in the past when the air pollution was high, the government has sprayed water in the air. Unfortunately, this mitigation does not appear to be effective, since the volume of water is minuscule compared to actual rain. How much influence does weather pattern have on air pollution? Lets compare the weather in winter versus other seasons. Temperature, wind speed and humidity are all lower in winter, but not by a large amount. Now, lets look at the relationship of each of these with the PM 2.5 level. Higher temperature (which disrupts the temperature inversion effect), wind speed and humidity have a negative correlation with the pollution level. On windy days, the pollution is clearly better. The median of the distribution for PM 2.5 levels is lower on windy days compared to on days without wind. In fact, the pollution level also depends on the wind direction, as seen in this plot. I selected only four major wind directions for simplicity. On the days where the wind comes from the south, the pollution level is lower likely because the Thai gulf is to the south of Bangkok. The clean ocean wind improves the air quality. Wind from the other three directions pass overland. However, having any wind is better than the stagnant atmospheric conditions on calm days. The shift in the median PM 2.5 level is smaller between rainy days and days with no rain. There are fewer rainy days during the winter season, so the data is somewhat noisy, but a difference can be observed in the cumulative density function. Traffic Index One of the sources of PM 2.5 particles is car engine exhaust. While campaigning for more public transportation usage is in general good for the environment, the effectiveness toward reducing PM 2.5 pollution is unclear. Here is why. We have seen that PM 2.5 levels are related to the time of day. The pollution is lower around 3 pm, but remains high during the night time. When plotted against the traffic data, the relationship with the pollution level is very noisy. There does not seem to be a strong correlation. Including the time of day and weekday versus weekend information into the model might make the relationship more clear. Autoregression process The current PM 2.5 value can also depend on the previous value. The partial autocorrelation plot below shows a strong correlation at 1 hr time lag, which means the PM 2.5 level is an autoregression process. Thus I include the 24 hour average values in the model, with the restriction that the model is only allowed to see the previous value for future predictions. The importance of this feature should be directly related to how long the particles stay in the atmosphere. Machine Learning Model The picture below show a dedrogram of all input features calculated from Spearman correlation. The dendrogram helps to identify redundant features that can be removed from the model. The number of fires within various distances and the level of PM 2.5 are closely related. Other features are further away. I ended up using all of these features in the model. To identify the major contributions to the pollution, I used a random forest regression to fit the model because of its simplicity and ease of interpretation. During hyper-parameter tuning, 25% of the data was allocated for the validation set. The model was retrained again using the entire dataset. The model achieves 0.99 R-squared on the training set. Since the purpose of this study is to understand the sources of the air pollution in the past, I focused on the training set. The plot below ranks the importance of each contributing factor. The importance is calculated from the decrease in the R-squared values upon permuting the column, and re-normalizing the sum of all columns. As expected the previous pollution level is the most important predictor. This is followed by the number of fires from the closest to the furthest. The number of fires as far away as 720 km has more influence on the air quality than the local humidity, traffic, or even rain. The hour of day is a more important predictor than the traffic index. Among the weather features, humidity is the most important feature. The influence of each feature is illustrated below using a tree interpreter for the data on Jan 13, 2019 at 8 am with 96 PM 2.5 level. We start with the average value of 26. The PM 2.5 level for the previous hour was 62, thus the model adds a value 20. There were 150 fires within a 240 km radius, thus the model adds 10 to the pollution level. The value is now 56. There are 1649 fires between 240-480 km, and 896 fires between 480-720 km, and the model adds a value of 9 and 8 respectively. The low wind speed and the morning rush hour (8 am) adds 8 to the model. These six top factors account for 81 out of the total 96 predicted for the PM 2.5 level. The remaining features to the right are less important and thus increase the predicted pollution value less. On a good day such as Feb 2, 2019 at 7 pm the PM 2.5 level was 10. The pollution level in the hour before was low, thus the model subtracts a value of 10. There were still a lot of fires in the area, and the model adds a value of 2. The wind speed was high, reducing the value by 2. The weather and traffic were good. The combination of many factors results in a low predicted PM 2.5 level of 10. Conclusions The PM 2.5 level has a complex relationship with various factors: number of fires, weather patterns, and traffic. But this analysis confirms the suspicion that many people have agricultural burning is the root cause of PM 2.5 pollution in Thailand. Burning activities as far as 720 km away from Bangkok, an area which extends into Myanmar, Laos, and Cambodia, can cause air problems in Bangkok. Solving this problem will not be easy. It will require a collaborative international effort among the Southeast Asian countries. I leave you with a fire map from March 17, 2019, one of the worst days ever! 772 15 772 772 15 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Sean McClure Mar 30, 2019 Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab) R and Python are the bread and butter of todays machine learning languages. R provides powerful statistics and quick visualizations, while Python offers an intuitive syntax, abundant support, and is the choice interface to todays major AI frameworks. In this article well look at the steps involved in creating libraries 32 min read 32 min read Share your ideas with millions of readers. Charlene Chambliss Mar 30, 2019 Cleaning, Analyzing, and Visualizing Survey Data in Python A tutorial using pandas, matplotlib, and seaborn to produce digestible insights from dirty data If you work in data at a D2C startup, theres a good chance you will be asked to look at survey data at least once. And since SurveyMonkey is one of the most popular survey platforms out there, theres a good chance itll be SurveyMonkey data. The way SurveyMonkey exports 10 min read 10 min read Matthew Stewart Mar 30, 2019 Handling Discriminatory Biases in Data for Machine Learning What do you do when data tells you to be racist? Of course algorithms are racist. Theyre made by people. Stephen Bush, New Statesman America Ethics in Machine Learning Most of the time, machine learning does not touch on particularly sensitive social, moral, or ethical issues. Someone gives us a data set and asks us to predict house prices based 13 min read 13 min read Abraham Kang Mar 30, 2019 Member-only Applied AI: Going From Concept to ML Components Opening your mind to different ways of applying machine learning to the real world. By Abraham Kang with special thanks to Kunal Patel and Jae Duk Seo for being a sounding board and providing input for this article. 6 min read 6 min read Michel Kana, Ph.D Mar 30, 2019 Wild Wide AI: responsible data science Who shoots first the new race or the human? Data Science can do good things for us: it improves life, it makes things more efficient, more effective and leads to a better experience. There are however some miss-steps that data-driven analysis has already exhibited. Here are few examples where data science tools were intentionally or unintentionally misused: In 2012 9 min read 9 min read Worasom Kundhikanjana machine learning, deep learning, and computer vision https://github.com/worasom/ More from Medium Luay Rahil in ILLUMINATION Elon Musk Eliminated Remote Work Because Working From Home \"Doesn\\'t Work\" Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Akshad Singi in Better Humans 4 Unsexy One-Minute Habits That Save Me 30+ Hours Every Week Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 361,\n",
       "  'url': 'https://towardsdatascience.com/cleaning-analyzing-and-visualizing-survey-data-in-python-42747a13c713',\n",
       "  'title': 'Cleaning, Analyzing, and Visualizing Survey Data in\\xa0Python',\n",
       "  'subtitle': 'A tutorial using ',\n",
       "  'claps': 652,\n",
       "  'responses': 7.0,\n",
       "  'reading_time': 10,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 0.00032205753271329025,\n",
       "  'text': \"Towards Data Science Mar 30 2019 Listen Save Cleaning Analyzing Visualizing Survey Data Python tutorial using panda matplotlib seaborn produce digestible insight dirty data work data D2C startup good chance asked look survey data least since SurveyMonkey one popular survey platform good chance itll SurveyMonkey data way SurveyMonkey export data necessarily ready analysis right box pretty close Ill demonstrate example question might want ask survey data extract answer quickly Well even write function make life easier plotting future question Well using panda matplotlib seaborn make sense data used Mockaroo generate data specifically survey question field used `` Custom List '' entered appropriate field could achieve effect using random.choice random module found easier let Mockaroo create whole thing tweaked data Excel mirrored structure SurveyMonkey export first reaction might Ugh horrible mean column name didnt read properly ton NaNs instead numerical representation like 0/1 1/2/3/4/5 actual text answer cellAnd actually reading MultiIndex dont worry bad might think going ignore MultiIndexes post Nobody really like working anyway team need insight ASAP well come hacky solution First order business weve asked find answer question vary age group age age -- n't column age group Well luckily u pretty easily define function create one try run like well get error Thats first row value age word age instead number Since first step convert age int fail need remove row DataFrame itll useful u later rename column well save separate variable notice since removing header 've lost information looking survey data Ideally list question option asked survey provided whoever want analysis keep separate way reference info document note look working OK let apply age_group function get age_group column Great Next let subset data focus first question answer first question vary age group Great answer variable go plot data going look good misnamed column Lets write quick function make renaming column simple Remember header earlier use create new_names_list renaming already array pas right rename first readability Isnt much nicer look Dont worry almost part get insight Notice groupby aggregation function ignore NaNs automatically make life significantly easier Lets say also dont really care analyzing under-30 customer right well plot age group OK well good 60+ group ha people group hard make fair comparison plot age group separate plot compare distribution wait might think dont really want write code 4 different plot Well course ha time Lets write another function u believe wa Jenny Bryan wonderful talk Code Smells Feels first tipped following find copying pasting code changing value really ought write function ha great guide deciding isnt worth write function something rule thumb like use would copying pasting 3 time write function also benefit convenience approach improve programming skill make people need read code happier course generated data uniform distribution would thus expect see significant difference group Hopefully survey data interesting Next let address another format question one need see interested age group given benefit Happily question actually easier deal former type Lets take look look since small DataFrame age_group appended already wo n't add Cool subsetted data cant aggregate count time like could question last question NaNs would excluded give true count response one would get number response age group overall definitely want point question understand interested different age group need preserve information tell u many people age group responded question One way go would re-encode response numerically want preserve relationship even granular level encode numerically take median average age group level interest really interested specific percentage people per age group chose interest level Itd easier convey info barplot text preserved Thats going next guessed time write another function Quick note new learner people wont say explicitly let clear visualization often made Generally speaking highly iterative process Even experienced data scientist dont write plot specification top head Generally start .plot kind='bar similar depending plot want change size color map get group properly sorted using order= specify whether label rotated set x- y-axis label invisible depending think best whoever using visualization dont intimidated long block code see people making plot Theyre usually created span minute testing different specification writing perfect code scratch one go plot another 2x2 benefit broken age group wed 4 benefit ha time Instead well loop benefit age group within benefit using couple loop 're interested 'd challenge refactor function happen many question formatted like Success wanted export individual set plot would simply add line plt.savefig _interest_by_age.png'.format benefit matplotlib would automatically save beautifully sharp rendering set plot make especially easy folk team use finding simply export plot folder people browse image able drag drop right PowerPoint presentation report could use tad padding would increase allowed height figure slightly Lets one example numerically encoding benefit mentioned earlier generate heatmap correlation interest different benefit lastly well generate correlation matrix plot correlation since data randomly generated would expect little correlation indeed find funny note SQL tutorial slightly negatively correlated drag-and-drop feature actually might expect see real data Lets one last type plot one thats closely related heatmap clustermap Clustermaps make correlation especially informative analyzing survey response use hierarchical clustering case group benefit together closely related instead eyeballing heatmap individual benefit positively negatively associated get little crazy 10+ benefit plot segmented cluster little easier look also easily change linkage type used calculation youre familiar mathematical detail hierarchical clustering available option single average ward wont get detail ward generally safe bet starting Long label often require little tweaking Id recommend renaming benefit shorter name prior using clustermap quick assessment show clustering algorithm belief drag-and-drop feature ready-made formula cluster together custom dashboard template SQL tutorial form another cluster Since correlation weak see height benefit link together form cluster tall mean probably base business decision finding Hopefully example illustrative despite weak relationship hope enjoyed quick tutorial working survey data writing function quickly generate visualization finding think know even efficient way thing feel free let know comment came needed produce insight individual question quickly possible 742 7 742 742 7 Towards Data Science home data science Medium publication sharing concept idea code Matthew Stewart Mar 30 2019 Handling Discriminatory Biases Data Machine Learning data tell racist course algorithm racist Theyre made people Stephen Bush New Statesman America Ethics Machine Learning time machine learning doe touch particularly sensitive social moral ethical issue Someone give u data set asks u predict house price based 13 min read 13 min read Share idea million reader Abraham Kang Mar 30 2019 Member-only Applied AI Going Concept ML Components Opening mind different way applying machine learning real world Abraham Kang special thanks Kunal Patel Jae Duk Seo sounding board providing input article 6 min read 6 min read Michel Kana Ph.D Mar 30 2019 Wild Wide AI responsible data science shoot first new race human Data Science good thing u improves life make thing efficient effective lead better experience however miss-steps data-driven analysis ha already exhibited example data science tool intentionally unintentionally misused 2012 9 min read 9 min read Kevin Luk Mar 30 2019 Member-only library load image Python difference Summarization Comparison .imread face computer vision project first need load image preprocessing various library perform imread want consolidate popular library loading image difference article go Libraries loading 3 min read 3 min read Abhishek Mukherjee Mar 30 2019 Trust interpretability machine learning machine learning model always need interpretable Given choice interpretable model inaccurate non-interpretable model accurate wouldnt rather choose non-interpretable accurate model word reason sacrificing accuracy altar interpretability 9 min read 9 min read Charlene Chambliss Machine Learning Engineer Primer AI Im Twitter blissfulchar LinkedIn http //www.linkedin.com/in/charlenechambliss/ Medium Terence Shin Towards Data Science 10 Best Data Visualizations 2022 Luay Rahil ILLUMINATION Elon Musk Eliminated Remote Work Working Home `` n't Work '' Anmol Tomar CodeX Say Goodbye Loops Python Welcome Vectorization Robert Ritz picky choosing partner data-driven approach Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Mar 30, 2019 Listen Save Cleaning, Analyzing, and Visualizing Survey Data in Python A tutorial using pandas , matplotlib , and seaborn to produce digestible insights from dirty data If you work in data at a D2C startup, theres a good chance you will be asked to look at survey data at least once. And since SurveyMonkey is one of the most popular survey platforms out there, theres a good chance itll be SurveyMonkey data. The way SurveyMonkey exports data is not necessarily ready for analysis right out of the box, but its pretty close. Here Ill demonstrate a few examples of questions you might want to ask of your survey data, and how to extract those answers quickly. Well even write a few functions to make our lives easier when plotting future questions. Well be using pandas , matplotlib , and seaborn to make sense of our data. I used Mockaroo to generate this data; specifically, for the survey question fields, I used \"Custom List\" and entered in the appropriate fields. You could achieve the same effect by using random.choice in the random module, but I found it easier to let Mockaroo create the whole thing for me. I then tweaked the data in Excel so that it mirrored the structure of a SurveyMonkey export. Your first reaction to this might be Ugh. Its horrible. I mean, the column names didnt read in properly, there are a ton of NaNs, instead of numerical representations like 0/1 or 1/2/3/4/5 we have the actual text answers in each cellAnd should we actually be reading this in with a MultiIndex? But dont worry, its not as bad as you might think. And were going to ignore MultiIndexes in this post. (Nobody really likes working with them anyway.) The team needs those insights ASAP so well come up with some hacky solutions. First order of business: weve been asked to find how the answers to these questions vary by age group. But age is just an age--we don\\'t have a column for age groups! Well, luckily for us, we can pretty easily define a function to create one. But if we try to run it like this, well get an error! Thats because we have that first row, and its value for age is the word age instead of a number. Since the first step is to convert each age to an int , this will fail. We need to remove that row from the DataFrame, but itll be useful for us later when we rename columns, so well save it as a separate variable. You will notice that, since removing headers , we\\'ve now lost some information when looking at the survey data by itself. Ideally, you will have a list of the questions and their options that were asked in the survey, provided to you by whoever wants the analysis. If not, you should keep a separate way to reference this info in a document or note that you can look at while working. OK, now lets apply the age_group function to get our age_group column. Great. Next, lets subset the data to focus on just the first question. How do the answers to this first question vary by age group? Great. We have the answers in a variable now. But when we go to plot this data, its not going to look very good, because of the misnamed columns. Lets write up a quick function to make renaming the columns simple: Remember headers from earlier? We can use it to create our new_names_list for renaming. Its already an array, so we can just pass it right in, or we can rename it first for readability. Isnt that so much nicer to look at? Dont worry, were almost to the part where we get some insights. Notice how groupby and other aggregation functions ignore NaNs automatically. That makes our lives significantly easier. Lets say we also dont really care about analyzing under-30 customers right now, so well plot only the other age groups. OK, this is all well and good, but the 60+ group has more people in it than the other groups, and so its hard to make a fair comparison. What do we do? We can plot each age group in a separate plot, and then compare the distributions. But wait, you might think. I dont really want to write the code for 4 different plots. Well of course not! Who has time for that? Lets write another function to do it for us. I believe it was Jenny Bryan , in her wonderful talk Code Smells and Feels, who first tipped me off to the following: If you find yourself copying and pasting code and just changing a few values, you really ought to just write a function. This has been a great guide for me in deciding when it is and isnt worth it to write a function for something. A rule of thumb I like to use is that if I would be copying and pasting more than 3 times, I write a function. There are also benefits other than convenience to this approach, such as that it: (All of which improve your programming skills and make the people who need to read your code happier!) This is, of course, generated data from a uniform distribution, and we would thus not expect to see any significant differences between groups. Hopefully your own survey data will be more interesting. Next, lets address another format of question. In this one, we need to see how interested each age group is in a given benefit. Happily, these questions are actually easier to deal with than the former type. Lets take a look: And look, since this is a small DataFrame, age_group is appended already and we won\\'t have to add it. Cool. Now we have the subsetted data, but we cant just aggregate it by count this time like we could with the other question the last question had NaNs that would be excluded to give the true count for that response, but with this one, we would just get the number of responses for each age group overall: This is definitely not what we want! The point of the question is to understand how interested the different age groups are, and we need to preserve that information. All this tells us is how many people in each age group responded to the question. So what do we do? One way to go would be to re-encode these responses numerically. But what if we want to preserve the relationship on an even more granular level? If we encode numerically, we can take the median and average of each age groups level of interest. But what if what were really interested in is the specific percentage of people per age group who chose each interest level? Itd be easier to convey that info in a barplot, with the text preserved. Thats what were going to do next. And you guessed it its time to write another function. Quick note to new learners: Most people wont say this explicitly, but let me be clear on how visualizations are often made. Generally speaking, it is a highly iterative process. Even the most experienced data scientists dont just write up a plot with all of these specifications off the top of their head. Generally, you start with .plot(kind=\\'bar\\') , or similar depending on the plot you want, and then you change size, color maps, get the groups properly sorted using order= , specify whether the labels should be rotated, and set x- or y-axis labels invisible, and more, depending on what you think is best for whoever will be using the visualizations. So dont be intimidated by the long blocks of code you see when people are making plots. Theyre usually created over a span of minutes while testing out different specifications, not by writing perfect code from scratch in one go. Now we can plot another 2x2 for each benefit broken out by age group. But wed have to do that for all 4 benefits! Again: who has time for that? Instead, well loop over each benefit, and each age group within each benefit, using a couple of for loops. But if you\\'re interested, I\\'d challenge you to refactor this into a function if you happen to have many questions that are formatted like this. Success! And if we wanted to export each individual set of plots, we would simply add the line plt.savefig(\\'{}_interest_by_age.png\\'.format(benefit)) , and matplotlib would automatically save a beautifully sharp rendering of each set of plots. This makes it especially easy for folks on other teams to use your findings; you can simply export them to a plots folder, and people can browse the images and be able to drag and drop them right into a PowerPoint presentation or other report. These could use a tad more padding, so if I were to do this again, I would increase the allowed height for the figure slightly. Lets do one more example: numerically encoding the benefits, as we mentioned earlier. Then we can generate a heatmap of the correlations between interest in different benefits. And lastly, well generate the correlation matrix and plot the correlations. Again, since the data is randomly generated, we would expect there to be little to no correlation, and that is indeed what we find. (It is funny to note that SQL tutorials are slightly negatively correlated with drag-and-drop features, which is actually what we might expect to see in real data!) Lets do one last type of plot, one thats closely related to the heatmap: the clustermap . Clustermaps make correlations especially informative in analyzing survey responses, because they use hierarchical clustering to (in this case) group benefits together by how closely related they are. So instead of eyeballing the heatmap for which individual benefits are positively or negatively associated, which can get a little crazy when you have 10+ benefits, the plot will be segmented into clusters, which is a little easier to look at. You can also easily change the linkage type used in the calculation, if youre familiar with the mathematical details of hierarchical clustering. Some of the available options are single, average, and ward I wont get into the details, but ward is generally a safe bet when starting out. Long labels often require a little tweaking, so Id recommend renaming your benefits to shorter names prior to using a clustermap. A quick assessment of this shows that the clustering algorithm believes drag-and-drop features and ready-made formulas cluster together, while custom dashboard templates and SQL tutorials form another cluster. Since the correlations are so weak, you can see that the height of when the benefits link together to form a cluster is very tall. (This means you should probably not base any business decisions on this finding!) Hopefully the example is illustrative despite the weak relationships. I hope you enjoyed this quick tutorial about working with survey data and writing functions to quickly generate visualizations of your findings! If you think you know an even more efficient way of doing things, feel free to let me know in the comments this is just what I came up with when I needed to produce insights on individual questions as quickly as possible. 742 7 742 742 7 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Matthew Stewart Mar 30, 2019 Handling Discriminatory Biases in Data for Machine Learning What do you do when data tells you to be racist? Of course algorithms are racist. Theyre made by people. Stephen Bush, New Statesman America Ethics in Machine Learning Most of the time, machine learning does not touch on particularly sensitive social, moral, or ethical issues. Someone gives us a data set and asks us to predict house prices based 13 min read 13 min read Share your ideas with millions of readers. Abraham Kang Mar 30, 2019 Member-only Applied AI: Going From Concept to ML Components Opening your mind to different ways of applying machine learning to the real world. By Abraham Kang with special thanks to Kunal Patel and Jae Duk Seo for being a sounding board and providing input for this article. 6 min read 6 min read Michel Kana, Ph.D Mar 30, 2019 Wild Wide AI: responsible data science Who shoots first the new race or the human? Data Science can do good things for us: it improves life, it makes things more efficient, more effective and leads to a better experience. There are however some miss-steps that data-driven analysis has already exhibited. Here are few examples where data science tools were intentionally or unintentionally misused: In 2012 9 min read 9 min read Kevin Luk Mar 30, 2019 Member-only What libraries can load image in Python and what are their difference? Summarization & Comparison of .imread() When we face computer vision project, first of all we need to load the images before any preprocessing. There are various libraries out there to perform imread() . Here I want to consolidate the popular libraries for loading image and their difference. This article will go through: Libraries for loading 3 min read 3 min read Abhishek Mukherjee Mar 30, 2019 Trust and interpretability in machine learning Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldnt you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability? 9 min read 9 min read Charlene Chambliss Machine Learning Engineer at Primer AI. Im on Twitter @blissfulchar, and heres my LinkedIn: https://www.linkedin.com/in/charlenechambliss/ More from Medium Terence Shin in Towards Data Science The 10 Best Data Visualizations of 2022 Luay Rahil in ILLUMINATION Elon Musk Eliminated Remote Work Because Working From Home \"Doesn\\'t Work\" Anmol Tomar in CodeX Say Goodbye to Loops in Python, and Welcome Vectorization! Robert Ritz Are you being too picky choosing a partner? A data-driven approach. Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 3181,\n",
       "  'url': 'https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e',\n",
       "  'title': 'Everything You Ever Wanted To Know About Computer Vision. Here’s A Look Why It’s So\\xa0Awesome.',\n",
       "  'subtitle': '-',\n",
       "  'claps': 604,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 11,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-25',\n",
       "  'clap_prop': 0.0002983477757037229,\n",
       "  'text': 'Towards Data Science Apr 25 2019 Listen Save Everything Ever Wanted Know Computer Vision Heres Look Awesome ne powerful compelling type AI computer vision youve almost surely experienced number way without even knowing Heres look work awesome going get better Computer vision field computer science focus replicating part complexity human vision system enabling computer identify process object image video way human recently computer vision worked limited capacity Thanks advance artificial intelligence innovation deep learning neural network field ha able take great leap recent year ha able surpass human task related detecting labeling object One driving factor behind growth computer vision amount data generate today used train make computer vision better Along tremendous amount visual data 3 billion image shared online every day computing power required analyze data accessible field computer vision ha grown new hardware algorithm ha accuracy rate object identification le decade today system reached 99 percent accuracy 50 percent making accurate human quickly reacting visual input Early experiment computer vision started 1950s wa first put use commercially distinguish typed handwritten text 1970s today application computer vision grown exponentially 2022 computer vision hardware market expected reach 48.6 billion Computer Vision Work One major open question Neuroscience Machine Learning exactly brain work approximate algorithm reality working comprehensive theory brain computation despite fact Neural Nets supposed mimic way brain work nobody quite sure thats actually true paradox hold true computer vision since decided brain eye process image difficult say well algorithm used production approximate internal mental process certain level Computer vision pattern recognition one way train computer understand visual data feed image lot image thousand million possible labeled subject various software technique algorithm allow computer hunt pattern element relate label example feed computer million image cat love subject algorithm let analyze color photo shape distance shape object border identifies profile cat mean finished computer theory able use experience fed unlabeled image find one cat Lets leave fluffy cat friend moment side let get technical simple illustration grayscale image buffer store image Abraham Lincoln pixel brightness represented single 8-bit number whose range 0 black 255 white way storing image data may run counter expectation since data certainly appears two-dimensional displayed Yet case since computer memory consists simply ever-increasing linear list address space Lets go back first picture imagine adding colored one thing start get complicated Computers usually read color series 3 value red green blue RGB 0255 scale pixel actually ha 3 value computer store addition position colorize President Lincoln would lead 12 x 16 x 3 value 576 number Thats lot memory require one image lot pixel algorithm iterate train model meaningful accuracy especially youre talking Deep Learning youd usually need ten thousand image merrier Evolution Computer Vision advent deep learning task computer vision could perform limited required lot manual coding effort developer human operator instance wanted perform facial recognition would perform following step manual work application would finally able compare measurement new image one stored database tell whether corresponded profile wa tracking fact wa little automation involved work wa done manually error margin wa still large Machine learning provided different approach solving computer vision problem machine learning developer longer needed manually code every single rule vision application Instead programmed feature smaller application could detect specific pattern image used statistical learning algorithm linear regression logistic regression decision tree support vector machine SVM detect pattern classify image detect object Machine learning helped solve many problem historically challenging classical software development tool approach instance year ago machine learning engineer able create software could predict breast cancer survival window better human expert However building feature software required effort dozen engineer breast cancer expert took lot time develop Deep learning provided fundamentally different approach machine learning Deep learning relies neural network general-purpose function solve problem representable example provide neural network many labeled example specific kind data itll able extract common pattern example transform mathematical equation help classify future piece information instance creating facial recognition application deep learning requires develop choose preconstructed algorithm train example face people must detect Given enough example lot example neural network able detect face without instruction feature measurement Deep learning effective method computer vision case creating good deep learning algorithm come gathering large amount labeled training data tuning parameter type number layer neural network training epoch Compared previous type machine learning deep learning easier faster develop deploy current computer vision application cancer detection self-driving car facial recognition make use deep learning Deep learning deep neural network moved conceptual realm practical application thanks availability advance hardware cloud computing resource Long Take Decipher Image short much Thats key computer vision thrilling Whereas past even supercomputer might take day week even month chug calculation required today ultra-fast chip related hardware along speedy reliable internet cloud network make process lightning fast crucial factor ha willingness many big company AI research share work Facebook Google IBM Microsoft notably open sourcing machine learning work allows others build work rather starting scratch result AI industry cooking along experiment long ago took week run might take 15 minute today many real-world application computer vision process happens continuously microsecond computer today able scientist call situationally aware Applications Computer Vision Computer vision one area Machine Learning core concept already integrated major product use every day CV Self-Driving Cars tech company leverage Machine Learning image application Computer vision enables self-driving car make sense surroundings Cameras capture video different angle around car feed computer vision software process image real-time find extremity road read traffic sign detect car object pedestrian self-driving car steer way street highway avoid hitting obstacle hopefully safely drive passenger destination CV Facial Recognition Computer vision also play important role facial recognition application technology enables computer match image people face identity Computer vision algorithm detect facial feature image compare database face profile Consumer device use facial recognition authenticate identity owner Social medium apps use facial recognition detect tag user Law enforcement agency also rely facial recognition technology identify criminal video feed CV Augmented Reality Mixed Reality Computer vision also play important role augmented mixed reality technology enables computing device smartphones tablet smart glass overlay embed virtual object real world imagery Using computer vision AR gear detect object real world order determine location device display place virtual object instance computer vision algorithm help AR application detect plane tabletop wall floor important part establishing depth dimension placing virtual object physical world CV Healthcare Computer vision ha also important part advance health-tech Computer vision algorithm help automate task detecting cancerous mole skin image finding symptom x-ray MRI scan Challenges Computer Vision Helping computer see turn hard Inventing machine see like deceptively difficult task hard make computer entirely sure human vision work first place Studying biological vision requires understanding perception organ like eye well interpretation perception within brain Much progress ha made charting process term discovering trick shortcut used system although like study involves brain long way go Many popular computer vision application involve trying recognize thing photograph example Outside recognition method analysis include application involves understanding pixel software safely labeled computer vision Conclusion Despite recent progress ha impressive still even close solving computer vision However already multiple healthcare institution enterprise found way apply CV system powered CNNs real-world problem trend likely stop anytime soon want get touch way know good joke connect Twitter LinkedIn Thanks reading 1K 1K 1K Towards Data Science home data science Medium publication sharing concept idea code Nishit Jain Apr 25 2019 overview Gradient Descent algorithm subtle yet powerful algorithm optimizes parameter Optimizing parameter ultimate goal every machine learning algorithm want get optimum value slope intercept get line best fit linear regression problem also want get optimum value parameter sigmoidal curve 8 min read 8 min read Share idea million reader Venelin Valkov Apr 25 2019 Cryptocurrency price prediction using LSTMs TensorFlow Hackers Part III Predict Bitcoin price using LSTM Deep Neural Network TensorFlow 2 TL DR Build train Bidirectional LSTM Deep Neural Network Time Series prediction TensorFlow 2 Use model predict future Bitcoin price Complete source code Google Colaboratory Notebook time youll build basic Deep Neural Network model predict Bitcoin price based historical 6 min read 6 min read Ahmad Tanehkar Apr 25 2019 Lets Apply Machine Learning Behavioral Economics Currently Machine Learning ML well applied behavioral economics social science lack use come unfamiliarity different approach 5 min read 5 min read Ashok Chilakapati Apr 25 2019 Concept Drift Model Decay Machine Learning Concept drift drift label time essentially data lead divergence decision boundary new data model built earlier data/labels 7 min read 7 min read Ren Bremer Apr 25 2019 use Python feature data analytics project Python tutorial Azure using OO NumPy panda SQL PySpark 1 Introduction lot company moving cloud consider tooling shall used data analytics On-premises company mostly use propriety software advanced analytics BI reporting However tooling may logical choice cloud environment 9 min read 9 min read Ilija Mihajlovic iOS developer computer science graduate passion machine learning computer vision Medium Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Shahrullohon Lutfillohonov Visual Perception Self-Driving Cars Part 5 Multi-Task Learning Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Betul Mescioglu Image Processing Tutorial Using scikit-imageImage Segmentation Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 25, 2019 Listen Save Everything You Ever Wanted To Know About Computer Vision. Heres A Look Why Its So Awesome. O ne of the most powerful and compelling types of AI is computer vision which youve almost surely experienced in any number of ways without even knowing. Heres a look at what it is, how it works, and why its so awesome (and is only going to get better). Computer vision is the field of computer science that focuses on replicating parts of the complexity of the human vision system and enabling computers to identify and process objects in images and videos in the same way that humans do. Until recently, computer vision only worked in limited capacity. Thanks to advances in artificial intelligence and innovations in deep learning and neural networks, the field has been able to take great leaps in recent years and has been able to surpass humans in some tasks related to detecting and labeling objects. One of the driving factors behind the growth of computer vision is the amount of data we generate today that is then used to train and make computer vision better. Along with a tremendous amount of visual data ( more than 3 billion images are shared online every day ), the computing power required to analyze the data is now accessible. As the field of computer vision has grown with new hardware and algorithms so has the accuracy rates for object identification. In less than a decade, todays systems have reached 99 percent accuracy from 50 percent making them more accurate than humans at quickly reacting to visual inputs. Early experiments in computer vision started in the 1950s and it was first put to use commercially to distinguish between typed and handwritten text by the 1970s, today the applications for computer vision have grown exponentially. By 2022, the computer vision and hardware market is expected to reach $48.6 billion How Does Computer Vision Work? One of the major open questions in both Neuroscience and Machine Learning is: How exactly do our brains work, and how can we approximate that with our own algorithms? The reality is that there are very few working and comprehensive theories of brain computation; so despite the fact that Neural Nets are supposed to mimic the way the brain works, nobody is quite sure if thats actually true. The same paradox holds true for computer vision since were not decided on how the brain and eyes process images, its difficult to say how well the algorithms used in production approximate our own internal mental processes. On a certain level Computer vision is all about pattern recognition. So one way to train a computer how to understand visual data is to feed it images, lots of images thousands, millions if possible that have been labeled, and then subject those to various software techniques, or algorithms, that allow the computer to hunt down patterns in all the elements that relate to those labels. So, for example, if you feed a computer a million images of cats (we all love them), it will subject them all to algorithms that let them analyze the colors in the photo, the shapes, the distances between the shapes, where objects border each other, and so on, so that it identifies a profile of what cat means. When its finished, the computer will (in theory) be able to use its experience if fed other unlabeled images to find the ones that are of cat. Lets leave our fluffy cat friends for a moment on the side and lets get more technical. Below is a simple illustration of the grayscale image buffer which stores our image of Abraham Lincoln. Each pixels brightness is represented by a single 8-bit number, whose range is from 0 (black) to 255 (white): This way of storing image data may run counter to your expectations, since the data certainly appears to be two-dimensional when it is displayed. Yet, this is the case, since computer memory consists simply of an ever-increasing linear list of address spaces. Lets go back to the first picture again and imagine adding a colored one. Now things start to get more complicated. Computers usually read color as a series of 3 values red, green, and blue (RGB) on that same 0255 scale. Now, each pixel actually has 3 values for the computer to store in addition to its position. If we were to colorize President Lincoln, that would lead to 12 x 16 x 3 values, or 576 numbers. Thats a lot of memory to require for one image, and a lot of pixels for an algorithm to iterate over. But to train a model with meaningful accuracy especially when youre talking about Deep Learning youd usually need tens of thousands of images, and the more the merrier. The Evolution Of Computer Vision Before the advent of deep learning, the tasks that computer vision could perform were very limited and required a lot of manual coding and effort by developers and human operators. For instance, if you wanted to perform facial recognition, you would have to perform the following steps: After all this manual work, the application would finally be able to compare the measurements in the new image with the ones stored in its database and tell you whether it corresponded with any of the profiles it was tracking. In fact, there was very little automation involved and most of the work was being done manually. And the error margin was still large. Machine learning provided a different approach to solving computer vision problems. With machine learning, developers no longer needed to manually code every single rule into their vision applications. Instead they programmed features, smaller applications that could detect specific patterns in images. They then used a statistical learning algorithm such as linear regression, logistic regression, decision trees or support vector machines (SVM) to detect patterns and classify images and detect objects in them. Machine learning helped solve many problems that were historically challenging for classical software development tools and approaches. For instance, years ago, machine learning engineers were able to create a software that could predict breast cancer survival windows better than human experts. However building the features of the software required the efforts of dozens of engineers and breast cancer experts and took a lot of time develop. Deep learning provided a fundamentally different approach to doing machine learning. Deep learning relies on neural networks, a general-purpose function that can solve any problem representable through examples. When you provide a neural network with many labeled examples of a specific kind of data, itll be able to extract common patterns between those examples and transform it into a mathematical equation that will help classify future pieces of information. For instance, creating a facial recognition application with deep learning only requires you to develop or choose a preconstructed algorithm and train it with examples of the faces of the people it must detect. Given enough examples (lots of examples), the neural network will be able to detect faces without further instructions on features or measurements. Deep learning is a very effective method to do computer vision. In most cases, creating a good deep learning algorithm comes down to gathering a large amount of labeled training data and tuning the parameters such as the type and number of layers of neural networks and training epochs. Compared to previous types of machine learning, deep learning is both easier and faster to develop and deploy. Most of current computer vision applications such as cancer detection, self-driving cars and facial recognition make use of deep learning. Deep learning and deep neural networks have moved from the conceptual realm into practical applications thanks to availability and advances in hardware and cloud computing resources. How Long Does It Take To Decipher An Image In short not much. Thats the key to why computer vision is so thrilling: Whereas in the past even supercomputers might take days or weeks or even months to chug through all the calculations required, todays ultra-fast chips and related hardware, along with the a speedy, reliable internet and cloud networks, make the process lightning fast. Once crucial factor has been the willingness of many of the big companies doing AI research to share their work Facebook, Google, IBM, and Microsoft, notably by open sourcing some of their machine learning work. This allows others to build on their work rather than starting from scratch. As a result, the AI industry is cooking along, and experiments that not long ago took weeks to run might take 15 minutes today. And for many real-world applications of computer vision, this process all happens continuously in microseconds, so that a computer today is able to be what scientists call situationally aware. Applications Of Computer Vision Computer vision is one of the areas in Machine Learning where core concepts are already being integrated into major products that we use every day. CV In Self-Driving Cars But its not just tech companies that are leverage Machine Learning for image applications. Computer vision enables self-driving cars to make sense of their surroundings. Cameras capture video from different angles around the car and feed it to computer vision software, which then processes the images in real-time to find the extremities of roads, read traffic signs, detect other cars, objects and pedestrians. The self-driving car can then steer its way on streets and highways, avoid hitting obstacles, and (hopefully) safely drive its passengers to their destination. CV In Facial Recognition Computer vision also plays an important role in facial recognition applications, the technology that enables computers to match images of peoples faces to their identities. Computer vision algorithms detect facial features in images and compare them with databases of face profiles. Consumer devices use facial recognition to authenticate the identities of their owners. Social media apps use facial recognition to detect and tag users. Law enforcement agencies also rely on facial recognition technology to identify criminals in video feeds. CV In Augmented Reality & Mixed Reality Computer vision also plays an important role in augmented and mixed reality, the technology that enables computing devices such as smartphones, tablets and smart glasses to overlay and embed virtual objects on real world imagery. Using computer vision, AR gear detect objects in real world in order to determine the locations on a devices display to place a virtual object. For instance, computer vision algorithms can help AR applications detect planes such as tabletops, walls and floors, a very important part of establishing depth and dimensions and placing virtual objects in physical world. CV In Healthcare Computer vision has also been an important part of advances in health-tech. Computer vision algorithms can help automate tasks such as detecting cancerous moles in skin images or finding symptoms in x-ray and MRI scans. Challenges of Computer Vision Helping computers to see turns out to be very hard. Inventing a machine that sees like we do is a deceptively difficult task, not just because its hard to make computers do it, but because were not entirely sure how human vision works in the first place. Studying biological vision requires an understanding of the perception organs like the eyes, as well as the interpretation of the perception within the brain. Much progress has been made, both in charting the process and in terms of discovering the tricks and shortcuts used by the system, although like any study that involves the brain, there is a long way to go. Many popular computer vision applications involve trying to recognize things in photographs; for example: Outside of just recognition, other methods of analysis include: Any other application that involves understanding pixels through software can safely be labeled as computer vision. Conclusion Despite the recent progress, which has been impressive, were still not even close to solving computer vision. However, there are already multiple healthcare institutions and enterprises that have found ways to apply CV systems, powered by CNNs, to real-world problems. And this trend is not likely to stop anytime soon. If you want to get in touch and by the way, you know a good joke you can connect with me on  Twitter  or  LinkedIn. Thanks for reading! 1K 1K 1K More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Nishit Jain Apr 25, 2019 An overview of the Gradient Descent algorithm The subtle yet powerful algorithm that optimizes parameters Optimizing parameters is the ultimate goal of every machine learning algorithm. You want to get the optimum value of the slope and the intercept to get the line of best fit in linear regression problems. You also want to get the optimum value for the parameters of a sigmoidal curve 8 min read 8 min read Share your ideas with millions of readers. Venelin Valkov Apr 25, 2019 Cryptocurrency price prediction using LSTMs | TensorFlow for Hackers (Part III) Predict Bitcoin price using LSTM Deep Neural Network in TensorFlow 2 TL;DR Build and train an Bidirectional LSTM Deep Neural Network for Time Series prediction in TensorFlow 2. Use the model to predict the future Bitcoin price. Complete source code in Google Colaboratory Notebook This time youll build a basic Deep Neural Network model to predict Bitcoin price based on historical 6 min read 6 min read Ahmad Tanehkar Apr 25, 2019 Lets Apply Machine Learning in Behavioral Economics Currently, Machine Learning (ML) is not well applied in behavioral economics or social science, and this lack of use comes from unfamiliarity with this different approach. 5 min read 5 min read Ashok Chilakapati Apr 25, 2019 Concept Drift and Model Decay in Machine Learning Concept drift is a drift of labels with time for the essentially the same data. It leads to the divergence of decision boundary for new data from that of a model built from earlier data/labels. 7 min read 7 min read Ren Bremer Apr 25, 2019 How to use Python features in your data analytics project Python tutorial in Azure using OO, NumPy, pandas, SQL, PySpark 1. Introduction A lot of companies are moving to cloud and consider what tooling shall be used for data analytics. On-premises, companies mostly use propriety software for advanced analytics, BI and reporting. However, this tooling may not be the most logical choice in a cloud environment. 9 min read 9 min read Ilija Mihajlovic iOS developer and computer science graduate, with a passion for machine learning and computer vision. More from Medium Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Shahrullohon Lutfillohonov Visual Perception for Self-Driving Cars! Part 5: Multi-Task Learning Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Betul Mescioglu Image Processing Tutorial Using scikit-imageImage Segmentation Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5454,\n",
       "  'url': 'https://towardsdatascience.com/visualizing-beta-distribution-7391c18031f1',\n",
       "  'title': 'Visualizing Beta Distribution and Bayesian\\xa0Updating',\n",
       "  'subtitle': 'Seeing is believing: build intuition by simulating…',\n",
       "  'claps': 587,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 0.0002899505700961678,\n",
       "  'text': 'Towards Data Science Apr 1 2019 Member-only Listen Save Visualizing Beta Distribution Bayesian Updating Seeing believing build intuition simulating visualizing inspecting every step Beta distribution one esoteric distribution compared Bernoulli Binomial Geometric distribution also rare practice doe readily available real-world analogy help intuition make matter worse online tutorial tend intimidate reader complex formula beta gamma function weird sounding name conjugate prior post Ill waive technical detail supplement badly needed intuition visual animation simulated experiment Introduction Beta distribution probability distribution probability doe mean Everyone started learning probability flipping coin let start coin First define p probability landing head coin fair likely coin land head half time case p 50 likely value p. wait also possible unfair coin behaves accidentally like fair coin rule possibility coin unfair even observe head half time Notice using probability describe probability p essence Beta distribution describes likely p take value 0 1 Beta distribution parametrized Beta Together describe probability p take certain value formula youll need get post Prior Prior believe conducting experiment look side coin see one side head side tail may believe coin fair one fair coin ha magnitude describes confident belief youre confident may assign high value 10 le confident guy would probably assign 3 difference summarized Sure curve centered p 50 quantify confidence Think imaginary coin flip actually flip coin 1 number head get 1 number tail imagine flipped 18 time got 9 head 9 tail 10 confident coin fair someone imago 4 trial resulted 2 head 2 tail 3 reason need subtract 1 setting 1 reduce numerator constant 1 give u uninformative prior assume zero imaginary coin flip Uninformative Prior flipping even looking coin know coin know nothing dont know anything say anything happen equally likely fair coin two-headed coin two-tailed coin mixture alloy ha one side heavier dont know anything probability landing head uniformly distributed special case Beta parametrized Beta =1 =1 Depending use case may may want uninformative prior Sometimes experiment dont want already know bias way interpret data case want leverage result earlier experiment dont learn everything scratch case however learn new evidence using Bayesian updating Bayesian Updating previously thought imaginary coin flip Although conceptual convenience good news Beta distribution doe distinguish imaginary real flip coin observe head simply update 1 vice versa process called Bayesian updating see proof Building Intuition Simulation Lets start uninformative prior suppose coin indeed fair expect happen flip coin observe roughly equal number head tail flip confident become coin fair probability density function grows sharper sharper p 50 Effect Prior consider case coin biased 20 towards head start uninformative prior instantly emerges center around p 20 surprising biased prior belief Note also simulated complement event p 80 important property Beta distribution trial end two possible outcome start prior belief coin fair well see peak mode Beta distribution converges slowly true value stronger prior belief slowly well accept truth differ case prior strong enough 100 unable converge true value 1000 iteration prior symmetric well see Beta closer prior converges faster peak higher one prior intuitive reality consistent believe gladly accept become confident believe contrast slower accept truth contradicts believe Baseball Batting Statistics concrete example real-world application let consider baseball batting average adapted post national batting average 0.27 new player join game record prior performance may compare national average see good prior formulated Beta =81 =219 give 0.27 expectation swing bat update along way 1000 bat observe 300 hit 700 miss posterior becomes Beta =81 300 =219 700 expectation 381 381 919 0.293 Summary post introduces Beta distribution demystifies basic property using simulation visualization built intuition mean describe probability probability distribution prior interacts evidence relates real-world scenario Bayesian updating powerful concept ha wide range application business intelligence signal filtering stochastic process modeling study application near future next post close inspection Google Analytics multi-armed bandit experiment first animation post actually come 8-armed bandit experiment Extended Reading following blog cover topic relevant AB testing in-depth review key concept mentioned article 766 3 766 766 3 Towards Data Science home data science Medium publication sharing concept idea code Rudradeb Mitra Apr 1 2019 Member-only Successful Adoption AI Need Female Leaders woman posse right quality lead AI project successfully enable environment collaboration inclusion build real-world AI product exciting breakthrough twenty-first century occur technology expanding concept mean human John Naisbitt Part Differences Men Women story woman men work differently 8 min read 8 min read Share idea million reader Manu Siddharth Jha Mar 31 2019 Member-only Artificial Intelligence next big thing Hollywood Movies captured imagination people ever since came limelight Right first motion picture late 1880s upcoming latest sci-fi blockbuster cinema ha become medium love joy passion movie lover Almost every country across globe 5 min read 5 min read Guy Tsror Mar 31 2019 Member-only romantic side data science Analyzing relationship year worth text message wa supposed private project one thought would upload private Google Colab notebook share whomever relevant one person referred OJ 11 min read 11 min read Salma Ghoneim Mar 31 2019 Member-only Object detection via color-based image segmentation using python tutorial contouring using python OpenCV Getting started already jupyter notebook IDE run python OpenCV installed skip Execution Tools hero today Anaconda free open-source distribution help installing different package sort mess isolated environment Wikipedias telling u 7 min read 7 min read Sahil Dhankhad Mar 31 2019 Member-only Forget APIs Python Scraping Using Beautiful Soup Import Data File web Part 2 APIs always every website Beautiful Soup going stay forever collect kind data website Today Data play critical role every industry data coming internet company 8 min read 8 min read Shaw Lu Data Scientist Coupang Medium Anil Tilbe Towards AI Bayesian Inference Best 5 Models 10 Best Practices Machine Learning Pedro Martins de Bastos Towards Data Science use Bayesian Inference prediction Python KARRI TIRUMALA VINAY Data Science Dying Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 1, 2019 Member-only Listen Save Visualizing Beta Distribution and Bayesian Updating Seeing is believing: build intuition by simulating, visualizing, and inspecting every step Beta distribution is one of the more esoteric distributions compared to Bernoulli, Binomial and Geometric distributions. It is also rare in practice because it does not have a readily available real-world analogy that helps intuition. To make matters worse, online tutorials tend to intimidate readers with complex formula (beta & gamma functions) and weird sounding names (conjugate prior). So in this post, Ill waive those technical details and supplement some badly needed intuition through visual animations and simulated experiments. Introduction Beta distribution is a probability distribution of probabilities. What does it mean? Everyone started learning probability by flipping a coin. So lets start with a coin. First, define p as the probability of landing a head . If the coin is fair, then it is most likely that the coin will land head half of the time. In this case, p = 50% is the most likely value for p. But wait, it is also possible to have an unfair coin that behaves accidentally like a fair coin. So we cannot rule out the possibility that the coin is unfair, even if we observe heads half of the time. Notice how I am using probability to describe probability p . This is the essence of Beta distribution: it describes how likely  p  can take on each value between 0 and 1. Beta distribution is parametrized by Beta(, ). Together and describe the probability that p takes on a certain value. Here is the only formula youll need to get through this post. Prior Prior is what we believe before conducting an experiment. If you look at both sides of a coin and see that one side is head and the other side is tail, you may believe that the coin is a fair one. A fair coin has = , and the magnitude describes how confident you are about your belief. If youre confident, then you may assign high values to = = 10. A less confident guy would probably assign = = 3. The difference is summarized below. Sure, both curve are centered at p = 50%, but how to quantify confidence? Think of  and  as imaginary coin flips before you actually flip the coin: - 1 is the number of heads you get, and - 1 is the number of tails. If you imagine that you have flipped 18 times, and got 9 heads and 9 tails ( = = 10) , you are more confident that the coin is fair, than someone who imagines only 4 trials that resulted in 2 heads and 2 tails ( = = 3). The reason we need to subtract 1 is that, by setting = = 1, we reduce numerator to a constant 1. This gives us the uninformative prior we assume zero imaginary coin flips. Uninformative Prior Before flipping or even looking at the coin, what do we know about the coin? We know nothing, and when we dont know anything, we say anything can happen. It is equally likely to be a fair coin, to be a two-headed coin, to be a two-tailed coin, or any mixture of alloy that has one side heavier than the other. When we dont know anything, the probability of landing head is uniformly distributed. This is a special case of Beta, and is parametrized as Beta(=1, =1). Depending on the use case, we may or may not want an uninformative prior. Sometimes during experiments, we dont want what we already know bias the way we interpret data. In other cases, we want to leverage the results from earlier experiments, so we dont have to learn everything from scratch. In both cases, however, we learn from new evidence using Bayesian updating. Bayesian Updating We have previously thought of  and  as imaginary coin flips. Although this is a conceptual convenience, the good news is that Beta distribution does not distinguish the imaginary and the real. If we flip the coin and observe a head, we simply update  + 1 (vice versa for  ). This process is called Bayesian updating (see here for a proof). Building Intuition: A Simulation Lets start with an uninformative prior, and suppose the coin is indeed fair. What do you expect to happen? As we flip the coin, we will observe a roughly equal number of heads and tails, and the more we flip, the more confident we become that the coin is fair. So the probability density function grows sharper and sharper at p = 50%. The Effect of Prior Now consider the case where the coin is biased 20% towards head, and we start with an uninformative prior. It instantly emerges up at and centers around p = 20%. This is not surprising because we are not biased in our prior belief. Note that I also simulated the complement event ( p = 80%). This is an important property of Beta distribution: each trial can only end up with two possible outcomes. If we start with a prior belief that the coin is fair, well see that the peak (mode) of Beta distribution converges more slowly to the true values. The stronger our prior belief, the more slowly well accept the truth if they differ. In the case below, the prior is strong enough ( = = 100) that  we are unable to converge to the true values in 1000 iterations. If the prior is not symmetric, well see that the Beta closer to the prior converges faster (and its peak higher) than the one further from prior. This is intuitive: if the reality is consistent with what we believe, we gladly accept it and become more confident with what we believe. In contrast, we are slower to accept the truth if it contradicts what we believe. Baseball Batting Statistics For a concrete example of real-world application, lets consider the baseball batting average (adapted from this post ). The national batting average is 0.27. If some new player joins the game with no records on prior performance, we may compare him to the national average to see if he is any good. The prior is formulated as Beta(=81, =219) to give the 0.27 expectation . As he swings his bat, we update  and  along the way. After 1000 bats, we observe 300 hits and 700 misses. The posterior becomes Beta(=81 + 300, =219 + 700), with expectation  381 / (381 + 919) = 0.293. Summary This post introduces Beta distribution and demystifies its basic properties using simulation and visualization. You should have built up some intuition on what it means to describe probability with a probability distribution, how prior interacts with evidence, and how it relates to real-world scenarios. Bayesian updating is a very powerful concept and has a wide range of applications in business intelligence, signal filtering, and stochastic process modeling. I will study some of those applications in the near future. The next post is a close inspection on Google Analytics multi-armed bandit experiment (the first animation in this post actually comes from an 8-armed bandit experiment). Extended Reading The following blogs covers topics relevant to AB testing, and more in-depth review of key concepts mentioned in this article. 766 3 766 766 3 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Rudradeb Mitra Apr 1, 2019 Member-only For the Successful Adoption of AI, We Need More Female Leaders Why women possess the right qualities to lead AI projects more successfully and enable an environment for collaboration and inclusion to build real-world AI products. The most exciting breakthroughs of the twenty-first century will not occur because of technology, but because of an expanding concept of what it means to be human John Naisbitt Part I: Differences between Men and Women A story of how women and men can work differently 8 min read 8 min read Share your ideas with millions of readers. Manu Siddharth Jha Mar 31, 2019 Member-only Is Artificial Intelligence the next big thing in Hollywood? Movies have captured the imagination of people ever since they came into the limelight. Right from the first motion picture in the late 1880s to the upcoming latest sci-fi blockbuster, cinema has become a medium of love, joy, and passion for all movie lovers. Almost every country across the globe 5 min read 5 min read Guy Tsror Mar 31, 2019 Member-only The romantic side of data science: Analyzing a relationship through a year worth of text messages This was supposed to be a private project, one I thought I would upload to a private Google Colab notebook, and share with just whomever it is relevant for (which is, one other person, referred to here as OJ). 11 min read 11 min read Salma Ghoneim Mar 31, 2019 Member-only Object detection via color-based image segmentation using python A tutorial on contouring using python & OpenCV. Getting started If you already have jupyter notebook or an IDE with which you can run python & OpenCV installed, just skip to Execution. Tools Our hero today is Anaconda. a free open-source distribution that helps with installing different packages & sorts out their messes into isolated environments. What Wikipedias telling us about 7 min read 7 min read Sahil Dhankhad Mar 31, 2019 Member-only Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2 APIs are not always there for you for every website, but Beautiful Soup is going to stay with you forever to collect any kind of data from any website. Today, Data play a critical role in every industry. And most of this data is coming from the internet. Most company 8 min read 8 min read Shaw Lu Data Scientist @ Coupang More from Medium Anil Tilbe in Towards AI Bayesian Inference: The Best 5 Models and 10 Best Practices for Machine Learning Pedro Martins de Bastos in Towards Data Science How to use Bayesian Inference for predictions in Python KARRI TIRUMALA VINAY Data Science is Dying? Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4956,\n",
       "  'url': 'https://towardsdatascience.com/machine-learning-with-big-data-86bcb39f2f0b',\n",
       "  'title': 'Handling Big Datasets for Machine\\xa0Learning',\n",
       "  'subtitle': '-',\n",
       "  'claps': 533,\n",
       "  'responses': 4.0,\n",
       "  'reading_time': 12,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-11',\n",
       "  'clap_prop': 0.0002632770934604045,\n",
       "  'text': \"Towards Data Science Mar 11 2019 Listen Save Handling Big Datasets Machine Learning 2.5 quintillion byte data created day 90 data world wa generated past two year prevalence data increase need learn deal large data Big Data like teenage sex everyone talk nobody really know everyone think everyone else everyone claim Dan Ariely Imagine downloading dataset full Tweets ever written data 2.3 billion people Facebook even data every webpage exists Internet analyze dataset isolated problem hit largest tech company current age datasets already becoming larger computer handle regularly work satellite data easily Terabyte range large even fit hard drive computer let alone process reasonable amount time eye-opening statistic regarding big data Storing data one thing processing developing machine learning algorithm work article discus easily create scalable parallelized machine learning platform cloud process large-scale data used research commercial non-commercial purpose done minimal cost compared developing supercomputer develop robust high-performance parallel cluster cloud also used local machine performance enhancement delve following topic post based content following GitHub repository found command required setting machine learning platform cloud found markdown file based tutorial Institute Applied Computational Science Harvard University Environment Setup Dockers Containers read one part post let part people set machine learning environment typically install everything directly operating system Oftentimes fine try download something like PyTorch TensorFlow Keras everything explodes spend hour Stack Overflow trying get thing work implore work like sake problem typically occurs dependency co-dependencies certain package specific version package Often need half package work would make sense start clean slate install version dependency required task hand ultimately save time stress using Anaconda easy efficient separate isolated container run without causing problem container called Conda environment Conda package manager Python think environment different computer know existence create new environment start blank slate need install package great part actually download package twice pointer created point specific version package want install already downloaded computer may seem pointless unless dependency issue computer promise worth knowing Another useful feature install package like one line using YAML .yml file file tell environment package want install dependency required downloaded need write file exported one line code environment already required package pretty neat right required command shown Gist example YAML file look like conda env export environment.yml command run another good reason separating thing environment like want get reproducible result data analysis widely depend version different package also operating system working creating environment.yml file contain dependency easier someone reproduce result created Conda environment essentially isolated rest system However additional thing want work environment Python package case use Docker create container application containerize whole thing using Docker case component encapsulated Docker container admit concept behind Docker container bit confusing Building docker image trivial task Fortunately however Jupyter folk created repo2docker repo2docker take GitHub repository automatically make docker image uploads docker image repository done using one line code running code code pop terminal look like following Simply copy paste URL browser access docker image get going read using repo2docker Another really useful thing use binder Binder build repo2docker provide service provide GitHub repository give working JupyterHub publish project demo etc GitHub repository associated tutorial run binder clicking link ReadMe section read using Binder Parallelization Dask Kubernetes ha taken u quite get parallelization part tutorial previous step necessary get Lets dive using Dask Kubernetes Dask ha two part associated 1 Dynamic task scheduling optimized computation like Airflow 2 Big Data collection like parallel Numpy array Pandas dataframes list Dask ha around couple year gradually growing momentum due popularity Python machine learning application Dask allows scaling 1000 core cluster Python application processed much faster regular laptop would refer anyone interested working Dask GitHub repository Tom Augspurger one main creator Dask found talked Dask doe Kubernetes come run Dask laptop allows u distribute code multiple core doe help u run code multiple system time run locally Ideally want run cloud provisioned cluster wed like cluster self-repairing wed like code respond failure expand onto machine need need cluster manager Kubernetes cluster manager think like operating system cluster provides service discovery scaling load-balancing self-healing Kubernetes think application stateless movable one machine another enable better resource utilization controlling master node cluster operating system run worker node perform bulk work node computer associated cluster loses connection break master node assign work someone new like bos would stopped working master worker node consist several piece software allow perform task get pretty complicated quickly give high-level overview Master Node Worker Node great isnt particularly helpful unless 100 computer disposal make use power Kubernetes Dask afford u Enter cloud Dask Cloud Deployment want run Dask speed machine learning code Python Kubernetes recommended cluster manager done local machine using Minikube 3 major cloud provider Microsoft Azure Google Compute Cloud Amazon Web Services probably familiar cloud computing since pretty much everywhere day common company computing infrastructure cloud since reduces capital expenditure computing equipment move operational expenditure requires le maintenance also significantly reduces running cost Unless working classified information strict regulatory requirement probably get away running thing cloud Using cloud allows leverage collective performance several machine perform task example performing hyperparameter optimization neural network need rerun model 10,000 time get best parameter selection fairly common problem would nonsensical run one computer take 2 week run model 100 computer likely finish task hour hope made good case make use cloud aware get quite expensive use powerful machine especially turn using set environment cloud must following First run following See http //docs.dask.org/en/latest/setup/kubernetes-helm.html detail Deep Learning Cloud several useful tool available building deep learning algorithm Kubernetes Dask example TensorFlow put cloud using tf.distributed kubeflow parallelism trivially used grid optimization since different model run worker node Examples found GitHub repository use research environmental scientist consulting work machine learning consultant regularly use either JupyterHub Kubernetes cluster Dask Harvards supercomputer Odyssey run infrastructure AWS real prejudice Azure Google Cloud wa taught use AWS first Example Cloud Deployment AWS section run setup Kubernetes Cluster running Dask AWS first thing need set account AWS able run following line code unless already account First download AWS command line interface configure private key supplied AWS install Amazons Elastic Container Service EKS Kubernetes using brew command Creating Kubernetes cluster ludicrously simple need run one command specify cluster name number node region case Boston choose us-east-1 run command must configure cluster following command set Helm Dask cluster Wait two minute complete install Dask Kubernetes command detail shell need command like exact pod name different cluster clone GitHub repository watch Dask go Kaggle Rossman Competition recommend got Dask cloud deployment running try running rossman_kaggle.ipynb example code Kaggle Rossman competition allowed user use data wanted try predict pharmacy sale Europe competition wa run 2015 step notebook run set coding environment multilayer perceptron order apply parallel cluster perform hyperparameter optimization step code split function run sklearn pipeline recommended way run large machine learning program several example repository run parallel cluster play Also feel free clone repository tinker much like learn learn Dask check following link dask/dask-tutorial Dask tutorial Contribute dask/dask-tutorial development creating account GitHub github.com Dask Dask 1.1.4 documentation Internally Dask encodes algorithm simple format involving Python dicts tuples function graph format docs.dask.org learn Dask Kubernetes dask/dask-kubernetes Native Kubernetes integration dask Contribute dask/dask-kubernetes development creating account github.com Dask Kubernetes Dask Kubernetes 0.7.0 documentation Currently designed run pod Kubernetes cluster ha permission launch pod kubernetes.dask.org learn Helm Kubernetes Helm Dask 1.1.4 documentation case might consider setting Kubernetes cluster one common cloud provider docs.dask.org struggling work step multiple walkthroughs go specific detail Adding Dask Jupyter Kubernetes Cluster post 're going set Dask Jupyter Kubernetes cluster running AWS n't ramhiser.com Setup private dask cluster Kubernetes alongside JupyterHub Jetstream Andrea Zonca 's blog post leverage software made available Pangeo community allow user Jupyterhub zonca.github.io setting cluster Google Cloud sadly could find one Microsoft Azure check link ogrisel/docker-distributed Experimental docker-compose setup bootstrap distributed docker-swarm cluster ogrisel/docker-distributed github.com hammerlab/dask-distributed-on-kubernetes Deploy dask-distributed google container engine using kubernetes hammerlab/dask-distributed-on-kubernetes github.com working parallel cluster perform machine learning big data big compute task Thanks reading Newsletter update new blog post extra content sign newsletter Newsletter Subscription Enrich academic journey joining community scientist researcher industry professional obtain mailchi.mp 585 3 585 585 3 Get email whenever Matthew Stewart publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Aidan Morrison Mar 11 2019 data-scientists need direction hate direction theyre given Everyone know beneath hype data-scientists struggle job Hang around beer kind data-science meetup youll hear end rant Data-scientists frequently hired pet plaything aspirational executive little need service 11 min read 11 min read Share idea million reader Mohamed Chrif Haidara Mar 11 2019 Dimensionality Reduction toolbox python article derived work friend Herv Trinh recent year volume data ha exploded 80 ha led emergence many model Machine Learning since easier train model important dataset However 5 min read 5 min read David Comfort Mar 11 2019 Member-only Build Reporting Dashboard using Dash Plotly blog post provide step-by-step tutorial build reporting dashboard using Dash Python framework building analytical web application Rather go basic building Dash app provide detailed guide building multi-page dashboard data 24 min read 24 min read Daniel Shenfeld Mar 11 2019 Member-only Build AI Moat Forging link better model better product Jerry Chen introduced concept system intelligence explain product powered AI data help company build deep moat protect profit market share competitor Successful company build virtuous cycle data data generate 7 min read 7 min read Yitong Ren Mar 11 2019 Member-only Get Started Using CNN+LSTM Forecasting method consider data low granularity recurring local pattern Predicting trend ha ancient discipline yet never fallen popularity Whether stock price financial market power energy consumption sale projection corporate planning series time-based data point representation world thinking 4 min read 4 min read Matthew Stewart Environmental Data Science PhD Harvard ML consultant Critical Future Blogger TDS Content Creator EdX http //mpstewart.net Medium Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Naga Sanjay Continuous Training ML model Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Towards Data Science Mar 11, 2019 Listen Save Handling Big Datasets for Machine Learning More than 2.5 quintillion bytes of data are created each day. 90% of the data in the world was generated in the past two years. The prevalence of data will only increase, so we need to learn how to deal with such large data. Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it. Dan Ariely Imagine downloading a dataset full of all the Tweets ever written, or the data of all the 2.3 billion people on Facebook, or even, the data for every webpage that exists on the Internet. How do you analyze such a dataset? This is not an isolated problem that only hits the largest tech companies. In the current age, datasets are already becoming larger than most computers can handle. I regularly work with satellite data and this can easily be in the Terabyte range too large to even fit on the hard drive of my computer, let alone to process it in a reasonable amount of time. Here are some eye-opening statistics regarding big data: Storing this data is one thing, but what about processing it and developing machine learning algorithms to work with it? In this article, we will discuss how to easily create a scalable and parallelized machine learning platform on the cloud to process large-scale data. This can be used for research, commercial, or non-commercial purposes and can be done with minimal cost compared to developing your own supercomputer. To develop a very robust and high-performance parallel cluster on the cloud (this can also be used on a local machine for performance enhancement) we will delve into the following topics: This post will be based on the contents of the following GitHub repository, that can be found here . All of the commands required for setting up the machine learning platform on the cloud can be found in the markdown file here . This is based on a tutorial by the Institute for Applied Computational Science at Harvard University. Environment Setup Dockers and Containers If you only read one part of this post, let it be this part. When people set up their machine learning environment, typically they install everything on the directly on their operating system. Oftentimes, this is fine, and then you try to download something like PyTorch, TensorFlow, or Keras and everything explodes and you spend hours on Stack Overflow trying to get things to work. I implore you not to work like this, for your own sake. This problem typically occurs from dependencies or co-dependencies of certain packages on specific versions of other packages. Often, you do not need half of these packages for your work. It would make more sense to start from a clean slate and only install the versions and dependencies that are required for the task at hand. This will ultimately save you time and stress. If you are using Anaconda, it is very easy and efficient to separate these into isolated containers such that they can all run without causing problems. These containers are called Conda environments. Conda is a package manager for Python You can think of these environments as different computers that do not know about the existence of each other. When I create a new environment, I start with a blank slate and need to install packages again. The great part about this is that you do not actually download the packages twice when doing this, a pointer is created which points to the specific version of the package you want to install that is already downloaded on your computer. This may seem pointless unless you have had dependency issues before on your computer, but I can promise you it is worth knowing about this. Another useful feature is that you can install all the packages you like in just one line by using a YAML (.yml) file. This is a file that tells the environment what packages you want to install and what dependencies are required to be downloaded. You do not need to write this file, it can be exported with one line of code from an environment where you already have all the required packages pretty neat right? All of the required commands are shown in the Gist below. Here is an example of what the YAML file looks like when the conda env export > environment.yml command is run. There is another good reason for separating things into environments like this. If I want to get reproducible results for data analysis that I am doing, it can widely depend on the versions of different packages and also the operating system that you are working on. By creating environment.yml files that contain all of the dependencies, it is easier for someone to reproduce your results. So what did we do when we created our Conda environment? We essentially isolated it from the rest of our system. However, what if we have additional things that we want to work with the environments that are not just Python packages. In this case, we use Docker to create containers. If your application: you can containerize the whole thing  using Docker . In this case, all these components will be encapsulated in a Docker container : I admit the concept behind Docker and containers is a bit confusing. Building a docker image is not a trivial task. Fortunately, however, the Jupyter folks created repo2docker for this. repo2docker takes a GitHub repository and automatically makes a docker image and uploads it to the docker image repository for you. This can be done using one line of code. After running the above code, you should have some code pop up in the terminal that looks like the following: Simply copy and paste the URL in your browser and you then have access to your docker image and can get going! You can read more about using repo2docker here . Another really useful thing to use is binder. Binder builds on repo2docker to provide a service where you provide a GitHub repository, and it gives you a working JupyterHub where you can publish your project, demo, etc. The GitHub repository associated with this tutorial can be run on binder by clicking on the link in the ReadMe section. You can read more about using Binder here . Parallelization with Dask and Kubernetes It has taken us quite a while to get to the parallelization part of the tutorial, but the previous steps were necessary to get here. Lets now dive into using Dask and Kubernetes. Dask has two parts associated with it: [1] Dynamic task scheduling optimized for computation like Airflow. [2] Big Data collections like parallel (Numpy) arrays, (Pandas) dataframes, and lists. Dask has only been around for a couple of years but is gradually growing momentum due to the popularity of Python for machine learning applications. Dask allows scaling up (1000 core cluster) of Python applications so that they can be processed much faster than on a regular laptop. I would refer anyone who is interested in working with Dask to the GitHub repository by Tom Augspurger (one of the main creators of Dask), which can be found here . So we have talked about Dask, where does Kubernetes come in here? If we run Dask on our laptop, it allows us to distribute our code to multiple cores at once, but it does not help us run the code on multiple systems at the same time. We have run it locally. Ideally, we want to run on a cloud provisioned cluster, and wed like this cluster to be self-repairing that is, wed like our code to respond to failures and expand onto more machines if we need them. We need a cluster manager. Kubernetes is a cluster manager. We can think of it like being an operating system for the cluster. It provides service discovery, scaling, load-balancing, and is self-healing. Kubernetes think of applications as stateless, and movable from one machine to another to enable better resource utilization. There is a controlling master node on which the cluster operating system runs, and worker nodes which perform the bulk of the work. If a node (computer associated with the cluster) loses connection or breaks, the master node will assign the work to someone new, just like your boss would if you stopped working. The master and worker nodes consist of several pieces of software which allow it to perform its task. It gets pretty complicated so I will quickly give a high-level overview. Master Node : Worker Node : Doing all of this is great, but it isnt particularly helpful unless we have 100 computers at our disposal to make use of the power that Kubernetes and Dask afford us. Enter the cloud. Dask Cloud Deployment If you want to run Dask to speed up your machine learning code in Python, Kubernetes is the recommended cluster manager. This can be done on your local machine using Minikube or on any of the 3 major cloud providers, Microsoft Azure, Google Compute Cloud, or Amazon Web Services. You are probably familiar with cloud computing since it is pretty much everywhere these days. It is now very common for companies to have all of their computing infrastructure on the cloud, since this reduces their capital expenditure on computing equipment and moves it to operational expenditure, requires less maintenance and also significantly reduces the running cost. Unless you are working with classified information or have very strict regulatory requirements, you can probably get away with running things on the cloud. Using the cloud allows you to leverage the collective performance of several machines to perform the same task. For example, if you are performing hyperparameter optimization on a neural network and it will need to rerun the model 10,000 times to get the best parameter selection (a fairly common problem) then it would be nonsensical to run it on one computer if it will take 2 weeks. If you can run this same model on 100 computers you will likely finish the task in a few hours. I hope I have made a good case for why you should make use of the cloud, but be aware that it can get quite expensive if you use very powerful machines (especially if you do not turn them off after using them!) To set up the environment on the cloud, you must do the following: First run the following and then See https://docs.dask.org/en/latest/setup/kubernetes-helm.html for all the details. Deep Learning on the Cloud There are several useful tools which are available for building deep learning algorithms with Kubernetes and Dask. For example, TensorFlow can be put on the cloud using tf.distributed of kubeflow . The parallelism can be trivially used during grid optimization since different models can be run on each worker node. Examples can be found on the GitHub repository here . What do you use? For my own research (I am an environmental scientist) and in my consulting work (machine learning consultant) I regularly use either JupyterHub, a Kubernetes cluster with Dask on Harvards supercomputer Odyssey, or I will run the same infrastructure on AWS (no real prejudice against Azure or the Google Cloud, I was just taught how to use AWS first). Example Cloud Deployment on AWS In this section, I will run through the setup of a Kubernetes Cluster running Dask on AWS. The first thing you need to do is set up an account on AWS, you will not be able to run the following lines of code unless you already have an account. First, we download the AWS command line interface and configure it with our private key supplied by AWS. We then install Amazons Elastic Container Service (EKS) for Kubernetes using the brew commands. Creating a Kubernetes cluster is now ludicrously simple, we only need to run one command, but you should specify the cluster name, the number of nodes, and the region you are in (in this case I am in Boston so I choose us-east-1 ) and then run the command. Now we must configure the cluster with the following commands: Now we set up Helm and Dask on the cluster Wait two minutes for this to complete and then we can install Dask. A few more Kubernetes commands. For more details and a shell, you will need a command like this. Your exact pod names will be different. Once you are in the cluster, you can clone the GitHub repository and watch Dask go! Kaggle Rossman Competition I recommend that once you have got the Dask cloud deployment up and running you try running the rossman_kaggle.ipynb . This is example code from the Kaggle Rossman competition, which allowed users to use any data they wanted to try and predict pharmacy sales in Europe. The competition was run in 2015. The steps in this notebook run you through how to set up your coding environment for a multilayer perceptron in order to apply it to a parallel cluster and then perform hyperparameter optimization. All of the steps in this code are split into functions which are then run in an sklearn pipeline (this is the recommended way to run large machine learning programs). There are several other examples on the repository that you can run on the parallel cluster and play with. Also, feel free to clone the repository and tinker with it as much as you like. Where can I learn more? To learn more about Dask, check out the following links: dask/dask-tutorial Dask tutorial. Contribute to dask/dask-tutorial development by creating an account on GitHub. github.com Dask - Dask 1.1.4 documentation Internally, Dask encodes algorithms in a simple format involving Python dicts, tuples, and functions. This graph format docs.dask.org To learn more about Dask with Kubernetes: dask/dask-kubernetes Native Kubernetes integration for dask. Contribute to dask/dask-kubernetes development by creating an account on github.com Dask Kubernetes - Dask Kubernetes 0.7.0 documentation Currently, it is designed to be run from a pod on a Kubernetes cluster that has permissions to launch other pods kubernetes.dask.org To learn more about Helm: Kubernetes and Helm - Dask 1.1.4 documentation If this is not the case, then you might consider setting up a Kubernetes cluster on one of the common cloud providers docs.dask.org If you are struggling to work through any of the above steps, there are multiple other walkthroughs that go through the specifics in more detail: Adding Dask and Jupyter to a Kubernetes Cluster In this post, we're going to set up Dask and Jupyter on a Kubernetes cluster running on AWS. If you don't have a ramhiser.com Setup private dask clusters in Kubernetes alongside JupyterHub on Jetstream | Andrea Zonca's blog In this post we will leverage software made available by the Pangeo community to allow each user of a Jupyterhub zonca.github.io For setting up the cluster on the Google Cloud (sadly could not find one for Microsoft Azure) check these links out: ogrisel/docker-distributed Experimental docker-compose setup to bootstrap distributed on a docker-swarm cluster. - ogrisel/docker-distributed github.com hammerlab/dask-distributed-on-kubernetes Deploy dask-distributed on google container engine using kubernetes - hammerlab/dask-distributed-on-kubernetes github.com Now you should have a working parallel cluster on which to perform machine learning on big data or for big compute tasks! Thanks for reading! Newsletter For updates on new blog posts and extra content, sign up for my newsletter. Newsletter Subscription Enrich your academic journey by joining a community of scientists, researchers, and industry professionals to obtain mailchi.mp 585 3 585 585 3 Get an email whenever Matthew Stewart publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Aidan Morrison Mar 11, 2019 Why data-scientists need more direction, but hate the directions theyre given Everyone knows that beneath the hype, data-scientists struggle in their jobs. Hang around for a few beers after some kind of data-science meetup, and youll hear no end to the rants: Data-scientists are frequently hired to be pets and playthings for aspirational executives with little need for their services, or 11 min read 11 min read Share your ideas with millions of readers. Mohamed Chrif Haidara Mar 11, 2019 Dimensionality Reduction toolbox in python This article is derived from my work with my friend Herv Trinh. In recent years, the volume of data has exploded by more than 80%. This has led to the emergence of many models of Machine Learning, since it is easier to train these models with an important dataset. However 5 min read 5 min read David Comfort Mar 11, 2019 Member-only How to Build a Reporting Dashboard using Dash and Plotly In this blog post, I will provide a step-by-step tutorial on how to build a reporting dashboard using Dash, a Python framework for building analytical web applications. Rather than go over the basics of building a Dash app, I provide a detailed guide to building a multi-page dashboard with data 24 min read 24 min read Daniel Shenfeld Mar 11, 2019 Member-only How to Build an AI Moat Forging the link between better models and better products Jerry Chen introduced the concept of systems of intelligence to explain how products powered by AI and data can help companies build deep moats to protect their profits and market share from competitors: Successful companies here can build a virtuous cycle of data because the more data you generate and 7 min read 7 min read Yitong Ren Mar 11, 2019 Member-only Get Started with Using CNN+LSTM for Forecasting A method you should consider when you have data of low granularity with recurring local pattern Predicting the trend has been an ancient discipline yet its never fallen from popularity. Whether it is stock price in financial market, power or energy consumption, or sales projection for corporate planning, a series of time-based data points can be the representation of how the world is thinking in any 4 min read 4 min read Matthew Stewart Environmental + Data Science PhD @Harvard | ML consultant @Critical Future | Blogger @TDS | Content Creator @EdX. https://mpstewart.net More from Medium Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Naga Sanjay Continuous Training of ML models. Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 5451,\n",
       "  'url': 'https://towardsdatascience.com/object-detection-via-color-based-image-segmentation-using-python-e9b7c72f0e11',\n",
       "  'title': 'Object detection via color-based image segmentation using\\xa0python',\n",
       "  'subtitle': 'A tutorial on contouring using…',\n",
       "  'claps': 496,\n",
       "  'responses': 4.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 0.0002450008224321963,\n",
       "  'text': 'Towards Data Science Mar 31 2019 Member-only Listen Save Object detection via color-based image segmentation using python tutorial contouring using python OpenCV Getting started already jupyter notebook IDE run python OpenCV installed skip Execution Tools hero today Anaconda free open-source distribution help installing different package sort mess isolated environment Wikipedias telling u Anaconda Anaconda free open-source distribution Python R programming language scientific computing data science machine learning application large-scale data processing predictive analytics etc aim simplify package management deployment Package version managed package management system conda Anaconda distribution used 12 million user includes 1400 popular data-science package suitable Windows Linux MacOS detailed tutorial download Anaconda anaconda Windows anaconda Linux Creating environment Open bash cmd type Type yes prompted download package open jupyter notebook browser important term Contours Contours explained simply curve joining continuous point along boundary color intensity contour useful tool shape analysis object detection recognition Thresholds Applying thresholding grayscale image make binary image set threshold value value threshold turned black value go white Execution need start gon na start simple example show color-based segmentation work Bear till get good stuff want try get image free following code Im gon na segment image 17 gray level measure level area using contouring function simply convert range intensity gray want contour highlight iteration unifying lie within range one intensity turn every intensity range black including greater smaller intensity second step threshold image color want contour right appears white others convert black step doesnt change much must done contouring work best black white threshold image applying step thresholding image would except white ring wouldve gray gray intensity 10th gray level 25515 10 way obtain area gray level really important move want stress importance topic Im Computer Engineering student Im working project called Machine learning intelligent tumor detection identification Color-based image segmentation used project help computer learn detect tumor dealing MRI scan program ha detect cancer level said MRI scan doe segmenting scan different grayscale level darkest filled cancerous cell closest white healthier part calculates degree membership tumor grayscale level information program able identify tumor stage project based soft computing fuzzy logic machine learning learn Fuzzy logic curing cancer Object detection get image free Pexels need crop image want contour leaf Since texture image irregular uneven meaning although arent many color intensity green color image change also brightness best thing unify different shade green one shade way apply contouring deal leaf one whole object Note result apply contouring image without pre-processing wanted see uneven nature leaf make OpenCV understand one object First know HSV representation color know converting RGB HSV like following Converting image HSV easier HSV get complete range one color HSV H stand Hue Saturation V value already know green color 60 255 255 green world lie within 45 100 50 75 255 255 60 15 100 50 60+ 15 255 255 15 approximation value take range convert 75 255 200 light color 3rd value must relatively large cause thats brightness color thats value make part white threshold image Since seem irregularity background well get largest contour using method largest contour course leaf get index leaf contour contour array get area center leaf Contours many feature make use contour perimeter convex hull bounding rectangle many others learn 591 5 591 591 5 Towards Data Science home data science Medium publication sharing concept idea code Sahil Dhankhad Mar 31 2019 Member-only Forget APIs Python Scraping Using Beautiful Soup Import Data File web Part 2 APIs always every website Beautiful Soup going stay forever collect kind data website Today Data play critical role every industry data coming internet company 8 min read 8 min read Share idea million reader Andrew Donaldson Mar 31 2019 Member-only Dont Fear Robots Impressive tech important remember Boston Dynamics video trailer movie isnt yet made Every time one Boston Dynamics video hit met mix amazement prediction doom hand new robot overlord impressive also well choreographed staged Impressive tech important 4 min read 4 min read Jasmine Vasandani Mar 31 2019 Member-only 10 Steps Teaching Data Science Well resource data science instructor increase demand data scientist ha parallel growth training program within company educational institution teach people data science addition focusing meeting job demand need resource instructor facilitating data science classroom 4 min read 4 min read Karan Bhanot Mar 31 2019 Member-only income 50K/yr Machine Learning tell Machine learning breaking ground numerous field including Finance could use Machine Learning model identify income individual found right dataset called Census Income Dataset 7 min read 7 min read Ane Berasategi Mar 31 2019 Member-only Semantic search brief post semantics search semantic search READ ORIGINAL POST BLOG Semantics branch linguistics studying meaning word symbolic use also including multiple meaning One morning shot elephant pajama got pajama Ill never know Groucho Marx sentence semantically ambiguous clear author 5 min read 5 min read Salma Ghoneim SDE Microsoft Passionate frontend development fascinated artificial intelligence Interested game development Medium Fnnnr tctt2022 pwnable 02 Pranjal Saxena Level Coding Step Step Guide Labeling Object Detection Training Images Using Python Chinmay Bhalerao Towards AI Working Computer Vision project code chunk help Mikolaj Buchwald YOLO COCO object recognition basic Python Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Mar 31, 2019 Member-only Listen Save Object detection via color-based image segmentation using python A tutorial on contouring using python & OpenCV. Getting started If you already have jupyter notebook or an IDE with which you can run python & OpenCV installed, just skip to Execution. Tools Our hero today is Anaconda . a free open-source distribution that helps with installing different packages & sorts out their messes into isolated environments. What Wikipedias telling us about Anaconda Anaconda is a free and open-source distribution of the Python and R programming languages for scientific computing (data science, machine learning applications, large-scale data processing, predictive analytics, etc. ), that aims to simplify package management and deployment . Package versions are managed by the package management system conda . The Anaconda distribution is used by over 12 million users and includes more than 1400 popular data-science packages suitable for Windows, Linux, and MacOS. Here are detailed tutorials on how to download Anaconda. anaconda for Windows & anaconda for Linux. Creating the environment Open the bash (cmd) and type this Type y (for yes) when prompted to download the packages. This will open jupyter notebook in the browser for you. Some important terms Contours Contours can be explained simply as a curve joining all the continuous points (along with the boundary), having the same color or intensity. The contours are a useful tool for shape analysis and object detection and recognition. Thresholds Applying thresholding on a grayscale image makes it a binary image. You set a threshold value, in which all values below this threshold are turned to black and all values above go white. Execution Now you have all you need to start. Were gonna start with a simple example just to show you how color-based segmentation works. Bear with me till we get to the good stuff. If you want to try this with me, you can get this image for free from here . In the following code, Im gonna segment this image into 17 gray levels. Then measure each levels area using contouring. In this function, I simply convert the range (of intensities) of gray that I want to contour (highlight) in this iteration by unifying all of those which lie within this range to one intensity. I turn every other intensity but this range to black (including greater & smaller intensities). The second step I threshold the image so that only the color that I want to contour right now appears white and all others convert to black. This step doesnt change much here but it must be done because contouring works best with black and white (thresholds) images. Before applying this step (thresholding) the image below would be the same except the white ring wouldve been gray (the gray intensity of the 10th gray level (25515*10 ) ) This way we obtain the area of each gray level. Is this really important? Before we move on, I want to stress the importance of this topic. Im a Computer Engineering student and Im working on a project called Machine learning for intelligent tumor detection and identification . Color-based image segmentation is used in this project to help the computer learn how to detect the tumor. When dealing with an MRI scan, the program has to detect the cancer level of said MRI scan. It does that by segmenting the scan into different grayscale levels in which the darkest is the most filled with cancerous cells and the closest to white is the healthier parts. Then it calculates the degree of membership of the tumor to each grayscale level. With this information, the program is able to identify the tumor and its stage. This project is based on soft computing, fuzzy logic & machine learning, you can learn more about it on Fuzzy logic and how it is curing cancer . Object detection You can get this image for free on Pexels from here . You just need to crop it. In this image, we want to contour the leaf only. Since the texture of this image is very irregular and uneven, meaning that although there arent many colors. The intensity of the green color in this image changes, also, its brightness. So, the best thing to do here is to unify all these different shades of green into one shade. This way when we apply contouring, it will deal with the leaf as one whole object. Note: This is the result if you apply contouring on the image without any pre-processing. I just wanted you to see how the uneven nature of the leaf makes OpenCV not understand that this is just one object. First, you have to know the HSV representation of your color , you can know it by converting its RGB to HSV just like the following. Converting the image to HSV : Its easier with HSV to get the complete range of one color. HSV, H stands for Hue, S for Saturation, V for value. We already know that the green color is [60, 255, 255]. All the greens in the world lie within [45, 100, 50] to [75, 255, 255] that is [60 15 , 100, 50] to [60+ 15 , 255, 255]. 15 is just an approximation value. We take this range and convert it to [75, 255, 200 ] or any other light color ( 3rd value must be relatively large) cause thats the brightness of the color, thats the value that will make this part be white when we threshold the image. Since there seem to be irregularities in the background as well, We can get the largest contour using this method, The largest contour is, of course, the leaf. We can get the index of the leaf contour in the contours array, from that we get the area and center of the leaf. Contours have many other features that you can make use of such as the contour perimeter, convex hull, bounding rectangle, and many others. You can learn more about it from here . 591 5 591 591 5 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Sahil Dhankhad Mar 31, 2019 Member-only Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2 APIs are not always there for you for every website, but Beautiful Soup is going to stay with you forever to collect any kind of data from any website. Today, Data play a critical role in every industry. And most of this data is coming from the internet. Most company 8 min read 8 min read Share your ideas with millions of readers. Andrew Donaldson Mar 31, 2019 Member-only Dont Fear the Robots Impressive as the tech here is, it is important to remember that the Boston Dynamics videos are trailers for a movie that isnt yet made. Every time one of these Boston Dynamics videos hit, they are met with a mix of amazement and predictions of doom at the hands of our new robot overlords. It is impressive. It is also very well choreographed and staged. Impressive as the tech here is, it is important 4 min read 4 min read Jasmine Vasandani Mar 31, 2019 Member-only 10 Steps to Teaching Data Science Well A resource for data science instructors. With the increase in demand for data scientists, there has been a parallel growth in training programs within companies and educational institutions that teach people data science. In addition to focusing on meeting the job demand, there need to be more resources for instructors facilitating data science classrooms. 4 min read 4 min read Karan Bhanot Mar 31, 2019 Member-only Will your income be more than $50K/yr? Machine Learning can tell Machine learning is breaking grounds in numerous fields including Finance. What if we could use Machine Learning models to identify incomes of individuals? I found just the right dataset for this, called Census Income Dataset. 7 min read 7 min read Ane Berasategi Mar 31, 2019 Member-only Semantic search A brief post on semantics, search, and semantic search READ THE ORIGINAL POST IN MY BLOG Semantics is a branch of linguistics studying the meanings of words, their symbolic use, also including their multiple meanings. One morning I shot an elephant in my pajamas. How he got into my pajamas Ill never know. Groucho Marx This sentence is semantically ambiguous: its not clear if the author 5 min read 5 min read Salma Ghoneim SDE at Microsoft, Passionate about frontend development, fascinated by artificial intelligence, Interested in game development. More from Medium Fnnnr [tctt2022] pwnable 02 Pranjal Saxena in Level Up Coding Step by Step Guide for Labeling Object Detection Training Images Using Python Chinmay Bhalerao in Towards AI Working on a Computer Vision project? These code chunks will help you!!! Mikolaj Buchwald YOLO and COCO object recognition basics in Python Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5867,\n",
       "  'url': 'https://towardsdatascience.com/the-complete-tensorflow-tutorial-for-newbies-dc3acc1310f8',\n",
       "  'title': 'The Complete TensorFlow Tutorial for\\xa0Newbies',\n",
       "  'subtitle': 'From installation to building a neural network for\\xa0hand…',\n",
       "  'claps': 461,\n",
       "  'responses': 4.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-18',\n",
       "  'clap_prop': 0.0002277124579460534,\n",
       "  'text': \"Towards Data Science Mar 18 2019 Member-only Listen Save Complete TensorFlow Tutorial Newbies installation building neural network hand sign recognition TensorFlow robust framework machine learning deep learning make easier build model deploy production popular framework among developer come surprise framework also available web-based machine learning TensorFlow.js on-device inference TensorFlow Lite Furthermore recent announcement TensorFlow 2.0 framework soon easier use syntax simplified fewer APIs support Julia programming language great time get started TensorFlow mastering important asset data scientist tutorial get running TensorFlow Note TensorFlow 2.0 stable focus previous stable version first install framework easiest way possible write function learn syntax use APIs Finally write model recognize hand sign Lets get started hands-on video tutorial machine learning deep learning artificial intelligence checkout YouTube channel Installation Lets install TensorFlow go installation process Windows machine Feel free post comment experiencing difficulty operating system Step 1 Download Anaconda first step download install Anaconda Anaconda platform make easier perform data science give access popular tool library also act package manager Download Anaconda distribution operating system follow installation step safe accept default setting Installation might take minute lot thing bundled Anaconda installation done search anaconda prompt Windows search bar Open application see something like Great Step 2 Install TensorFlow install Tensorflow simply type Wait installation complete voil set write code using TensorFlow Getting warmed rest tutorial follow notebook also grab utility needed throughout tutorial Refer whenever feel stuck Computing sigmoid function Lets compute sigmoid function using TensorFlow full code block walk code TensorFlow program usually split two part construction phase computation phase construction phase use placeholder create variable need define type variable give name simply use built-in sigmoid function Note construction phase value computed fact code doe run need computation phase computation phase create session assign result computation another variable Notice function take z input use x inside function hence need feed_dict entire block cell called TensorFlow graph TensorFlow program structured Understanding structure key mastering TensorFlow comfortable able write advanced program Computing cost let compute cost function classification problem used compute cross-entropy function writing scratch let see TensorFlow make easy u achieve result Thats one line define loss function Notice used placeholder recognize construction computation phase code cell One-hot encoding One-hot encoding technique transform mutliclass label vector 0 1 Lets see TensorFlow TensorFlow make easy manipulate data comfortable general structure let move building actual neural network classify hand sign Hand Signs Recognition little project build hand sign recognition system Specifically neural network recognize hand expressing number 0 5 Lets load dataset take look sample image see building model first flatten image normalize feature one-hot encode label Onto building model Create placeholder First write function create placeholder feature matrix label matrix write function initialize weight matrix bias matrix Awesome must define forward propagation Note code cell comment show equivalent syntax numpy Finally define function compute cost ready combine everything model Wait backpropagation Unlike previous post wrote backpropagation scratch deep learning framework take care automatically line code Putting everything single model combine function model use mini-batch gradient descent train neural network run model following line code get Great test accuracy good used small part entire dataset train long main objective tutorial wa get used TensorFlow get overview API Good job ready use TensorFlow advanced neural network application following post explore different neural network structure use TensorFlow Keras another deep learning framework build Stay tuned keep learning 461 3 461 461 3 Get email whenever publish Get freebie course announcement VIP invitation event straight inbox Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Karl Schriek Mar 18 2019 Introducing Mercury-ML open-source messenger machine learning god ancient Roman mythology god Mercury wa known messenger god Wearing winged shoe winged hat zipped Mount Olympus kingdom men saw god wa known wasnt strongest 7 min read 7 min read Share idea million reader Supratim Haldar Mar 18 2019 stop training neural-network using callback useful hack Tensorflow Keras Introduction Often training deep neural network want stop training training accuracy reach certain desired threshold Thus achieve want optimal model weight avoid wastage resource time computation power brief tutorial let learn achieve 2 min read 2 min read Aman Kumar Mar 18 2019 5 prerequisite data science 5 point worth evaluating become data scientist Data science buzz word market n't Sexiest job time 29 increase demand year year 344 increase since 2013 easy get good salary hike etc etc etc Lucrative enough 5 min read 5 min read Nhan Tran Mar 18 2019 Machine Learning Simple Linear Regression Python post guide first step approach Machine Learning using Simple Linear Regression Linear First let say shopping Walmart Whether buy good pay 2.00 parking ticket apple price 1.5 buy x item apple populate price list easy predict 6 min read 6 min read Shadab Hussain Mar 17 2019 Optimizing Jupyter Notebook Tips Tricks nbextensions Jupyter Notebooks web-based interactive tool machine learning data science community us lot used quick testing reporting tool even highly sophisticated learning material online course blog Im going list 6 min read 6 min read Marco Peixeiro Senior data scientist Author Instructor write hands-on article focus practical skill Medium Dharmaraj Deploying Deep Learning Model using Flask API Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Towards Data Science Mar 18, 2019 Member-only Listen Save The Complete TensorFlow Tutorial for Newbies From installation to building a neural network for hand signs recognition TensorFlow is a robust framework for machine learning and deep learning. It makes it easier to build models and deploy them for production. It is the most popular framework among developers. This comes with no surprise, as the framework is also available for web-based machine learning ( TensorFlow.js ) and for on-device inference ( TensorFlow Lite ). Furthermore, with the recent announcement of TensorFlow 2.0 , the framework will soon be easier to use, as the syntax will be simplified with fewer APIs, and it will support the Julia programming language. It is a great time to get started with TensorFlow, and mastering it is an important asset data scientists. This tutorial will get you up and running with TensorFlow. Note that TensorFlow 2.0 is not stable, so we will focus on the previous stable version. We will first install the framework in the easiest way possible, then we will write a few functions to learn the syntax and to use a few APIs. Finally, we will write a model that will recognize hand signs. Lets get started! For hands-on video tutorials on machine learning, deep learning, and artificial intelligence, checkout my YouTube channel . Installation Lets install TensorFlow! We will go through the installation process on a Windows machine. Feel free to post a comment if you are experiencing difficulties on other operating systems. Step 1 Download Anaconda The first step is to download and install Anaconda . Anaconda is a platform that makes it easier to perform data science; it gives access to the most popular tools and libraries and it also acts as a package manager. Download the Anaconda distribution for your operating system and follow the installation steps. It is safe to accept the default settings. Installation might take a few minutes, because a lot of things are bundled with Anaconda. Once installation is done, search for anaconda prompt in your Windows search bar. Open the application and you should see something like this: Great! Step 2 Install TensorFlow Now, to install Tensorflow, simply type: Wait for the installation to complete and voil! You are now set to write code using TensorFlow! Getting warmed up The rest of the tutorial will follow this notebook. You can also grab any utilities needed throughout the tutorial. Refer to it whenever you feel stuck! Computing the sigmoid function Lets compute the sigmoid function using TensorFlow. Here is the full code block. We will then walk through what the code is doing. A TensorFlow program is usually split into two parts: a construction phase and a computation phase . During the construction phase, we use a placeholder to create a variable. We need to define the type of variable and give it a name. Then, we simply use the built-in sigmoid function. Note that during the construction phase, there are no values being computed. In fact, the code does not run at all. That is why we need a computation phase. During the computation phase, we create a session and assign the result of our computation to another variable. Notice that the function takes z as input, but use x inside the function, hence the need of feed_dict . And the entire block cell is called a TensorFlow graph . This is how all TensorFlow programs are structured: Understanding this structure is key to mastering TensorFlow. Once you are comfortable with this, you will be able to write more advanced programs. Computing the cost Now, lets compute the cost function for a classification problem. We are used to compute the cross-entropy function and writing it from scratch. Now, lets see how TensorFlow makes it easy for us to achieve the same result: Thats it! Only one line to define the loss function! Notice again how we used the placeholders, and recognize the construction and computation phases of the code cell above. One-hot encoding One-hot encoding is a technique to transform mutliclass labels to vectors of 0s and 1s. Lets see how we can do it in TensorFlow: Again, TensorFlow makes it very easy to manipulate our data. Now that you are more comfortable to the general structure, lets move on to building an actual neural network to classify hand signs! Hand Signs Recognition In this little project, we will build a hand signs recognition system. Specifically, our neural network will recognize if a hand is expressing a number from 0 to 5. Lets load the dataset and take a look at a sample image: And you should see: Before building our model, we will first flatten the images, normalize their features, and one-hot encode the labels: Onto building the model now! Create placeholders First, we write a function to create placeholders for the feature matrix and label matrix: Then, we write a function to initialize the weight matrix and bias matrix: Awesome! Now, we must define forward propagation: Note that in the code cell above, the comments show the equivalent syntax in numpy . Finally, we define a function to compute the cost: Now, we are ready to combine everything into a model! Wait What about backpropagation? Unlike previous posts where we wrote backpropagation from scratch, deep learning frameworks take care of it automatically with a few lines of code! Putting everything into a single model Now, we combine all our functions into a model, and we will use mini-batch gradient descent to train the neural network: Now, we run the model with the following line of code: And you should get: Great! The test accuracy is not that good, because we only used a small part of the entire dataset, and we did not train for very long. The main objective of this tutorial was to get used to TensorFlow and to get an overview to its API. Good job! You are now ready to use TensorFlow for more advanced neural networks and applications. In following posts, we will explore different neural network structures and use TensorFlow or Keras (another deep learning framework) to build them. Stay tuned and keep learning! 461 3 461 461 3 Get an email whenever I publish! Get freebies, course announcement, VIP invitations to events and more straight into your inbox! Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Karl Schriek Mar 18, 2019 Introducing Mercury-ML: an open-source messenger of the machine learning gods In the ancient Roman mythology, the god Mercury was known as the messenger of the gods. Wearing winged shoes and a winged hat, he zipped between Mount Olympus and the kingdoms of men and saw to it that the will of the gods was known. He wasnt the strongest, the 7 min read 7 min read Share your ideas with millions of readers. Supratim Haldar Mar 18, 2019 How to stop training a neural-network using callback? An useful hack with Tensorflow and Keras Introduction Often, when training a very deep neural network, we want to stop training once the training accuracy reaches a certain desired threshold. Thus, we can achieve what we want (optimal model weights) and avoid wastage of resources (time and computation power). In this brief tutorial, lets learn how to achieve 2 min read 2 min read Aman Kumar Mar 18, 2019 5 prerequisites for data science. 5 points worth evaluating to become a data scientist. Data science the buzz word in market!! isn't it? Sexiest job of the time, 29% increase in demand year over year and a 344% increase since 2013, easy to get a good salary hike. etc. etc. etc. :) Lucrative enough 5 min read 5 min read Nhan Tran Mar 18, 2019 Machine Learning: Simple Linear Regression with Python In this post we will guide you the very first step to approach Machine Learning using Simple Linear Regression. What is Linear? First, lets say that you are shopping at Walmart. Whether you buy goods or not, you have to pay $2.00 for parking ticket. Each apple price $1.5, and you have to buy an (x) item of apple. Then we can populate a price list as below: Its easy to predict 6 min read 6 min read Shadab Hussain Mar 17, 2019 Optimizing Jupyter Notebook: Tips, Tricks, and nbextensions Jupyter Notebooks are a web-based and interactive tool that the machine learning and data science community uses a lot. They are used for quick testing, as a reporting tool or even as highly sophisticated learning materials in online courses. So here in this blog, Im going to list down a 6 min read 6 min read Marco Peixeiro Senior data scientist | Author | Instructor. I write hands-on articles with a focus on practical skills. More from Medium Dharmaraj Deploying Deep Learning Model using Flask API Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 5453,\n",
       "  'url': 'https://towardsdatascience.com/forget-apis-do-python-scraping-using-beautiful-soup-import-data-file-from-the-web-part-2-27af5d666246',\n",
       "  'title': 'Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part\\xa02',\n",
       "  'subtitle': '-',\n",
       "  'claps': 459,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 8,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 0.00022672455140398807,\n",
       "  'text': 'Towards Data Science Mar 31 2019 Member-only Listen Save Forget APIs Python Scraping Using Beautiful Soup Import Data File web Part 2 APIs always every website Beautiful Soup going stay forever collect kind data website oday Data play critical role every industry data coming internet company invest million dollar one tech gain user without profited invested product return Internet vast contains information one topic nerdy professor need importance extracting information web becoming increasingly loud clear time adding information facebook twitter LinkedIn giving feedback Yelp information treated Data kind data internet come many different way example comment restaurant feedback Yelp Twitter discussion Reddit user discussion Stock price etc collect data organize analyze going talk tutorial several way extracting importing data Internet use APIs retrieve information major website Thats everybody using day import data internet primary site like Twitter Twitch Instagram Facebook provides APIs access website dataset data available structured form website doesnt provide API think dont want u use user data dont offer lack knowledge topic going import data web without using APIs processed please look part 1 series everything connected like dot Something dont know data File Starter Data Science Import data File new Data Science field must working hard learn Data Science concept fast towardsdatascience.com Beautiful Soup Beautiful Soup best Library scrap data particular website Internet comfortable work also par extract structured data HTML Beautiful Soup automatically transforms incoming text Unicode outgoing version UTF-8 dont remember encoding except document doesnt define encoding Beautiful Soup cant catch one mention original encoding Rules run program please use Jupyter python environment run program Instead running whole program taking precaution program doesnt break website Please check website term condition start pulling data sure read statement legal use data Basic-Getting Familiar HTML HTML code play essential role extracting data website processed let u jump basic HTML tag got tiny bit knowledge HTML tag move ahead next level basic syntax HTML webpage Every tag serf block inside webpage 1 DOCTYPE html HTML document must start type declaration 2 HTML document contained html /html 3 meta script declaration HTML document head /head 4 visible portion HTML document body /body tag 5 Title heading defined h1 h6 tag 6 Paragraphs defined p tag useful tag include hyperlink table table tr table row td table column Lets Check HTML page List Asian country area Wikipedia need additional citation verification .improve article adding citation reliable source Unsourced en.wikipedia.org Let u take Wikipedia page scrapping google chrome go page first right-click open browser inspector inspect webpage result see table inside wiki table sortable inspect find table information fantastic yeah going amazing see beautiful soup Lets Start DIY project know data located going start scrapping data process need install import library face trouble installation use sudo front every line Requests meant used human communicate language suggests dont manually join query string URLs form-encode POST data Requests enable send HTTP/1.1 request utilizing Python combine content like header form data multipart file parameter simple Python library also enables obtain response data Python way BS4 BeautifulSoup Beautiful Soup Python library extracting data HTML XML file work favourite parser produce natural way operating examining transforming parse tree usually save programmer hour day work begin studying source code given web page building BeautifulSoup soup object BeautifulSoup function.Now need use Beautiful Soap function help u parse apply HTML fetched Wikipedia page going use Beautiful Soup parse HTML data collected URL variable assign different variable store data Beautiful Soup format called Soup get concept structure underlying HTML web page use Beautiful Soups prettify function check get prettify function visit link look Wikipedia page Asian country see little bit information country area wikipedia table already setup-which make work easy Lets look prettify HTML Beginning HTML table tag class identifier wikitable sortable remember class future use go program see table made look row begin finish tr /tr tag first row header ha th tag data row underneath every club ha td tag Using td tag going tell Python secure data go ahead let work Beautiful Soup function demonstrate capture deliver data u HTML website title function Beautiful Soup return HTML tag heading content within use information start preparing attack HTML know data remains within HTML table firstly give Beautiful Soup retrieve occurrence table tag within page add array called all_tables table class wikitable sortable link country name title table class wikitable sortable connection country name title going extract link within used find_all URL extract title name country create list Countries extract name country link add list country convert list country Pandas DataFrame work python interested scrapping data high volume consider using Scrapy powerful python scraping framework also try integrate code public APIs performance data retrieval significantly higher scraping webpage example take look Facebook Graph API help get hidden data shown Facebook webpage Consider using database backend like MySQL collect information get big carry u end Beautiful Soup tutorial Confidently provides quite get working examine scraping next project Weve introduced request fetch URL HTML data Beautiful Soup parse HTML Pandas convert data data frame proper presentation find tutorial notebook question please feel free ask next tutorial going talk APIs Feel Free contact LinkedIn References 1. http //www.gregreda.com/2013/03/03/web-scraping-101-with-python/ 2. http //www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/ 3. http //github.com/stewync/Web-Scraping-Wiki-tables-using-BeautifulSoup-and-Python/blob/master/Scraping 2BWiki 2Btable 2Busing 2BPython 2Band 2BBeautifulSoup.ipynb 4. http //en.wikipedia.org/wiki/List_of_Asian_countries_by_area 5. http //www.crummy.com/software/BeautifulSoup/ 450 3 450 450 3 Towards Data Science home data science Medium publication sharing concept idea code Andrew Donaldson Mar 31 2019 Member-only Dont Fear Robots Impressive tech important remember Boston Dynamics video trailer movie isnt yet made Every time one Boston Dynamics video hit met mix amazement prediction doom hand new robot overlord impressive also well choreographed staged Impressive tech important 4 min read 4 min read Share idea million reader Jasmine Vasandani Mar 31 2019 Member-only 10 Steps Teaching Data Science Well resource data science instructor increase demand data scientist ha parallel growth training program within company educational institution teach people data science addition focusing meeting job demand need resource instructor facilitating data science classroom Heres 4 min read 4 min read Karan Bhanot Mar 31 2019 Member-only income 50K/yr Machine Learning tell Machine learning breaking ground numerous field including Finance could use Machine Learning model identify income individual found right dataset called Census Income Dataset used information dataset predict someone would earn income 7 min read 7 min read Ane Berasategi Mar 31 2019 Member-only Semantic search brief post semantics search semantic search READ ORIGINAL POST BLOG Semantics branch linguistics studying meaning word symbolic use also including multiple meaning One morning shot elephant pajama got pajama Ill never know Groucho Marx sentence semantically ambiguous clear author 5 min read 5 min read Sik-Ho Tsang Mar 31 2019 Review DPN Dual Path Networks Image Classification Better ResNet DenseNet PolyNet ResNeXt Winner ILSVRC 2017 Object Localization Challenge story DPN Dual Path Network briefly reviewed work National University Singapore Beijing Institute Technology National University Defense Technology Qihoo 360 AI Institute ResNet enables feature re-usage DenseNet enables new feature exploration DPN pick advantage ResNet 6 min read 6 min read Sahil Dhankhad Data Science Machine Learning Researcher LakeheadU Learning something everyday Data www.linkedin.com/in/sahil-dhankhad-303350135 Medium Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Antonio Blago ILLUMINATION Become Data Analyst 2023 Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Mar 31, 2019 Member-only Listen Save Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2 APIs are not always there for you for every website, but Beautiful Soup is going to stay with you forever to collect any kind of data from any website. T oday, Data play a critical role in every industry. And most of this data is coming from the internet. Most company, invest millions of dollars in one tech to gain users without having profited from that invested product return. The Internet is so vast that it contains more information about one topic than your nerdy professor. And the need & importance of extracting information from the web is becoming increasingly loud and clear. Most time, when we are adding any information in your facebook, twitter, LinkedIn and giving feedback on Yelp, this information is treated as Data. And this kind of data from the internet comes in many different ways example comment, restaurant feedback on Yelp, Twitter discussion, Reddit user discussion and Stock price etc. You can collect all this data, organize it and analyze it. That what we are going to talk about in this tutorial. There are several ways of extracting or importing data from the Internet. You can use APIs, to retrieve information from any major website. Thats what everybody is using these days to import data from the internet all primary site like Twitter, Twitch, Instagram, Facebook provides APIs to access their website dataset. And all this data available in a structured form. But most of the website doesnt provide an API. I think they dont want us to use their users data or they dont offer it because of lack of knowledge. So, In this topic, we are going to import data from the web without using any APIs. But before we processed, please have a look at our part 1 of this series because everything connected like dots. Something You dont know about data File if you just a Starter in Data Science, Import data File If you are new into Data Science field, then you must be working so hard to learn Data Science concept so fast. Now towardsdatascience.com What is Beautiful Soup Beautiful Soup is the best Library to scrap the data from a particular website or the Internet. And it is most comfortable to work on also. It parses and extracts structured data from HTML . Beautiful Soup automatically transforms incoming texts to Unicode and outgoing versions to UTF-8. You dont have to remember about encodings except the document doesnt define an encoding, and Beautiful Soup cant catch one. Then you have to mention the original encoding. Rules: To run your program, please use Jupyter python environment to run your program. Instead of running the whole program at once. We are just taking precaution, so your program doesnt break the website. Please check out the website term and conditions before you start pulling out data from there. Be sure you read the statement about the legal use of data. Basic-Getting Familiar with HTML HTML code plays an essential role in extracting data from the website. So, before we processed, let us jump to the basic of the HTML tags. If you have got a tiny bit of knowledge of HTML tags, you can move ahead to the next level. This is the basic syntax of an HTML webpage. Every <tag> serves a block inside the webpage: 1. <!DOCTYPE html> : HTML documents must start with a type declaration. 2. The HTML document is contained between <html> and </html> . 3. The meta and script declaration of the HTML document is between <head> and </head> . 4. The visible portion of the HTML document is between <body> and </body> tags. 5. Title headings are defined with the <h1> through <h6> tags. 6. Paragraphs are defined with the <p> tag. Other useful tags include <a> for hyperlinks, <table> for tables, <tr> for table rows, and <td> for table columns. Lets Check your HTML page List of Asian countries by area - Wikipedia needs additional citations for verification .improve this article by adding citations to reliable sources. Unsourced en.wikipedia.org Let us take a Wikipedia page to do the scrapping. If you have google chrome, then go to the page, first right-click on it and open your browser inspector to inspect the webpage. From the result you can see the table is inside the wiki table sortable and if you inspect it more you can find all of your table information there, its fantastic yeah!!!. Its going to be more amazing to see what you can do with beautiful soup. Lets Start Your DIY project Now we know about our data and where it is located. So, we are going to start scrapping our data. Before we process, You need to install or import some libraries. if you face any trouble in your installation, you can use sudo in front of every line. Requests  It is meant to be used by humans to communicate with the language. This suggests you dont have to manually join query strings to URLs, or form-encode your POST data. Requests will enable you to send HTTP/1.1 requests utilizing Python. In it, you can combine content like headers, form data, multipart files, and parameters by through simple Python libraries. It also enables you to obtain the response data of Python in the same way. BS4 BeautifulSoup  Beautiful Soup is a Python library for extracting data out of HTML and XML files. It works with your favourite parser to produce natural ways of operating, examining and transforming the parse tree. It usually saves programmers hours or days of work. We begin by studying the source code for a given web page and building a BeautifulSoup (soup)object with the BeautifulSoup function.Now, we need to use Beautiful Soap function which will help us parse and apply with the HTML we fetched from our Wikipedia page: Then we are going to use Beautiful Soup to parse the HTML data that we have collected in our URL variable, and we assign a different variable to store the data in Beautiful Soup format called Soup. To get a concept of the structure of the underlying HTML in our web page, use Beautiful Soups prettify function and check it. This is what we get from the prettify() function : If you visit this   link  and have a look to our Wikipedia page for the Asian countries, we can see there is little bit more information about the country areas. The wikipedia table already setup-which make our work more easy. Lets have a look for it in our prettify HTML: And there it is,Beginning with an HTML <table> tag with a class identifier of wikitable sortable. We will remember this class for future use. If you go down in your program, you will see how the table is made up, and you will have a look at the rows begin and finish with <tr> and </tr> tags. The first row of headers has <th> tags while the data rows underneath for every club has <td> tags. Using the <td> tags that we are going to tell Python to secure our data from. Before we go ahead, lets work out some Beautiful Soup functions to demonstrate how it captures and can deliver data to us from the HTML website. If we do the title function, Beautiful Soup will return the HTML tags for the heading and the content within them. We can use this information to start preparing our attack on the HTML . We know the data remains within an HTML table so firstly, we give Beautiful Soup off to retrieve all occurrences of the <table> tag within the page and add them to an array called all_tables. Under table class wikitable sortable we have links with country name as the title. Under table class wikitable sortable we have connections with country name as the title. Now, we are going to extract all the links within <a> , we used find_all() . From the URL, we have to extract the title which is the name of countries. To do that, we have to create a list Countries so that we can extract the name of countries from the link and add it to the list countries. Now, we have to convert the list countries into Pandas DataFrame to work in python. If you are interested in scrapping the data in high volume, you should consider using Scrapy , a powerful python scraping framework and also try to integrate your code with some publics APIs. The performance of data retrieval is significantly higher than scraping webpages. For example, take a look at Facebook Graph API , which can help you get hidden data which is not shown on Facebook webpages. Consider using a database backend like MySQL to collect your information when it gets too big. And that carries us to the end of our Beautiful Soup tutorial. Confidently, it provides you quite to get working on to examine some scraping out for your next project. Weve introduced request to fetch the URL and HTML data, Beautiful Soup to parse the HTML and Pandas to convert the data into a data frame for proper presentation. You can find this tutorial notebook here. If you have question, please feel free to ask. In next tutorial we are going to talk about the APIs. Feel Free to contact me on LinkedIn . References: 1. http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/ . 2. http://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/ . 3. https://github.com/stewync/Web-Scraping-Wiki-tables-using-BeautifulSoup-and-Python/blob/master/Scraping%2BWiki%2Btable%2Busing%2BPython%2Band%2BBeautifulSoup.ipynb 4. https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area 5. https://www.crummy.com/software/BeautifulSoup/ 450 3 450 450 3 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Andrew Donaldson Mar 31, 2019 Member-only Dont Fear the Robots Impressive as the tech here is, it is important to remember that the Boston Dynamics videos are trailers for a movie that isnt yet made. Every time one of these Boston Dynamics videos hit, they are met with a mix of amazement and predictions of doom at the hands of our new robot overlords. It is impressive. It is also very well choreographed and staged. Impressive as the tech here is, it is important 4 min read 4 min read Share your ideas with millions of readers. Jasmine Vasandani Mar 31, 2019 Member-only 10 Steps to Teaching Data Science Well A resource for data science instructors. With the increase in demand for data scientists, there has been a parallel growth in training programs within companies and educational institutions that teach people data science. In addition to focusing on meeting the job demand, there need to be more resources for instructors facilitating data science classrooms. Heres a 4 min read 4 min read Karan Bhanot Mar 31, 2019 Member-only Will your income be more than $50K/yr? Machine Learning can tell Machine learning is breaking grounds in numerous fields including Finance. What if we could use Machine Learning models to identify incomes of individuals? I found just the right dataset for this, called Census Income Dataset. I used the information in the dataset to predict if someone would earn an income 7 min read 7 min read Ane Berasategi Mar 31, 2019 Member-only Semantic search A brief post on semantics, search, and semantic search READ THE ORIGINAL POST IN MY BLOG Semantics is a branch of linguistics studying the meanings of words, their symbolic use, also including their multiple meanings. One morning I shot an elephant in my pajamas. How he got into my pajamas Ill never know. Groucho Marx This sentence is semantically ambiguous: its not clear if the author 5 min read 5 min read Sik-Ho Tsang Mar 31, 2019 Review: DPN Dual Path Networks (Image Classification) Better Than ResNet, DenseNet, PolyNet, ResNeXt, Winner of ILSVRC 2017 Object Localization Challenge In this story, DPN (Dual Path Network) is briefly reviewed. This is a work by National University of Singapore, Beijing Institute of Technology, National University of Defense Technology, and Qihoo 360 AI Institute. ResNet enables feature re-usage while DenseNet enables new features exploration. DPN picks the advantages from both ResNet 6 min read 6 min read Sahil Dhankhad Data Science and Machine Learning Researcher. LakeheadU. Learning something everyday about Data. ( www.linkedin.com/in/sahil-dhankhad-303350135 ) More from Medium Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Antonio Blago in ILLUMINATION Why You Should (not) Become a Data Analyst in 2023! Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 3185,\n",
       "  'url': 'https://towardsdatascience.com/concept-drift-and-model-decay-in-machine-learning-a98a809ea8d4',\n",
       "  'title': 'Concept Drift and Model Decay in Machine\\xa0Learning',\n",
       "  'subtitle': '-',\n",
       "  'claps': 455,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-25',\n",
       "  'clap_prop': 0.00022474873831985746,\n",
       "  'text': 'Towards Data Science Apr 25 2019 Listen Save Concept Drift Model Decay Machine Learning Concept drift drift label time essentially data lead divergence decision boundary new data model built earlier data/labels Scoring randomly sampled new data detect drift allowing u trigger expensive re-label/re-train task needed basis Nothing last forever even carefully constructed model mound well-labeled data predictive ability decay time Broadly two way model decay Due data drift due concept drift case data drift data evolves time potentially introducing previously unseen variety data new category required thereof impact previously labeled data case concept drift interpretation data change time even general distribution data doe cause end user interpret model prediction deteriorated time same/similar data fix requires re-labeling affected old data re-training model data concept simultaneously drift well vexing matter Rather like Lakers play Patriots World Series concept/game data/players change would lot head scratching stand post detection concept drift post-production scenario model trained older data place classifying new incoming data Continuous re-labeling old data retraining thereof prohibitively expensive want detect concept drift act needed ease illustration visualization stick 2-d i.e two feature two class divvy feature space approach detection generic drift detected model need updated detail depend application general focus post concept drift Data drift subject next post series look code snippet illustration full code reproducing reported result downloaded github Update 8/21/2020 Came across nice article list reference topic neptune.ai Shibsankar Das Best Practices Dealing Concept Drift 1 Concept Drift Concept drift arises interpretation data change time even data may agreed upon belonging class past claim belong class B understanding property B changed since pure concept drift Clearly need explanation context example piece text legitimately labelled belonging one class 1960 belonging different one 2019 prediction model built 1960 going largely error data 2019 See Figure 1 extreme example perhaps one serf illustrate problem rule game change 2 Shifting Decision Boundaries Concept drift seen morphing decision boundary time Consider simulated 2-class situation Figure 2 continual morphing data decision boundary crux concept drift way detect course make effort label least new data routine basis look degradation predictive ability model feel degradation longer tolerable need rebuild model data re-labeled per current understanding data 3 Simulating Concept Drift continue similar problem shown Figure 2 quantitative light trigger rebuild signal performance model decay threshold value problem statement 3.1 Generate data class B start 1000 data point Every new batch add 125 point class data decision boundary w different batch 3.2 Build SVM classifier build SVM model labeled data obtain f1-score test/sampled data using 3.3 Detect decay Twenty five randomly chosen data point 10 batch labeled compared prediction latest model hand f1-score sample fall threshold 0.925 trigger re-label/re-train task 3.4 Re-label data decision boundary rotates deviate farther farther model decision boundary model prediction continually get worse f1-score sampled data drop 0.925 re-label data per current data decision boundary simple geometry 2-features linear data/model decision boundary misclassified new data easy figure general re-label data 3.5 Re-train complete loop model need re-trained using updated label recover predictive ability 4 Results Figure 3 confirms expecting drifting several batch newly sampled data plotted along starting/final f1-score ha deteriorated 0.925 data decision boundary plot show amount new data would misclassified previously built model Re-labeling per final data decision boundary rebuilding model point recovers perfect f1-score data plotted rebuild show updated model decision boundary perfectly separate two class Figure 4 show model decay concept drift followed recovery upon re-label rebuild task total angle rotation proxy concept drift 5 Conclusions Concept drift driven model decay expensive fix given need manual re-labeling old data Drift detected via global measure worsening f1-scores sampled new data could complete re-label data labeling manual rather formula driven likely possible real situation however Even rough identification old data need re-labeled could help lowering manual work Something requires work move data drift associated issue next post Originally published http //xplordat.com April 25 2019 536 2 536 536 2 Towards Data Science home data science Medium publication sharing concept idea code Ren Bremer Apr 25 2019 use Python feature data analytics project Python tutorial Azure using OO NumPy panda SQL PySpark 1 Introduction lot company moving cloud consider tooling shall used data analytics On-premises company mostly use propriety software advanced analytics BI reporting However tooling may logical choice cloud environment 9 min read 9 min read Share idea million reader Ryan Burge Apr 25 2019 Member-only Religious Composition Two Major Parties make lot data visualization time pretty good idea result calculation going look like write code fact result obvious dont even post anywhere 4 min read 4 min read Ashutosh Singh Apr 25 2019 Member-only Classifying Products Banned Approved using Text Mining- Part II part explain optimize existing Machine Learning model Part deployment ML model using Flask previous article series discussed business problem shown train model using fastText classification Banned Approved product based information like Product Name product description specification 5 min read 5 min read Favio Vzquez Apr 25 2019 Jungle Koalas Pandas Optimus Spark expect newest library Databricks Koalas Optimus framework Apache Spark 3.x excited data science probably know Spark+AI latest summit started yesterday April 24th 2019 great thing talk spin-off youve following co-created 8 min read 8 min read Rinu Gour Apr 25 2019 Member-only Complete Guide Learn R R Programming Technology open source programming language Also R programming language latest cutting-edge tool R Basics hottest trend Moreover R command line interface C.L.I consists prompt usually character History R John Chambers colleague developed R Bell Laboratories Basically 8 min read 8 min read Ashok Chilakapati Medium Samuele Mazzanti Towards Data Science Using Causal ML Instead A/B Testing Anil Tilbe Towards AI Bayesian Inference Best 5 Models 10 Best Practices Machine Learning Freedom Preetham Statistical Sauce Confidence Tolerance Prediction Intervals Statistical Forecasts Moez Ali Top AutoML Python library 2022 Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 25, 2019 Listen Save Concept Drift and Model Decay in Machine Learning Concept drift is a drift of labels with time for the essentially the same data. It leads to the divergence of decision boundary for new data from that of a model built from earlier data/labels. Scoring randomly sampled new data can detect the drift allowing us to trigger the expensive re-label/re-train tasks on an as needed basis Nothing lasts forever. Not even a carefully constructed model on mounds of well-labeled data. Its predictive ability decays over time. Broadly, there are two ways a model can decay. Due to data drift or due to concept drift. In case of data drift, data evolves with time potentially introducing previously unseen variety of data and new categories required thereof. But there is no impact to previously labeled data. In case of concept drift our interpretation of the data changes with time even while the general distribution of the data does not. This causes the end user to interpret the model predictions as having deteriorated over time for the same/similar data. The fix here requires a re-labeling of the affected old data and re-training the model. Both data and concept can simultaneously drift as well, further vexing the matters Rather like having the Lakers play the Patriots in the World Series, when both the concept/game and the data/players change there would be a lot of head scratching in the stands This post is about the detection of concept drift in a post-production scenario where a model trained on older data is in place for classifying new incoming data. Continuous re-labeling of old data and retraining thereof is prohibitively expensive so we want to detect concept drift and act on it as needed. For ease of illustration and visualization we stick to 2-d (i.e. two features) and two classes that divvy up the feature space. But the approach to detection is generic. Once the drift is detected, the model needs to be updated. The details will depend on the application but in general: The focus of this post is concept drift. Data drift is the subject of the next post in this series. We look at some code snippets for illustration but the full code for reproducing the reported results here can be downloaded from github . Update (8/21/2020) : Came across a nice article (and with a list of references) on this topic at neptune.ai by Shibsankar Das. Best Practices for Dealing with Concept Drift . 1. Concept Drift Concept drift arises when our interpretation of the data changes over time even while the data may not have. What we agreed upon as belonging to class A in the past, we claim now that it should belong to class B, as our understanding of the properties of A and B have changed since. This is pure concept drift. Clearly needs further explanation and context. For example a piece of text can legitimately be labelled as belonging to one class in 1960 but belonging to a different one in 2019. So the predictions from model built in 1960 are going to be largely in error for the same data in 2019. See Figure 1 below for an extreme example perhaps but one that serves to illustrate the problem when the rules of the game change. 2. Shifting Decision Boundaries Concept drift can be seen as the morphing of decision boundaries over time. Consider a simulated 2-class situation in Figure 2. So the continual morphing of the data decision boundary is at the crux of concept drift. The only way to detect it then of course is to make the effort to label at least some of the new data on a routine basis and look for degradation in the predictive ability of the model. When we feel that the degradation is no longer tolerable we will need to rebuild the model with the data re-labeled as per the current understanding of the data. 3. Simulating Concept Drift We continue with a similar problem as shown in Figure 2 but in a quantitative light so as to trigger a rebuild! signal when the performance of the model decays to a threshold value. Here is the problem statement. 3.1 Generate data The classes A and B each will start off with 1000 data points. Every new batch adds 125 points to each class. The data decision boundary ( w ) is different for each batch. 3.2 Build an SVM classifier We build an SVM model on labeled data and obtain the f1-score for test/sampled data using the same. 3.3 Detect decay Twenty five randomly chosen data points (10% of the batch) are labeled and compared with predictions from the latest model at hand. When the f1-score of the sample falls below a threshold (0.925 here) we trigger a re-label/re-train task. 3.4 Re-label As the data decision boundary rotates and deviates farther and farther from the models decision boundary, the model predictions continually get worse. When the f1-score for the sampled data drops to about 0.925 we re-label all the data as per the current data decision boundary. Now, in our simple geometry with 2-features and linear data/model decision boundaries the misclassified new data is easy to figure out. But in general it will not be so we re-label all the data. 3.5 Re-train to complete the loop The model needs to be re-trained using the updated labels so as to recover its predictive ability. 4. Results Figure 3 below confirms what we have been expecting. After drifting over several batches, the newly sampled data is plotted along with the starting/final (by when the f1-score has deteriorated to 0.925) data decision boundaries. These plots show the amount of new data that would be misclassified by the previously built model. Re-labeling as per the final data decision boundary and rebuilding the model at that point recovers the perfect f1-score. All the data is plotted after a rebuild to show that the updated models decision boundary perfectly separates the two classes. Figure 4 below shows the model decay as the concepts drift followed by a recovery upon re-label & rebuild tasks. The total angle of rotation is the proxy for concept drift. 5. Conclusions Concept drift driven model decay can be expensive to fix given the need for manual re-labeling of old data. Drift can be detected via global measures such as worsening f1-scores for sampled new data. We could do a complete re-label of all data here as our labeling is not manual but rather formula driven. It will likely not be possible in a real situation however. Even a rough identification of the old data that needs to be re-labeled could help with lowering the manual work. Something that requires further work. We will move on to data drift and the associated issues in the next post. Originally published at  http://xplordat.com  on April 25, 2019. 536 2 536 536 2 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ren Bremer Apr 25, 2019 How to use Python features in your data analytics project Python tutorial in Azure using OO, NumPy, pandas, SQL, PySpark 1. Introduction A lot of companies are moving to cloud and consider what tooling shall be used for data analytics. On-premises, companies mostly use propriety software for advanced analytics, BI and reporting. However, this tooling may not be the most logical choice in a cloud environment. 9 min read 9 min read Share your ideas with millions of readers. Ryan Burge Apr 25, 2019 Member-only The Religious Composition of the Two Major Parties I make a lot of data visualizations. Most of the time I have a pretty good idea of what the results of the calculations are going to look like before I write the code. In fact, some of the results are so obvious that I dont even post them anywhere 4 min read 4 min read Ashutosh Singh Apr 25, 2019 Member-only Classifying Products as Banned Or Approved using Text Mining- Part II In this part, we will explain how to optimize the existing Machine Learning model in Part I and the deployment of this ML model using Flask. In the previous article of this series, We have discussed the business problem, shown how to train the model using fastText and classification of Banned or Approved products based on information like(Product Name, product description, and specifications). 5 min read 5 min read Favio Vzquez Apr 25, 2019 The Jungle of Koalas, Pandas, Optimus and Spark What to expect from the newest library from Databricks (Koalas), the Optimus framework and Apache Spark 3.x If you are as excited about data science as me, you probably know that the Spark+AI latest summit started yesterday (April 24th 2019). And there are great things to talk about. But I will do it with a spin-off. If youve been following me you now that I co-created a 8 min read 8 min read Rinu Gour Apr 25, 2019 Member-only A Complete Guide to Learn R R Programming Technology is an open source programming language. Also, the R programming language is the latest cutting-edge tool. R Basics is the hottest trend. Moreover, the R command line interface (C.L.I) consists of a prompt, usually the > character. History of R John Chambers and colleagues developed R at Bell Laboratories. Basically 8 min read 8 min read Ashok Chilakapati More from Medium Samuele Mazzanti in Towards Data Science Using Causal ML Instead of A/B Testing Anil Tilbe in Towards AI Bayesian Inference: The Best 5 Models and 10 Best Practices for Machine Learning Freedom Preetham in Statistical Sauce Confidence, Tolerance, and Prediction Intervals for Statistical Forecasts. Moez Ali Top AutoML Python libraries in 2022 Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 711,\n",
       "  'url': 'https://towardsdatascience.com/4-steps-to-break-into-data-science-in-2020-4750418c726c',\n",
       "  'title': '4 Steps to Break Into Data Science in\\xa02020',\n",
       "  'subtitle': 'Go from 0 to Junior Data Scientist in 1\\xa0year',\n",
       "  'claps': 398,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-12-30',\n",
       "  'clap_prop': 0.0001965934018709962,\n",
       "  'text': 'Towards Data Science Dec 30 2019 Member-only Listen Save 4 Steps Break Data Science 2020 2020 almost mean time year take piece paper make list goal want accomplish next year Nothing wrong probably know easy make exhaustive list near-impossible time-consuming goal make feel overwhelmed likely motivated much youre planning enroll data science next year Id say youve made great decision field widely accepted job everywhere salary great even management slowly figuring data science needed start let slightly demotivate yes necessary one year isnt enough learn entire field Dont get wrong 1 year enough land first job chance wont go 0 data science team lead year manage please share story comment section said let explore skill youll need learn enough get started 1 Brushing Math skill Youve likely heard harsh math prerequisite data science amount math youll need know vary much depending job role general answer much math need get started would say le think reasoning follow tempting dive deep every somewhat related field like calculus linear algebra probability statistic need know time stop Dont get wrong time world guest become expert above-mentioned field otherwise make sure youre wasting time break field junior-level data scientist youll need know math intuitive level Youll need know certain situation thats intuition part come wouldnt spend much time solving complex math task hand youre good intuitive level know code thats enough Theres plenty time get deeper math get job need learn everything beforehand dont advanced math degree already wouldnt suggest spend 23 month brushing math skill 2 Programming Yes coding skill essential data science get job industry coding skill lacking likely know need wont know also likely youll suffer SOCPS Stack Overflow Copy Paste Syndrome possibly even without reading question answer Theres nothing wrong looking elegant solution online know write basic solution youve never written line code start small read book Python R role data science get complete picture dive deeper syntax Dont worry memorizing everything make sure know look get stuck youve already read book finished course programming know syntax dont know approach problem spend time learning algorithm data structure Also go common coding interview question get creative juice flowing piss Youre satisfied programming skill Thats awesome spend time analysis library like Numpy Pandas much time youll spend coding also vary much wont complete beginner one need library knowledge Id say 34 month enough complete beginner around 1 month youre learning analysis library 3 Databases highly likely data youre analyzing come sort database Thats typical work environment get different book online course wont get nicely formatted CSV file often youll need domain knowledge someone domain knowledge also good amount SQL knowledge youll analysis programming language like Python R dont spend much time learning SQL analytic function PLSQL/T-SQL advanced stuff SQL work case rely mostly joining couple table perform analysis much time youll spend depends way youll use prior knowledge starter dont spend month 4 let learn Data Science youve followed step dont prior knowledge probably August September 2020 lot time ha passed prerequisite needed land first job Well precise Youre looking job data science weve covering prerequisite far would suggest next 2 month get comfortable basic data analysis visualization library like havent already probably since learning prerequisite without clear indication need boring Dont go tutorial download datasets web perform solid analysis go online see others done dataset see improve 2 month period also get acquainted machine learning algorithm like Maybe wont use practice provide base learning advanced algorithm like XGBoost Neural network later Like analysis library make sure follow tutorial tutorial good quality work feel like try implementing algorithm scratch Numpy thats mandatory Whats next couple month left 2020 create GitHub account upload 35 finest analysis/ML piece potential employer see Also make nice looking resume cover letter really feel like document learning journey form online blog Online presence help career development dont publish nonsense content daily basis trust judgment thats start sending resume company want work nothing else sincerely hope 2020 year Go crush Loved article Become Medium member continue learning without limit Ill receive portion membership fee use following link extra cost Join Medium referral link Dario Radei Medium member portion membership fee go writer read get full access every story medium.com 413 1 413 413 1 Subscribe Never miss thing Data Science News pocket Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Boris Shabash Dec 30 2019 Member-only Reasoning Probability Pyro Model Good Enough core probabilistic programming designed answer question uncertainty popular example online discus use probabilistic programming aid neural network making capable dealing novel example havent trained However tutorial going 5 min read 5 min read Share idea million reader John Daniel Dec 30 2019 Support Team SVM Topic Overview Understanding Support Vector Machine 2020 Support Vector Machine SVM powerful versatile Machine Learning model SVM used classification regression problem outlier detection one popular model Machine Learning Data Scientist interested Machine Learning toolbox 6 min read 6 min read Rebecca Weng Dec 30 2019 Member-only Cryptography Crash Course Intimidated Conceptual overview suggestion reading/watching last post talked importance mindful handle data ha access protect information law changing isnt covered academia vs. industry post decided give quick crash 7 min read 7 min read Jackson Gilkey Dec 30 2019 Member-only Creating Graphs Python using Networkx intro building first Graph Python youre interested Graph Theory analysis Python wondering get started blog Well start presenting key concept implementing Python using handy Networkx Package Graph Theory Terminology Graph G V E data structure 4 min read 4 min read Laura Uzcategui Dec 30 2019 Journey Machine Learning AI Lets walk together journey transition Software Engineering Machine Learning first blog Machine Learning ML journey Ive quite interested thing data beginning career Software Engineer interested following baby step journey ML fundamental 5 min read 5 min read Dario Radei Data Scientist Tech Writer betterdatascience.com Medium Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Anna Wu Google Data Scientist Interview Questions Step-by-Step Solutions Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Dec 30, 2019 Member-only Listen Save 4 Steps to Break Into Data Science in 2020 2020 is almost here, which means its that time of the year when you take a piece of paper and make a list of goals you want to accomplish in the next year. Nothing wrong with that, but you probably know that its very easy to make an exhaustive list of near-impossible, time-consuming goals that will only make you feel overwhelmed, and very likely not motivated because theres so much to do. If youre planning to enroll in data science in the next year, Id say youve made a great decision. The field is widely accepted, there are jobs everywhere, salaries are great, and even the management is slowly figuring out why data science is needed. But before we start, let me to slightly demotivate you ( yes, its necessary ) one year isnt enough to learn the entire field. Dont get me wrong, 1 year is enough for you to land your first job, but the chances are you wont go from 0 to data science team lead in a year ( if you manage to do so please share your story in the comment section ). With that being said, lets explore all the skills youll need and how to learn just enough of them to get you started. 1. Brushing up the Math skills Youve most likely heard of harsh math prerequisites of data science. The amount of math youll need to know will vary much depending on the job role, but as a general answer to how much math you will need to get started, I would say: less than you think . The reasoning will follow. Its tempting to dive deep into every somewhat related field like calculus, linear algebra, probability, or statistics but you need to know when its time to stop . Dont get me wrong, if you have all of the time in the world be my guest, become an expert in the above-mentioned fields, but otherwise, make sure youre not wasting your time. To break into the field as a junior-level data scientist youll need to know math, but more on the intuitive level . Youll need to know what to do in certain situations thats where the intuition part comes in but I wouldnt spend much time solving complex math tasks by hand. If youre good on the intuitive level and know how to code thats enough. Theres plenty of time to get deeper into math after you get a job no need to learn everything beforehand . If you dont have an advanced math degree already I wouldnt suggest you spend more than 23 months brushing up the math skills. 2. What about Programming? Yes, coding skills are essential to data science. If you get a job in the industry and your coding skills are lacking, most likely you will know what you need to do, but you wont know how to do it . Its also likely youll suffer from SOCPS ( Stack Overflow Copy Paste Syndrome ), possibly even without reading the questions and answers. Theres nothing wrong with looking for more elegant solutions online, but you should know how to write a basic solution by yourself. If youve never written a line of code before, start small, read a book on Python or R and their role in data science ( to get a complete picture ). Then dive deeper into syntax. Dont worry about memorizing everything, just make sure to know where to look when you get stuck. If youve already read a book or finished a course on programming and you know the syntax, but dont know how to approach the problem, spend some time learning algorithms and data structures . Also go through most common coding interview questions, as those will get your creative juices flowing ( or piss you off ). Youre satisfied with your programming skills? Thats awesome! Now spend some time with analysis libraries like Numpy and Pandas. How much time youll spend on coding will also vary much. It wont be the same for complete beginners, or the ones who just need library knowledge. Id say 34 months will be enough for complete beginners, and around 1 month if youre learning the analysis libraries only. 3. Databases? Its highly likely that data youre analyzing will come from some sort of a database. Thats where a typical work environment gets different from books or online courses you wont get a nicely formatted CSV file . More often than not youll need a domain knowledge ( or someone with domain knowledge ), and also a good amount of SQL knowledge. If youll be doing analysis in programming languages like Python or R, then dont spend too much time learning SQL analytic functions, PLSQL/T-SQL and all of that more advanced stuff. Your SQL work, in this case, will rely mostly on joining a couple of tables on which you can perform the analysis. How much time youll spend here depends on the way youll use them and on the prior knowledge, but for starters dont spend more than a month here. 4. Now lets learn some Data Science If youve followed each step from above and you dont have some prior knowledge than its probably August or September of 2020. A lot of time has passed by, but you have all the prerequisites needed to land your first job. Well, not all to be precise. Youre looking for a job in data science, and weve only been covering prerequisites so far. I would suggest that for the next 2 months you get comfortable with the basic data analysis and visualization libraries, like: That is if you havent already ( you probably have since learning the prerequisites without a clear indication of why you need them can be boring ). Dont just go over tutorials, download some datasets from the web and perform a solid analysis . Then go online and see what others have done on the same dataset to see where you can improve. In the same 2 month period you should also get acquainted with some of machine learning algorithms, like: Maybe you wont use some of them in practice, but they will provide you with a base for learning more advanced algorithms like XGBoost and Neural networks later on. Like with the analysis libraries, make sure to not follow tutorial by tutorial, but to do good quality work by yourself. If you feel like it, try implementing the algorithms from scratch in Numpy but thats not mandatory. Whats next? With only a couple of months left in 2020, create a GitHub account a there upload 35 of your finest analysis/ML pieces for potential employers to see. Also, make a nice looking resume and cover letter. If you really feel like it, document your learning journey in the form of an online blog. Online presence can only help you in career development, that is if you dont publish nonsense content on a daily basis but I trust your judgment. And thats it, start sending your resume to the companies you want to work for theres nothing else you can do . I sincerely hope 2020 will be your year. Go crush it. Loved the article? Become a  Medium member  to continue learning without limits. Ill receive a portion of your membership fee if you use the following link, with no extra cost to you. Join Medium with my referral link - Dario Radei As a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story medium.com 413 1 413 413 1 Subscribe - Never miss a thing. Data Science News in your pocket. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Boris Shabash Dec 30, 2019 Member-only Reasoning With Probability and Pyro Is My Model Good Enough? At its core, probabilistic programming is designed to answer questions about uncertainty. Some very popular examples online discuss the use of probabilistic programming to aid with neural networks and making them more capable of dealing with novel examples they havent been trained on. However, in this tutorial I am going 5 min read 5 min read Share your ideas with millions of readers. John Daniel Dec 30, 2019 The Support Team SVM Topic Overview: Understanding Support Vector Machine 2020 A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model. SVM can be used for classification or regression problem and outlier detection. It is one of the most popular models in Machine Learning that any Data Scientist interested in Machine Learning should have in their toolbox 6 min read 6 min read Rebecca Weng Dec 30, 2019 Member-only Cryptography Crash Course for the Intimidated Conceptual overview and suggestions for reading/watching In my last post, I talked about the importance of being mindful when you handle data. Who has access? How can you protect the information? How are some laws changing? What is and isnt covered in academia vs. industry. For this post, I decided to give myself a quick crash 7 min read 7 min read Jackson Gilkey Dec 30, 2019 Member-only Creating Graphs in Python using Networkx An intro to building your first Graph in Python If youre interested in doing Graph Theory analysis in Python and wondering where to get started then this is the blog for you. Well start by presenting a few key concepts and then implementing them in Python using the handy Networkx Package. Some Graph Theory Terminology A Graph G(V, E) is a data structure 4 min read 4 min read Laura Uzcategui Dec 30, 2019 On the Journey to Machine Learning / AI. Lets walk together through the journey to transition between Software Engineering and Machine Learning This is my first blog on Machine Learning (ML) and my journey through it. Ive been quite interested in all things data from the very beginning of my career as a Software Engineer. If you are interested in following baby steps on the journey to ML, its fundamentals and how 5 min read 5 min read Dario Radei Data Scientist & Tech Writer | betterdatascience.com More from Medium Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Anna Wu Google Data Scientist Interview Questions (Step-by-Step Solutions!) Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5078,\n",
       "  'url': 'https://towardsdatascience.com/kedro-prepare-to-pimp-your-pipeline-f8f68c263466',\n",
       "  'title': 'Kedro: Prepare to Pimp your\\xa0Pipeline',\n",
       "  'subtitle': 'A new Python library for production-ready data pipelines',\n",
       "  'claps': 357,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 10,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-06-04',\n",
       "  'clap_prop': 0.00017634131775865738,\n",
       "  'text': 'Towards Data Science Jun 4 2019 Listen Save Kedro New Tool Data Science new Python library production-ready data pipeline post introduce Kedro new open source tool data scientist data engineer brief description likely become standard part every professional toolchain describe use tutorial able complete fifteen minute Strap spaceflight future Suppose data scientist working senior executive make key financial decision company asks provide ad-hoc analysis thanks delivering useful insight planning Great Three month line newly-promoted executive CEO asks re-run analysis next planning meetingand code broken youve overwritten key section file remember exact environment used time maybe code OK one big Jupyter notebook file path hard coded meaning go laboriously check change one new data input great everyday principle software engineering unfamiliar data scientist extensive background programming fact many data scientist self-taught learning program part research project necessary job maybe data scientist argue code doe need high standard arent working production system code feed business decision process considered production code Data scientist may consider primary output work code doesnt mean code write shouldnt follow standard expected software engineering team fact following minimum characteristic detail principle take look useful blog post Thomas Huijskens QuantumBlack Kedro Kedro development workflow tool allows create portable data pipeline applies software engineering best practice data science code reproducible modular well-documented use Kedro worry le write production-ready code Kedro doe heavy lifting youll standardise way team collaborates allowing work efficiently Many data scientist need perform routine task data cleaning processing compilation may favourite activity form large percentage day day task Kedro make easier build data pipeline automate heavy lifting reduce amount time spent kind task Kedro ha created QuantumBlack advanced analytics firm wa acquired consultancy giant McKinsey Company 2015 QuantumBlack used Kedro 60+ project McKinsey decided open source goal enable staff client third-party developer use Kedro build upon extending open tool need data professional able complete regular task effectively potentially share enhancement community Kedro workflow scheduler Kedro workflow scheduler like Airflow Luigi Kedro make easy prototype data pipeline Airflow Luigi complementary framework great managing deployment scheduling monitoring alerting Kedro pipeline like machine build car part Airflow Luigi tell different machine switch order work together produce car QuantumBlack built Kedro-Airflow plugin providing faster prototyping time reducing barrier entry associated moving pipeline workflow scheduler need know Kedros documentation ha designed beginner get started creating Kedro project Python 3.5+ basic knowledge Python might find learning curve challenging bear follow documentation guidance fifteen minute spaceflight Kedro easy-to-follow fifteen minute tutorial based following scenario 2160 space tourism industry booming Globally thousand space shuttle company taking tourist moon back able source three fictional datasets amenity offered space shuttle customer review company information want train linear regression model us datasets predict price shuttle hire However get train model need prepare data data engineering process preparing data model building creating master table Getting set first step pip install kedro new Python 3.5+ virtual environment recommend conda confirm correctly installed typing see Kedro graphic shown additionally give version Kedro using wrote tutorial based version 0.14.0 May 2019 since active open source project may time change codebase cause minor change example project endeavour keep example code date although blog post consult Kedro documentation release note hit upon inconsistency Kedro workflow building Kedro project typically follow standard development workflow shown diagram tutorial walk step Set project template keep thing simple QuantumBlack team provided code need simply need clone example project Github Repo need set project template tutorial code Spaceflights project make sure necessary dependency please run following Python 3.5+ virtual environment Set data Kedro us configuration file make project code reproducible across different environment may need reference datasets different location set data Kedro project typically add datasets data folder configure registry data source manages loading saving data tutorial make use three datasets spaceflight company shuttling customer moon back us two data format .csv .xlsx file stored data/01_raw/ folder project directory work datasets provided Kedro project conf/base/catalog.yml file act registry datasets use Registering dataset simple adding named entry .yml file include file location path parameter given dataset type data versioning Kedro come support type data csv look catalog.yml file Spaceflights tutorial see following entry datasets used check whether Kedro load data correctly inspect first five row data open terminal window start IPython session Kedro project directory type exit want end session continue tutorial Create run pipeline next part workflow create pipeline set node Python function perform distinct individual task typical project stage project comprises three step Data engineering pipeline reviewed raw datasets Spaceflights project time consider data engineering pipeline process data prepares model within data science pipeline pipeline preprocesses two datasets merges third master table file data_engineering.py inside node folder youll find preprocess_companies preprocess_shuttles function specified node within pipeline pipeline.py function take dataframe output pre-processed data preprocessed_companies preprocessed_shuttles respectively .. Kedro run data engineering pipeline determines whether datasets registered data catalog conf/base/catalog.yml dataset registered persisted automatically path specified without need specify code function dataset registered Kedro store memory pipeline run remove afterwards tutorial preprocessed data registered conf/base/catalog.yml CSVLocalDataSet chosen simplicity choose available dataset implementation class save data example database table cloud storage like S3 Azure Blob Store etc others data_engineering.py file also includes function create_master_table data engineering pipeline us join together company shuttle review dataframes single master table Data science pipeline data science pipeline build model us datasets comprised price prediction model us simple LinearRegression implementation scikit-learn library code found file src/kedro_tutorial/nodes/price_prediction.py test size random state parameter prediction model specified conf/base/parameters.yml Kedro feed catalog pipeline executed Combining pipeline project pipeline specified pipeline.py de_pipeline preprocess data use create_master_table combine preprocessed_shuttles preprocessed_companies review master_table dataset ds_pipeline create feature train evaluate model first node ds_pipeline output 4 object X_train X_test y_train y_test used train model final node evaluate model two pipeline merged together de_pipeline ds_pipeline order add pipeline together significant since Kedro automatically detects correct execution order node overall pipeline would result specified ds_pipeline de_pipeline pipeline executed invoke kedro run see output similar following problem getting tutorial code running check working Python 3.5 environment dependency installed project still problem head Stack Overflow guidance community behaviour youre observing appears problem Kedro please feel raise issue Kedros Github repository Kedro runner two different way run Kedro pipeline specify default Kedro us SequentialRunner invoke kedro run Switching use ParallelRunner simple providing additional flag follows Thats end tutorial run basic Kedro Theres plenty documentation various configuration option optimisation use project example Contributions welcome start working Kedro want contribute example change Kedro project would welcome chance work Please consult contribution guide Acknowledgements Spaceflights example based tutorial written Kedro team QuantumBlack Labs Yetunde Dada Ivan Danov Dmitrii Deriabin Lorena Balan Gordon Wrigley Kiyohito Kunii Nasef Khan Richard Westenra Nikolaos Tsaousis kindly given early access code documentation order produce tutorial TowardsDataScience Thanks 441 3 441 441 3 Towards Data Science home data science Medium publication sharing concept idea code Shuvashish Chatterjee Jun 4 2019 Alexa find parking AI guide vacant parking slot Detecting parking lot occupancy security cam footage using deep learning Finding vacant spot parking lot tough ask even difficult manage lot incoming traffic varies lot slot vacant instant time need slot commuter finding difficult reach particular slot 8 min read 8 min read Share idea million reader Emanuele Fabbiani Jun 4 2019 Member-only Lessons real Machine Learning project part 2 trap data exploration fall pitfall data exploration get away second story series Im going brutally shrink intro Im writing share real enterprise-level Machine Learning project taught team curious know feel free check first chapter Jupyter 5 min read 5 min read Aditya Mandal Jun 4 2019 Evolution Machine Translation 1949 Warren Weaver researcher Rockefeller Foundation presented set proposal machine based translation based information theory success code breaking Second World War year machine translation research began earnest many US university described 10 min read 10 min read Ziad SALLOUM Jun 4 2019 Member-only Eligibility Traces Reinforcement Learning Sometimes looking backward isnt bad Update best way learning practicing Reinforcement Learning going http //rl-lab.com Eligibility Traces short straight forward manner Eligibility Traces kind mathematical trick improves performance Temporal Difference method Reinforcement Learning benefit Eligibility Traces 7 min read 7 min read Adam King Jun 4 2019 Member-only Optimizing deep learning trading bot using state-of-the-art technique Lets teach deep RL agent make even money using feature engineering Bayesian optimization last article used deep reinforcement learning create Bitcoin trading bot dont lose money Although agent profitable result werent impressive time going step notch massively improve model profitability reminder 17 min read 17 min read Jo Stichbury Rnin technology writer podcast host Cat herder Dereferences NULL Medium Khuyen Tran Prefect Blog Orchestrate Data Science Project Prefect 2.0 Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Naga Sanjay Continuous Training ML model Madison Hunter Towards Data Science Write Good Code Documentation Data Scientists Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Jun 4, 2019 Listen Save Kedro: A New Tool For Data Science A new Python library for production-ready data pipelines In this post, I will introduce  Kedro  , a new open source tool for data scientists and data engineers. After a brief description of what it is and why it is likely to become a standard part of every professionals toolchain, I will describe how to use it in a tutorial that you should be able to complete in fifteen minutes. Strap in for a spaceflight to the future! Suppose you are a data scientist working for a senior executive who makes key financial decisions for your company. She asks you to provide an ad-hoc analysis, and when you do, she thanks you for delivering useful insights for her planning. Great! Three months down the line, the newly-promoted executive, now your CEO, asks you to re-run the analysis for the next planning meetingand you cannot. The code is broken because youve overwritten some of the key sections of the file, and you cannot remember the exact environment you used at the time. Or maybe the code is OK, but its in one big Jupyter notebook with all the file paths hard coded, meaning that you have to go through laboriously to check and change each one for the new data inputs. Not so great! Some everyday principles of software engineering are unfamiliar to data scientists who do not all have an extensive background in programming. In fact, many data scientists are self-taught, learning to program as part of a research project or when necessary in their jobs. And, maybe, a few data scientists will argue that their code does not need to be of a high standard because they arent working on a production system. Any code that feeds some business decision process should be considered as production code Data scientists may not consider the primary output of their work to be code, but this doesnt mean that the code they write shouldnt follow the standards expected by a software engineering team. In fact, it should have the following, minimum, characteristics: For more detail on these principles, take a look at a useful blog post by Thomas Huijskens from QuantumBlack . What is Kedro? Kedro is a development workflow tool that allows you to create portable data pipelines. It applies software engineering best practices to your data science code so it is reproducible, modular and well-documented. If you use Kedro you can worry less about how to write production-ready code (Kedro does the heavy lifting for you) and youll standardise the way your team collaborates, allowing you all to work more efficiently. Many data scientists need to perform the routine tasks of data cleaning, processing and compilation that may not be their favourite activities but form a large percentage of their day to day tasks. Kedro makes it easier to build a data pipeline to automate the heavy lifting and reduce the amount of time spent on this kind of task. Kedro has been created by QuantumBlack , an advanced analytics firm, that was acquired by the consultancy giant, McKinsey & Company, in 2015. QuantumBlack have used Kedro on more than 60+ projects with McKinsey and have now decided to open source it. Their goal is to enable staff, clients and third-party developers to use Kedro and build upon it. By extending this open tool for their own needs, data professionals are able to complete their regular tasks effectively and potentially share their enhancements to the community. Is Kedro a workflow scheduler? Kedro is not a workflow scheduler like Airflow and Luigi. Kedro makes it easy to prototype your data pipeline, while Airflow and Luigi are complementary frameworks that are great at managing deployment, scheduling, monitoring and alerting. A Kedro pipeline is like a machine that builds a car part. Airflow or Luigi tell different machines to switch on or off in order to work together and produce a car. QuantumBlack have built a Kedro-Airflow plugin, providing faster prototyping time and reducing the barriers to entry associated with moving pipelines to both workflow schedulers. What do I need to know? Kedros documentation has been designed for beginners to get started creating their own Kedro projects (in Python 3.5+). If you have just the very basic knowledge of Python then you might find the learning curve more challenging but bear with it and follow the documentation for guidance. A fifteen minute spaceflight with Kedro My easy-to-follow fifteen minute tutorial will be based on the following scenario: It is 2160 and the space tourism industry is booming. Globally, there are thousands of space shuttle companies taking tourists to the moon and back. You have been able to source three (fictional) datasets about the amenities offered in each space shuttle, customer reviews and company information. You want to train a linear regression model that uses the datasets to predict the price of shuttle hire. However, before you get to train the model, you need to prepare the data by doing some data engineering, which is the process of preparing data for model building by creating a master table. Getting set up The first step, is to pip install kedro into a new Python 3.5+ virtual environment (we recommend conda ), and confirm that it is correctly installed by typing You should see the Kedro graphic as shown below: This will additionally give you the version of Kedro that you are using. I wrote this tutorial based on version 0.14.0 in May 2019 and since this is an active and open source project, there may, over time, be changes to the codebase which cause minor changes to the example project. We will endeavour to keep the example code up to date, although not this blog post, so you should consult Kedro documentation and release notes if you hit upon any inconsistencies. Kedro workflow When building a Kedro project, you will typically follow a standard development workflow as shown in the diagram below. In this tutorial, I will walk through each of the steps. Set up the project template To keep things simple, the QuantumBlack team have provided all the code you need, so you simply need to clone the example project from its Github Repo . You do not need to set up a project template in this tutorial. Once you have the code for the Spaceflights project, to make sure you have the necessary dependencies for it, please run the following in a Python 3.5+ virtual environment: Set up the data Kedro uses configuration files to make a projects code reproducible across different environments, when it may need to reference datasets in different locations. To set up data for a Kedro project, you will typically add datasets to the data folder and configure the registry of data sources that manages the loading and saving of data This tutorial makes use of three datasets for spaceflight companies shuttling customers to the moon and back, and uses two data formats: .csv and .xlsx. The files are stored in the data/01_raw/ folder of the project directory. To work with the datasets provided, all Kedro projects have a conf/base/catalog.yml file which acts as a registry of the datasets in use. Registering a dataset is as simple as adding a named entry into the .yml file to include file location (path), parameters for the given dataset, type of data and versioning. Kedro comes with support for a few types of data, such as csv, and if you look at the catalog.yml file for the Spaceflights tutorial, you will see the following entries for the datasets used: To check whether Kedro loads the data correctly, and to inspect the first five rows of data, open a terminal window and start an IPython session in the Kedro project directory (and just type ( exit() when you want to end the session and continue with the tutorial): Create and run the pipeline The next part of the workflow is to create a pipeline from a set of nodes, which are Python functions that perform distinct, individual tasks. In a typical project, this stage of the project comprises of three steps: Data engineering pipeline We have reviewed the raw datasets for the Spaceflights project, and it is now time to consider the data engineering pipeline that processes the data and prepares it for the model within the data science pipeline. This pipeline preprocesses two datasets and merges them with a third into a master table. In the file data_engineering.py inside the nodes folder youll find the preprocess_companies and preprocess_shuttles functions (specified as nodes within the pipeline in pipeline.py ). Each function takes in a dataframe and outputs a pre-processed data, preprocessed_companies and preprocessed_shuttles respectively.. When Kedro runs the data engineering pipeline, it determines whether datasets are registered in the data catalog ( conf/base/catalog.yml ). If a dataset is registered, it is persisted automatically to the path specified without a need to specify any code in the function itself. If a dataset is not registered, Kedro stores it in memory for the pipeline run and removes it afterwards. In the tutorial, the preprocessed data is registered in conf/base/catalog.yml : CSVLocalDataSet is chosen for its simplicity, but you can choose any other available dataset implementation class to save the data, for example, to a database table, cloud storage (like S3, Azure Blob Store etc.) and others. The data_engineering.py file also includes a function, create_master_table which the data engineering pipeline uses to join together the companies, shuttles and reviews dataframes into a single master table. Data science pipeline The data science pipeline, which builds a model that uses the datasets, is comprised of a price prediction model, which uses a simple LinearRegression implementation from the scikit-learn library. The code is found in the file src/kedro_tutorial/nodes/price_prediction.py . The test size and random state parameters for the prediction model are specified in conf/base/parameters.yml and Kedro feeds them into the catalog when the pipeline is executed. Combining the pipelines The projects pipelines are specified in pipeline.py : The de_pipeline will preprocess the data, then use create_master_table to combine preprocessed_shuttles , preprocessed_companies and reviews into the master_table dataset. The ds_pipeline will then create features, train and evaluate the model. The first node of the ds_pipeline outputs 4 objects: X_train , X_test , y_train , y_test ] which are then used to train the model and, in a final node, to evaluate the model. The two pipelines are merged together in de_pipeline + ds_pipeline . The order in which you add the pipelines together is not significant since Kedro automatically detects the correct execution order for all the nodes. The same overall pipeline would result if you specified ds_pipeline + de_pipeline . Both pipelines will be executed when you invoke kedro run, and you should see output similar to the following: If you have any problems getting the tutorial code up and running, you should check that you are working in a Python 3.5 environment and have all the dependencies installed for the project. If there are still problems, you should head over to Stack Overflow for guidance from the community and, if the behaviour youre observing appears to be a problem with Kedro, please feel to raise an issue on Kedros Github repository . Kedro runners There are two different ways to run a Kedro pipeline. You can specify: By default, Kedro uses a SequentialRunner when you invoke kedro run. Switching to use ParallelRunner is as simple as providing an additional flag as follows: Thats the end of the tutorial to run through the basics of Kedro. Theres plenty more documentation on the various configuration options and optimisations you can use in your own projects, for example: Contributions welcome! If you start working with Kedro and want to contribute an example or changes to the Kedro project, we would welcome the chance to work with you. Please consult the contribution guide . Acknowledgements The Spaceflights example is based on a tutorial written by the Kedro team at QuantumBlack Labs (Yetunde Dada, Ivan Danov, Dmitrii Deriabin, Lorena Balan, Gordon Wrigley, Kiyohito Kunii, Nasef Khan, Richard Westenra and Nikolaos Tsaousis), who have kindly given me early access to their code and documentation in order to produce this tutorial for TowardsDataScience. Thanks all! 441 3 441 441 3 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Shuvashish Chatterjee Jun 4, 2019 Alexa find me a parking. Can AI guide you to a vacant parking slot? Detecting parking lot occupancy from a security cam footage using deep learning Finding a vacant spot in a parking lot is a tough ask. It is even difficult to manage these lots if incoming traffic varies a lot. Which slots are vacant at this instant? What time do we need more slots? Are commuters finding it difficult to reach a particular slot 8 min read 8 min read Share your ideas with millions of readers. Emanuele Fabbiani Jun 4, 2019 Member-only Lessons from a real Machine Learning project, part 2: the traps of data exploration How to fall into the pitfalls of data exploration and get away This is the second story of the series, so Im going to brutally shrink the intro. Im writing to share what a real, enterprise-level Machine Learning project taught me and my team. If you are curious to know more, feel free to check out the first chapter: from Jupyter to 5 min read 5 min read Some Aditya Mandal Jun 4, 2019 Evolution of Machine Translation In 1949, Warren Weaver, a researcher at Rockefeller Foundation, presented a set of proposals for machine based translations which were based on information theory and successes in code breaking during the Second World War. After few years, the machine translation research began in earnest in many US universities. As described 10 min read 10 min read Ziad SALLOUM Jun 4, 2019 Member-only Eligibility Traces in Reinforcement Learning Sometimes looking backward isnt that bad Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com What is Eligibility Traces ? In short and a straight forward manner, Eligibility Traces is a kind of mathematical trick that improves the performance of Temporal Difference methods, in Reinforcement Learning. Here are the benefits of Eligibility Traces: 7 min read 7 min read Adam King Jun 4, 2019 Member-only Optimizing deep learning trading bots using state-of-the-art techniques Lets teach our deep RL agents to make even more money using feature engineering and Bayesian optimization In the last article, we used deep reinforcement learning to create Bitcoin trading bots that dont lose money. Although the agents were profitable, the results werent all that impressive, so this time were going to step it up a notch and massively improve our models profitability. As a reminder, the 17 min read 17 min read Jo Stichbury Rnin technology writer and podcast host. Cat herder. Dereferences NULL. More from Medium Khuyen Tran in The Prefect Blog Orchestrate Your Data Science Project with Prefect 2.0 Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Naga Sanjay Continuous Training of ML models. Madison Hunter in Towards Data Science How to Write Good Code Documentation for Data Scientists Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5464,\n",
       "  'url': 'https://towardsdatascience.com/for-the-successful-future-of-ai-women-have-to-take-the-lead-180f09be4e50',\n",
       "  'title': 'For the Successful Future of AI, Women Have to Take the\\xa0Lead',\n",
       "  'subtitle': 'Why women possess the right qualities to…',\n",
       "  'claps': 339,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 8,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 0.00016745015888006964,\n",
       "  'text': \"Towards Data Science Apr 1 2019 Member-only Listen Save Successful Adoption AI Need Female Leaders woman posse right quality lead AI project successfully enable environment collaboration inclusion build real-world AI product exciting breakthrough twenty-first century occur technology expanding concept mean human John Naisbitt Part Differences Men Women story woman men work differently dive woman lead AI team want share fascinating story heard Tania Biland 3rd-year student Lucerne University Applied Sciences Arts story narrated Tania Last semester class got split three different group order develop safety technology solution Swiss German brand Group 1 woman group Group 2 men Group 3 Four woman one man 4 week work team present work Group 1 composed woman developed safety solution woman dark jury wa male decided tell story using persona music video order make feel woman experiencing daily basis also put emphasis fact everyone ha mother sister wife life probably dont want her/them suffer end solution wa rather simple technologically using light provide safety connected audience emotionally Group 2 mostly composed men presented high-tech solution using AI GPS video conference based argument fact number pointed competitive advantage Group 3 4 woman 1 man outcome didnt seem finished man group could agree led woman therefore spend much time discussing group dynamic instead working group different output also approached problem differently group group 1 decided start defining others work preference style order distribute responsibility keeping hierarchy flat possible hand two group elected leader team turned leader perceived dictator lead heavy conflict team spent hour discussing arguing group wa working productive science tell u gender difference science landscape regard gender difference effect behavior still evolving ha come clear set scientific explanation different behavior yet compiling research two main factor influence behavior story told Tania woman developed solution Collaborative Leadership Style adhocracy culture adapting leading position based task almost flat hierarchy derived argumentation involving stakeholder case mother wife user showing empathy problem saw bigger picture also built simpler solution wa actually finished story wa able connect dot AI project never end moving prototype phase real-world application Part II Making AI success AI product adopted Based experience three main reason AI Machine Learning ML solution move prototyping phase real-world Interestingly none three point relate technical challenge overcome creating right team make AI successfully adopted order solve challenge build successful AI product need focus collaborative community-driven approach take account opinion different stakeholder especially under-represented step achieve Step 1 Involve different group esp woman middle talent pyramid technology company focus hiring people top talent pyramid primarily historical reason fewer woman example Computer Science class le 10 percent woman However many talented woman hidden middle pyramid educating online course lack opportunity encouragement give example wa talking president Geek Girls Carrot organization promoting woman tech organizing AI workshop 125 woman applied 25 seat naturally leave behind 100 talented woman Imagine involve 100 woman instead top would give lot woman opportunity work new technology like AI Step 2 Build communal collaborative bottom-up team different stakeholder Next need collaboration men woman well different stakeholder launch product successfully real market achieved forming inclusive project community build AI product based common value belief often bigger vision Proving point past six month brought together group 50 male female student build ML model Within short time member started collaborating helping build model Four subgroup got formed one wa driven two woman supported two men data tagger group men 4 month group two woman two male built accurate model beginning woman much willing collaborate men However interestingly saw men group also ended behaving collaboratively woman group wa fascinating Step 3 Create right Organizational Structure collaboration could create organizational structure practice dont need empowerment design everybody powerful one powerless seen achieved connecting intrinsic extrinsic motivation related money creating incentive structure competitive case built community mentor wa top pyramid followed community manager engineer working building model finally data tagger Members team striving move ladder reach next level created extrinsic motivation However monetary compensation people level wa fostered collaboration context role leader bos foster Collaborative Leadership organizational structure decrease need control people give opportunity learn grow together 2 Part III Connecting Part Part II woman lead AI team story beginning female group followed Collaborative Leadership Style showing customer empathy willingness collaborate Considering limited experiment solar project saw approach use community build product helped well foster collaboration build trust among community member none mentioned quality generalized following graphic aim summarize reason many woman great fit Collaborative Leadership conclusion arguing think holistically best create right environment look beyond gender race cultural background focus collaborate human build better future Finally would like thank people men woman helped article creating experience content also refining text want receive update AI Challenges get expert interview practical tip boost AI skill subscribe monthly newsletter also Facebook LinkedIn Twitter 336 336 336 Towards Data Science home data science Medium publication sharing concept idea code Manu Siddharth Jha Mar 31 2019 Member-only Artificial Intelligence next big thing Hollywood Movies captured imagination people ever since came limelight Right first motion picture late 1880s upcoming latest sci-fi blockbuster cinema ha become medium love joy passion movie lover Almost every country across globe 5 min read 5 min read Share idea million reader Guy Tsror Mar 31 2019 Member-only romantic side data science Analyzing relationship year worth text message wa supposed private project one thought would upload private Google Colab notebook share whomever relevant one person referred OJ lightbulb clean share 11 min read 11 min read Salma Ghoneim Mar 31 2019 Member-only Object detection via color-based image segmentation using python tutorial contouring using python OpenCV Getting started already jupyter notebook IDE run python OpenCV installed skip Execution Tools hero today Anaconda free open-source distribution help installing different package sort mess isolated environment Wikipedias telling u 7 min read 7 min read Sahil Dhankhad Mar 31 2019 Member-only Forget APIs Python Scraping Using Beautiful Soup Import Data File web Part 2 APIs always every website Beautiful Soup going stay forever collect kind data website Today Data play critical role every industry data coming internet company 8 min read 8 min read Andrew Donaldson Mar 31 2019 Member-only Dont Fear Robots Impressive tech important remember Boston Dynamics video trailer movie isnt yet made Every time one Boston Dynamics video hit met mix amazement prediction doom hand new robot overlord impressive also well choreographed staged Impressive tech important 4 min read 4 min read Rudradeb Mitra Back writing 2years Building Omdena 7 startup Mentor Google Startups Speaker Book Author Deeply spiritual Medium Luay Rahil ILLUMINATION Elon Musk Eliminated Remote Work Working Home `` n't Work '' Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Mark Vassilevskiy 5 Unique Passive Income IdeasHow Make 4,580/Month Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Apr 1, 2019 Member-only Listen Save For the Successful Adoption of AI, We Need More Female Leaders Why women possess the right qualities to lead AI projects more successfully and enable an environment for collaboration and inclusion to build real-world AI products. The most exciting breakthroughs of the twenty-first century will not occur because of technology, but because of an expanding concept of what it means to be human John Naisbitt Part I: Differences between Men and Women A story of how women and men can work differently Before we dive into why more women should lead AI teams, I want to share a fascinating story I heard from Tania Biland, a 3rd-year student of Lucerne University of Applied Sciences and Arts. The story as narrated by Tania: Last semester, our class got split into three different groups in order to develop a safety technology solution for Swiss or German brands: Group 1:  Only women (my group) Group 2:  Only men Group 3:  Four women and one man After 4 weeks of work, each team had to present their work. Group 1  , composed of only women, developed a safety solution for women in the dark. As the jury was only male we decided to tell a story using a persona, music, and videos in order to make them feel what women are experiencing on a daily basis. We also put emphasis on the fact that everyone has a mother, sister or wife in their life and that they probably dont want her/them to suffer. In the end, our solution was rather  simple  ,  technologically  : using light to provide safety but connected to the audience  emotionally. Group 2  , mostly composed of men, presented a more  high-tech solution using AI  ,  GPS  and  video conferences  . They based their arguments on  facts and numbers  and pointed out their  competitive advantages  . In Group 3,  with 4 women and 1 man, the outcome didnt seem finished. The only man in the group could not agree to be led by women and they, therefore, spend too much time discussing group dynamics instead of working. The groups not only had different outputs but also approached the problem differently. My group (group 1) decided to start by defining each others work preferences and styles in order to distribute some responsibilities and keeping a hierarchy as flat as possible. On the other hand, the two other groups elected a leader for the team. It turned out that these leaders were more perceived as dictators, which lead to heavy conflicts where the teams spent hours discussing and arguing while our group was just working and productive. What science tells us about gender differences The science landscape with regards to gender differences and effects on behavior is still evolving and has not come up with a clear set of scientific explanations for different behaviors yet. By compiling most of the research, there are two main factors that influence behaviors: In the above story, as told by Tania, women developed the solution in a Collaborative Leadership Style (  adhocracy culture  ), adapting the leading position based on the tasks with an almost flat hierarchy. They derived their argumentation by involving all stakeholders (in this case the mothers and wives = users), showing empathy for their problems. They saw the bigger picture and also built a simpler solution that was actually finished. Through the story, I was able to connect the dots on why most AI projects never end up moving out from the prototype phase to a real-world application. Part II: Making AI a success Why AI products are not adopted? Based on my experience, there are three main reasons why most AI and Machine Learning (ML) solutions do not move from the prototyping phase to the real-world: Interestingly, none of the three points relate to the technical challenges, and all of them can be overcome by creating the right team. How to make AI more successfully adopted? In order to solve the above challenges and build more successful AI products, we need to focus on a more collaborative and community-driven approach . This takes into account opinions from different stakeholders, especially those who are under-represented. Below are steps to achieve that: Step 1. Involve different groups esp. women from the middle of the talent pyramid In technology, most companies focus on hiring people at the top of the talent pyramid, where for primarily historical reasons, are fewer women. For example, most Computer Science classes have less than 10 percent of women. However, many talented women are hidden in the middle of the pyramid, educating themselves through online courses but lack opportunities and encouragement. To give an example, I was talking with the president of Geek Girls Carrot, which is an organization promoting women in tech. They are organizing an AI workshop where over 125 women applied but they had only 25 seats , so naturally, they have to leave behind more than 100 talented women . Imagine, if we can involve most of the other 100 women instead of only at the top. This would give a lot more women the opportunity to work in new technologies like AI. Step 2. Build a communal and collaborative bottom-up team with different stakeholders Next, we need more collaboration between men and women as well as different stakeholders to launch products successfully in the real market. This can be achieved through forming inclusive project communities that build AI products based on common values, beliefs, and often a bigger vision. Proving the point, in the past six months, we brought together a group of more than 50 male and female students to build an ML model. Within a short time, members started collaborating and helping each other to build the models. Four subgroups got formed, and one of them was driven by two women and supported by two men (data taggers). The other groups were all men. In 4 months, the group with the two women and two male built the most accurate model. From the beginning, the women were much more willing to collaborate than men. However, more interestingly, I saw that men in the group also ended up behaving more collaboratively because of the other women in the group. This was fascinating!! Step 3. Create the right Organizational Structure for collaboration What if we could create organizational structures and practices that dont need empowerment because, by design, everybody is powerful and no one powerless? I have seen that this can be achieved by connecting intrinsic and extrinsic motivations (which is not related to money) and creating an incentive structure which is not competitive. In my case, I built the community where the mentor was at the top of the pyramid, followed by the community manager, then engineers working on building models and finally data taggers. Members from each team were striving to move up the ladder to reach the next level, which created an extrinsic motivation. However, the monetary compensation for people on the same level was the same. This fostered collaboration. In this context, the role of a leader is not to be a boss but to foster Collaborative Leadership . Such an organizational structure will decrease the need to control people and will give opportunities to learn and grow together[2]. Part III: Connecting Part I and Part II. Why women should lead AI teams In the story from the beginning, the female group followed a more Collaborative Leadership Style by showing more customer empathy and willingness to collaborate. Considering the limited experiment in the solar project, we saw that the approach to use the community to build products helped as well to foster collaboration and build trust among community members. While none of the mentioned qualities can be generalized, the following graphic aims to summarize some of the reasons why many women are a great fit for Collaborative Leadership . In conclusion, I am arguing: We should think more holistically and do our best to create the right environment where we look beyond gender, race, and cultural background and focus on how we can collaborate as humans to build a better future. Finally, I would like to thank all the people (both men and women) who helped me with this article creating the experiences and content but also refining the text. If you want to receive updates on our AI Challenges, get expert interviews, and practical tips to boost your AI skills, subscribe to our monthly newsletter. We are also on Facebook , LinkedIn , and Twitter . 336 336 336 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Manu Siddharth Jha Mar 31, 2019 Member-only Is Artificial Intelligence the next big thing in Hollywood? Movies have captured the imagination of people ever since they came into the limelight. Right from the first motion picture in the late 1880s to the upcoming latest sci-fi blockbuster, cinema has become a medium of love, joy, and passion for all movie lovers. Almost every country across the globe 5 min read 5 min read Share your ideas with millions of readers. Guy Tsror Mar 31, 2019 Member-only The romantic side of data science: Analyzing a relationship through a year worth of text messages This was supposed to be a private project, one I thought I would upload to a private Google Colab notebook, and share with just whomever it is relevant for (which is, one other person, referred to here as OJ). But then, a lightbulb why not clean it up and share 11 min read 11 min read Salma Ghoneim Mar 31, 2019 Member-only Object detection via color-based image segmentation using python A tutorial on contouring using python & OpenCV. Getting started If you already have jupyter notebook or an IDE with which you can run python & OpenCV installed, just skip to Execution. Tools Our hero today is Anaconda. a free open-source distribution that helps with installing different packages & sorts out their messes into isolated environments. What Wikipedias telling us about 7 min read 7 min read Sahil Dhankhad Mar 31, 2019 Member-only Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2 APIs are not always there for you for every website, but Beautiful Soup is going to stay with you forever to collect any kind of data from any website. Today, Data play a critical role in every industry. And most of this data is coming from the internet. Most company 8 min read 8 min read Andrew Donaldson Mar 31, 2019 Member-only Dont Fear the Robots Impressive as the tech here is, it is important to remember that the Boston Dynamics videos are trailers for a movie that isnt yet made. Every time one of these Boston Dynamics videos hit, they are met with a mix of amazement and predictions of doom at the hands of our new robot overlords. It is impressive. It is also very well choreographed and staged. Impressive as the tech here is, it is important 4 min read 4 min read Rudradeb Mitra Back to writing after 2years, Building Omdena, 7 startups, Mentor@Google for Startups, Speaker, Book Author, Deeply spiritual. More from Medium Luay Rahil in ILLUMINATION Elon Musk Eliminated Remote Work Because Working From Home \"Doesn\\'t Work\" Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Mark Vassilevskiy 5 Unique Passive Income IdeasHow I Make $4,580/Month Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 1155,\n",
       "  'url': 'https://towardsdatascience.com/overloading-operators-in-python-2e24da0d36d7',\n",
       "  'title': 'Overloading Operators in\\xa0Python',\n",
       "  'subtitle': 'And a bit on overloading methods as well (but I’ll try not to overload…',\n",
       "  'claps': 337,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 8,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-09-11',\n",
       "  'clap_prop': 0.0001664622523380043,\n",
       "  'text': \"Towards Data Science Sep 11 2019 Member-only Listen Save Overloading Operators Python bit overloading general Ill try overload u learning program Python run concept behind operator overloading relatively early course learning path like aspect Python language matter pretty much anything learning overloaded operator necessarily tie concept broadening scope topic somewhat obfuscating route individual learning curve mind try keep on-topic without pulling many area learning Python- Object-Oriented Programming naturally tie however Id like focus overloading operator broader complex topic overloading function also warrant least mention Overloading context programming refers ability function operator behave different way depending parameter passed function operand operator act 1 Python operator overloading also known operator ad-hoc polymorphism particular form syntactic sugar enables powerful convenient way prescribe operator er operation infix expression specified type Said another way operator overloading give extended meaning operator beyond pre-defined operational meaning classic operator-overloading example Python plus sign binary i.e. two operand operator add pair number also concatenates pair list string asterisk similarly overloaded multiplier number also repetition operator list string Comparison operator == exhibit similar behavior however overloaded operator Python user ought somewhat careful considering type checking MITs Professor John Guttag reminds u type checking Python strong programming language e.g. Java better Python 3 Python 2 example pretty clear mean used compare two string two number value 4 3 Rather arbitrarily designer Python 2 decided False numeric value le value type str designer Python 3 modern language decided since expression dont obvious meaning generate error message 2 well good operator used one user-defined data type i.e. created class operand case- say attempting add pair x coordinate shown here- compiler throw error since doesnt know add two object overloading done existing operator Python handful along corresponding magic method operator invoke using corresponding method create/access/edit inner working see end article Like nomenclature rapidly-evolving field doesnt seem consensus call them- theyre somewhat commonly referred magic methods- called magic since theyre invoked directly- seems closest standard perhaps since alternative special method sound well special herald colorful moniker- dunder method shorthand double-underscore method i.e. dunder-init-dunder Anyhow theyre special type method limited method associated operator __init__ __call__ example fact quite quick aside- printing ha associated magic method __str__ print plain Point class lone __init__ would get not-so-user-friendly output shown Adding __str__ method Point class remedy Interestingly format also invokes __str__ method print doe turn using x coordinate walk example overloading method Python concept somewhat common practice learning Python probably since get create class something mathematically familiar coordinate point one create number useful magic method one user-defined class use overload operator noteworthy aspect operator overloading position operand relation operator Take le operator example- call __lt__ method first left/preceding operand word expression x shorthand x.__lt__ first operand user-defined class need corresponding __lt__ method order able use may seem like nuisance actually add handy flexibility designing one class since could customize operator function doe class addition providing syntactic convenience writing infix expression use Professor Guttag point overloading provides automatic access polymorphic method defined using __lt__ built-in method sort one method 2 light distinction first second operand Python also provides u set reverse method __radd__ __rsub__ __rmul__ Keep mind reverse method called left operand doe support corresponding operation operand different type Pythonista redditor named Rhomboid explains way better ever could humbly defer take someone explain __radd__ simple term read documentation dont understand One final caveat- awesome flexibility ought keep original intention operator mind example len generally understood used return length sequence overloading method requires integer returned otherwise return TypeError brief toe-dip vaster choppier water overloading function According wikipedia pertains ability create multiple function name different implementation function may differ arity type parameter concept way useful language C++ Java doe really comport Pythonic way thing stackoverflow pointed use method overloading Python one helpful thread matter Clearly Python handle case different manner said reading method overloading helped discern important Pythonic concept polymorphism defined ability leverage interface different underlying form data type class Polymorphism signature feature class Python allows commonly-named method utilized across many class subclass furthermore enables function use object belonging one class way doe object different class without needing aware distinction across class 3 allows duck typing special case dynamic typing us characteristic polymorphism including late binding dynamic dispatch evaluate object type start single vs. multiple static vs. dynamic dispatch way beyond current level understanding Ill leave Sources 1 http //stackabuse.com/overloading-functions-and-operators-in-python/ 2 Guttag John V .. Introduction Computation Programming Using Python MIT Press MIT Press Kindle Edition 3 http //www.digitalocean.com/community/tutorials/how-to-apply-polymorphism-to-classes-in-python-3 4 title image http //www.osgpaintball.com/event/operation-overlord-scenario-paintball/ http //www.reddit.com/r/learnpython/comments/3cvgpi/can_someone_explain_radd_to_me_in_simple_terms_i/ List python class special method magic method Micropyramid List python class special method magic method magic function allow u override add default micropyramid.com Python Operator Overloading operator overloading Python Python operator work built-in class operator behaves www.programiz.com Python Tutorial Magic Methods so-called magic method nothing wizardry already seen previous chapter www.python-course.eu Overloading Functions Operators Python Overloading Overloading context programming refers ability function operator stackabuse.com Operator overloading computer programming operator overloading sometimes termed operator ad hoc polymorphism specific case en.wikipedia.org Python Operator Overloading Python Magic Methods DataFlair Python tutorial going discus Python Operator Overloading example operator overloading data-flair.training Operator Function Overloading Custom Python Classes Real Python might wondered built-in operator function show different behavior object different realpython.com use method overloading Python trying implement method overloading Python class def stackoverflow self print 'first method def stackoverflow.com Python function overloading know Python doe support method overloading 've run problem ca n't seem solve stackoverflow.com Function Overloading Python Recently one conversation Practo found guy complaining bad medium.com Disclaimer Mistakes misinterpretation abuse concept idea taken far mine mine 341 341 341 Towards Data Science home data science Medium publication sharing concept idea code Sergi Lehkyi Sep 11 2019 Football Winners Win Losers Lose Exploring 5 Years European Football Intro notebook explore modern metric football xG xGA xPTS influence sport analytics Expected Goals xG measure quality shot based several variable assist type shot angle distance goal whether wa headed shot 17 min read 17 min read Share idea million reader MILA JONES Sep 11 2019 Effective Ways Data Analytics Help Make Better Entrepreneur Check Business Intelligence BI data analytics remove uncertainty business provide insight help decision making forecasting Business Intelligence data analytics integral part successful business venture Business analytics ha dedicated market industry often sought-after method skip guesswork accelerate pace growth data analytics get invaluable insight business 4 min read 4 min read Mark Nagelberg Sep 11 2019 Member-only Take Python Skills Next Level Fluent Python intermediate programmer ticket advanced Python Youve programming Python although know way around dicts list tuples set function class feeling Python knowledge heard pythonic code fall short Youre intermediate Python programmer 7 min read 7 min read Javier Rodriguez Zaurin Sep 11 2019 RecoTour II neural recommendation algorithm second series post recommendation algorithm python first series wrote quite ago quickly went number algorithm implemented tried using Kaggles Ponpare dataset find related 15 min read 15 min read Vikash Kumar Sep 11 2019 Python Vs R Whats Best Machine Learning thinking build machine learning project stuck choosing right programming language project Well article going help clear doubt related characteristic Python R. Lets get started basic R Python 5 min read 5 min read Jay Kim Medium Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Herman Michaels Better Programming Setup Data Classes Python Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Yang Zhou TechToFreedom 5 Levels Using Context Managers Python Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Towards Data Science Sep 11, 2019 Member-only Listen Save Overloading Operators in Python and a bit on overloading in general (but Ill try not to overload you) Most of us learning to program in Python run into concepts behind operator overloading relatively early during the course of our learning path. But, like most aspects of Python (and other languages; and, for that matter, pretty much anything), learning about overloaded operators necessarily ties into other concepts, both broadening the scope of topic and somewhat obfuscating the route through our individual learning curve. With that in mind, I will try to keep on-topic without pulling in too many other areas of learning Python- some Object-Oriented Programming naturally ties in, however; and, while Id like to focus on overloading operators , the broader, more complex topic of overloading functions also warrants at least some mention. Overloading, in the context of programming, refers to the ability of a function or an operator to behave in different ways depending on the parameters that are passed to the function, or the operands that the operator acts on [1]. In Python, operator overloading (also known as operator ad-hoc polymorphism) in particular is a form of syntactic sugar that enables powerful and convenient ways to prescribe an operators, er, operations, as an infix expression of specified types. Said another way, operator overloading gives extended meaning to operators beyond their pre-defined operational meaning. The classic operator-overloading example in Python is the plus sign, a binary (i.e., two operands) operator that not only adds a pair of numbers, but also concatenates a pair of lists or strings. The asterisk is similarly overloaded as not only a multiplier for numbers, but also as a repetition operator for lists or strings. Comparison operators (such as >, ==, or !=) exhibit similar behavior; however, for all of these overloaded operators, we as Python users ought be somewhat careful when considering type checking. As MITs Professor John Guttag reminds us, type checking in Python is not as strong as in some other programming languages (e.g., Java), but it is better in Python 3 than in Python 2. For example, it is pretty clear what < should mean when it is used to compare two strings or two numbers. But what should the value of 4 < 3 be? Rather arbitrarily, the designers of Python 2 decided that it should be False because all numeric values should be less than all values of type str. The designers of Python 3 and most other modern languages decided that since such expressions dont have an obvious meaning, they should generate an error message. [2] This is all well and good, but what if an operator is being used on one or more user-defined data types (i.e., from a created class) as an operand? In such a case- say, attempting to add a pair of (x, y) coordinates, as shown here- the compiler will throw an error since it doesnt know how to add the two objects. And, while overloading can only be done on existing operators in Python, there are a handful of them, along with the corresponding magic method each of these operators invoke; using these corresponding methods, we can create/access/edit their inner workings (see end of article). Like other nomenclature in this rapidly-evolving field, there doesnt seem to be a consensus on what to call them- theyre somewhat commonly referred to as magic methods- called magic since theyre not invoked directly- and that seems to be closest to standard, perhaps since the alternative special method sounds, well, not so special. Some herald a more colorful moniker- dunder methods as a shorthand for double-underscore methods (i.e., dunder-init-dunder). Anyhow, theyre a special type of method, and are not only limited to the methods associated with operators ( __init__() or __call__() , as examples); in fact, there are quite a few of them. Just as a quick aside- printing has its own associated magic method, __str__() . If we were to print the plain Point class with just the lone __init__() , we would get the not-so-user-friendly output shown above. Adding the __str__() method into the Point class will remedy that. Interestingly, format() also invokes the same __str__() method that print() does. It turns out that using (x, y) coordinates to walk through examples of overloading, methods, and other Python concepts is a somewhat common practice when learning Python, probably since we get to create our own class with something as mathematically familiar as coordinate points. From there, one can create a number of useful magic methods for ones user-defined class and use them to overload operators. A noteworthy aspect of operator overloading is the position of each operand in relation to its operator. Take the less than operator < as an example- it calls the __lt__() method for the first (or left/preceding) operand. In other words, the expression x < y is shorthand for x.__lt__(y)  ; if the first operand is a user-defined class, it needs to have its own corresponding __lt__() method in order to be able to use <. That may seem like a nuisance, but it actually adds some handy flexibility for designing ones classes, since we could customize what any operators function does for a class. In addition to providing the syntactic convenience of writing infix expressions that use <, Professor Guttag points out, this overloading provides automatic access to any polymorphic method defined using __lt__() . The built-in method sort is one such method. [2] In light of this distinction between first and second operands, Python also provides us with a set of reverse methods, such as __radd__(), __rsub__(), __rmul__() , and so on. Keep in mind that these reverse methods are only called if the left operand does not support the corresponding operation and the operands are of different types. A Pythonista redditor named Rhomboid explains it way better than I ever could, so I humbly defer to his take: Can someone explain __radd__ to me in simple terms? I read the documentation and I dont understand it. One final caveat- while its awesome that we have this flexibility, we ought to keep the original intention of the operators in mind. For example, len() is generally understood to be used to return the length of a sequence; so overloading this method requires that an integer is returned (otherwise it will return a TypeError). and now for a brief toe-dip into the vaster, choppier waters of overloading functions . According to wikipedia, this pertains to the ability to create multiple functions of the same name with different implementations. The functions may differ by the arity or types of their parameters. This concept is way more useful in other languages (C++, Java), and does not really comport with the Pythonic way of doing things, as some on stackoverflow have pointed out: How do I use method overloading in Python? and one more helpful thread on the matter: Clearly, Python handles such cases in a different manner. With that said, reading up on method overloading helped me discern an important Pythonic concept: polymorphism. This is defined as the ability to leverage the same interface for different underlying forms such as data types or classes . Polymorphism is a signature feature of classes in Python, in that it allows commonly-named methods to be utilized across many classes or subclasses, which furthermore enables functions to use objects belonging to one class in the same way that it does for objects of a different class, all without needing to be aware of distinctions across classes. [3] . This allows for duck typing, a special case of dynamic typing that uses the characteristics of polymorphism (including late binding and dynamic dispatch ) to evaluate object types. From here, this all starts into a single vs. multiple, and static vs. dynamic dispatch, which is way beyond my current level of understanding; so Ill leave that be for now. Sources: [1] https://stackabuse.com/overloading-functions-and-operators-in-python/ [2] Guttag, John V.. Introduction to Computation and Programming Using Python (The MIT Press) . The MIT Press. Kindle Edition. [3] https://www.digitalocean.com/community/tutorials/how-to-apply-polymorphism-to-classes-in-python-3 [4] (title image)  https://www.osgpaintball.com/event/operation-overlord-scenario-paintball/ https://www.reddit.com/r/learnpython/comments/3cvgpi/can_someone_explain_radd_to_me_in_simple_terms_i/ List of python class special methods or magic methods - Micropyramid List of python class special methods or magic methods. magic functions allow us to override or add the default micropyramid.com Python Operator Overloading What is operator overloading in Python? Python operators work for built-in classes. But same operator behaves www.programiz.com Python Tutorial: Magic Methods The so-called magic methods have nothing to do with wizardry. You have already seen them in previous chapters of our www.python-course.eu Overloading Functions and Operators in Python What is Overloading? Overloading, in the context of programming, refers to the ability of a function or an operator to stackabuse.com Operator overloading In computer programming, operator overloading, sometimes termed operator ad hoc polymorphism , is a specific case of en.wikipedia.org Python Operator Overloading and Python Magic Methods - DataFlair In this Python tutorial, we are going to discuss Python Operator Overloading, examples of operator overloading in data-flair.training Operator and Function Overloading in Custom Python Classes - Real Python You might have wondered how the same built-in operator or function shows different behavior for objects of different realpython.com How do I use method overloading in Python? I am trying to implement method overloading in Python: class A: def stackoverflow(self): print 'first method' def stackoverflow.com Python function overloading I know that Python does not support method overloading, but I've run into a problem that I can't seem to solve in a stackoverflow.com Function Overloading in Python Recently in one of the conversations at Practo, I found some guys complaining that its so bad that we do not have medium.com Disclaimer: Mistakes, misinterpretation, abuses of concepts, and ideas taken too far are mine and only mine. 341 341 341 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Sergi Lehkyi Sep 11, 2019 Football: Why Winners Win and Losers Lose Exploring 5 Years of European Football Intro In this notebook we will explore modern metrics in football (xG, xGA and xPTS) and its influence in sport analytics. Expected Goals (xG) measures the quality of a shot based on several variables such as assist type, shot angle and distance from goal, whether it was a headed shot 17 min read 17 min read Share your ideas with millions of readers. MILA JONES Sep 11, 2019 Effective Ways How Data Analytics Help to Make a Better Entrepreneur Check out how Business Intelligence (BI) and data analytics remove uncertainty in business and provide insights that help in decision making and forecasting. Business Intelligence and data analytics are an integral part of any successful business venture. Business analytics has its dedicated market in the industry and is often a sought-after method to skip the guesswork and accelerate the pace of growth. With data analytics, you get invaluable insights into your business. 4 min read 4 min read Mark Nagelberg Sep 11, 2019 Member-only Take your Python Skills to the Next Level With Fluent Python The intermediate programmers ticket to advanced Python Youve been programming in Python for a while, and although you know your way around dicts, lists, tuples, sets, functions, and classes, you have a feeling your Python knowledge is not where it should be. You have heard about pythonic code and yours falls short. Youre an intermediate Python programmer 7 min read 7 min read Javier Rodriguez Zaurin Sep 11, 2019 RecoTour II: neural recommendation algorithms This is the second of a series of posts on recommendation algorithms in python. In the first of the series, that I wrote quite a while ago, I quickly went through a number of algorithms that I implemented and tried using Kaggles Ponpare dataset. You can find all the related 15 min read 15 min read Vikash Kumar Sep 11, 2019 Python Vs R: Whats Best for Machine Learning Are you thinking to build a machine learning project and stuck between choosing the right programming language for your project? Well, then this article is going to help you clear the doubts related to the characteristics of Python and R. Lets get started with the basics. R and Python both 5 min read 5 min read Jay Kim More from Medium Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Herman Michaels in Better Programming How to Setup Data Classes in Python Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Yang Zhou in TechToFreedom 5 Levels of Using Context Managers in Python Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 6164,\n",
       "  'url': 'https://towardsdatascience.com/make-data-acquisition-easy-with-aws-lambda-python-in-12-steps-33fe201d1bb4',\n",
       "  'title': '<strong class=\"markup--strong markup--h3-strong\">Make Data Acquisition Easy with AWS &amp; Lambda (Python) in 12\\xa0Steps</strong>',\n",
       "  'subtitle': '<strong class=\"markup--strong markup--h4-strong\">Goodbye to complex ETL pipelines</strong>',\n",
       "  'claps': 303,\n",
       "  'responses': 6.0,\n",
       "  'reading_time': 15,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-06-27',\n",
       "  'clap_prop': 0.0001496678411228941,\n",
       "  'text': 'Towards Data Science Jun 27 2019 Member-only Listen Save Make Data Acquisition Easy AWS Lambda Python 12 Steps Goodbye complex ETL pipeline SQL database complication article serve brief introduction AWS Lambda building fully serverless data pipeline article written people least basic mean basic understanding Python brand new AWS create AWS account Lambda function pull data Craigslist store S3 bucket u automatically daily Data scientist enthusiast continuously trying learn new technology make life easier Like many data science community self taught statistic CS background past life wa mechanical engineer feel like eon ago year gotten pretty good Python AWS certification Ive always learned much others thought wa turn contribute community Feel free connect LinkedIn check Github profile find article useful Lambda Often run scenario wrote Python function need run every day could set Cron machine computer option turn cloud old way AWS wa spin EC2 instance SSH give function maybe docker image set Cron job forth isnt bad bit pain Also need something run daily really need EC2 instance running day Enter Lambda yes know could also use Batch Lambda tutorial AWS ha ton service perhaps favorite AWS Lambda Basically let focus writing code dealing annoying thing like VPCs EC2 instance MySQL database etc write Python give code Lambda execute code Cloud Even better trigger code variety way every minute day put something S3 bucket etc Lambda totally awesome cheap ha quickly become used AWS service twelve concise step set Lambda function automatically pull data Craigslist every day store data JSON S3 one example could use tutorial run Python function automatically interval specify Admittedly article bit long wanted make sure would easily reproducible especially beginner ever get stuck feel free reach try debug final note everything well within free tier wont cost penny Lambda give million free request month unless plan firing baby every second good Step One Create Free AWS Account Head http //aws.amazon.com signup free account Note need verify phone number whole process take minute end select Basic plan give solid amount compute power free 12 month Step Two Create Bucket Store Data S3 Buckets place store data object data specific basically like folder computer cloud Go Services Storage S3 brought S3 splash screen Click Create bucket give bucket name pick region closest named bucket my-super-sweet-bucket name whatever want know bucket name unique globally cant awesome bucket name giving name region stick default click Next bucket created create bucket make note called need later Step Three Create Lambda Function bucket store data Lambda time create Lambda function hover Services top Compute find Lambda Since havent made Lambda yet get splash screen introducing world Lambda Click Create Function button make Lambda Next asked name function specify programming language give role Quick Aside Role Amazon Web Services vast mysterious beast lot thing going One thing worth noting nearly everything AWS us rule Least Permissions basically say default nothing get permission access anything example let say create user named Bob AWS Network default Bob log cant access AWS service Bob login cant use S3 EC2 Lambda anything else Poor Bob Unless explicitly stated access given Anyways role way programmatically granting access Basically tell AWS Lambda function want able access S3 Without role Lambda function like Poor Bob access Dont make Lambda like Bob Anyways name Lambda function whatever want named mine creatively pullCraigslistAds Since 2019 modern men woman using Python 3.7 2.7 nonsense Note Lambda used lot othe programming language use Python Permissions set Create new role basic Lambda permission fix role later tutorial Step 4 Choose Upload .zip file code whole point Lambda execute code u youve created Lambda function notice code editor right browser Often time youcan write code right editor However tutorial going use outside built Python library like Requests need thing bit different uploading zip file dont need import library dont come Python write inline need import library need Zip file Lambda run Python code problem-o dont need library like Pandas Requests Matplotlib anything else installed Pip tutorial using python-craigslist library go Upload .zip file option Step 5 Functions Code Note rush want skip step clone repository provide final Zip file need give Lambda clone repo jump Step 8 arent lazy want follow along create new folder anywhere machine save code file called lambda_function.py Basically using Python library called craigslist scrape Craigslist u first line import library Line 7 define function giving input event context want get nerdy read check AWS Docs high level event pass function meta data trigger context something pass runtime info function function dont need mess much next part series need use event example trigger Lambda something put S3 bucket use event get file name often handy Anyways rest function fairly straightforward used Python instantiate class tell data pull store JSON Lines 28 31 send data S3 using Boto3 youve never used Boto3 Python SDK plain English interact AWS via Python Boto3 let put stuff S3 invoke Lambda create bucket etc want use AWS resource Python script Boto3 answer use one basic feature output S3 bucket line 29 need update bucket name whatever named bucket Step 2 reference sample data function data scraped Craigslist ad apartment final note sample code could anything example could call API dump data JSON store JSON NoSQL database outputting JSON S3 bucket could many thing Step 6 Pip Install Dependencies Lambda run Python cloud doe version Python Anyone us Python regularly highly dependent library open source pre-written blob code use import panda pd anyone example happens Lambda go back inline import request Note sure save function clicking test else testing previous version code without request included good rule thumb always saving Lambda function easy forget debugging past version Basically Lambda fails Python environment doesnt request library get library several way Ive found easiest already lambda_function.py file saved folder Next pip install dependency folder Looking back code dependency craigslist library json datetime boto3 library required builtin Lambda already ha recently idea could pip install current directory work quite nicely install python-craigslist dependency folder along lambda_function.py Step 7 Create Zip File Thinking back Lambda function AWS chose option upload Zip file friend time make aforementioned Zip file Select everything folder put zip file naming lambda_function.zip use 7Zip literally highlight file folder right click choose 7Zip add Zip file rename lambda_function.zip directory look something like although need Zip file Step 8 Upload Zip file Lambda First feel judging using Windows dont like Anyways click Upload button select newly created zip file youve done click Save button top right bring lambda_function.py inline editor Lambda environment needed library Also point forward need make change code edit line instead repackage zip file reason would need repackage zip file new dependency another library happens repeat Steps 68 Step 9 Create Environmental Variable Going fully environmental variable bit beyond scope already lengthy tutorial basically Lambda look code call environmental variable number_of_posts many post pull Lets nice Craigslist overload system set something low like 5 10 creates environmental variable code go get invoked Step 10 Set timeout value Incoming AWS Developers Associate exam question default Lambda allow 3 second execute example simply isnt enough time change timeout value Basic Settings max 15 minute able get away 3 minute mean set function pull lot ad say 200 would need time Note Lambda charge memory compute time Step 10 Add Permission Lambda Function Im sure point article feeling length like Jane Austen novel think back Step 3 created Lambda function let know would need give Lambda permission put stuff S3 bucket Weve arrived time Heres concisely possible Step 11 Test Function Finally Lambda ready tested test see article hasnt bunch baloney stuff actually work Luckily Lambda ha test function built right look right next save button Test button Click ask create test input going skip needed pulling API scraping Basically Lambda going respond certain type input could give test input dont need need run give name click Create test Test event click Test function run Gods good see universal sign proper execution form giant green block think back function outputting data form JSON blob S3 theory data S3 bucket go S3 bucket data Huzzah Lets move onto last step automation Step 12 Automate beauty Lambda triggered invoked variety way Lambda function happen anytime object go bucket another Lambda executes time interval going use latter set run based time interval Go top function choose CloudWatch Events set standard time interval event fire Lambda CloudWatch many thing essentially monitoring tool getting data AWS Environment want get AWS certification need learn CloudWatch come many exam Go bottom keep Create new rule rule basically governs event fire creatively name Daily description creative juice really flowing say Run day Select Schedule expression put rate 1 day value Note either use rate function take Cron scheduler Cron ugly going go rate use whatever float boat thats Click Add apply new spiffy rule Lambda function click Save top apply Lambda function Lambda function pull data daily dump S3 Summary Lambda wonderful developer data scientist looking get data EC2 database hardware anything else pain let u simply write code let AWS take care annoying stuff tutorial created Python function could run locally machine set run automatically day AWS using Lambda first time go may slow get used Lambda start deploying stuff rapidly AWS Lambda bit scary first get going stick ha ton us data scientist especially data acquisition ease mentioned found article useful feel free connect LinkedIn share use case Lambda 429 9 429 429 9 Towards Data Science home data science Medium publication sharing concept idea code Rachel Wiles Jun 27 2019 solved problem handwritingrecognition Deep learning enables recognition text 99.73 accuracy go Deep learning ha widely used recognise handwriting offline handwriting recognition text analysed written information analysed binary output character background Although shift towards digital stylus writing give information pen stroke 6 min read 6 min read Share idea million reader Pier Paolo Ippolito Jun 27 2019 Member-only Understanding Cancer using Machine Learning Use Machine Learning ML Medicine becoming important One application example Cancer Detection Analysis Note Towards Data Sciences editor allow independent author publish article accordance rule guideline endorse author contribution rely author work without seeking professional advice See Reader Terms detail 8 min read 8 min read Siddhant Bhambri Jun 27 2019 Member-only Reinforcement Learning Job Scheduling thought writing first Medium article intrigued since believed could give platform share contribute something would like read fellow community member Scientific publication research paper hard comprehend layperson would 5 min read 5 min read George Seif Jun 27 2019 Member-only Nvidias New Data Science Workstation Review Benchmark Data science hot write newsletter learner called Mighty Knowledge new issue contains link key lesson best content including quote book article podcasts video every one picked specifically learning live wiser happier fuller life Sign 7 min read 7 min read Sascha Heyer Jun 27 2019 Member-only Kubeflow Components Pipelines Kubeflow large ecosystem stack different open source tool ML tool want keep thing simple therefore cover component pipeline experiment pipeline component get basic required build ML workflow many tool integrated Kubeflow cover upcoming post Kubeflow originated Google 6 min read 6 min read Shawn Cochran Data music enthusiast Medium Prakshal Jain Clairvoyant Blog Executing Snowflake Queries Lambda Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Haq Nawaz Dev Genius Develop AWS Glue ETL pipeline Python shell Taimur Ijlal Geek Culture passed AWS Certified Solutions Architect Professional exam Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Jun 27, 2019 Member-only Listen Save Make Data Acquisition Easy with AWS & Lambda (Python) in 12 Steps Goodbye to complex ETL pipelines, SQL databases and other complications This article will serve as a brief introduction to AWS Lambda and building a fully serverless data pipeline. This article is written for people with at least basic (and I mean basic ) understanding of Python, but you can be brand new to AWS. We will create an AWS account and then have a Lambda function pull data from Craigslist and store it in a S3 bucket for us automatically, daily. About Me: Data scientist and enthusiast who is continuously trying to learn new technologies that make my life easier. Like many in the data science community I am self taught and not from a statistics or CS background. In a past life I was a mechanical engineer, but that feels like eons ago. Over the years I have gotten pretty good with Python and AWS (I have a few of the certifications), and because Ive always learned so much from others I thought it was my turn to contribute to the community. Feel free to connect with me on LinkedIn or check out my Github profile if you find this article useful. What Is Lambda Often I run into this scenario of I wrote this Python function and need it to run every day. You could set up a Cron on your machine, but what if your computer is off? The other option is to turn to the cloud. The old way to do this with AWS was to spin up an EC2 instance, SSH into it, give it your function or maybe a docker image, set up a Cron job there, and so forth. That isnt too bad, but it is a bit of a pain. Also, if I just need something to run daily do I really need a EC2 instance running all day? Enter Lambda (yes I know you could also use Batch for this, but this is a Lambda tutorial). AWS has a ton of services, but perhaps my favorite is AWS Lambda. Basically it lets you focus on writing code and not dealing with annoying things like VPCs, EC2 instances, MySQL databases, etc. Just write some Python, give that code to Lambda, and it will execute that code in the Cloud. Even better, you can trigger that code in a variety of ways: every minute, once a day, when you put something into an S3 bucket, etc. Lambda is totally awesome (and cheap) and has quickly become my most used AWS service. What We Will Be Doing In twelve, concise steps we will set up a Lambda function that will automatically pull data from Craigslist every day and store that data (in JSON) in S3. This is just one example, but you could use this tutorial to run any Python function automatically at some interval you specify. Admittedly this article is a bit long, but I wanted to make sure it would be easily reproducible, especially for beginners. If you ever get stuck feel free to reach out to me and we can try to debug it. A final note is everything here should be well within free tier, so it wont cost you a penny. Lambda gives you a million free requests a month, so unless you plan on firing this baby every second you should be good. Step One: Create A Free AWS Account Head over to https://aws.amazon.com and signup for a free account. Note you will need to verify your phone number, but the whole process should only take a few minutes. At the end select the Basic plan, which gives you a solid amount of compute power for free for 12 months. Step Two: Create A Bucket To Store Our Data S3 Buckets are a place we can store data (or object data to be more specific). Its basically like a folder on your computer, but in the cloud! Go to Services > Storage > S3 and you will be brought to the S3 splash screen. Click Create bucket and give your bucket a name and pick the region closest to you. I named by bucket my-super-sweet-bucket , and you can name yours whatever you want. Just know the bucket names have to be unique globally, so you cant have my awesome bucket name. After giving it the name and region just stick with all the defaults (click Next until your bucket is created). This will create your bucket, and make a note of what you called it as we will need it later. Step Three: Create A Lambda Function Now that we have a bucket to store our data from Lambda, its time to create our Lambda function. If you hover over Services at the top and under Compute you will find Lambda. Since you havent made a Lambda yet you will get a splash screen introducing you to the world of Lambda. Click the Create a Function button to make your Lambda. Next you will be asked to name your function, specify a programming language, and give it a role. Quick Aside What is a Role? Amazon Web Services is a vast and mysterious beast, with a lot of things going on. One thing worth noting is nearly everything in AWS uses a rule of Least Permissions, which basically says by default nothing gets permissions to access anything. For example, lets say you create a user named Bob in your AWS Network. By default, when Bob logs in, he cant access any AWS service. Bob can login but he cant use S3, EC2, Lambda or anything else. Poor Bob. Unless it is explicitly stated then no access is given. Anyways, a role is a way of programmatically granting access. Basically it will tell AWS that This is my Lambda function and I want it to be able to access S3. Without this role your Lambda function will be just like Poor Bob with no access. Dont make your Lambda like Bob. Anyways, you can name your Lambda function whatever you want. I named mine creatively pullCraigslistAds. Since its 2019, and we are modern men and women, we will be using Python 3.7 and not any of that 2.7 nonsense. Note that Lambda can be used with a lot of othe programming languages, but here we will use Python. For now Permissions can be set to Create a new role with basic Lambda permissions. We fix our role later in the tutorial. Step 4: Choose Upload a .zip file for your code The whole point of Lambda is to have it execute some code for us. Once youve created your Lambda function you will notice there is a code editor right there in the browser. Often times youcan just write code right there in the editor. However, in this tutorial we are going to use some outside (not built into Python) libraries, like Requests, we will need to do things a bit different by uploading a zip file. If you dont need to import any libraries that dont come with Python you can just write it inline. If you do need to import libraries we will need the Zip file. That is, Lambda can run your Python code no problem-o but what if you dont need libraries like Pandas, Requests, Matplotlib or anything else you installed with Pip. In this tutorial we will be using the python-craigslist library, so we go with the Upload .zip file option. Step 5: Our Functions Code Note: If you are in a rush and want to skip some steps you can clone  this repository  that will provide you the final Zip file you need to give to Lambda. If you just clone the repo you can jump to Step 8. If you arent lazy and want to follow along then create a new folder anywhere on your machine and save the code below into a file called lambda_function.py . What is this doing Basically it is using a Python library called craigslist to scrape Craigslist for us. The first few lines import our libraries. Line 7 is where we define our function, giving it the input of event and context . If you want to get nerdy and read into it, check out the AWS Docs . At a high level, event passes the function meta data about the trigger and context i s something that passes runtime info to the function. For this function we dont need to mess with them much, but in the next part of this series we will need to use event. For example, if we trigger a Lambda when something is put into an S3 bucket we can use event to get that file name, which is often handy. Anyways, the rest of the function is fairly straightforward for those who have used Python: instantiate our class, tell it what data to pull, and store that in some JSON. Lines 28 to 31 are how we send that data to S3 using Boto3. If youve never used Boto3, it is a Python SDK, or in plain English it is how you can interact with AWS via Python. Boto3 lets you put stuff in S3, invoke a Lambda, create a bucket, etc. If you want to use AWS resources from a Python script than Boto3 is your answer. Here we use one of its most basic features: output to our S3 bucket. On line 29 you will need to update the bucket name to whatever you named your bucket in Step 2. For reference, below is some sample data from our function, which is just data scraped from a Craigslist ad for apartments. A final note is that this is just our sample code, but this could be anything. For example, it could call an API, dump that data to JSON and store that JSON in a NoSQL database. Here we are outputting JSON to an S3 bucket but this could be many things. Step 6: Pip Install Our Dependencies Lambda runs Python in the cloud for you, but what does its version of Python have? Anyone who uses Python regularly is highly dependent on libraries, which are open source pre-written blobs of code we can use (import pandas as pd, anyone)? As an example, what happens in our Lambda (if I go back to inline) if I import requests? Note: Be sure to save your function before clicking test, or else it is testing the previous version of the code without requests included. A good rule of thumb is to always be saving your Lambda function, its easy to forget to do it and then you are debugging the past version. Basically our Lambda fails because this Python environment doesnt have the requests library. So how do we get it (or any library)? There are several ways, but this is how Ive found it easiest. We already have our lambda_function.py file saved in a folder. Next pip install your dependencies into that same folder. Looking back at our code, the only dependency we have is the craigslist library (json, datetime, os and boto3 are the other libraries required and all are builtin so Lambda already has them). Until recently I had no idea you could do this to pip install to your current directory, but it works quite nicely: This will install python-craigslist (and its dependencies) into your folder along with your lambda_function.py Step 7: Create A Zip File Thinking back to our Lambda function on AWS, we chose the option to upload a Zip file. Now, my friends, is the time to make the aforementioned Zip file! Select everything in your folder and put it into a zip file, naming it lambda_function.zip. I use 7Zip so I literally just highlight all the files and folders, right click, choose 7Zip and add it to a Zip file. Then I rename it to lambda_function.zip. Your directory should now look something like below, although now all you need is that Zip file. Step 8: Upload your Zip file to Lambda First, I can feel you judging me for using Windows and I dont like it. Anyways, click the Upload button and select your newly created zip file. Once youve done, that click the Save button in the top right. This will bring your lambda_function.py into the inline editor and your Lambda environment will now have all your needed libraries! Also from this point forward if you need to make changes to the code you can just edit it in line instead of having to repackage your zip file. The only reason you would need to repackage the zip file is if you have a new dependency on another library. If that happens then repeat Steps 68. Step 9: Create Your Environmental Variable Going fully into environmental variables is a bit beyond the scope of this already lengthy tutorial, but basically you can do them in Lambda. If you look at our code, we call for an environmental variable number_of_posts, which is how many posts we will pull. Lets be nice to Craigslist and not overload their system, so just set it to something low like 5 or 10. This creates the environmental variable that our code will go out and get when it is invoked. Step 10: Set your timeout value Incoming AWS Developers Associate exam question! By default a Lambda will allow 3 seconds to execute. For this example that simply isnt enough time. You can change this timeout value under the Basic Settings. The max is 15 minutes, but we should be able to get away with 3 minutes. If you were mean and set your function to pull a lot of ads, say 200, you would need more time. Note that Lambda charges you for memory and compute time. Step 10: Add Permission For Your Lambda Function Im sure at this point this article is feeling length like a Jane Austen novel, but think back to Step 3 where we created our Lambda function. I let you know that we would need to give Lambda some permissions to put stuff in an S3 bucket. Weve arrived at that time now. Heres how we do this (as concisely as possible): Step 11: Test Your Function Finally our Lambda is ready to be tested! So how do we test to see if this article hasnt been a bunch of baloney and this stuff actually works? Luckily Lambda has a test function built right in! If you look right next to the save button there is a Test button. Click that! It will ask you to create a test input, but we are going to skip that for now as its not needed when we are just pulling from an API or scraping. Basically if your Lambda is going to respond to a certain type of input you could give it a test input here. We dont need that, we just need it to run so just give it a name and click Create to test it out. Now that you have a Test event click Test again and your function should run! If the Gods are good you should see the universal sign for proper execution in the form of a giant green block. If we think back to what our function is doing, it is outputting data, in the form of a JSON blob to S3. So in theory we should now have data in our S3 bucket. If you go to your S3 bucket you should now have data in there! Huzzah! Lets move onto our last step: automation! Step 12: Automate! The beauty of Lambda is it can be triggered, or invoked, in a variety of ways. You can have Lambda functions happen anytime an object goes to a bucket, when another Lambda executes, or on a time interval. We are going to use the latter here an set it to run based on a time interval. Go to the top of your function and choose CloudWatch Events. This is how you can set a standard time interval event to fire our Lambda. CloudWatch is many things, but essentially its a monitoring tool for getting data about your AWS Environment. If you do want to get any of the AWS certifications you will need to learn about CloudWatch as it comes up in many of the exams. Go down to the bottom and keep it as Create a new rule. A rule is basically what governs when this event fires. We will creatively name it Daily and in the description, because the creative juices are now really flowing, we will say Run once a day. Select Schedule expression and put in rate(1 day) for the value. Note this can either use the rate() function or take a Cron scheduler. Cron is ugly so I am going to go with the rate, but use whatever floats your boat. And thats it! Click Add to apply your new, spiffy rule to your Lambda function and then click Save at the top to apply it to your Lambda function. Your Lambda function will now pull data daily and dump it to S3! Summary Lambda is wonderful for a developer or data scientist just looking to get data as theres no EC2, theres no databases, theres no hardware or anything else that is a pain. It lets us just simply write code and let AWS take care of all the annoying stuff. In this tutorial we created a Python function, that we could run locally on our machine, and set it up to run automatically once a day on AWS using Lambda. The first time you go through this it may be slow, but once you get used to Lambda you can start deploying stuff rapidly. AWS and Lambda can be a bit scary when you first get going but stick with it as it has tons of uses for a data scientist, especially for data acquisition with ease. As I mentioned if you found this article useful feel free to connect with me on LinkedIn and share your use cases for Lambda. 429 9 429 429 9 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Rachel Wiles Jun 27, 2019 Have we solved the problem of handwritingrecognition? Deep learning enables recognition of text to 99.73% accuracy. Where do we go from here? Deep learning has been widely used to recognise handwriting. In offline handwriting recognition, text is analysed after being written. The only information that can be analysed is the binary output of a character against a background. Although shifts towards digital stylus for writing gives more information, such as pen stroke 6 min read 6 min read Share your ideas with millions of readers. Pier Paolo Ippolito Jun 27, 2019 Member-only Understanding Cancer using Machine Learning Use of Machine Learning (ML) in Medicine is becoming more and more important. One application example can be Cancer Detection and Analysis. Note from Towards Data Sciences editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each authors contribution. You should not rely on an authors works without seeking professional advice. See our Reader Terms for details. 8 min read 8 min read Siddhant Bhambri Jun 27, 2019 Member-only Reinforcement Learning in Job Scheduling The thought of writing my first Medium article intrigued me since I believed this could give me a platform to share and contribute something I would like to read from fellow community members. Scientific publications and research papers can be so hard to comprehend by a layperson that it would 5 min read 5 min read George Seif Jun 27, 2019 Member-only Nvidias New Data Science Workstation a Review and Benchmark Data science is hot. I write a newsletter for learners called Mighty Knowledge. Each new issue contains links and key lessons from the very best content including quotes, books, articles, podcasts, and videos. Each and every one is picked out specifically for learning how to live a wiser, happier, and fuller life. Sign up 7 min read 7 min read Sascha Heyer Jun 27, 2019 Member-only Kubeflow Components and Pipelines Kubeflow is a large ecosystem, a stack of different open source tools ML tools. I want to keep things simple therefore we cover components, pipelines, and experiments. With pipelines and components, you get the basics that are required to build ML workflows. There are many more tools integrated into Kubeflow and I will cover them in the upcoming posts. Kubeflow is originated at Google. 6 min read 6 min read Shawn Cochran Data and music enthusiast More from Medium Prakshal Jain in Clairvoyant Blog Executing Snowflake Queries in Lambda Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Haq Nawaz in Dev Genius Develop AWS Glue ETL pipeline with Python shell Taimur Ijlal in Geek Culture How I passed the AWS Certified Solutions Architect Professional exam Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 1263,\n",
       "  'url': 'https://medium.com/datadriveninvestor/deploy-your-machine-learning-model-using-flask-made-easy-now-635d2f12c50c',\n",
       "  'title': 'Deploy your Machine Learning Model using Flask\\u200a—\\u200aMade Easy\\xa0Now',\n",
       "  'subtitle': '-',\n",
       "  'claps': 291,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Data Driven Investor',\n",
       "  'date': '2019-09-11',\n",
       "  'clap_prop': 0.00014374040187050224,\n",
       "  'text': \"DataDrivenInvestor Sep 11 2019 Listen Save Deploy Machine Learning Model using Flask Made Easy quick cheat sheet deploy ML model 10 min Youve built machine learning model using python turn super cool good prediction result would nice way share thing friend family simpler way easiest way deploying model using flask start learning machine learning initially running simple supervised learning model important easiest one understand use Regression model blog assumption person ha fair knowledge running python code familiar simple ML library like Sci-kit Learn P andas N umpy Different user different choice come type python IDE want use popular IDEs Spyder Jupyter Notebook PyCharm editor run python compiler DDI Editor 's Pick 5 Machine Learning Books Turn Novice Expert Data Driven booming growth Machine Learning industry ha brought renewed interest people Artificial www.datadriveninvestor.com want run line line check output idle go Jupyter notebook want run chunk code altogether Spyder better choice want whole project look organized ha lot file want make look structured way good go PyCharm IDE blog using Spyder framework IDE provided Anaconda Navigator use anything choice comfortable post building simple regression model using scikit-learns built-in LinearRegression function First let u look data look simple data set 4 attribute 8 row column numerical column need data pruning cleaning data convert categorical data numerical build classifier regression model data used clean ready applied classifier Normally first clean data like filling empty value data set converting categorical data numerical data normalization etc part data cleaning pruning blog purpose mainly serving purpose deploying model using flask considered simpler data already clean requires pre-processing class label last salary column predicting using model model built saving trained model using library called pickle Pickle Python pickle module used serializing de-serializing Python object structure object Python pickled saved disk pickle doe serializes object first writing file Pickling way convert python object list dict etc character stream idea character stream contains information necessary reconstruct object another python script Heres code building simple regression model saving model serializing using pickle run model trained model get saved directory project file stored local machine model ready saved time start building flask model Setting flask-app localhost Make sure flask installed `` pip install flask `` flask code explained three section 1.Loading saved model load model.pkl file initialize flask app 2 Redirecting API home page index.html initializing app tell Flask want web page load line app.route `` '' method `` GET '' '' POST '' tell Flask load home page website use app.route define function used redirect number URI respect API start flask server redirects index.html file default case 3 Redirecting API predict result salary Since POST request reading input value request.form.values input value variable int_features convert array use model predict round final prediction two decimal place click predict button index.html predicts salary value entered user 3 input pass variable `` output `` outputted model sends back index.html template prediction_text Let u look index.html file prediction_text placeholder see output prediction salary predicted model placed index.html file 4 Starting flask server call app.run run web page locally hosted computer following script start flask server localhost default port 5000 making http //127.0.0.1:5000/ paste http //127.0.0.1:5000/ http //localhost:5000 browser press enter see server working overall structure project project ha four major part 1. model.py contains code Machine Learning model predict employee salary based training data hiring.csv file 2. app.py contains Flask APIs receives employee detail GUI API call computes predicted value based model return 3. template folder contains HTML template index.html allow user enter employee detail display predicted employee salary 4. static folder contains cs folder style.css file ha styling required index.html file Running project 1 Ensure project home directory Create machine learning model running command command prompt `` python model.py `` would compile linear regression classifier build trained model creates serialized version model save file named model.pkl 2 Run app.py using command start Flask API `` python app.py `` default flask run port 5000 3 Navigate URL http //127.0.0.1:5000/ http //localhost:5000 able view homepage Enter valid numerical value 3 input box hit button Predict everything go well able see predicted salary value HTML page folk easy deploy application using flask hope liked post start working building application code project found GitHub page http //github.com/MaajidKhan/DeployMLModel-Flask Thanks reading keep eye next article deploying deep learning model using Flask 382 2 382 382 2 DataDrivenInvestor empowerment data knowledge expertise subscribe DDIntel http //ddintel.datadriveninvestor.com Heba Tarek Sep 11 2019 Data Structure Queue article talk Queue data structure know implement queue defined ordered list data Stack Programmer 's Guide Creating Eclectic Bookshelf Data Driven Investor Every developer bookshelf possible set text cabinet myriad every collectionwww.datadriveninvestor.com Also Queue defined First In-First list FIFO mean 3 min read 3 min read Share idea million reader Rick Cesari Sep 11 2019 Whats Story Key Building Brand person story tell Isak Dinesen Karen Blixen author Africa novel Heres truth human want need product service basic primal drive instinctive urge engage tool making hunting-gathering 5 min read 5 min read Benny Lim Sep 11 2019 Member-only Bad Apple Apple fan isnt excited latest release iPhone iPad Apple Watch Apple finally announced latest lineup iPhone last night iPhone 11 iPhone 11 Pro iPhone 11 Pro Max accompany released new 10.2 inch iPad replaces 9.7 4 min read 4 min read Chris Gardener Sep 11 2019 Might Need Change bought printer year ago cheap nothing fancy print-the-odd-boarding-pass-at-home printer cost 30 Business Moves Stomach Make Allowances Gut Feelings Data Driven Investor Gut feeling turn feeling science quite clear gut know youwww.datadriveninvestor.com specifically chose cheap one rarely need print thing actively try avoid fact didnt need anything special 4 min read 4 min read Alissa Knight Sep 11 2019 Member-only Undivided Fall Decoupling Network Segmentation Micro-Segmentation Software Defined Perimeter Introduction today law regulation even latest version PCI-DSS HIPAA HITECH make network segmentation micro-segmentation compulsory comply rule 5 min read 5 min read Maajid Khan Deep Learning Engineer| AI Machine Learning Data Science Enthusiast DataAnalytics Python Java GIT Hadoop C++ Blogger Sports DataStructures Medium Dharmaraj Deploying Deep Learning Model using Flask API Nate DiRenzo Towards Data Science Deploying ML Models using Streamlit Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Maria Gusarova Build Beautiful Machine Learning Web App Streamlit Python Tutorial Python code included Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'DataDrivenInvestor Sep 11, 2019 Listen Save Deploy your Machine Learning Model using Flask Made Easy Now A quick cheat sheet to deploy your ML model in 10 min. Youve built a machine learning model using python. It turns out to be super cool with good prediction results. It would be nice if theres a way you can share this thing with your friends and family. So, how can we do this? Is there a simpler way of doing this? The easiest way of doing it is by deploying the model using flask. When we start learning machine learning, initially we do it by running a simple supervised learning model. The most important and the easiest one to understand and use is a Regression model . This blog is under the assumption that the person has a fair knowledge of running python code and is familiar with simple ML libraries like Sci-kit Learn , P andas , N umpy . Different users have different choices when it comes to the type of python IDE they want to use. Some of the popular IDEs are Spyder , Jupyter Notebook, PyCharm editor or you can just run it on python compiler. DDI Editor\\'s Pick: 5 Machine Learning Books That Turn You from Novice to Expert | Data Driven The booming growth in the Machine Learning industry has brought renewed interest in people about Artificial www.datadriveninvestor.com When you want to run line by line and check the output, its idle to go with Jupyter notebooks. When you want to run a chunk of code altogether, Spyder is the better choice and if you want your whole project to look organized, has a lot of files and you want to make it look in a structured way, its good to go with PyCharm  IDE. In our blog, we will be using Spyder framework as an IDE provided by Anaconda Navigator . You can use anything of your choice and with which you are comfortable with. In this post, we will be building a simple regression model using scikit-learns built-in LinearRegression() function. First, let us have a look at how our data looks. Its a very simple data set with 4 attributes and 8 rows. All the columns are numerical columns, so we need to do any data pruning or cleaning up the data to convert categorical data into numerical before you build your classifier (regression model). This data used here is clean and is ready to be applied for a classifier. Normally, you have to first clean up your data like filling up empty values in your data set, converting your categorical data into numerical data, normalization, etc; as part of our data cleaning and pruning. But this blog purpose is mainly serving the purpose of deploying the model using flask, so we have considered simpler data which is already clean and requires no further pre-processing. Here, the class label will be the last salary column, which we will be predicting using our model. After our model is built, we will be saving our trained model using a library called pickle. Pickle Python pickle module is used for serializing and de-serializing a Python object structure. Any object in Python can be pickled so that it can be saved on disk. What pickle does is that it serializes the object first before writing it to file. Pickling is a way to convert a python object (list, dict, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another python script. Heres the code of building a simple regression model and then saving the model by serializing it using pickle. When you run this, your model is now trained and your model gets saved in the directory where your project files are stored in your local machine. Now that your model is ready and saved. Its time we start building a flask model. Setting up flask-app over the localhost: Make sure flask is installed. ``` pip install flask ``` The flask code can be explained in three sections: 1.Loading the saved model We load the model.pkl file and initialize the flask app. 2. Redirecting the API to the home page index.html After initializing the app, we have to tell Flask what we want to do when the web page loads. The line @app.route(\"/\", methods = [\"GET\",\"POST\"]) tells Flask what to do when we load the home page of our website. We use @app.route(/) to define functions which are used to redirect them into any number of URI with respect to the API. So, when you start the flask server, it redirects to index.html file by default in our case. 3. Redirecting the API to predict the result (salary) Since it is a POST request, it will be reading the input values from request.form.values(). Now that we have the input values in the variable int_features , we will convert it into an array and then use the model to predict it and round the final prediction to two decimal places. When we click on the predict button in index.html, it predicts the salary for the values entered by the user (3 inputs), then passes on the variable ``` output ``` outputted from the model and sends it back to index.html template as prediction_text . Let us have a look over index.html file: The {{ prediction_text }} placeholder you see here is where your output prediction(salary predicted) from the model will be placed in our index.html file. 4. Starting the flask server This will call app.run() and run our web page locally, hosted on your computer. The following script starts the flask server on localhost and default port (5000) making http://127.0.0.1:5000/ Just paste  http://127.0.0.1:5000/  (or)  http://localhost:5000  on browser and press enter to see the server working. The overall structure of the project: This project has four major parts: 1. model.py This contains code for our Machine Learning model to predict employee salaries based on training data in hiring.csv file. 2. app.py This contains Flask APIs that receives employee details through GUI or API calls, computes the predicted value based on our model and returns it. 3. template This folder contains the HTML template ( index.html ) to allow user to enter employee detail and displays the predicted employee salary. 4. static This folder contains the css folder with style.css file which has the styling required for out index.html file. Running the project: 1. Ensure that you are in the project home directory. Create the machine learning model by running below command from command prompt - ``` python model.py ``` This would compile our linear regression classifier, build the trained model and creates a serialized version of our model and save it as a file named model.pkl 2. Run app.py using below command to start Flask API ``` python app.py ``` By default, flask will run on port 5000. 3. Navigate to URL  http://127.0.0.1:5000/  (or)  http://localhost:5000 You should be able to view the homepage. Enter valid numerical values in all 3 input boxes and hit the button Predict . If everything goes well, you should be able to see the predicted salary value on the HTML page! That is all folks. It is that easy to deploy an application using flask. I hope you liked this post. Now, you can start working on this and building your own application. The code for this project can be found at my GitHub page: https://github.com/MaajidKhan/DeployMLModel-Flask Thanks for reading, and keep an eye out for my next article on deploying a deep learning model using Flask! 382 2 382 382 2 More from DataDrivenInvestor empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com Heba Tarek Sep 11, 2019 Data Structure Queue In this article, we will talk about the Queue as a data structure, and we will know how to implement it. A queue can be defined as an ordered list of data, the same as Stack. A Programmer\\'s Guide to Creating an Eclectic Bookshelf | Data Driven Investor Every developer should have a bookshelf. The possible set of texts in his cabinet are myriad, but not every collectionwww.datadriveninvestor.com Also, a Queue is defined as the First In-First Out list (FIFO), which means 3 min read 3 min read Share your ideas with millions of readers. Rick Cesari Sep 11, 2019 Whats Your Story? The Key to Building Your Brand To be a person is to have a story to tell. ~ Isak Dinesen (Karen Blixen), author of Out of Africa and other novels Heres the truth humans want and need products and services. Its a basic, primal drive, this instinctive urge to engage in tool making and hunting-gathering 5 min read 5 min read Benny Lim Sep 11, 2019 Member-only A Bad Apple Why this Apple fan isnt excited about the latest release of iPhone, iPad and Apple Watch Apple finally announced their latest lineup of the iPhone last night the iPhone 11, iPhone 11 Pro and iPhone 11 Pro Max. To accompany it, they released a new 10.2 inch iPad which replaces the 9.7 4 min read 4 min read Chris Gardener Sep 11, 2019 You Might Not Need to Change WHAT You Do, But WHO You Do it For I bought a printer a few years ago. Just a cheap, nothing fancy, print-the-odd-boarding-pass-at-home printer. It cost me about 30. A Business Moves On Its Stomach: How to Make Allowances for Gut Feelings | Data Driven Investor Gut feelings, as it turns out, are more than a feeling. The science is quite clear on it: Your gut knows more than youwww.datadriveninvestor.com I specifically chose a cheap one, as I rarely have the need to print things. I actively try to avoid it, in fact. So, I didnt need anything special. 4 min read 4 min read Alissa Knight Sep 11, 2019 Member-only Undivided We Fall: Decoupling Network Segmentation from Micro-Segmentation in the Software Defined Perimeter Introduction As of today, no laws or regulations, even the latest version of PCI-DSS, HIPAA, and HITECH, do not make network segmentation or micro-segmentation compulsory to comply with the rule. 5 min read 5 min read Maajid Khan Deep Learning Engineer| AI | Machine Learning | Data Science Enthusiast | DataAnalytics | Python | Java | GIT | Hadoop | C++ | Blogger | Sports | DataStructures More from Medium Dharmaraj Deploying Deep Learning Model using Flask API Nate DiRenzo in Towards Data Science Deploying ML Models using Streamlit Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Maria Gusarova Build A Beautiful Machine Learning Web App With Streamlit | Python Tutorial (Python code included) Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 876,\n",
       "  'url': 'https://towardsdatascience.com/making-3-easy-maps-with-python-fb7dfb1036',\n",
       "  'title': 'Making 3 Easy Maps With\\xa0Python',\n",
       "  'subtitle': 'Mapping Starbucks locations in Los Angeles\\xa0County',\n",
       "  'claps': 267,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-22',\n",
       "  'clap_prop': 0.00013188552336571856,\n",
       "  'text': 'Towards Data Science Apr 22 2019 Member-only Listen Save Making 3 Easy Maps Python working geospatial data Ive often needed visualize data natural way possible map Wouldnt nice could use Python quickly easily create interactive map data Well using data set Starbucks location Los Angeles County tutorial end introductory post able create Lets need get familiar data snapshot first row need worry latitude longitude zip field analysis needed Python import loading Starbucks data loading LA County GeoJSON Basic Point Map Creating basic point map Starbucks LA County latitude/longitude pair dataframe pretty straightforward Opening laPointMap.html see following map clearly see Starbucks LA County little red dot within LA County region course customize color shape dot Choropleth Map actually didnt know choropleth map wa playing map Python turn useful visualizing aggregated geospatial data choropleth map answer question zip code LA County Starbucks choropleth map essentially color zip code based value variable number Starbucks store case Lets first go basic code needed create one Since Ive personally found difficult understand get component place choropleth let take look separate visual see work choropleth need know color fill zip code 90001 example check panda dataframe referenced data field search key_on column zip code find column listed column numStores know need fill color corresponding 3 store zip code 90001 look GeoJSON referenced geo_path field find zip code 90001 associated shape info tell shape draw zip code map link ha necessary information Lets look resulting choropleth laChoropleth.html see come nice color bar top reference Heatmap choropleth map see area south LA County seem Starbucks store general get bit specific maybe figure lot Starbucks store small vicinity Basically let create heatmap highlight Starbucks hotspot LA County main parameter heatmap need trial error radius control big circle around Starbucks store blur control much circle blend together higher radius mean given Starbucks influence wider area higher blur mean two Starbucks away still contribute hotspot parameter Lets see picture heatmap laHeatmap.html Hmm cool kind seems like everything red Heatmaps might valuable zoom Lets zoom bit see identify specific hotspot Nice pretty clear map hotspot not-hotspots notspots map One stand Downtown Los Angeles understandably thats regret havent yet found way embed actual interactive version map Medium post wa able show screenshots strongly encourage run small bit code post play interactive map totally different experience hope post helped bit Ill see next one full notebook containing code used analysis found GitHub 333 1 333 333 1 Towards Data Science home data science Medium publication sharing concept idea code Anas Al-Masri Apr 22 2019 Member-only Creating Voice Recognition Calculator Android App Automatic Speech Recognition one famous topic Machine Learning nowadays lot newcomer every day investing time expertise post build simple end-to-end voice-activated calculator app take speech input return speech output 7 min read 7 min read Share idea million reader Nimish Mishra Apr 22 2019 Member-only Data analytics MODIS data MODIS Moderate Resolution Imaging Spectroradiometer imaging sensor board NASAs satellite Terra Aqua orbiting earth capturing imagery understand study various phenomenon earth surface extract whole bunch product image ranging geolocation cloud mask atmospheric product 10 min read 10 min read Marco Peixeiro Apr 22 2019 Member-only Introduction Natural Language Processing NLP Bias AI practical guide working natural data removing bias AI Natural language processing NLP field revolutionized deep learning voice assistant Gmails Smart Compose deep learning ha made possible machine understand u intuitive way course working natural data different working tabular 7 min read 7 min read Sambasivarao K Apr 22 2019 Member-only Region Interest Pooling technique made object detection faster viable major hurdle going image classification object detection fixed size input requirement network existing fully connected layer object detection proposal different shape need converting proposal fixed shape 4 min read 4 min read Avishalom Shalit Apr 22 2019 Innocent Interpretations Suspicious Statistics General Election Data Exploration part 1 Looking 2019 election Israel result appear weird sure evidence actual malfeasance simpler explanation favourite analogy statistic made Cassie Kozyrkov analogy English/American legal system Null Hypothesis presumption innocence leading acquittal rejection due presentation evidence guilt beyond reasonable doubt P value 5 min read 5 min read Ritvik Kharkar Software Engineer Mathemagician Home Chef Medium Nik Piepenbreier Better Programming Make Awesome Maps Python Geopandas Alex Mitrani Towards Data Science Creating Choropleth Maps Pythons Folium Library Vinod Dhole JovianData Science Machine Learning Interesting Heatmaps Using Python Folium MecSimCalc Level Coding Python tutorial use Folium publish interactive map Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 22, 2019 Member-only Listen Save Making 3 Easy Maps With Python In working with geospatial data, Ive often needed to visualize this data in the most natural way possible: a map. Wouldnt it be nice if we could use Python to quickly and easily create interactive maps of your data? Well be using a data set on all Starbucks locations in Los Angeles County for this tutorial. By the end of this introductory post you will be able to create: Lets do it! You will need To get familiar with the data, heres a snapshot of the first few rows: We only need to worry about the latitude, longitude, and zip fields for this analysis. Here are the needed Python imports, loading the Starbucks data, and loading the LA County GeoJSON: Basic Point Map Creating a basic point map of all Starbucks in LA County from the latitude/longitude pairs in our dataframe is pretty straightforward. Opening up laPointMap.html , we see the following map: We can clearly see all the Starbucks in LA County as little red dots within the LA County region. Of course, you can customize any of the colors and shapes of the dots. Choropleth Map I actually didnt know what a choropleth map was before playing with maps in Python but it turns out they are very useful in visualizing aggregated geospatial data. Our choropleth map will answer the question: Which zip codes in LA County have the most Starbucks? . The choropleth map essentially colors in each zip code based on the value of some other variable, the number of Starbucks stores in our case. Lets first go over the basic code needed to create one: Since Ive personally found it more difficult to understand how to get all the components in place for a choropleth, lets take a look at a separate visual to see how it works. The choropleth needs to know what color to fill in for zip code 90001, for example. It checks the pandas dataframe referenced by the data field, searches the key_on column for the zip code and finds the other column listed in columns which is numStores . It then knows that it needs to fill in the color corresponding to 3 stores in zip code 90001 . It then looks in the GeoJSON referenced by the geo_path field, and finds zip code 90001 and its associated shape info , which tells it which shape to draw for that zip code on the map. Through these links, it has all the necessary information. Lets look at the resulting choropleth in laChoropleth.html ! We see that it comes with a nice color bar at the top for reference. Heatmap In the choropleth map above, we see that areas in south LA County seem to have more Starbucks stores in general, but can we get a bit more specific? Can we maybe figure out where there are a lot of Starbucks stores in a small vicinity? Basically, lets create a heatmap to highlight Starbucks hotspots in LA County. The main parameters in the heatmap that need some trial and error are radius which controls how big the circles are around each Starbucks store and blur which controls how much the circles blend together. A higher radius means any given Starbucks influences a wider area and a higher blur means that two Starbucks which are further away from each other can still contribute to a hotspot. The parameters are up to you! Lets see a picture of our heatmap in laHeatmap.html. Hmm cool but it kind of seems like everything is red. Heatmaps might be more valuable if you zoom in. Lets zoom in a bit and see if we can identify more specific hotspots. Nice! Its pretty clear from the above map that we have some hotspots and some not-hotspots (notspots?) in the map. One that stands out is in Downtown Los Angeles (understandably). And thats about it! My only regret is that I havent yet found a way to embed the actual interactive versions of these maps in a Medium post so I was only able to show you screenshots. I strongly encourage you to run the small bits of code through this post to play with the interactive maps for yourself. Its a totally different experience. I hope this post helped a bit and Ill see you in the next one! The full notebook containing all code used in this analysis can be found at my GitHub here . 333 1 333 333 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Anas Al-Masri Apr 22, 2019 Member-only Creating A Voice Recognition Calculator Android App Automatic Speech Recognition is one of the most famous topics in Machine Learning nowadays, with a lot of newcomers every day investing their time and expertise into it. In this post, we will build a simple end-to-end voice-activated calculator app that takes speech as input and returns speech as output 7 min read 7 min read Share your ideas with millions of readers. Nimish Mishra Apr 22, 2019 Member-only Data analytics with MODIS data MODIS (or Moderate Resolution Imaging Spectroradiometer) is an imaging sensor on board the NASAs satellites Terra and Aqua, orbiting the earth capturing imagery to understand and study various phenomena on earths surface. You can extract a whole bunch of products from these images, ranging from geolocation, cloud mask, atmospheric products 10 min read 10 min read Marco Peixeiro Apr 22, 2019 Member-only Introduction to Natural Language Processing (NLP) and Bias in AI A practical guide to working with natural data and removing bias in AI Natural language processing (NLP) is a field that is being revolutionized by deep learning. From voice assistants to Gmails Smart Compose, deep learning has made it possible for machines to understand us in a more intuitive way. Of course, working with natural data is very different than working with tabular 7 min read 7 min read Sambasivarao. K Apr 22, 2019 Member-only Region of Interest Pooling The technique which made object detection faster and viable. The major hurdle for going from image classification to object detection is fixed size input requirement to the network because of existing fully connected layers. In object detection, each proposal will be of a different shape. So there is a need for converting all the proposals to fixed shape as 4 min read 4 min read Avishalom Shalit Apr 22, 2019 Innocent Interpretations for Some Suspicious Statistics; General Election Data Exploration. (part 1) Looking at the 2019 elections in Israel. Some results appear weird, sure, but is there evidence of actual malfeasance, or is there a simpler explanation? My favourite analogy in statistics(made by Cassie Kozyrkov) is the analogy to the English/American legal system. The Null Hypothesis being the presumption of innocence (leading to acquittal) and the rejection of that can only be due to the presentation of evidence of guilt beyond a reasonable doubt. the P value we 5 min read 5 min read Ritvik Kharkar Software Engineer Mathemagician Home Chef More from Medium Nik Piepenbreier in Better Programming Make Awesome Maps in Python and Geopandas Alex Mitrani in Towards Data Science Creating Choropleth Maps with Pythons Folium Library Vinod Dhole in JovianData Science and Machine Learning Interesting Heatmaps Using Python Folium MecSimCalc in Level Up Coding Python tutorial on how to use Folium to publish an interactive map Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 1753,\n",
       "  'url': 'https://towardsdatascience.com/boost-your-efficiency-and-process-excel-files-with-python-cae650c85d6c',\n",
       "  'title': 'Boost your efficiency and process Excel-files with\\xa0Python',\n",
       "  'subtitle': 'Load, transform, modify and save Excel-files…',\n",
       "  'claps': 261,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-11-01',\n",
       "  'clap_prop': 0.00012892180373952263,\n",
       "  'text': \"Towards Data Science Nov 1 2019 Member-only Listen Save Boost efficiency process Excel-files Python Load transform modify save Excel-files Python improve reporting process work data get touch excel Even dont use client colleague use Excel great made table calculation smaller data set always hated kind excel sheet one million row hundred column workbook slow tend crash calculation started use python handling large excel file offer another big advantage create code reproducible provide documentation well Lets jump Reading Excel-files Python file want process contains nearly 1 million row 16 column Python provides read_excel read Excel-files DataFrame see data look clean far column header seems wrong lot excel map contain headline information guide reader skip part define header row argument header= 1 specifies want use second row excel sheet header previous row skipped Use Pandas calculation typical question marketing department could much sale different country year finished calculation 86 ms. One big advantage processing Excel-files Python kind calculation much faster done Excel complex operation greater speed advantage Another requirement could sale department need data country grouped year category Since want supply data national market save calculation different worksheet Saving result Excel next step want save file Excel supply sale marketing department create pd.ExcelWriter object create different worksheet Easy isnt Lets look new created workbook see DataFrames saved correctly specified worksheet sent great result department receive mail next day ask formatting visualization Since transform kind data every month decide perform task Python well Formatting visualization add formatting visualization create writer object see first part code first example create writer object xlsxwriter give u access Excel-features chart formatting gain access feature need get workbook object workbook writer.book worksheet object worksheet writer.sheet 'Sales_Sums example perform modification first sheet add chart specify range data =Sales_Sums B 2 B 7 add worksheet cell A9 way add formatting sale data add 3 color scale range B2 B7 visually highlight low high value also adjust width first second column worksheet.set_column 0,1,30 also format column header sale data rename 2019 Sales Data last step save file result much better provides big advantage compared Excel reproduce exactly file next month one click Conclusion Python great processing Excel-files handle large file much easier create reproducible code provide documentation colleague also saw easily access advanced feature Python could automate whole reporting process reading Creating chart Using Pandas XlsxWriter create Excel chart introduction creation Excel file chart using Pandas XlsxWriter pandas-xlsxwriter-charts.readthedocs.io Excel report Pandas pivot Generating Excel Reports Pandas Pivot Table previous pivot table article described use panda pivot_table function combine present data pbpython.com Formatting Excel-files Python Tutorial 2 Adding formatting XLSX File previous section created simple spreadsheet using Python XlsxWriter module converted xlsxwriter.readthedocs.io enjoy Medium Towards Data Science didnt sign yet feel free use referral link join community 386 2 386 386 2 Towards Data Science home data science Medium publication sharing concept idea code Benedikt Droste Data Analyst management consultancy Interested data science web scraping storytelling http //www.linkedin.com/in/benedikt-droste-893b1b189/ Medium Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Anmol Tomar CodeX Say Goodbye Loops Python Welcome Vectorization Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Towards Data Science Nov 1, 2019 Member-only Listen Save Boost your efficiency and process Excel-files with Python Load, transform, modify and save Excel-files with Python to improve your reporting processes If you work with data, you will get in touch with excel. Even if you dont use it by yourself, your clients or colleagues will use it. Excel is great for what it is made: table calculation for smaller data sets. But I always hated the kind of excel sheets with one million rows and hundreds of columns. This workbooks are slow and tend to crash after some calculation. So I started to use python for handling large excel files which offer another big advantage: You create code which is reproducible and provide documentation as well. Lets jump in! Reading Excel-files with Python The file we want to process contains nearly 1 million rows and 16 columns: Python provides read_excel() to read Excel-files as DataFrame: As you can see the data looks clean so far but our column header seems to be wrong. A lot of excel maps contain headlines or other information to guide the reader. We can skip this parts and define a header row: The argument header=[1] specifies that we want to use the second row in the excel sheet as header. All previous rows are skipped. Use Pandas to do some calculations A typical question of the marketing department could be how much sales we had for the different countries in each year: We finished this calculation in 86 ms. One big advantage of processing Excel-files with Python is that any kind of calculation is much faster done as in Excel itself. The more complex the operations, the greater the speed advantages. Another requirement could be that the sales department needs the data for each country grouped by years and categories. Since they want to supply the data to the national markets, we have to save the calculations in different worksheets: Saving the results as Excel In a next step, we want to save our files as Excel again to supply it to the sales and marketing department. We will create a pd.ExcelWriter object and create the different worksheets: Easy, isnt it? Lets have a look at the new created workbook: As you can see our DataFrames were saved correctly to the specified worksheets. After we sent our great result to both departments, we receive a mail on the next day: They ask for some formatting and visualization. Since we have to transform this kind of data every month, we decide to perform the tasks in Python as well. Formatting and visualization To add formatting and visualization, we have to create a writer object again: As you can see the first part of the code is the same as in the first example. We create a writer object. xlsxwriter gives us access to Excel-features such as charts and formatting. To gain access to this features, we need to get the workbook object workbook = writer.book and the worksheet object worksheet = writer.sheet['Sales_Sums'] . In this example, we will perform the modifications on our first sheet. We add a chart, specify the range for the data ( =Sales_Sums!$B$2:$B$7' ) and add it to our worksheet in cell A9 . In the same way we add formatting for our sales data. We add a 3 color scale on the range B2:B7 to visually highlight low or high values. We also adjust the width of the first and second column worksheet.set_column(0,1,30) . We also format the column header for our sales data and rename it to 2019 Sales Data . In a last step, we save out the file: This result is much better and provides a big advantage compared to Excel. We can reproduce exactly the same file next month with one click. Conclusion Python is great for processing Excel-files. You can handle large files much easier, you create reproducible code and you provide a documentation for your colleagues. We also saw the we have easily access to advanced features of Python. You could automate your whole reporting process. Further reading: Creating charts: Using Pandas and XlsxWriter to create Excel charts An introduction to the creation of Excel files with charts using Pandas and XlsxWriter. pandas-xlsxwriter-charts.readthedocs.io Excel reports with Pandas pivots: Generating Excel Reports from a Pandas Pivot Table The previous pivot table article described how to use the pandas pivot_table function to combine and present data in an pbpython.com Formatting Excel-files with Python: Tutorial 2: Adding formatting to the XLSX File In the previous section we created a simple spreadsheet using Python and the XlsxWriter module. This converted the xlsxwriter.readthedocs.io If you enjoy Medium and Towards Data Science and didnt sign up yet, feel free to use my referral link to join the community. 386 2 386 386 2 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Benedikt Droste Data Analyst in a management consultancy | Interested in data science, web scraping & storytelling | https://www.linkedin.com/in/benedikt-droste-893b1b189/ More from Medium Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Anmol Tomar in CodeX Say Goodbye to Loops in Python, and Welcome Vectorization! Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 3193,\n",
       "  'url': 'https://towardsdatascience.com/cryptocurrency-price-prediction-using-lstms-tensorflow-for-hackers-part-iii-264fcdbccd3f',\n",
       "  'title': 'Cryptocurrency price prediction using LSTMs | TensorFlow for Hackers (Part\\xa0III)',\n",
       "  'subtitle': 'Predict Bitcoin\\xa0price…',\n",
       "  'claps': 242,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-25',\n",
       "  'clap_prop': 0.00011953669158990221,\n",
       "  'text': 'Towards Data Science Apr 25 2019 Listen Save Cryptocurrency price prediction using LSTMs TensorFlow Hackers Part III Predict Bitcoin price using LSTM Deep Neural Network TensorFlow 2 TL DR Build train Bidirectional LSTM Deep Neural Network Time Series prediction TensorFlow 2 Use model predict future Bitcoin price Complete source code Google Colaboratory Notebook time youll build basic Deep Neural Network model predict Bitcoin price based historical data use model however want carry risk action might asking something along line still get rich cryptocurrency course answer fairly nuanced well look might build model help along crazy journey might money problem one possible solution plan Data Overview dataset come Yahoo Finance cover available time writing data Bitcoin-USD price Lets load Pandas dataframe Note sort data Date case sample data interested total 3201 data point representing Bitcoin-USD price 3201 day ~9 year interested predicting closing price future date course Bitcoin made people really rich went really poor question remains though happen Lets look one possible model think Shall dataset somewhat different previous example data sorted time recorded equal interval 1 day sequence data called Time Series Time Series Temporal datasets quite common practice energy consumption expenditure calorie calorie weather change stock market analytics gathered user product/app even possibly love heart produce Time Series might interested plethora property regarding Time Series stationarity seasonality autocorrelation well known Autocorrelation correlation data point separated interval known lag Seasonality refers presence cyclical pattern interval doesnt every spring time series said stationarity ha constant mean variance Also covariance independent time One obvious question might ask watching Time Series data value current time step affect next one a.k.a Time Series forecasting many approach use purpose well build Deep Neural Network doe forecasting u use predict future Bitcoin price Modeling model weve built far allow operating sequence data Fortunately use special class Neural Network model known Recurrent Neural Networks RNNs purpose RNNs allow using output model new input model process repeated indefinitely One serious limitation RNNs inability capturing long-term dependency sequence e.g dependency today price 2 week ago One way handle situation using Long short-term memory LSTM variant RNN default LSTM behavior remembering information prolonged period time Lets see use LSTM Keras Data preprocessing First going squish price data range 0 1 Recall help optimization algorithm converge faster going use MinMaxScaler scikit learn scaler expects data shaped x add dummy dimension using reshape applying Lets also remove NaNs since model wont able handle well use isnan mask filter NaN value reshape data removing NaNs Making sequence LSTMs expect data 3 dimension need split data sequence preset length shape want obtain also want save data testing Lets build sequence process building sequence work creating sequence specified length position 0 shift one position right e.g 1 create another sequence process repeated possible position used save 5 data testing datasets look like model use 2945 sequence representing 99 day Bitcoin price change training going predict price 156 day future model POV Building LSTM model creating 3 layer LSTM Recurrent Neural Network use Dropout rate 20 combat overfitting training might wondering deal Bidirectional CuDNNLSTM Bidirectional RNNs allows train sequence data forward backward reversed direction practice approach work well LSTMs CuDNNLSTM Fast LSTM implementation backed cuDNN Personally think good example leaky abstraction crazy fast output layer ha single neuron predicted Bitcoin price use Linear activation function activation proportional input Training Well use Mean Squared Error loss function Adam optimizer Note want shuffle training data since using Time Series lightning-fast training thanks Google free T4 GPUs following training loss Predicting Bitcoin price Lets make model predict Bitcoin price use scaler invert transformation price longer scaled 0 1 range rather succinct model seems well test data Care try currency Conclusion Congratulations built Bidirectional LSTM Recurrent Neural Network TensorFlow 2 model preprocessing pipeline pretty generic used datasets Complete source code Google Colaboratory Notebook One interesting direction future investigation might analyzing correlation different cryptocurrencies would affect performance model Originally published http //www.curiousily.com 356 3 356 356 3 Towards Data Science home data science Medium publication sharing concept idea code Ahmad Tanehkar Apr 25 2019 Lets Apply Machine Learning Behavioral Economics Currently Machine Learning ML well applied behavioral economics social science lack use come unfamiliarity different approach 5 min read 5 min read Share idea million reader Ashok Chilakapati Apr 25 2019 Concept Drift Model Decay Machine Learning Concept drift drift label time essentially data lead divergence decision boundary new data model built earlier data/labels Scoring randomly sampled new data detect drift allowing u trigger expensive 7 min read 7 min read Ren Bremer Apr 25 2019 use Python feature data analytics project Python tutorial Azure using OO NumPy panda SQL PySpark 1 Introduction lot company moving cloud consider tooling shall used data analytics On-premises company mostly use propriety software advanced analytics BI reporting However tooling may logical choice cloud environment 9 min read 9 min read Ryan Burge Apr 25 2019 Member-only Religious Composition Two Major Parties make lot data visualization time pretty good idea result calculation going look like write code fact result obvious dont even post anywhere 4 min read 4 min read Ashutosh Singh Apr 25 2019 Member-only Classifying Products Banned Approved using Text Mining- Part II part explain optimize existing Machine Learning model Part deployment ML model using Flask previous article series discussed business problem shown train model using fastText classification Banned Approved product based information like Product Name product description specification 5 min read 5 min read Venelin Valkov Adventures Artificial Intelligence http //curiousily.com Medium Andrew datascience Towards Data Science Time series prediction LSTM Tensorflow Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Dmytro Sazonov Trading Data Analysis Trading algorithm work Ren Heinrich DataDrivenInvestor analyzed 200 DeFi Projects Found Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 25, 2019 Listen Save Cryptocurrency price prediction using LSTMs | TensorFlow for Hackers (Part III) Predict Bitcoin price using LSTM Deep Neural Network in TensorFlow 2 TL;DR Build and train an Bidirectional LSTM Deep Neural Network for Time Series prediction in TensorFlow 2. Use the model to predict the future Bitcoin price. Complete source code in Google Colaboratory Notebook This time youll build a basic Deep Neural Network model to predict Bitcoin price based on historical data. You can use the model however you want, but you carry the risk for your actions . You might be asking yourself something along the lines: Can I still get rich with cryptocurrency? Of course, the answer is fairly nuanced. Here, well have a look at how you might build a model to help you along the crazy journey. Or you might be having money problems? Here is one possible solution: Here is the plan: Data Overview Our dataset comes from Yahoo! Finance and covers all available (at the time of this writing) data on Bitcoin-USD price. Lets load it into a Pandas dataframe: Note that we sort the data by Date just in case. Here is a sample of the data were interested in: We have a total of 3201 data points representing Bitcoin-USD price for 3201 days (~9 years). Were interested in predicting the closing price for future dates. Of course, Bitcoin made some people really rich and for some went really poor. The question remains though, will it happen again? Lets have a look at what one possible model thinks about that. Shall we? Our dataset is somewhat different from our previous examples. The data is sorted by time and recorded at equal intervals (1 day). Such a sequence of data is called  Time Series  . Time Series Temporal datasets are quite common in practice. Your energy consumption and expenditure (calories in, calories out), weather changes, stock market, analytics gathered from the users for your product/app and even your (possibly in love) heart produce Time Series . You might be interested in a plethora of properties regarding your Time Series stationarity , seasonality and autocorrelation are some of the most well known. Autocorrelation is the correlation of data points separated by some interval (known as lag). Seasonality refers to the presence of some cyclical pattern at some interval (no, it doesnt have to be every spring). A time series is said to be stationarity if it has constant mean and variance. Also, the covariance is independent of the time. One obvious question you might ask yourself while watching at Time Series data is: Does the value of the current time step affects the next one? a.k.a. Time Series forecasting . There are many approaches that you can use for this purpose. But well build a Deep Neural Network that does some forecasting for us and use it to predict future Bitcoin price. Modeling All models weve built so far do not allow for operating on sequence data. Fortunately, we can use a special class of Neural Network models known as Recurrent Neural Networks (RNNs) just for this purpose. RNNs allow using the output from the model as a new input for the same model. The process can be repeated indefinitely. One serious limitation of RNNs is the inability of capturing long-term dependencies in a sequence (e.g. Is there a dependency between today`s price and that 2 weeks ago?). One way to handle the situation is by using an Long short-term memory (LSTM) variant of RNN . The default LSTM behavior is remembering information for prolonged periods of time. Lets see how you can use LSTM in Keras. Data preprocessing First, were going to squish our price data in the range [0, 1]. Recall that this will help our optimization algorithm converge faster: Were going to use the MinMaxScaler from scikit learn : The scaler expects the data to be shaped as (x, y), so we add a dummy dimension using reshape before applying it. Lets also remove NaNs since our model wont be able to handle them well: We use isnan as a mask to filter out NaN values. Again we reshape the data after removing the NaNs. Making sequences LSTMs expect the data to be in 3 dimensions. We need to split the data into sequences of some preset length. The shape we want to obtain is: We also want to save some data for testing. Lets build some sequences: The process of building sequences works by creating a sequence of a specified length at position 0. Then we shift one position to the right (e.g. 1) and create another sequence. The process is repeated until all possible positions are used. We save 5% of the data for testing. The datasets look like this: Our model will use 2945 sequences representing 99 days of Bitcoin price changes each for training. Were going to predict the price for 156 days in the future (from our model POV). Building LSTM model Were creating a 3 layer LSTM Recurrent Neural Network. We use Dropout with a rate of 20% to combat overfitting during training: You might be wondering about what the deal with Bidirectional and CuDNNLSTM is? Bidirectional RNNs allows you to train on the sequence data in forward and backward (reversed) direction. In practice, this approach works well with LSTMs. CuDNNLSTM is a Fast LSTM implementation backed by cuDNN. Personally, I think it is a good example of leaky abstraction, but it is crazy fast! Our output layer has a single neuron (predicted Bitcoin price). We use Linear activation function which activation is proportional to the input. Training Well use Mean Squared Error as a loss function and Adam optimizer. Note that we do not want to shuffle the training data since were using Time Series. After a lightning-fast training (thanks Google for the free T4 GPUs), we have the following training loss: Predicting Bitcoin price Lets make our model predict Bitcoin prices! We can use our scaler to invert the transformation we did so the prices are no longer scaled in the [0, 1] range. Our rather succinct model seems to do well on the test data. Care to try it on other currencies? Conclusion Congratulations, you just built a Bidirectional LSTM Recurrent Neural Network in TensorFlow 2. Our model (and preprocessing pipeline) is pretty generic and can be used for other datasets. Complete source code in Google Colaboratory Notebook One interesting direction of future investigation might be analyzing the correlation between different cryptocurrencies and how would that affect the performance of our model. Originally published at  https://www.curiousily.com  . 356 3 356 356 3 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ahmad Tanehkar Apr 25, 2019 Lets Apply Machine Learning in Behavioral Economics Currently, Machine Learning (ML) is not well applied in behavioral economics or social science, and this lack of use comes from unfamiliarity with this different approach. 5 min read 5 min read Share your ideas with millions of readers. Ashok Chilakapati Apr 25, 2019 Concept Drift and Model Decay in Machine Learning Concept drift is a drift of labels with time for the essentially the same data. It leads to the divergence of decision boundary for new data from that of a model built from earlier data/labels. Scoring randomly sampled new data can detect the drift allowing us to trigger the expensive 7 min read 7 min read Ren Bremer Apr 25, 2019 How to use Python features in your data analytics project Python tutorial in Azure using OO, NumPy, pandas, SQL, PySpark 1. Introduction A lot of companies are moving to cloud and consider what tooling shall be used for data analytics. On-premises, companies mostly use propriety software for advanced analytics, BI and reporting. However, this tooling may not be the most logical choice in a cloud environment. 9 min read 9 min read Ryan Burge Apr 25, 2019 Member-only The Religious Composition of the Two Major Parties I make a lot of data visualizations. Most of the time I have a pretty good idea of what the results of the calculations are going to look like before I write the code. In fact, some of the results are so obvious that I dont even post them anywhere 4 min read 4 min read Ashutosh Singh Apr 25, 2019 Member-only Classifying Products as Banned Or Approved using Text Mining- Part II In this part, we will explain how to optimize the existing Machine Learning model in Part I and the deployment of this ML model using Flask. In the previous article of this series, We have discussed the business problem, shown how to train the model using fastText and classification of Banned or Approved products based on information like(Product Name, product description, and specifications). 5 min read 5 min read Venelin Valkov Adventures in Artificial Intelligence https://curiousily.com More from Medium Andrew D #datascience in Towards Data Science Time series prediction with LSTM in Tensorflow Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Dmytro Sazonov in Trading Data Analysis Trading algorithm that !works Ren & Heinrich in DataDrivenInvestor I analyzed 200 DeFi Projects. Here Is What I Found Out. Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5469,\n",
       "  'url': 'https://towardsdatascience.com/is-artificial-intelligence-the-next-big-thing-in-hollywood-535688dfe388',\n",
       "  'title': 'Is Artificial Intelligence the next big thing in Hollywood?',\n",
       "  'subtitle': '-',\n",
       "  'claps': 232,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 0.00011459715887957567,\n",
       "  'text': \"Towards Data Science Mar 31 2019 Member-only Listen Save Artificial Intelligence next big thing Hollywood Movies captured imagination people ever since came limelight Right first motion picture late 1880s upcoming latest sci-fi blockbuster cinema ha become medium love joy passion movie lover Almost every country across globe ha movie industry film industry USA ha distinction biggest famous recognizable industry world today none Hollywood Hollywood universally adored moviegoer due ability churn high-quality cinema best technology Computer-generated imagery visual effect always hallmark Hollywood industry well-known using best technician equipment create breath-taking cinema last decade Hollywood ha increasingly investing experimenting innovative technology Artificial Intelligence AI AI machine learning algorithm achieve task quickly scale capable accomplishing variety thing real-time require dedicated team people used effectively new age technology bring best edits best performance advanced visual effect possible today Artificial Intelligence Game Changer Movie production studio Hollywood nowadays looking hire engineer good training AI-based deep learning machine learning algorithm could visual effect specialist work example work making digital character look realistic smoothing effect etc done easily intelligent algorithm Advanced algorithm capability automatically render advanced visual effect AI therefore help creative artist focus effort important work waste valuable time meticulously editing effect Last year superhero blockbuster Avengers Infinity War American actor Josh Brolin play titular character super villain Thanos visual effect Thanos on-screen appearance wa done well looked real life-like visual effect team accomplished feat wa using AI algorithm track Brolins facial expression included minute detail wrinkle using another algorithm map face render body Thanos Using machine learning algorithm whole process could done real-time wa done without AI would taken least week team get result using face mapping swapping technology Disney recently came robot acrobat could perform well human acrobat robot acrobat could edited using AI made look like human counterpart Thus actor could focus part job le dangerous early 2000s Peter Jacksons Lord Rings series used AI-driven software generate huge army seen three movie 2016 20th Century Fox partnered IBM Research develop movie trailer using AI movie Morgan IBMs AI cognitive system Watson wa used create horror movie trailer kept audience edge seat Apart editing performance side filmmaking process AI may soon used decide whether film made According Variety Belgian company called Scriptbook ha come AI-based algorithm analyse screenplay predict whether movie commercially successful help production studio make calculated decision movie need made Using AI Hyper-personalized User Targeting Movie studio typically spend third budget marketing movie could spend marketing money wisely targeting right audience would make efficient utilization budget Using high-end AI tool possible deliver hyper-personalized content customer per preference American movie production company Fox formed partnership Google Cloud develop Merlin AI-based learning program main objective Merlin analyse trailer identify basic pattern audience preference different type movie Online streaming giant Netflix us AI program recommendation subscriber also generate targeted mini trailer user click doe mouse-hover show consider watching kind personalized relationship wasnt available traditional Hollywood company company product removed cut direct customer relationship Studios dont know exactly watching movie TV show situation however slowly changing Subscription-based movie ticketing service MoviePass trying emulate Netflix style data-driven model around theatrical movie business Online video streaming coupled AI enables movie production company build direct-to-consumer relationship long-term basis powerful aspect Hollywood near future Disney one big movie production company understood immense value model brings ha fact scrapped deal Netflix launching subscription-based video on-demand service next year Conclusion AI machine learning yet widely adopted movie company around world due fact dont fully understand Filmmakers understand AI already started using element machine learning deep learning specific area editing performance movie industry Hollywood part world growing fast pace year year According Box office Mojo number movie released Hollywood last year totalled whopping 873 number going keep increasing future Therefore important movie studio cater every moviegoer truly understanding need expectation Success going knock door company able achieve feat technology like AI play crucial role building robust customer relationship Originally published www.greatlearning.in April 1 2019 231 231 231 Towards Data Science home data science Medium publication sharing concept idea code Guy Tsror Mar 31 2019 Member-only romantic side data science Analyzing relationship year worth text message wa supposed private project one thought would upload private Google Colab notebook share whomever relevant one person referred OJ lightbulb clean share 11 min read 11 min read Share idea million reader Salma Ghoneim Mar 31 2019 Member-only Object detection via color-based image segmentation using python tutorial contouring using python OpenCV Getting started already jupyter notebook IDE run python OpenCV installed skip Execution Tools hero today Anaconda free open-source distribution help installing different package sort mess isolated environment Wikipedias telling u 7 min read 7 min read Sahil Dhankhad Mar 31 2019 Member-only Forget APIs Python Scraping Using Beautiful Soup Import Data File web Part 2 APIs always every website Beautiful Soup going stay forever collect kind data website Today Data play critical role every industry data coming internet company 8 min read 8 min read Andrew Donaldson Mar 31 2019 Member-only Dont Fear Robots Impressive tech important remember Boston Dynamics video trailer movie isnt yet made Every time one Boston Dynamics video hit met mix amazement prediction doom hand new robot overlord impressive also well choreographed staged Impressive tech important 4 min read 4 min read Jasmine Vasandani Mar 31 2019 Member-only 10 Steps Teaching Data Science Well resource data science instructor increase demand data scientist ha parallel growth training program within company educational institution teach people data science addition focusing meeting job demand need resource instructor facilitating data science classroom Heres 4 min read 4 min read Manu Siddharth Jha Medium Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Luay Rahil ILLUMINATION Elon Musk Eliminated Remote Work Working Home `` n't Work '' Akshad Singi Better Humans 4 Unsexy One-Minute Habits Save 30+ Hours Every Week Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Mar 31, 2019 Member-only Listen Save Is Artificial Intelligence the next big thing in Hollywood? Movies have captured the imagination of people ever since they came into the limelight. Right from the first motion picture in the late 1880s to the upcoming latest sci-fi blockbuster, cinema has become a medium of love, joy, and passion for all movie lovers. Almost every country across the globe has its own movie industry. The film industry of USA has the distinction of being the biggest, most famous, and most recognizable industry in the world today. It is none other than Hollywood. Hollywood is universally adored by moviegoers due to its ability to churn out high-quality cinema with the best technology. Computer-generated imagery and visual effects were always a hallmark of Hollywood. The industry is well-known for using the best technicians and equipment to create breath-taking cinema. Over the last decade, Hollywood has been increasingly investing and experimenting with innovative technologies such as Artificial Intelligence (AI). AI and machine learning algorithms can achieve tasks quickly and at scale. They are capable of accomplishing a variety of things in real-time that require dedicated teams of people. If used effectively, these new age technologies can bring out the best edits, the best performances, and the most advanced visual effects possible today. Artificial Intelligence The Game Changer Movie production studios in Hollywood are nowadays looking to hire engineers who are good at training AI-based deep learning and machine learning algorithms that could do a visual effects specialists work. For example, work such as making a digital character look realistic, or smoothing out an effect, etc. can be done easily by intelligent algorithms. Advanced algorithms have the capability to automatically render advanced visual effects. AI, therefore, helps creative artists focus their efforts in doing other important work than waste their valuable time in meticulously editing an effect. Last years superhero blockbuster, Avengers Infinity War, had American actor Josh Brolin play the titular character of the super villain, Thanos. The visual effects of Thanos on-screen appearance was done so well that it looked very real and life-like. How the visual effects team accomplished this feat was by using an AI algorithm to track Brolins facial expressions which included minute details such as his wrinkles, and then using another algorithm to map his face renders on to the body of Thanos. Using machine learning algorithms, the whole process could be done in real-time. If this was done without AI, it would have taken at least a few weeks for the team to get the same results using face mapping and swapping technology. Disney recently came up with robot acrobats that could perform as well as human acrobats. The robot acrobats could be edited using AI and made to look like their human counterparts. Thus, the actors could focus on parts of the job which is less dangerous. In the early 2000s, Peter Jacksons Lord of the Rings series used AI-driven software to generate the huge armies seen in all three movies. In 2016, 20th Century Fox partnered with IBM Research to develop a movie trailer using AI for the movie Morgan. IBMs AI cognitive system, Watson, was used to create a horror movie trailer that kept audiences on the edge of their seats. Apart from the editing and performance sides of the filmmaking process, AI may soon be used to decide whether a film is to be made or not. According to Variety, a Belgian company called Scriptbook has come up with AI-based algorithms that can analyse a screenplay and predict whether or not a movie will be commercially successful. This will help production studios to make calculated decisions on which movies need to be made. Using AI for Hyper-personalized User Targeting Movie studios typically spend over a third of their budget on marketing their movies. If they could spend their marketing money wisely by targeting the right audience, it would make a more efficient utilization of their budget. Using high-end AI tools, its possible to deliver hyper-personalized content to customers as per their preferences. American movie production company, Fox, formed a partnership with Google Cloud to develop Merlin, an AI-based learning program. The main objective of Merlin is to analyse trailers and identify the basic patterns in audiences preferences for different types of movies. Online streaming giant, Netflix, uses AI for program recommendations for its subscribers and also to generate targeted mini trailers when a user clicks or does a mouse-hover on a show to consider watching it. This kind of a personalized relationship wasnt available with traditional Hollywood companies. Most of these companies products were removed or have been cut off from direct customer relationships. Studios dont know exactly who is watching their movies and TV shows. This situation is however slowly changing. Subscription-based movie ticketing service, MoviePass, is trying to emulate a Netflix style data-driven model around the theatrical movie business. Online video streaming coupled with AI, enables movie production companies to build direct-to-consumer relationships on a long-term basis. This will be a powerful aspect of Hollywood in the near future. Disney is one of the big movie production companies that have understood the immense value that this model brings. It has, in fact, scrapped its deal with Netflix and launching its own subscription-based video on-demand service next year. Conclusion AI and machine learning are yet to be widely adopted by movie companies around the world. This is due to the fact that they dont fully understand them. Filmmakers who do understand AI have already started using elements of machine learning and deep learning in very specific areas such as editing and performance. The movie industry in Hollywood and other parts of the world is growing at a fast pace year after year. According to Box office Mojo, the number of movies released in Hollywood last year totalled to a whopping 873. This number is only going to keep increasing in the future. Therefore, its important for movie studios to cater to every moviegoer by truly understanding their needs and expectations. Success is going to knock at the door of those companies who are able to achieve this feat. This is where technologies like AI will play a crucial role in building robust customer relationships. Originally published at  www.greatlearning.in  on April 1, 2019. 231 231 231 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Guy Tsror Mar 31, 2019 Member-only The romantic side of data science: Analyzing a relationship through a year worth of text messages This was supposed to be a private project, one I thought I would upload to a private Google Colab notebook, and share with just whomever it is relevant for (which is, one other person, referred to here as OJ). But then, a lightbulb why not clean it up and share 11 min read 11 min read Share your ideas with millions of readers. Salma Ghoneim Mar 31, 2019 Member-only Object detection via color-based image segmentation using python A tutorial on contouring using python & OpenCV. Getting started If you already have jupyter notebook or an IDE with which you can run python & OpenCV installed, just skip to Execution. Tools Our hero today is Anaconda. a free open-source distribution that helps with installing different packages & sorts out their messes into isolated environments. What Wikipedias telling us about 7 min read 7 min read Sahil Dhankhad Mar 31, 2019 Member-only Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2 APIs are not always there for you for every website, but Beautiful Soup is going to stay with you forever to collect any kind of data from any website. Today, Data play a critical role in every industry. And most of this data is coming from the internet. Most company 8 min read 8 min read Andrew Donaldson Mar 31, 2019 Member-only Dont Fear the Robots Impressive as the tech here is, it is important to remember that the Boston Dynamics videos are trailers for a movie that isnt yet made. Every time one of these Boston Dynamics videos hit, they are met with a mix of amazement and predictions of doom at the hands of our new robot overlords. It is impressive. It is also very well choreographed and staged. Impressive as the tech here is, it is important 4 min read 4 min read Jasmine Vasandani Mar 31, 2019 Member-only 10 Steps to Teaching Data Science Well A resource for data science instructors. With the increase in demand for data scientists, there has been a parallel growth in training programs within companies and educational institutions that teach people data science. In addition to focusing on meeting the job demand, there need to be more resources for instructors facilitating data science classrooms. Heres a 4 min read 4 min read Manu Siddharth Jha More from Medium Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Luay Rahil in ILLUMINATION Elon Musk Eliminated Remote Work Because Working From Home \"Doesn\\'t Work\" Akshad Singi in Better Humans 4 Unsexy One-Minute Habits That Save Me 30+ Hours Every Week Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 767,\n",
       "  'url': 'https://medium.com/swlh/flutter-custom-tab-indicator-for-tabbar-d72bbc6c9d0c',\n",
       "  'title': 'Flutter: Custom tab indicator for\\xa0TabBar',\n",
       "  'subtitle': '-',\n",
       "  'claps': 215,\n",
       "  'responses': 3.0,\n",
       "  'reading_time': 3,\n",
       "  'publication': 'The Startup',\n",
       "  'date': '2019-12-30',\n",
       "  'clap_prop': 0.00010619995327202056,\n",
       "  'text': \"Startup Dec 30 2019 Member-only Listen Save Flutter Custom tab indicator TabBar Welcome tutorial create custom tab indicator TabBar Flutter connect Instagram Lets start seeing end goal- Customization Options implement Customize tab bar three way 3 Implementing custom decoration TabBar indicator setting indicator property TabBar SPOILER Third option best one dont re-invent wheel 2nd option limited Lets Start 1 Add TabBar talk implementing TabBar Flutter need help implementing TabBar flutter check documentation flutter team Work tab Working tab common pattern apps follow Material Designguidelines Flutter includes convenient flutter.dev TabBar class API doc TabBar class material library Dart programming language api.flutter.dev starter code- 2 Implementing custom tab indicator Lets see done code snippet- 4 Overriding createBoxPainter onChanged method return BoxPainter paint decoration case _painter parameter omitted chance painter change 3 Implementing custom box painter create custom painter type BoxPainter Lets see added code snippet- 4 Overriding paint method draw circle based radius paint position 4 Updating indicator let update TabBar indicator Lets check output- Check project gist Thank staying till end flutter spinner blogs- Custom Spinner Tooltip Flutter Welcome tutorial building Custom Spinner tool tip medium.com Flutter Custom Spinner Thumb Welcome tutorial Custom Spinner Thumb medium.com Flutter Custom Spinner Thumb -Part 2 Welcome tutorial building custom spinner thumb medium.com posting flutter stay tuned Gursheesh Singh Chandigarh Chandigarh India Professional Profile LinkedIn View Gursheesh Singh 's professional profile LinkedIn LinkedIn world 's largest business network helping www.linkedin.com Gursheesh Singh Welcome back Instagram Sign check friend family interest capturing sharing www.instagram.com Gursheesh singh latest Tweets Gursheesh singh GursheeshChawla Business Solution developer Flutter developer Android twitter.com 486 5 486 486 5 Sign Top 5 Stories Startup Get smarter building thing Join 176,621+ others receive Startup 's top 5 story tool idea book delivered straight inbox week Take look Emails sent shadow_kelvin777 ymail.com Startup Get smarter building thing Follow join Startups +8 million monthly reader +760K follower Nick Wolny Dec 30 2019 Member-only Learned 3-Step System Organizing Creativity Top Music Conservatory Apply framework career skill want master college education get shaft day Student loan created financial crisis underemployment working job unrelated field study snarl young people year graduation rise hustle culture made u grind day night spawn burnout epidemic 5 min read 5 min read Share idea million reader Josh White Dec 30 2019 Member-only Best Writers Better Everyone Else give possession story reader Theres saying work art thats appropriate idea Heres saying paraphrased take writer reader write story finish art writing isnt complete clear comprehensive reader two may ask Heres 5 min read 5 min read Andrew Scott Dec 30 2019 Member-only Welcome Python Meet Dunders quick introduction several __magic__ method python Dunder Dunder method name common pronunciation python built-in method name start end double underscore Since Dunder easier say double underscore name stuck method also sometimes referred Special Methods Magic Methods However thing magic 4 min read 4 min read R. Shawn McBride Dec 30 2019 Member-only Keep Right Leaders Ideas Private Businesses Leadership valuable explored valuable leadership recent article keeping right leader tricky Good leader case option go elsewhere since survival enterprise could tied keeping right people 7 min read 7 min read Ajanthan Mani Dec 30 2019 Member-only Agile Personal Growth reading lot lot article personal life improvement year gave different useful way personal growth practicing way article suggested quite frankly gave day 4 min read 4 min read TheBoringDeveloper Flutter enthusiast Medium Maneesha Erandi Flutter Bottom Navigation Bar Animation Jackie Moraa Lottie Animations Flutter Part 1 Nabin Dhakal CodeX Routes Navigation Flutter Kaushiki Kumari Flutter Popular Packages Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"The Startup Dec 30, 2019 Member-only Listen Save Flutter: Custom tab indicator for TabBar Welcome to this tutorial to create a custom tab indicator for TabBar in Flutter. You can connect with me on Instagram Lets start by seeing our end goal- Customization Options We can implement Customize tab bar in three ways: 3. Implementing a custom decoration for TabBar indicator and then setting it to indicator property of TabBar SPOILER Third option is the best one as we dont have to re-invent the wheel and the 2nd option is limited. Lets Start 1. Add TabBar: Here we will not talk about implementing TabBar in Flutter. If you need help implementing TabBar in flutter, check out this documentation from flutter team: Work with tabs Working with tabs is a common pattern in apps that follow the Material Designguidelines. Flutter includes a convenient flutter.dev TabBar class API docs for the TabBar class from the material library, for the Dart programming language. api.flutter.dev Here is the starter code- 2. Implementing custom tab indicator: Lets see what we have done in the above code snippet- 4. Overriding the  createBoxPainter([onChanged])  method which returns a BoxPainter that will paint this decoration (In our case _painter ). The parameter can be omitted if there is no chance that the painter will change. 3. Implementing custom box painter: Now, we will create our custom painter which will be of type BoxPainter. Lets see what we have added in the above code snippet- 4. Overriding the paint() method which draws the circle based on radius, paint and position 4. Updating the indicator: Now lets update the TabBar indicator Lets check the output- Check the project here or gist here . Thank you for staying till the end More flutter spinner blogs- Custom Spinner Tooltip Flutter Welcome to this tutorial for building a Custom Spinner tool tip medium.com Flutter Custom Spinner Thumb Welcome to this tutorial for Custom Spinner Thumb. medium.com Flutter Custom Spinner Thumb -Part 2 Welcome to this tutorial for building a custom spinner thumb medium.com I will be posting more about flutter, so stay tuned :) Gursheesh Singh - Chandigarh, Chandigarh, India | Professional Profile | LinkedIn View Gursheesh Singh's professional profile on LinkedIn. LinkedIn is the world's largest business network, helping www.linkedin.com Gursheesh Singh Welcome back to Instagram. Sign in to check out what your friends, family & interests have been capturing & sharing www.instagram.com Gursheesh singh The latest Tweets from Gursheesh singh (@GursheeshChawla). Business Solution developer | Flutter developer | Android twitter.com 486 5 486 486 5 Sign up for Top 5 Stories By The Startup Get smarter at building your thing. Join 176,621+ others who receive The Startup's top 5 stories, tools, ideas, books delivered straight into your inbox, once a week. Take a look. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from The Startup Get smarter at building your thing. Follow to join The Startups +8 million monthly readers & +760K followers. Nick Wolny Dec 30, 2019 Member-only I Learned This 3-Step System For Organizing Creativity From A Top Music Conservatory Apply its framework to any career skill you want to master A college education gets the shaft these days. Student loans have created a financial crisis, underemployment (working a job unrelated to your field of study) snarls young people for years after graduation, and the rise of hustle culture made us grind day and night, only to spawn a burnout epidemic 5 min read 5 min read Share your ideas with millions of readers. Josh White Dec 30, 2019 Member-only What the Best Writers Do Better Than Everyone Else They give possession of their stories to the reader. Theres a saying about works of art thats appropriate for this idea. Heres the saying, paraphrased: It takes a writer and a reader to write a story to a finish. The art of writing isnt complete until its clear and comprehensive for the reader. Why two? You may ask. Heres 5 min read 5 min read Andrew Scott Dec 30, 2019 Member-only Welcome to Python, Meet the Dunders A quick introduction to several of the __magic__ methods in python What is a Dunder? Dunder method name is the common pronunciation for pythons built-in method names that start and end with double underscores. Since Dunder is easier to say than double underscore, the name stuck. These methods are also sometimes referred to as Special Methods or Magic Methods. However, the only thing magic or 4 min read 4 min read R. Shawn McBride Dec 30, 2019 Member-only How Can We Keep The Right Leaders? Ideas For Private Businesses Leadership can be valuable. I explored just how valuable leadership can be in a recent article. But keeping the right leaders can be tricky. Good leaders, in most cases, have options to go elsewhere. And since the very survival of our enterprises could be tied to keeping the right people 7 min read 7 min read Ajanthan Mani Dec 30, 2019 Member-only Agile for Personal Growth I have been reading lots and lots of articles about personal life improvements over this year, and each of them gave me different useful ways for personal growth. I have been practicing some of the ways the articles suggested, and quite frankly, I gave up after a few days. 4 min read 4 min read TheBoringDeveloper Flutter enthusiast More from Medium Maneesha Erandi Flutter Bottom Navigation Bar Animation Jackie Moraa Lottie Animations with Flutter | Part 1 Nabin Dhakal in CodeX Routes and Navigation in Flutter Kaushiki Kumari Flutter Popular Packages Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 1749,\n",
       "  'url': 'https://towardsdatascience.com/introduction-to-mesa-agent-based-modeling-in-python-bcb0596e1c9a',\n",
       "  'title': 'Introduction to Mesa: Agent-based Modeling in\\xa0Python',\n",
       "  'subtitle': '-',\n",
       "  'claps': 176,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 8,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-11-01',\n",
       "  'clap_prop': 8.693577570174706e-05,\n",
       "  'text': 'Towards Data Science Nov 1 2019 Member-only Listen Save Introduction Mesa Agent-based Modeling Python Python-based alternative NetLogo Repast MASON agent-based modeling Agent-based modeling relies simulating action interaction autonomous agent evaluate effect system often used predict projection obtain given complex phenomenon main purpose obtain explanatory insight agent behave given particular set rule Agent-based modeling ha extensively used numerous industry biology social science network business article cover necessary step kick-start agent-based modeling project using open-source python module called Mesa 4 section tutorial 1 Setup Setup pretty straightforward Mesa Make sure create new virtual environment name environment mesaenv Open terminal change directory mesaenv activate virtual environment using following code Virtual Environment Run following command activate virtual environment depending use case Python module tutorial requires three module Base folder Create base folder called Mesa use store python file following file base folder end section Feel free download case got lost somewhere tutorial done let proceed next section 2 Schelling Segregation Model using famous Schelling Segregation model use case tutorial Please noted introductory tutorial official mesa site based Boltzmann Wealth model Schelling Segregation model better use case explain use agent-based modeling explain racial segregation issue difficult eradicated Although actual model quite simple provides explanatory insight individual might self-segregate even though explicit desire Lets look explanation model provided Mesa official github page Schelling segregation model classic agent-based model demonstrating even mild preference similar neighbor lead much higher degree segregation would intuitively expect model consists agent square grid grid cell contain one agent Agents come two color red blue happy certain number eight possible neighbor color unhappy otherwise Unhappy agent pick random empty cell move step happy model keep running unhappy agent default number similar neighbor agent need happy set 3 mean agent would perfectly happy majority neighbor different color e.g Blue agent would happy five Red neighbor three Blue one Despite model consistently lead high degree segregation agent ending neighbor different color Model.py Create new python file called model.py Agent start single agent class code quite straightforward Model model class go part one-by-one clearer understanding agent-based modeling work First foremost create Schelling class define init function constructor Variables system consists least basic agent class model class Lets start writing model first need define 5 main variable Remember add required parameter input parameter init function Grid need set grid using space module mesa Scheduler Next need scheduler scheduler special model component control order agent activated common scheduler random activation activates agent per step random order also another type called Simultaneous activation Check API reference find Data Collection Data collection essential ensure obtained necessary data step simulation use built-in datacollection module case need know whether agent happy Agents Setup setup agent using following code last part init function set following parameter running variable enables conditional shut model condition met set false agent happy Step class requires step function represent run Run.py Create new python file called run.py type following code Server.py Create new python file called server.py Import Add following import statement file HappyElement Create class called HappyElement add two function Draw function Define function called schelling_draw part act visualization part running server 3 Visualization Lets test running following code terminal Make sure base folder Mesa web browser launched see following ouput Canvas visualization canvas grid defined earlier dot represents agent Setting modify setting test affect simulation Modify affect current simulation Remember click Reset button make change FPS define frame per second speed simulation Start Step Reset following button disposal Running Click Start button see following change canvas Chart chart bottom show number agent happy number step Result model stop certain point depends setting set noticed agent pretty much segregated 4 Conclusion Lets recap learned today started simple step setup install necessary python module learned Schelling Segregation Model model easily using Mesa created 3 python file explored in-depth basic usage component available Mesa ran server see visualization modified setting play around simulation fact official Mesa github provided u lot example explore Check following link find Hope enjoyed tutorial great day see next tutorial Reference 358 3 358 358 3 Get email whenever Ng Wai Foong publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Ng Wai Foong Senior AI Engineer Yoozoo Content Writer NLP datascience programming machinelearning Linkedin http //www.linkedin.com/in/wai-foong-ng-694619185/ Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Anmol Tomar CodeX Say Goodbye Loops Python Welcome Vectorization Soumodeep Das Guide Time Series Analysis Python Dr. Vernica Gephi Meet useful network analysis tool Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Nov 1, 2019 Member-only Listen Save Introduction to Mesa: Agent-based Modeling in Python Python-based alternative to NetLogo, Repast, or MASON for agent-based modeling Agent-based modeling relies on simulating the actions and interactions of autonomous agents to evaluate their effects on the system. It is often used to predict the projections that we will obtain given a complex phenomena. The main purpose is to obtain explanatory insight on how the agents will behave given a particular set of rules. Agent-based modeling has been extensively used in numerous industry such as biology, social sciences, network and business. This article covers the necessary steps to kick-start your agent-based modeling project using an open-source python module called Mesa. There are 4 sections in this tutorial: 1. Setup Setup is pretty straightforward for Mesa. Make sure to create a new virtual environment. I name the environment as mesaenv. Open up your terminal and change the directory to mesaenv and activate the virtual environment using the following code: Virtual Environment Run the following command to activate the virtual environment depending on your use case. Python modules This tutorial requires three modules: Base folder Create a base folder called Mesa that you will use to store all the python files. You should have the following files in the base folder at the end of this sections: Feel free to download it in case you got lost somewhere in the tutorial. Once you are done, lets proceed to the next section. 2. Schelling Segregation Model We will be using the famous Schelling Segregation model as use case for this tutorial. Please be noted that the introductory tutorial on the official mesa site is based on Boltzmann Wealth model. Schelling Segregation model is a better use case to explain how we can use agent-based modeling to explain why racial segregation issue is difficult to be eradicated. Although the actual model is quite simple, it provides explanatory insights at how individuals might self-segregate even though when they have no explicit desire to do so. Lets have a look at the explanation for this model provided by the Mesa official github page: The Schelling segregation model is a classic agent-based model, demonstrating how even a mild preference for similar neighbors can lead to a much higher degree of segregation than we would intuitively expect. The model consists of agents on a square grid, where each grid cell can contain at most one agent. Agents come in two colors: red and blue. They are happy if a certain number of their eight possible neighbors are of the same color, and unhappy otherwise. Unhappy agents will pick a random empty cell to move to each step, until they are happy. The model keeps running until there are no unhappy agents. By default, the number of similar neighbors the agents need to be happy is set to 3. That means the agents would be perfectly happy with a majority of their neighbors being of a different color (e.g. a Blue agent would be happy with five Red neighbors and three Blue ones). Despite this, the model consistently leads to a high degree of segregation, with most agents ending up with no neighbors of a different color. Model.py Create a new python file called model.py. Agent: We will start off with the single agent class. The code is quite straightforward: Model: For the model class. We will go through each part one-by-one to have a clearer understanding of how agent-based modeling works. First and foremost, create a Schelling class and define a init function as constructor. Variables: The system will consists of at least a basic agent class and a model class. Lets start by writing the model first. We will need to define 5 main variables: Remember to add the required parameters as input parameters in the init function. Grid: We will need to set the grid using the space module under mesa. Scheduler: Next up, we will need to have a scheduler. The scheduler is a special model component which controls the order in which agents are activated. The most common scheduler is the random activation which activates all the agents once per step, in random order. There is also another type called Simultaneous activation. Check out the API reference to find out more. Data Collection: Data collection is essential to ensure that we obtained the necessary data after each step of the simulation. You can use the built-in datacollection module. In this case, we only need to know whether the agent is happy or not. Agents Setup: We will now setup the agent using the following code: The last part of the init function is to set the following parameters: The running variable enables conditional shut off of the model once a condition is met. We will set it to false once all the agent are happy. Step: This class requires a step function that represent each run. Run.py Create a new python file called run.py and type in the following code. Server.py Create a new python file called server.py. Import: Add the following import statement to the file: HappyElement: Create a class called HappyElement and add two functions: Draw function: Define a function called schelling_draw. This part acts as the visualization part when running the server. 3. Visualization Lets test it out by running the following code at the terminal. Make sure that you are at the base folder ( Mesa ). A web browser will be launched and you should see the following ouput: Canvas The visualization of the canvas grid that we have defined earlier. Each dot represents an agent. Setting You can modify the setting to test out how it will affects the simulation. Modify this will not affect the current simulation. Remember to click on the Reset button to make changes. FPS You can define your own frame per second to speed up the simulation. Start, Step, Reset You will have the following buttons at your disposal: Running Click on the Start button and you should see the following changes to the canvas. Chart There is a chart at the bottom to show the number of agents that are happy against the number of step. Result The model should stop at a certain point depends on the setting that you have set. You can noticed that the agents are pretty much segregated. 4. Conclusion Lets recap on what we have learned today. We started off with some simple steps to setup and install the necessary python modules. Then, we learned about the Schelling Segregation Model and how we can model it easily using Mesa. We created 3 python files and explored further in-depth on the basic usage of components available in Mesa. After that, we ran the server to see the visualization. We modified some settings and play around with the simulation. In fact, the official Mesa github provided us with a lot more examples that we can explore. Check out the following link to find out more. Hope you enjoyed the tutorial. Have a great day and see you again in the next tutorial. Reference 358 3 358 358 3 Get an email whenever Ng Wai Foong publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ng Wai Foong Senior AI Engineer@Yoozoo | Content Writer #NLP #datascience #programming #machinelearning | Linkedin: https://www.linkedin.com/in/wai-foong-ng-694619185/ More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Anmol Tomar in CodeX Say Goodbye to Loops in Python, and Welcome Vectorization! Soumodeep Das A Guide on Time Series Analysis in Python Dr. Vernica What is Gephi? Meet this useful network analysis tool Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 370,\n",
       "  'url': 'https://towardsdatascience.com/repetition-in-songs-a-python-tutorial-3dbd1c279f19',\n",
       "  'title': 'Repetition in Songs: A Python\\xa0Tutorial',\n",
       "  'subtitle': 'One of Ed Sheeran songs as a case\\xa0study',\n",
       "  'claps': 144,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 8,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 7.112927102870214e-05,\n",
       "  'text': \"Towards Data Science Mar 30 2019 Listen Save Repetition Songs Python Tutorial One Ed Sheeran song case study Everyone ha heard song know song sound like carelessly say everyone define song word benefit doubt song according Wikipedia single work music typically intended sung human voice distinct fixed pitch pattern using sound silence variety form often include repetition section journal article called complexity Songs computer scientist Donald Knuth capitalized tendency popular song devolve long content-rich ballad highly repetitive text may waste time agreeing notion doe raise question like repetitiveness really help song become hit music really becoming repetitive time attempt teach basic python code form case study going test hypothesis popular song really repetitive one favorite song One way test hypothesis figure unique word calculate fraction word total number word song tutorial well cover Prerequisite Knowledge get tutorial follow along running code purpose case study streamline hypothesis asking two major question Let 's get started analyzing already Basic 1 String list character character anything type keyboard one keystroke like letter number backslash However Python recognizes string anything delimited quotation mark either double quote single quote beginning end character text example Hello world case study string lyric seen 2 Variables typically descriptive name word symbol used assign store value word storage placeholder datatype quite handy order refer value time variable always assigned equal sign followed value variable way view code output use print function may already know Jupyter notebook output viewed without print function store lyric assign variable named perfect_lyrics 3 Lists created simply putting different comma-separated value square bracket number item may different type integer float string etc. even another list item example gotten sense list look like Let go back data Since one aim figure number unique word used mean need bit counting i.e count word order achieve put string list separate word using .split method Therefore dataset look like Input Output output notice word ha separated independent string whole lyric make list called split_lyrics also variable also need separate unique word rest word count would need use python function function block organized reusable code used perform single related action Python ha many built-in function like print -to show output list -to create list len count number character list string etc Python also allows create function creating function case study using function case study start set function separate unique word whole lyric need set print function i.e Input Output count number word whole lyric would need len function Input Output unique word extracted analysis ha helped answer first question many unique word used compared whole lyric song Simply put 129 unique word used 290 word total next goal figure second part question repetitive word used many time used order answer question need learn data structure Intermediate 4 Dictionaries Python unordered collection item data structure value element dictionary ha key value pair key-value pair map key associated value Dictionaries optimized retrieve value key known define dictionary enclosing comma-separated list key-value pair curly brace colon separate key associated value example simple German-English dictionary 5 Loops great trying run block code python two type loop analysis focus loop loop used iterate element sequence often used piece code want repeat n number time work like element list dictionary example Going back dataset know repeated word need know number time word appeared lyric need use dictionary store word corresponding count loop iterate counting process word appears First need store unique word dictionary Input Output need use loop count number time unique word appears whole lyric Input Output found number time word appears let 's sort view highest lowest using sorted function seem many word occurred aim find popular word word narrow list 10 word slicing learn slicing Input Output changing back dictionary additional question 10 popular word song called perfect Ed Sheeran easily extract information using key method dictionary data structure creating list word Input Output output code confidently say popular word used song called Perfect Ed Sheeran appeared 24 time Lets improve analysis little Advanced 6 Visualizing data python Various technique developed presenting data visually analysis focus data visualization strictly library Python namely Matplotlib Lets quickly visualize analysis top 10 popular word case study Summary Insights key insight draw case study spot three Conclusion case study doe help see greater percentage song made repetitive word However conclude one song Donald Knuth theory true need analyze lot song conclude hit song result repetitiveness word song Although good point Perfect Ed Sheeran wa actually Hit song still world Reading want improve Python skill article may help P.S Like anyone learn data analyst want notified next project update learning feel free sign newsletter Thanks TDS Editors 144 2 144 144 2 Towards Data Science home data science Medium publication sharing concept idea code Alex Blyakhman Mar 30 2019 Member-only Getting Started Google BigQuerys Machine Learning Titanic Dataset still Beta BigQuery ML ha available since mid last year however didnt get around working Google cloud-based Machine Learning offering recently non-data scientist first impression whats like ability run ML model 12 min read 12 min read Share idea million reader Jeremie Harris Mar 30 2019 problem data science job posting Every notice something realize probably noticed long time ago start see everywhere wonder people arent talking every wa yesterday wa scrolling 5 min read 5 min read Marco Cerliani Mar 30 2019 Member-only Predictive Maintenance detect Faults Sensors CNN interesting approach python code graphic representation Machine Learning topic Predictive Maintenance becoming popular passage time challenge easy heterogenous useful good knowledge domain touch people know underlying system work 6 min read 6 min read Tim Darling Mar 30 2019 Member-only Executives Guide Implementing AI Machine Learning lesson Ive learned applying AI/Machine Learning support business objective Chief Analytics Officer Ive bridge gap business need data scientist gap bridged experience difference well value promise artificial intelligence AI machine learning realized 7 min read 7 min read Sik-Ho Tsang Mar 30 2019 Review DeepPose Cascade CNN Human Pose Estimation Using Cascade Convolutional Neural Networks Refinement State-of-the-art Performance Four Datasets story DeepPose Google Human Pose Estimation reviewed formulated Deep Neural Network DNN -based regression problem towards body joint cascade DNN high precision pose estimate achieved 2014 CVPR paper 900 citation 6 min read 6 min read Okoh Anita Full-Stack Data Scientist Building Data Product Python Analytics Machine Learning New Technologies Obsessed| Self-care Conscious Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Towards Data Science Mar 30, 2019 Listen Save Repetition in Songs: A Python Tutorial One of Ed Sheeran songs as a case study Everyone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung by the human voice with distinct and fixed pitches and patterns using sound and silence and a variety of forms that often include the repetition of sections . In his journal article called The complexity of Songs, computer scientist Donald Knuth capitalized on the tendency of popular songs to devolve from long and content-rich ballads to highly repetitive texts. As some may waste no time agreeing with his notion, it does raise some questions like: Does repetitiveness really help songs become a hit? Is music really becoming more repetitive over time? In an attempt to teach some basic python code in the form of a case study, I am going to test this hypothesis (Are popular songs really repetitive?) with one of my favorite songs. One way to test this hypothesis is to figure out the unique words and calculate the fraction of those words to the total number of words in a song. In this tutorial, well cover: Prerequisite Knowledge To get the most out of this tutorial, you can follow along by running the codes yourself. For the purpose of this case study, we will streamline our hypothesis by asking two major questions: Let's get started analyzing already The Basic 1. A String  is a list of characters. A character is anything you can type on the keyboard in one keystroke, like a letter, a number, or a backslash. However, Python recognizes strings as anything that is delimited by quotation marks either a double quote ( ) or a single quote ( ) at the beginning and end of a character or text. For example: Hello world For this case study, a string is our lyrics as seen below 2. Variables   are typically descriptive names, words or symbols used to assign or store values. In other words, they are storage placeholders for any datatype. It is quite handy in order to refer to a value at any time. A variable is always assigned with an equal sign, followed by the value of the variable. (A way to view your code output is to use a print function. As you may already know with Jupyter notebook, an output can be viewed without a print function) To store the lyrics, we will assign it a variable named perfect_lyrics . 3. Lists   can be created  simply by putting different comma-separated values between square brackets. It can have any number of items and they may be of different types (integer, float, string etc.). It can even have another list as an item. For example: Now that we have gotten a sense of what a list looks like. Let go back to our data. Since one of our aims is to figure out the number of unique words used, it means we will need to do a bit of counting i.e to count each word. In order to achieve these, we will not only have to put our string into a list but will have to separate each word using a  .split()  method. Therefore our dataset will look like this Input Output From the above output, you will notice that each word has been separated into independent strings. And the whole lyrics make up the list called split_lyrics (which is also a variable) We will also need to separate the unique words of the rest of the words and the count. To do this, we would need to use python functions. A function is a block of organized, reusable code that is used to perform a single, related action. Python has many built-in functions like print( )-to show your output, list( )-to create a list, len( )- to count the number of characters in a list or string, etc. Python also allows you to create your own functions. But we will not be creating our own function in this case study. We will be using a few functions in the case study but we will start with the set( ) function. To separate the unique words from the whole lyrics, we need a set( ) and a print ( ) function i.e Input Output To count the number of words in the whole lyrics, we would need a len ( ) function Input & Output Doing the same for the unique words extracted Our above analysis has helped answer our first question:  How many unique words were used as compared to the whole lyrics of the song? Simply put, 129 unique words were used in over 290 words in total. Our next goal is to figure out the second part of the question What are the most repetitive words used and how many times were they used? In order to answer this question, we will need to learn more data structures. Intermediate 4. Dictionaries   in Python are unordered collections of items. While data structures have only values as an element, A dictionary has a  key:value  pair. Each key-value pair maps the key to its associated value. Dictionaries are optimized to retrieve values when the key is known. You can define a dictionary by enclosing a comma-separated list of key-value pairs in curly braces ( {} ). A colon ( : ) separates each key from its associated value. An example is a simple German-English dictionary: 5. Loops  are great when trying to run the same block of code over and over again. In python, there are two types of loops: for and while. For this analysis, we will focus more on the for loop . The for loop is used to iterate over elements of a sequence. It is often used when you have a piece of code which you want to repeat n number of times. It works like this: for all elements in a list or dictionary, do this For example Going back to our dataset To know the most repeated word(s), we need to know the number of times each word appeared in the lyrics. To do that, we will need to use both a dictionary (to store each word with their corresponding count) and a for loop (to iterate the counting process as each word appears) First, we need to store the unique words in a dictionary Input & Output Then we need to use the for loop again to count the number of times each unique word appears in the whole lyrics. Input & Output Now we have found out the number of times each word appears, let's sort them out to view them from highest to lowest using the sorted ( ) function There seem to be too many words that only occurred once. Because our aim is to find the most popular word or words, we will narrow our list to the 10 words by slicing it. You can learn more about slicing here Input & Output Then changing back to a dictionary An additional question will be: What are the 10 most popular words in the song called perfect by Ed Sheeran? We can easily extract this information by using the key ( )method under the dictionary data structure and then creating a list of those words. Input & Output From the above output code, we can confidently say that the most popular word used in the song called Perfect by Ed Sheeran is I which appeared 24 times Lets improve our analysis a little further Advanced 6. Visualizing data with python. Various techniques have been developed for presenting data visually but in this analysis, we will focus data visualization strictly on a library in Python , namely Matplotlib. Lets quickly visualize the analysis on the top 10 most popular words in our case study Summary Insights What are some key insights we can draw from this case study? I can spot out three Conclusion This case study does help to see that a greater percentage of a song is made up of repetitive words. However, we can not conclude with just one song that Donald Knuth theory is true. We will need to analyze a lot more songs to conclude that hit songs are as a result of the repetitiveness of words in a song . Although it is good to point out that Perfect by Ed Sheeran was actually a Hit song (and still is in my world). Further Reading If you want to improve your Python skills, these are some articles that may help P.S Like me, anyone can learn to be a data analyst and if you want to be notified on my next project or updates on my learning, feel free to sign up to my  newsletter Thanks to TDS Editors 144 2 144 144 2 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Alex Blyakhman Mar 30, 2019 Member-only Getting Started with Google BigQuerys Machine Learning Titanic Dataset While still in Beta, BigQuery ML has been available since mid last year; however, I didnt get around to working with this Google cloud-based Machine Learning offering until recently. As a non-data scientist, my first impression whats not to like? After all, the ability to run ML models from 12 min read 12 min read Share your ideas with millions of readers. Jeremie Harris Mar 30, 2019 The problem with data science job postings Every once in a while, you notice something that you realize you probably should have noticed a long time ago. You start to see it everywhere. You wonder why more people arent talking about it. For me, every once in a while was yesterday when I was scrolling through the 5 min read 5 min read Marco Cerliani Mar 30, 2019 Member-only Predictive Maintenance: detect Faults from Sensors with CNN An interesting approach with python code and graphic representations In Machine Learning the topic of Predictive Maintenance is becoming more popular with the passage of time. The challenges are not easy and very heterogenous: its useful to have a good knowledge of the domain or to be in touch with people who know how the underlying system works. 6 min read 6 min read Tim Darling Mar 30, 2019 Member-only An Executives Guide to Implementing AI and Machine Learning Some lessons Ive learned applying AI/Machine Learning to support business objectives As a Chief Analytics Officer, Ive had to bridge the gap between business needs and data scientists. How that gap is bridged is, in my experience, the difference between how well the value and promise of artificial intelligence (AI) and machine learning is realized. 7 min read 7 min read Sik-Ho Tsang Mar 30, 2019 Review: DeepPose Cascade of CNN (Human Pose Estimation) Using Cascade of Convolutional Neural Networks for Refinement, State-of-the-art Performance on Four Datasets In this story, DeepPose, by Google, for Human Pose Estimation, is reviewed. It is formulated as a Deep Neural Network (DNN)-based regression problem towards body joints. With a cascade of DNN, high precision pose estimates are achieved. This is a 2014 CVPR paper with more than 900 citations. 6 min read 6 min read Okoh Anita Full-Stack Data Scientist | Building Data Product with Python, Analytics, and Machine Learning | New Technologies Obsessed| Self-care Conscious More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 372,\n",
       "  'url': 'https://towardsdatascience.com/trust-and-interpretability-in-machine-learning-b7be41f01704',\n",
       "  'title': 'Trust and interpretability in machine\\xa0learning',\n",
       "  'subtitle': '-',\n",
       "  'claps': 131,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 9,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 6.470787850527764e-05,\n",
       "  'text': 'Towards Data Science Mar 30 2019 Listen Save Trust interpretability machine learning machine learning model always need interpretable Given choice interpretable model inaccurate non-interpretable model accurate wouldnt rather choose non-interpretable accurate model word reason sacrificing accuracy altar interpretability going try clarify make model interpretable Often interpretability equated simplicity definition obviously ambiguous simple one person may another importantly advanced machine learning useful modeling complex system system called complex reason simple Demanding useful model system simple measure make little sense Perhaps go consider think interpretability traditional modeling setting Consider mathematical model internal combustion engine car doubt anyone would consider simple hand also doubt anyone would consider model internal combustion engine non-interpretable either primarily derive model deductive manner well established physical theory thermodynamics fluid dynamic use definition interpretability model considered interpretable derived least motivated trustworthy theory definition interpretability serf dual purpose understanding trust help u understand model tend understand thing deductive manner going known unknown Also definition trust model derived trust place underlying theory Indeed situation understanding trust necessary scenario interested determining causal factor behind behavior system scenario must insist corresponding model must interpretable according definition model realm physical science belong category One argue purely inductive blackbox model suitable scenario However many situation understanding might nice mean must situation really matter ability make trustworthy prediction situation could provide alternate source trust model need bound definition interpretability given common argument merit Remember machine learning way systematically building model preferably large amount data using inductive reasoning Constraining model interpretable deductive manner seriously limit accuracy question becomes generate trust blackbox model little insight inner working credible basis trust could testing testing form basis trust regular software test model need able formalize expectation could formalize expectation completely would correspond complete specification model case would really need machine learning modeling methodology really need able formalize expectation aspect model consider important easy either many concept care fairness lend convenient mathematical treatment worth pointing significant progress ha made developing testing methodology testing machine learning model personally find idea using metamorphic relation formalizing expectation particularly promising still long way concrete methodology allow u perform comprehensive testing blackbox model inability contributes trust deficit blackbox model One could question efficacy expectation-based comprehensive testing goal machine learning find undiscovered pattern data insisting model meet expectation amount pre-defining model defeat whole purpose Following line reasoning one would argue long data representative algorithm powerful enough capture pattern little reason trust model expect model result generalize overall population extent expect generalize encapsulated model performance accuracy score Thus essence asked delegate trust trifecta data algorithm performance score first need dissuade notion single performance accuracy score form sufficient basis trusting model performance score usually point estimate model expected generalize average population given current data Trust hand nuanced multidimensional concept encapsulated single coarse grained score One imagine defining granular performance score e.g population segment would require certain level understanding population determining consider important different forming expectation Let u examine data aspect argument indeed quite easy convince oneself data representative population interested contain relevant pattern spurious one Unfortunately rarely case degree data non-representative depends quite acutely situation Nonetheless identify certain high level scenario first scenario would good understanding population complete control data collection mechanism scenario choose data representative high degree confidence expect resulting model prediction applicable overall population However note good enough understanding population able draw representative sample task hand mean already understanding feature important prediction Hence case debatable blackbox model terribly useful Opinion polling predicting election result good example scenario second scenario complete control data collection prediction affect data collected scenario assume data collection mechanism unbiased wait long enough would representative sample population course lot ifs buts go assumption Firstly one doe know long long enough Thus one need assume time scale data collected long enough produce representative sample Furthermore population might change meantime Thus additional assumption time scale population change much longer time scale representative sample generated long justify assumption estimated performance reliable model predicting stock price example scenario long making investment large enough tip market whole decision make result prediction affect stock price third scenario one data collection impacted prediction moderate high risk appetite wrong prediction example product recommender system model recommender system trained data consisting ordered list product different user bought/clicked Based data model predict user likely buy/click based model prediction system decide user get see limit buy/click-on Thus prediction bias data collection product recommender system one circumvent problem somewhat keeping exploration budget fraction case system show user random set product regardless prediction model observation resulting randomized prediction used estimate performance model One still ha address concern aforementioned second scenario order access reliability estimate fourth final scenario data collection impacted prediction little risk appetite wrong prediction example suppose build model predict whether someone default mortgage loan payment mortgage loan approved based prediction prediction person default loan approved case way knowing whether person would actually defaulted difficult imagine situation institution would randomly approve otherwise loan sake data exploration situation difficult gauge reliability estimated performance resulting model without additional information Thus great idea blindly expect data representative population scenario given constraint problem hand simply might possible get unbiased representative sample Understanding limitation one data collection mechanism able deduce implication limitation honesty report part model result go long way building trust Let u consider algorithm aspect argument widespread belief flexible algorithm better flexibility equips algorithm capture complex pattern history actual successful application machine learning anything go belief would appear utterly misplaced computer vision success came able encode symmetry picture model form convolutional neural network natural language processing able build extremely accurate cross-purpose language model could encode knowledge language including structure word context model recommender system collaborative filtering algorithm including matrix factorization method make strong assumption affinity user towards item Whether would like slap label interpretability model objective fact build better model understand domain context model need operate best model come flexible algorithm come algorithm well constrained domain knowledge right amount flexibility capture relevant pattern data seen word understanding used quite time discussion realized difficult build trust without understanding end boil one perceives machine learning Yes machine learning incredibly powerful inductive modeling technique combined big data big compute allows u model system solve problem previously reach entry machine learning imply exit everything else including common sense Machine learning one element wider modeling family includes deductive modeling well domain knowledge better understand leverage interconnection element go towards robust complex system modeling Trust contextual trust multiple source eventually flow knowledge integrity specifically trust knowledge integrity individual building model Trust well adoption model come opinion wider audience convinced modeler knowledge understand limitation model machine learning otherwise integrity report 181 181 181 Towards Data Science home data science Medium publication sharing concept idea code Christine Calo Mar 30 2019 Microsoft Introduction AI Part 1 Machine Learning bit like wanted learn Artificial Intelligence although felt little intimidated math involved Maybe thought concept difficult understand would depth recently completed Microsoft Introduction AI course 17 min read 17 min read Share idea million reader Okoh Anita Mar 30 2019 Repetition Songs Python Tutorial One Ed Sheeran song case study Everyone ha heard song know song sound like carelessly say everyone define song word benefit doubt song according Wikipedia single work music typically intended sung 8 min read 8 min read Alex Blyakhman Mar 30 2019 Member-only Getting Started Google BigQuerys Machine Learning Titanic Dataset still Beta BigQuery ML ha available since mid last year however didnt get around working Google cloud-based Machine Learning offering recently non-data scientist first impression whats like ability run ML model 12 min read 12 min read Jeremie Harris Mar 30 2019 problem data science job posting Every notice something realize probably noticed long time ago start see everywhere wonder people arent talking every wa yesterday wa scrolling 5 min read 5 min read Marco Cerliani Mar 30 2019 Member-only Predictive Maintenance detect Faults Sensors CNN interesting approach python code graphic representation Machine Learning topic Predictive Maintenance becoming popular passage time challenge easy heterogenous useful good knowledge domain touch people know underlying system work 6 min read 6 min read Abhishek Mukherjee http //www.linkedin.com/in/abhimukh/ Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Akshad Singi Better Humans 4 Unsexy One-Minute Habits Save 30+ Hours Every Week Anna Wu Google Data Scientist Interview Questions Step-by-Step Solutions Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Mar 30, 2019 Listen Save Trust and interpretability in machine learning Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldnt you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability? Before going any further we should try to clarify what makes a model interpretable. Often, interpretability is equated with simplicity. This definition is obviously ambiguous; what is simple for one person may not be so for another. More importantly, advanced machine learning is most useful for modeling complex systems. These systems are called complex for a reason they are not simple! Demanding that useful models for such systems should be simple, by any measure, makes very little sense. Perhaps, we can go further if we consider how we think about interpretability in a traditional modeling setting. Consider the mathematical model of an internal combustion engine in a car. I doubt if anyone would consider that to be simple. But, on the other hand, I also doubt if anyone would consider the model of an internal combustion engine to be non-interpretable, either. This is primarily because we can derive this model in a deductive manner from well established physical theories such as thermodynamics and fluid dynamics. Why not use this as our definition of interpretability? A model should be considered to be interpretable if it can be derived (or at least motivated) from a trustworthy theory. This definition of interpretability serves the dual purpose of understanding and trust. It helps us understand the model because we tend to understand things in a deductive manner by going from the known to the unknown. Also, with such a definition, the trust in the model is derived from the trust that we place in the underlying theory. Indeed, there are situations where both understanding and trust are necessary scenarios where we are interested in determining the causal factors behind the behavior of a system. In such scenarios, we must insist that the corresponding models must be interpretable according to the above definition. Most models in the realm of physical sciences belong to this category. One can argue that purely inductive blackbox models are not suitable for such scenarios. However, there are many other situations where understanding might be nice to have, but by no means is it a must have. In these situations what really matters is the ability to make trustworthy predictions. In these situations, if we could provide an alternate source of trust, then our models need not be bound by the definition of interpretability given above. This is a common argument, and there is merit to it. Remember, machine learning is a way of systematically building models from (preferably) large amounts of data using inductive reasoning. Constraining these models to be interpretable in a deductive manner can seriously limit their accuracy. So then the question becomes how can we generate trust in a blackbox model where we have little to no insight into its inner workings. A credible basis for trust could be testing . After all, testing forms the basis of our trust in regular software. But to test a model we need be able to formalize our expectations about it. If we could formalize our expectations completely then that would correspond to a complete specification of the model itself. In that case, we would not really need machine learning or any other modeling methodology. What we really need to be able to do is to formalize our expectations about the aspects of the model that we consider important. This is not easy either, because many of the concepts that we care about, such as fairness , do not lend themselves to a convenient mathematical treatment. It is worth pointing out that significant progress has been made in developing testing methodologies for testing machine learning models. I personally find the idea of using metamorphic relations for formalizing expectations to be particularly promising. But, we are still a long way from having concrete methodologies that will allow us to perform comprehensive testing of blackbox models, and this inability of ours contributes to a trust deficit in blackbox models. One could question the efficacy of such expectation-based comprehensive testing. After all, the goal of machine learning is to find undiscovered patterns in data. By insisting that the models meet our expectations amounts to pre-defining the model, which defeats the whole purpose. Following this line of reasoning, one would argue that as long as the data is representative and our algorithms are powerful enough to capture the patterns, there is little reason not to trust the model we should expect the model results to generalize to the overall population, and the extent to which we should expect them to generalize is encapsulated in the models performance (accuracy) scores. Thus, in essence we are asked to delegate our trust to the trifecta of data, algorithms and performance scores. We first need to dissuade ourselves from the notion that a single performance (accuracy) score can form sufficient basis for trusting the model. A performance score is usually a point estimate of how a model is expected to generalize on an average over a population given the current data. Trust, on the other hand, is a nuanced multidimensional concept that cannot be encapsulated in such a single coarse grained score. One can imagine defining more granular performance scores e.g. by population segments. But, that would require a certain level of understanding of the population and determining what we consider important this is not very different from forming expectations. Let us examine the data aspect of this argument. It is, indeed, quite easy to convince oneself that if the data is representative of the population we are interested in, then it should contain all the relevant patterns and no spurious ones. Unfortunately, that is rarely the case. The degree to which the data can be non-representative depends quite acutely on the situation. Nonetheless, we can identify certain high level scenarios. In the first scenario, we would have a good understanding of the population and complete control over the data collection mechanism. In this scenario, we can choose our data to be representative, and with a high degree of confidence we can expect our resulting models predictions to be applicable to the overall population. However, note that having a good enough understanding of the population to be able to draw a representative sample for the task at hand means that we already have some understanding of which features are important for the prediction. Hence, in this case it is debatable if blackbox models are terribly useful. Opinion polling for predicting election results is a good example of this scenario. In the second scenario, we do not have complete control over the data collection, but our predictions do not affect the data collected. In this scenario, if we assume that the data collection mechanism is unbiased then were we to wait long enough, we would have a representative sample of the population. Of course, there are a lot of ifs and buts that go with this assumption. Firstly, one does not know how long is long enough. Thus one needs to assume that the time scale over which the data is collected is long enough to produce a representative sample. Furthermore, the population itself might change in the meantime. Thus, an additional assumption is that the time scale over which population changes is much longer than the time scale over which a representative sample is generated. As long as we can justify those assumptions, then the estimated performance will be reliable. A model for predicting the stock prices is an example of such a scenario as long as we are not making investments that are large enough to tip the market as a whole, the decisions that we make as the result of the predictions should not affect the stock prices. The third scenario is one where the data collection is impacted by the predictions, but we have a moderate to high risk appetite for wrong predictions. An example of this is a product recommender system. The model for a recommender systems will be trained on data consisting of ordered lists of products that different users have bought/clicked on. Based on this data the model will predict what a user is most likely to buy/click on and based on the models predictions the system will decide what the user gets to see, which limits what (s)he can buy/click-on. Thus the prediction biases the data collection. In product recommender systems, one can circumvent this problem, somewhat, by keeping an exploration budget for a fraction of the cases the system shows the user a random set of products regardless of the prediction of the model. The observations resulting from these randomized predictions can then be used to estimate the performance of the model. One still has to address the concerns of the aforementioned second scenario in order to access the reliability of these estimates. In the fourth and final scenario, the data collection is impacted by the predictions, but we have little to no risk appetite for wrong predictions. For example, suppose we have to build a model to predict whether someone will default on their mortgage loan payments. The mortgage loan will be approved or not based on the prediction. If the prediction is that the person will default, then the loan will not be approved, and in that case there is no way of knowing whether this person would have actually defaulted or not. It is difficult to imagine a situation where an institution would randomly approve (or otherwise) a loan for the sake of data exploration. In these situations, it is very difficult to gauge the reliability of the estimated performance of the resulting model without additional information. Thus, it is not such a great idea to blindly expect the data to be representative of the population. In most scenarios, given the constraints of the problem at hand, it simply might not be possible to get an unbiased representative sample. Understanding the limitations of ones data collection mechanism, being able to deduce the implications of those limitations, and having the honesty to report those as a part of the models results goes a long way in building trust. Let us now consider the algorithm aspect of the argument. It is a widespread belief that the more flexible an algorithm is the better it is, because flexibility equips an algorithm to capture more complex patterns. But if the history of the actual successful applications of machine learning are anything to go by, then this belief would appear to be utterly misplaced. In computer vision, success came when we were able to encode the symmetries in pictures into models in the form of convolutional neural networks. In natural language processing we are now able to build extremely accurate cross-purpose language models because we could encode our knowledge about languages, including structure and word context, into these models. In recommender systems most collaborative filtering algorithms including matrix factorization methods, make strong assumptions about the affinity of a user towards an item. Whether we would like to slap the label of interpretability on these models or not, it is an objective fact that we build better models when we understand the domain and the context in which the model needs to operate. The best models do not come from the most flexible algorithms, they come from algorithms that are well constrained by domain knowledge and have just the right amount of flexibility to capture the relevant patterns in the data. We have seen the word understanding being used quite a few times in the above discussion. What we should have realized by now is that it is difficult to build trust without understanding. In the end, it boils down to how one perceives machine learning. Yes, machine learning is an incredibly powerful inductive modeling technique. When combined with big data and big compute, it allows us to model systems and solve problems that were previously out of our reach. But the entry of machine learning should not imply the exit of everything else, including common sense. Machine learning is one element in the wider modeling family that includes deductive modeling as well as domain knowledge. The better we understand and leverage the interconnections between these elements, the further we will go towards robust complex system modeling. Trust is contextual and trust can have multiple sources, but eventually it flows from knowledge and integrity; specifically in our trust in the knowledge and integrity of the individuals who are building the models. Trust as well as adoption of models will come, in my opinion, only when the wider audience is convinced that the modelers have the knowledge to understand the limitations of their models (machine learning or otherwise), and the integrity to report them. 181 181 181 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Christine Calo Mar 30, 2019 Microsoft Introduction to AI Part 1 Machine Learning Are you a bit like me and have wanted to learn about Artificial Intelligence although felt a little intimidated by the maths involved? Maybe you thought the concepts were too difficult to understand and you would be out of your depth. I recently completed the Microsoft Introduction to AI course 17 min read 17 min read Share your ideas with millions of readers. Okoh Anita Mar 30, 2019 Repetition in Songs: A Python Tutorial One of Ed Sheeran songs as a case study Everyone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung 8 min read 8 min read Alex Blyakhman Mar 30, 2019 Member-only Getting Started with Google BigQuerys Machine Learning Titanic Dataset While still in Beta, BigQuery ML has been available since mid last year; however, I didnt get around to working with this Google cloud-based Machine Learning offering until recently. As a non-data scientist, my first impression whats not to like? After all, the ability to run ML models from 12 min read 12 min read Jeremie Harris Mar 30, 2019 The problem with data science job postings Every once in a while, you notice something that you realize you probably should have noticed a long time ago. You start to see it everywhere. You wonder why more people arent talking about it. For me, every once in a while was yesterday when I was scrolling through the 5 min read 5 min read Marco Cerliani Mar 30, 2019 Member-only Predictive Maintenance: detect Faults from Sensors with CNN An interesting approach with python code and graphic representations In Machine Learning the topic of Predictive Maintenance is becoming more popular with the passage of time. The challenges are not easy and very heterogenous: its useful to have a good knowledge of the domain or to be in touch with people who know how the underlying system works. 6 min read 6 min read Abhishek Mukherjee https://www.linkedin.com/in/abhimukh/ More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Akshad Singi in Better Humans 4 Unsexy One-Minute Habits That Save Me 30+ Hours Every Week Anna Wu Google Data Scientist Interview Questions (Step-by-Step Solutions!) Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 697,\n",
       "  'url': 'https://medium.com/datadriveninvestor/data-science-ai-journey-part-1-f81ba77d5f42',\n",
       "  'title': 'Data Science & AI Journey: Part\\xa01',\n",
       "  'subtitle': 'Quick dev environment set-up to start learning data\\xa0science',\n",
       "  'claps': 127,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Data Driven Investor',\n",
       "  'date': '2019-04-07',\n",
       "  'clap_prop': 6.273206542114702e-05,\n",
       "  'text': 'DataDrivenInvestor Apr 7 2019 Listen Save Data Science AI Journey Part 1 Quick dev environment set-up start learning data science someone new data science machine learning setting development environment tricky task option available could spend lot time searching right tutorial online fiddling different command tutorial learn quickly set stable Python development environment locally using Anaconda also discover option setting environment cloud free recently started practical data community Accra first meetup attendee various background completely new data science artificial intelligence introduction brief discussion first practice session Funsho Olaniyi taking u quick introduction Python Pandas realized lot attendee didnt Python installed machine took several minute get environment set follow session along post aim facilitate process setting Python development environment first time 8 Skills Need Become Data Scientist Data Driven Investor Numbers scare nothing satisfying beautiful excel sheet speak several language www.datadriveninvestor.com favorite recommended environment research online shouldnt take much time find Jupyter Notebook data scientist favorite local development environment Part Project Jupyter Jupyter Notebook open-source web application allows create share document contain live code equation visualization narrative text mean easily code document project see output interface save format easily shared others Installation using Anaconda distribution experienced user already version Python installed computer may wish install Jupyter using pip one following command depending version Python pip install jupyter pip3 install jupyter new user strongly recommended install Jupyter using Anaconda distribution make process easier faster includes Python Jupyter Notebook commonly used package scientific computing data science install Jupyter using Anaconda go http //jupyter.org/ scroll Jupyter Notebook section click Install Notebook first try online want following page see link download Anaconda Follow select operating system version Python want install installation completed follow instruction install complication simply go terminal type following command directory jupyter notebook second open new tab default browser done Go ahead create first notebook clicking New top right corner Familiarise intuitive interface start coding either following tutorial starting project User Interface Tour handy get quick grasp everything also click Help shortcut even libraries-specific help Feel free let know need help get started think write article Jupyter Overview Bonus Python Notebooks cloud aim story wa mainly help quickly set Python environment locally start learning data science Hopefully Jupyter Notebook server running local machine able code straight browser tab continue learning data science machine learning might find situation need faster better alternative One could want train model faster Instead relying CPU central processing unit train complex model might want switch GPUs graphic processing unit TPUs tensor processing unit speed training process Due high cost currently available GPU board good option probably train model cloud Fortunately lot option available many allow get free credit processing power detailed article topic check story Kwadwo Agyapon-Ntra list option Google Colaboratory Google Colab short Kaggle Kernels Kaggle Azure Notebooks Microsoft CoCalc Datalore Binder article Data School cover detailed comparison platform hope wa helpful quickly get started data science learning journey planning publish similar story document learning journey help learner Feel free clap share also follow Medium Twitter connect LinkedIn value learn share 127 1 127 127 1 DataDrivenInvestor empowerment data knowledge expertise subscribe DDIntel http //ddintel.datadriveninvestor.com Philippe A. Abdoulaye Apr 7 2019 Member-only CIOs Urgently Learn Paris AWS Summit 2019 Last week attended Paris 2019 AWS Summit wa third participation four year Paris already 2015 London 2017 Paris year AWS Summits exciting moment bring technologist together connect collaborate learn AWS Amazon usual thing 5 min read 5 min read Share idea million reader Manu Siddharth Jha Apr 7 2019 Artificial Intelligence Win Football Matches Football sport universally loved adored people walk life one popular sport 5 min read 5 min read Anupra Chandran Apr 6 2019 Autoencoders Like Google Genome Making Genomic Data Digestible Using Autoencoder Neural Networks lucky living time much access information mean access pretty much knowledge world right fingertip 12 min read 12 min read Holly Atkinson Apr 6 2019 Member-only initialised Drizzle create hybrid web app DApp post assumes already general understanding web development using Ruby-on-Rails JavaScript ReactJS 7 min read 7 min read Harsha Angeri Apr 6 2019 Slo-Mo AI Life recognize people bet cant none real dont exist generated using AI Artificial Intelligence machine Nvidia Shocked come across cool startup education space 5 min read 5 min read Ulrich Mabou write blog feel free check latest post http //ulrich.bearblog.dev/blog/ Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Anmol Tomar CodeX Say Goodbye Loops Python Welcome Vectorization Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'DataDrivenInvestor Apr 7, 2019 Listen Save Data Science & AI Journey: Part 1 Quick dev environment set-up to start learning data science For someone new to data science or machine learning, setting up a development environment can be a tricky task. With all the options available, you could spend a lot of time searching for the right tutorial online and fiddling with different commands. In this tutorial, you will learn how to quickly set up a stable Python development environment locally using Anaconda. You will also discover some options for setting up your environment in the cloud for free. Who is this for? We recently started a practical data community in Accra. During our first meetup, we had attendees from various backgrounds, some of them completely new to data science or artificial intelligence. After introductions and brief discussions, we had our first practice session with   Funsho Olaniyi   taking us through a quick introduction to Python and Pandas. We realized that a lot of our attendees who didnt have Python installed on their machines took several minutes to get their environment set up and follow the session along. This post aims to facilitate the process of setting up a Python development environment for those doing it the first time. 8 Skills You Need to Become a Data Scientist - Data Driven Investor Numbers do not scare you? There is nothing more satisfying than a beautiful excel sheet? You speak several languages www.datadriveninvestor.com Our favorite and recommended environment If you do your own research online, it shouldnt take you much time to find out that Jupyter Notebook is data scientists favorite local development environment. Part of Project Jupyter , The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. It means that you can easily code and document your project, see the outputs in the same interface, and save it in a format that can easily be shared with others. Installation using the Anaconda distribution If you are an experienced user and you already have a version of Python installed on your computer, you may wish to install Jupyter using pip with one of the following commands depending on which version of Python you have: pip install jupyter Or pip3 install jupyter For new users, it is strongly recommended to install Jupyter using the Anaconda distribution . It makes the process easier and faster because it includes Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science. To install Jupyter using Anaconda, go to https://jupyter.org/ , scroll down to The Jupyter Notebook section and click on Install the Notebook. You can first try it online if you want to. On the following page, you should see a link to download Anaconda . Follow it, select your operating system and version of Python you want to install. Once the installation is completed(just follow the instructions; it should install with no complication), simply go to your terminal and type the following command in any directory: jupyter notebook After a few seconds, it should open a new tab on your default browser. You are done! Go ahead and create your first notebook by clicking on New in the top right corner. Familiarise yourself with the intuitive interface and start coding, either by following a tutorial or starting your own project. The User Interface Tour is very handy to get a quick grasp of everything you can do; you can also click on Help for shortcuts and even libraries-specific help. Feel free to let me know if you need any further help to get started or if you think I should write an article on Jupyter Overview. Bonus: Python Notebooks in the cloud The aim of this story was mainly to help you quickly set up a Python environment locally to start learning data science. Hopefully, by now you should have a Jupyter Notebook server running on your local machine and you should be able to code straight from a browser tab. As you continue learning data science and machine learning, you might find yourself in situations where you need a faster or better alternative. One of such could be that you want to train a model faster. Instead of relying on your CPU (central processing unit) to train complex models, you might want to switch to GPUs (graphics processing units) or TPUs (tensor processing units) to speed up the training process. Due to the high cost of currently available GPU boards, a good option will probably be to train your models in the cloud. Fortunately, they are a lot of options available to do that, and many of them will allow you to get some free credits for processing power. For a detailed article on the topic, check out this story by   Kwadwo Agyapon-Ntra   . Below is a list of some of the options: Google Colaboratory (or Google Colab for short) Kaggle Kernels (from Kaggle) Azure Notebooks (from Microsoft) CoCalc Datalore Binder This article from Data School covers a detailed comparison of those platforms. I hope that this was helpful to quickly get you started on your data science learning journey. I am planning to publish similar stories to document my own learning journey and help other learners. Feel free to clap and share. You can also follow me here on Medium, on Twitter , or connect with me on LinkedIn . #value #learn #share 127 1 127 127 1 More from DataDrivenInvestor empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com Philippe A. Abdoulaye Apr 7, 2019 Member-only What CIOs Should Urgently Learn from Paris AWS Summit 2019 Last week, I attended the Paris 2019 AWS Summit, it was my third participation in four years; Paris already in 2015, London in 2017, and again Paris this year. AWS Summits are exciting moments, they bring technologists together to connect, collaborate, and learn about AWS. Amazon as usual did things 5 min read 5 min read Share your ideas with millions of readers. Manu Siddharth Jha Apr 7, 2019 Artificial Intelligence Can Win Football Matches Football is a sport that is universally loved and adored by people from all walks of life. It is one of the most popular sports in the 5 min read 5 min read Anupra Chandran Apr 6, 2019 Autoencoders: Like Google, But for your Genome Making Genomic Data More Digestible Using Autoencoder Neural Networks Were so lucky to be living in a time with so much access to information. I mean, we have access to pretty much all the knowledge in the world, all right at our fingertips! 12 min read 12 min read Holly Atkinson Apr 6, 2019 Member-only How I initialised Drizzle to create a hybrid web app / DApp This post assumes you already have a general understanding of web development using Ruby-on-Rails, JavaScript and ReactJS. 7 min read 7 min read Harsha Angeri Apr 6, 2019 A Slo-Mo AI Life Can you recognize any of these people? I bet you cant as none of them are real. They dont exist. They were generated using an AI (Artificial Intelligence) machine by Nvidia. Shocked??? Have you come across these below cool startups in education space? 5 min read 5 min read Ulrich Mabou I now write on my own blog; feel free to check out my latest posts: https://ulrich.bearblog.dev/blog/ More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Anmol Tomar in CodeX Say Goodbye to Loops in Python, and Welcome Vectorization! Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4512,\n",
       "  'url': 'https://towardsdatascience.com/master-geographic-data-science-with-real-world-projects-exercises-96ac1ad14e63',\n",
       "  'title': 'Master Geographic Data Science with Real World projects & Exercises',\n",
       "  'subtitle': 'Real World projects & Exercises',\n",
       "  'claps': 125,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-05-20',\n",
       "  'clap_prop': 6.174415887908172e-05,\n",
       "  'text': 'Towards Data Science May 20 2019 Member-only Listen Save Getting started Geographic Data Science Python Tutorials Real World project Exercises first article three-part series article Getting started Geographic Data Science Python learn reading manipulating analysing Geographic data Python article series designed sequential first article lay foundation second one get intermediate advanced level Geographic data science topic third part cover relevant real-world project wrapping cement learning tutorial also ha simple exercise help learn practice code dataset Google Colab Jupyter notebook available link end article focus Geographic Vector data series another coming series learn satellite image raster data analysis series contains Three part first part tutorial learn basic loading processing geographic data using Geopandas Geopandas workhorse Geographic data science Python built top Pandas Numpy library Like Pandas Dataframe Geopandas data structure contains GeodataFrame GeoSeries Geopandas provides capability read manipulate geographic data easily also perform many essential geospatial operation including among geometric operation projection geographic analysis also visualize plot map Geopandas- provides high-level interface Matplotlib library- using .plot method GeodataFrame/GeoSeries learning objective part Introduction Geographic Data science 1 Reading Geographic data tutorial use mainly 3 datasets Geographic Vector data come different format Shapefiles Geopackage Geojson etc Loading Geodata Formats Geopandas straightforward use .read_file Let see example reading data case read country dataset First created variable hold file path used Geopandas .read_file method read country dataset Geopandas take care geometry column enables u carry geoprocessing task example plotting map good way start data exploration look first row shape data well general statistic data possible following command first 5 row data look like One example reading data Geopandas time read city dataset come Geojson file technique used read country dataset apply carry also exploration using .head .shape .describe get feeling dataset exploration go ahead plot map Plotting map Geopandas easy available .plot function Since two datasets country city data overlay display map set subplots using Matplotlib pas axis Geopandas .plot function output map two-three line code able produce nice map time small exercise part Exercise 1.1 Read river data Exercise 1.2 Read first 5 row river dataset Exercise 1.3 Visualize river dataset 2 Coordinate system Projections Coordinate reference system represent data two dimensional planar relates actual place earth glue hold attribute respective location Geodataframes ha .crs attribute give original CRS used data easy transform project coordinate However perform projection necessary CRS order carry geographic analysis get right value analysis country city river CRS Let u check country CRS output code init epsg:4326 .EPSG stand European Petroleum Survey Group authority maintains spatial reference system code 4326 indicates Geographic Coordinate System used case WGS84 World Geodetic System 1984 Different CRS different measurement coordinate defined decimal degree others defined meter common process reproject data one format another Geographic data processing source useful visualizing comparing different Projections Mercator vs. Robinson Compare Map Projections Compare map projection Mercator Robinson map-projections.net project data Mercator Mercator projection latitude-longitude quadrangle stretched along x-axis y-axis move away equator first let u geometry column country dataset output code print latitude longitude Polygons coordinate decimal degree 0 POLYGON 117.7036079039552 4.163414542001791 1 POLYGON 117.7036079039552 4.163414542001791 2 POLYGON -69.51008875199994 -17.506588197999 3 POLYGON -69.51008875199994 -17.506588197999 4 POLYGON -69.51008875199994 -17.506588197999 Let u project data see change example project EPSG:3395 widely used Mercator projection geometry column data look like 0 POLYGON 13102705.69639943 460777.6522179524 1 POLYGON 13102705.69639943 460777.6522179524 2 POLYGON -7737827.684867887 -1967028.7849201 3 POLYGON -7737827.684867887 -1967028.7849201 4 POLYGON -7737827.684867887 -1967028.7849201 Due projection geometry longer measured decimal style point metre unit easier understand difference map Let u plot original country projected country Notice different scale x map Mercator projection distorts size object go equator pole Africa look small Greenland appears much larger size try overlay projected data unprojected data data align properly Let u see plot city top projected country Remember projected city see city overlayed properly projected country dataset fall near Africa proper place order align properly also need project projection country dataset EPSG:3395 exercise Exercise 2.1 Convert city data EPSG:3395 projection plot city top countries_proj 3 Write Geographic Data easily save new data created local disk helpful want access file another time without carrying operation Let u save projected country Remember projected Geopandas ha .to_file method save file might want download file since using collab configure Google drive erased close session Google Colab guessed also need save projected city exercise 2.1 right last exercise part Exercise 3.1 Save projected city file created exercise 2.1 file Conclusion tutorial covered basic loading writing Geographic data well Geographic coordinate system projection next tutorial learn Geoprocessing manipulation Geographic data using Geopandas code available GitHub repository shakasom/GDS Geographic data science tutorial series Contribute shakasom/GDS development creating account GitHub github.com also go directly run Google Collaboraty Jupyter Notebooks directly link Google Colaboratory Edit description colab.research.google.com 179 5 179 179 5 Enjoy read Reward writer Beta tip go Abdishakur third-party platform choice letting know appreciate story Get email whenever Abdishakur publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Nick Latocha May 20 2019 know hire data scientist CEO forward email popular management newsletter subject contains two letter A.I Straight away sink seat Everyone hailing AI Data Scientist saviour company 4 min read 4 min read Share idea million reader Jovan Medford May 20 2019 Member-only Rudimentary k-Nearest Neighbors intuitive ML model Sometimes data science find picking missile launcher order kill mere ant case though going discussing probably intuitive machine learning algorithm fooled however spite simplicity kNN 5 min read 5 min read Kunal Dhariwal May 20 2019 Member-only Create Virtual Personal Assistant know Cortana Siri Google Assistant right ever imagined make virtual personal assistant customize want Today well Well building personal assistant scratch python Oh getting let 5 min read 5 min read Bilal Maqsood May 20 2019 Schedulers YARN concept configuration FIFO capacity fair scheduler choice start delving world big data number new word acronym start showing YARN also one 5 min read 5 min read Rob Salgado May 20 2019 Member-only Hyperparameter Tuning Google Cloud Platform Scikit-Learn Google Cloud Platforms AI Platform formerly ML Engine offer hyperparameter tuning service model take extra time effort learn use instead running code already virtual machine 10 min read 10 min read Abdishakur Writing Geospatial Data Science AI ML DL Python SQL GIS Top writer 1m view Medium Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Nik Piepenbreier Better Programming Make Awesome Maps Python Geopandas Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science May 20, 2019 Member-only Listen Save Getting started with Geographic Data Science in Python Tutorials, Real World projects & Exercises This is the first article of a three-part series of articles in Getting started Geographic Data Science with Python. You will learn about reading, manipulating and analysing Geographic data in Python. The articles in this series are designed to be sequential where the first article lays the foundation and the second one gets into intermediate and advanced level Geographic data science topics. The third part covers a relevant and real-world project wrapping up to cement your learning. Each tutorial also has some simple exercises to help you learn and practice. All code, dataset and Google Colab Jupyter notebooks are available from the link at the end of this article. I will focus only on Geographic Vector data in this series. In another coming series, We will learn about satellite images and raster data analysis. The series contains Three parts: This is the first part. In this tutorial, we will learn the basics of loading and processing geographic data using Geopandas. Geopandas, the workhorse of Geographic data science in Python, is built on top of Pandas and Numpy libraries. Like Pandas Dataframe, Geopandas data structure contains GeodataFrame and GeoSeries. Geopandas provides not only the capability to read and manipulate geographic data easily but also can perform many essential geospatial operations including among other geometric operations, projections and geographic analysis. You can also visualize and plot maps with Geopandas- It provides a high-level interface to the Matplotlib library- by using the .plot() method on GeodataFrame/GeoSeries. These are the learning objectives for this part, Introduction to Geographic Data science: 1. Reading Geographic data In this tutorial we will use mainly 3 datasets: Geographic (Vector) data comes in different formats (Shapefiles, Geopackage, Geojson etc). Loading most of Geodata Formats with Geopandas is straightforward. We can use .read_file(). Let see an example of reading the data. In this case, we will read the countries dataset. First, we created a variable to hold the file path and then we have used Geopandas, .read_file() method to read the countries dataset. Geopandas takes care of the geometry column which enables us to carry out geoprocessing tasks, for example, plotting maps. A good way to start your data exploration is to look at the first few rows, the shape of the data, as well as general statistics of the data. This is possible through the following commands. This is how the first 5 rows of the data looks like. One more example of reading data in Geopandas and this time we will read the cities dataset. It comes as Geojson file but the same techniques we have used to read the countries dataset apply here. We can carry out also the same exploration using .head() , .shape() and .describe() to get a feeling of what this dataset is about. Once we do the explorations, we can go ahead and plot maps. Plotting maps in Geopandas is easy and available through .plot() function. Since we have two datasets countries and cities data, we can overlay them and display it as a map. Here we set up the subplots using Matplotlib and pass the axis to Geopandas .plot() function. This is the output map. With just two-three lines of code, we are able to produce this nice map. It is time for small exercise from your part. Exercise 1.1: Read the rivers data Exercise 1.2: Read the first 5 rows of the rivers dataset Exercise 1.3: Visualize rivers dataset. 2. Coordinate systems and Projections Coordinate reference systems represent how our data as two dimensional (planar) relates to actual places on earth. It is the glue that holds the attributes to their respective locations. Geodataframes has .crs attribute that can give you the original CRS used in the data. It is easy to transform and project these coordinates. However, to perform projections, it is necessary to have the same CRS in order to carry out geographic analysis and get the right values out the analysis. The countries, cities and rivers have the same CRS. Let us check the countries CRS. This is the output of the above code {init: epsg:4326}.EPSG stands for European Petroleum Survey Group and is an authority that maintains spatial reference systems. The code 4326 indicates which Geographic Coordinate System is used, in this case (WGS84) The World Geodetic System of 1984. Different CRS have different measurements. For some, the coordinates are defined in decimal degrees while others are defined in meters. It is a common to process and reproject data from one format to another in Geographic data processing. This source is very useful in visualizing and comparing different Projections: Mercator vs. Robinson: Compare Map Projections Compare the map projections Mercator and Robinson map-projections.net We will project our data into Mercator. The Mercator projection, latitude-longitude quadrangles are stretched along the x-axis and y-axis as you move away from the equator. But first, let us the geometry column of the countries dataset. This is the output of the above code. It just prints out the latitude and longitude of the Polygons. These coordinates are in decimal degrees now. 0 (POLYGON ((117.7036079039552 4.163414542001791 1 (POLYGON ((117.7036079039552 4.163414542001791 2 (POLYGON ((-69.51008875199994 -17.506588197999 3 (POLYGON ((-69.51008875199994 -17.506588197999 4 (POLYGON ((-69.51008875199994 -17.506588197999 Let us project this data and see the changes. In this example, we project to EPSG:3395 which is the widely used Mercator projection. Now our geometry column data looks like this: 0 (POLYGON ((13102705.69639943 460777.6522179524 1 (POLYGON ((13102705.69639943 460777.6522179524 2 (POLYGON ((-7737827.684867887 -1967028.7849201 3 (POLYGON ((-7737827.684867887 -1967028.7849201 4 (POLYGON ((-7737827.684867887 -1967028.7849201 Due to the projection, the geometry is no longer measured in decimal style points but in a metre unit. It is easier to understand the difference in maps. Let us plot both the original countries and the projected countries. Notice the different scales of x and y in both maps. The Mercator projection distorts the size of the objects as we go further from the equator to the poles. That is why Africa looks small and Greenland appears much larger than its size. If you try to overlay the projected data with unprojected data, then your data will not align properly. Let us see if we can plot cities on the top of projected countries. Remember we have not projected the cities. As you can see the cities are not overlayed properly in the projected countries dataset. They fall near Africa and that is not their proper place. In order to align them properly, we also need to project the same projection of the countries dataset, EPSG:3395 and that is an exercise for you. Exercise 2.1: Convert the cities data into EPSG:3395 projection and plot cities on top of countries_proj. 3. Write Geographic Data We can easily save any new data created to our local disk. This is helpful when you want to access that file in another time without carrying out the same operations again. Let us save our projected countries. Remember we have projected it. Geopandas has .to_file() method. And that will save your file. You might want to download this file since we are using collab and did not configure it with Google drive. This will be erased when you close your session in Google Colab. If you have guessed that we also need to save the projected cities from exercise 2.1, you are right. That is your last exercise for this part. Exercise 3.1: Save the projected cities file you created in exercise 2.1 into a file Conclusion In this tutorial, we have covered the basics of loading and writing Geographic data as well as Geographic coordinate system and projections. In the next tutorial, we will learn the Geoprocessing and manipulation of Geographic data using Geopandas. The code is available in this GitHub repository: shakasom/GDS Geographic data science tutorials series. Contribute to shakasom/GDS development by creating an account on GitHub. github.com You can also go directly and run Google Collaboraty Jupyter Notebooks directly from this link: Google Colaboratory Edit description colab.research.google.com 179 5 179 179 5 Enjoy the read? Reward the writer. Beta Your tip will go to Abdishakur through a third-party platform of their choice, letting them know you appreciate their story. Get an email whenever Abdishakur publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Nick Latocha May 20, 2019 What you should know before you hire a data scientist Your CEO forwards you an email from a popular management newsletter. The subject contains the two letters A.I. Straight away, you sink into your seat. Everyone is hailing AI, and the Data Scientist, as the saviour of your company. 4 min read 4 min read Share your ideas with millions of readers. Jovan Medford May 20, 2019 Member-only Rudimentary k-Nearest Neighbors The most intuitive ML model Sometimes in data science you find yourself picking up a missile launcher in order to kill a mere ant. In this case though, we are going to be discussing what is probably the most intuitive machine learning algorithm. Do not be fooled however, in spite of its simplicity, the kNN 5 min read 5 min read Kunal Dhariwal May 20, 2019 Member-only Create your own Virtual Personal Assistant You know about Cortana, Siri and Google Assistant, right? Have you ever imagined that you can make your own virtual personal assistant and customize it as you want? Today, well be doing it here. Well be building a personal assistant from scratch in python. Oh, Before getting into it, let 5 min read 5 min read Bilal Maqsood May 20, 2019 Schedulers in YARN: from concepts to configurations FIFO, capacity or fair scheduler, the choice is yours. When we start delving into the world of big data, a number of new words and acronyms start showing up YARN is also one of them. 5 min read 5 min read Rob Salgado May 20, 2019 Member-only Hyperparameter Tuning On Google Cloud Platform With Scikit-Learn Google Cloud Platforms AI Platform (formerly ML Engine) offers a hyperparameter tuning service for your models. Why should you take the extra time and effort to learn how to use it instead of just running the code you already have on a virtual machine? 10 min read 10 min read Abdishakur Writing about Geospatial Data Science, AI, ML, DL, Python, SQL, GIS | Top writer | 1m views. More from Medium Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Nik Piepenbreier in Better Programming Make Awesome Maps in Python and Geopandas Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 369,\n",
       "  'url': 'https://towardsdatascience.com/what-library-can-load-image-in-python-and-what-are-their-difference-d1628c6623ad',\n",
       "  'title': 'What libraries can load image in Python and what are their difference?',\n",
       "  'subtitle': 'Summarization & Comparison of…',\n",
       "  'claps': 124,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 3,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 6.125020560804907e-05,\n",
       "  'text': 'Towards Data Science Mar 30 2019 Member-only Listen Save library load image Python difference Summarization Comparison imread face computer vision project first need load image preprocessing various library perform imread want consolidate popular library loading image difference article go Library loading image four library usually used loading image Colour channel loading image usually plt.imshow img used plot image Lets plot doge may spot OpenCV image look odd matplotlib PIL skimage represent image RGB Red Green Blue order OpenCV reverse order BGR Blue Green Red Easy Fix convert image BGR RGB using cv2.cvtColor img cv2.COLOR_BGR2RGB plotting using plt.imshow Efficiency may ask one efficient library loading image function defined track time result follow Pillow Image.Open seems efficient based result study may go back source code find difference Cheatsheet combined information Jupyter Notebook Feel free download cheatsheet happy coding Source http //blog.csdn.net/renelian1572/article/details/78761278 http //github.com/ZhangXinNan/LearnPractice/blob/master/cv/opencv/test_cvlib.py 157 1 157 157 1 Towards Data Science home data science Medium publication sharing concept idea code Abhishek Mukherjee Mar 30 2019 Trust interpretability machine learning machine learning model always need interpretable Given choice interpretable model inaccurate non-interpretable model accurate wouldnt rather choose non-interpretable accurate model word reason sacrificing accuracy altar interpretability 9 min read 9 min read Share idea million reader Christine Calo Mar 30 2019 Microsoft Introduction AI Part 1 Machine Learning bit like wanted learn Artificial Intelligence although felt little intimidated math involved Maybe thought concept difficult understand would depth recently completed Microsoft Introduction AI course 17 min read 17 min read Okoh Anita Mar 30 2019 Repetition Songs Python Tutorial One Ed Sheeran song case study Everyone ha heard song know song sound like carelessly say everyone define song word benefit doubt song according Wikipedia single work music typically intended sung 8 min read 8 min read Alex Blyakhman Mar 30 2019 Member-only Getting Started Google BigQuerys Machine Learning Titanic Dataset still Beta BigQuery ML ha available since mid last year however didnt get around working Google cloud-based Machine Learning offering recently non-data scientist first impression whats like ability run ML model 12 min read 12 min read Jeremie Harris Mar 30 2019 problem data science job posting Every notice something realize probably noticed long time ago start see everywhere wonder people arent talking every wa yesterday wa scrolling 5 min read 5 min read Kevin Luk journey Data Science Medium Black_Raven James Ng Geek Culture Face Recognition 46 line code Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Mar 30, 2019 Member-only Listen Save What libraries can load image in Python and what are their difference? Summarization & Comparison of . imread() When we face computer vision project, first of all we need to load the images before any preprocessing. There are various libraries out there to perform imread() . Here I want to consolidate the popular libraries for loading image and their difference. This article will go through: Library for loading image There are four libraries that are usually used for loading images. Colour channel After loading the image, usually plt.imshow(img) will be used to plot the images. Lets plot some doge ! You may spot that the OpenCV image above looks odd. It is because matplotlib, PIL and skimage represent image in RGB (Red, Green, Blue) order, while OpenCV is in reverse order ! ( BGR Blue, Green, Red) Easy Fix Just convert the image from BGR to RGB using cv2.cvtColor(img, cv2.COLOR_BGR2RGB) before plotting using plt.imshow() . Efficiency So, you may ask which one is the most efficient library in loading the image. Here a function is defined to track the time: The result is as follow: Pillow Image.Open() seems to be the most efficient based on the result. For further study, we may go back to the source code to find out more about the difference! Cheatsheet I have combined the above information into a Jupyter Notebook. Feel free to download the cheatsheet and happy coding! Source: https://blog.csdn.net/renelian1572/article/details/78761278 https://github.com/ZhangXinNan/LearnPractice/blob/master/cv/opencv/test_cvlib.py 157 1 157 157 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Abhishek Mukherjee Mar 30, 2019 Trust and interpretability in machine learning Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldnt you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability? 9 min read 9 min read Share your ideas with millions of readers. Christine Calo Mar 30, 2019 Microsoft Introduction to AI Part 1 Machine Learning Are you a bit like me and have wanted to learn about Artificial Intelligence although felt a little intimidated by the maths involved? Maybe you thought the concepts were too difficult to understand and you would be out of your depth. I recently completed the Microsoft Introduction to AI course 17 min read 17 min read Okoh Anita Mar 30, 2019 Repetition in Songs: A Python Tutorial One of Ed Sheeran songs as a case study Everyone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung 8 min read 8 min read Alex Blyakhman Mar 30, 2019 Member-only Getting Started with Google BigQuerys Machine Learning Titanic Dataset While still in Beta, BigQuery ML has been available since mid last year; however, I didnt get around to working with this Google cloud-based Machine Learning offering until recently. As a non-data scientist, my first impression whats not to like? After all, the ability to run ML models from 12 min read 12 min read Jeremie Harris Mar 30, 2019 The problem with data science job postings Every once in a while, you notice something that you realize you probably should have noticed a long time ago. You start to see it everywhere. You wonder why more people arent talking about it. For me, every once in a while was yesterday when I was scrolling through the 5 min read 5 min read Kevin Luk My journey into Data Science More from Medium Black_Raven (James Ng) in Geek Culture Face Recognition in 46 lines of code Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 3190,\n",
       "  'url': 'https://towardsdatascience.com/lets-apply-machine-learning-in-behavioral-economics-eb952d0f2300',\n",
       "  'title': 'Let’s Apply Machine Learning in Behavioral Economics',\n",
       "  'subtitle': '-',\n",
       "  'claps': 120,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-25',\n",
       "  'clap_prop': 5.9274392523918454e-05,\n",
       "  'text': 'Towards Data Science Apr 25 2019 Listen Save Lets Apply Machine Learning Behavioral Economics Currently Machine Learning ML well applied behavioral economics social science lack use come unfamiliarity different approach However time generation scientist growing machine learning move social science make impact technique made computer vision interaction machine learning behavioral economics mutually beneficial one hand ML used mine broad set data find behavioral-type variable contribute emergence different behavior hand ML algorithm embedded identify bias wrong assumption would reach higher performance article aim provide sense first-mentioned application ML behavioral economics research identifying variable important shaping people behavior specifically paper simple explanation ML technique Random Forest Gradient Boosting Machine help research related field behavioral economics social psychology Behavioral economics field study examines effect psychological cognitive emotional cultural social factor human decision decision deviate implied rational thinking word human considered sufficiently rational agent psychological variable context emphasized essential determinant human decision approach help predict human behavior better certain situation directional deviation rational thinking allows designing nudge policy improve people decision Daniel Kahneman excellent book Thinking Fast Slow state human pattern seeker order understand pattern behavioral economist consider psychological characteristic individual context analyzing people behavior However difficult apply individual context-based approach massive scale Machine learning significantly resolve challenge detecting pattern searching large set data variable influential shaping pattern Machine learning often pattern recognition help automatically detect pattern data use detected pattern tool predicting future action However focus ML ha prediction power le attention ha given interpretation power example although beneficial using ML cancer diagnosed accurately earlier essential thing ML provide insight variable weight increasing cancer risk knowledge help researcher policymakers control variable better reduce cancer risk Put differently ML give u prediction power target variable also knowledge find input variable crucial predicting target variable sense application ML behavioral economics work let u look decision tree model family machine learning algorithm go observation item conclusion item target value One form model target variable ha discrete set value called classification tree model branch tree represent conjunction input variable lead leaf represent class label target variable kind decision tree model regression tree target variable ha continuous value Two famous model use ensemble decision tree reach high accuracy prediction Random Forest Gradient Boosting Machine goal decision-tree based algorithm establish model predicts value target variable based several input variable vital aspect model limitation number input variable model word le concern curse dimensionality general step building model making interpretation using technique follow first data divided two set one set training data typically includes larger portion data model built set test data model validated creating effective model based training testing data reach model predict observed behavior data model give u knowledge identify variable contribution prediction target variable process data analysis referred feature importance example hundred input data regarding massive amount observation including individual context related variable using methodology find variable critical leading people show different behavior model also provide possibility local interpretation word understand top input variable important individual prediction example two person may show behavior entirely different reason Therefore looking individual prediction one specific behavior grouping top variable mostly explain behavior help u understand feature influential driving behavior possibility due nature decision-tree based model specific path branch leave representing target value observation figure 1 powerful ability machine learning thus resolve challenge data scale also enable u match right policy person population thousand million people word machine learning make possible targeting right nudge right people right context LinkedIn http //www.linkedin.com/in/atanehkar/ 180 2 180 180 2 Towards Data Science home data science Medium publication sharing concept idea code Ashok Chilakapati Apr 25 2019 Concept Drift Model Decay Machine Learning Concept drift drift label time essentially data lead divergence decision boundary new data model built earlier data/labels Scoring randomly sampled new data detect drift allowing u trigger expensive 7 min read 7 min read Share idea million reader Ren Bremer Apr 25 2019 use Python feature data analytics project Python tutorial Azure using OO NumPy panda SQL PySpark 1 Introduction lot company moving cloud consider tooling shall used data analytics On-premises company mostly use propriety software advanced analytics BI reporting However tooling may logical choice cloud environment 9 min read 9 min read Ryan Burge Apr 25 2019 Member-only Religious Composition Two Major Parties make lot data visualization time pretty good idea result calculation going look like write code fact result obvious dont even post anywhere 4 min read 4 min read Ashutosh Singh Apr 25 2019 Member-only Classifying Products Banned Approved using Text Mining- Part II part explain optimize existing Machine Learning model Part deployment ML model using Flask previous article series discussed business problem shown train model using fastText classification Banned Approved product based information like Product Name product description specification 5 min read 5 min read Favio Vzquez Apr 25 2019 Jungle Koalas Pandas Optimus Spark expect newest library Databricks Koalas Optimus framework Apache Spark 3.x excited data science probably know Spark+AI latest summit started yesterday April 24th 2019 great thing talk spin-off youve following co-created 8 min read 8 min read Ahmad Tanehkar Behavioral Economics Consumer Psychology Researcher Medium Samuele Mazzanti Towards Data Science Using Causal ML Instead A/B Testing Anil Tilbe Towards AI Bayesian Inference Best 5 Models 10 Best Practices Machine Learning Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 25, 2019 Listen Save Lets Apply Machine Learning in Behavioral Economics Currently, Machine Learning (ML) is not well applied in behavioral economics or social science, and this lack of use comes from unfamiliarity with this different approach. However, over time the generation of scientists who are growing up with machine learning will move into social science and make the impact that these techniques have made in computer vision. The interactions between machine learning and behavioral economics can be mutually beneficial. On the one hand, ML can be used to mine a broad set of data and find the behavioral-type variables that contribute to the emergence of different behaviors. On the other hand, ML algorithms that are embedded to identify biases and wrong assumptions would reach higher performance. This article aims to provide a sense for the first-mentioned application of ML in behavioral economics research, which is identifying variables that are important in shaping peoples behavior. More specifically this paper is a simple explanation of how ML techniques such as Random Forest and Gradient Boosting Machine can help in research related to fields such as behavioral economics or social psychology. Behavioral economics is a field of study that examines the effects of psychological, cognitive, emotional, cultural and social factors on human decisions and how these decisions deviate from those implied by rational thinking. In other words, humans will not be considered as sufficiently rational agents, and psychological variables, and contexts will be emphasized as essential determinants of humans decisions. This approach helps to predict human behavior better, and in certain situations where there are directional deviations from rational thinking, allows for designing nudge policies to improve peoples decisions. As Daniel Kahneman in his excellent book Thinking, Fast and Slow states: We (humans) are pattern seekers. In order to understand these patterns, behavioral economists consider the psychological characteristics of individuals and the context in analyzing people behavior. However, it is difficult to apply this individual and context-based approach on a massive scale. Machine learning can significantly resolve this challenge by detecting patterns and searching in a large set of data for variables that are influential in shaping the patterns. Machine learning is often about pattern recognition, and it helps to automatically detect patterns in data and then use the detected patterns as a tool for predicting future actions. However, the most focus on ML has been on its prediction power, and less attention has been given to its interpretation power. For example, although it is very beneficial that using ML cancer can be diagnosed more accurately and earlier, but the more essential thing that ML can provide is the insight about the variables having more weight in increasing cancer risk. This knowledge helps researchers and policymakers to control those variables better and reduce cancer risk. Put differently ML gives us not only the prediction power over a target variable but also the knowledge to find out which input variables are more crucial in predicting that target variable. To have a sense how this application of ML to behavioral economics works, let us look at the decision trees models, a family of machine learning algorithms that go from observations about an item to conclusions about the items target value. One form of this model where the target variable has a discrete set of values is called classification trees. In these models, branches of trees represent conjunctions of input variables that lead to the leaves which represent the class labels of a target variable. The other kind of decision tree models is regression trees, in which the target variable has continuous values. Two famous models that use an ensemble of decision trees to reach the high accuracy in prediction are Random Forest and Gradient Boosting Machine. The goal of these decision-tree based algorithms is to establish a model that predicts the value of a target variable based on several input variables. The vital aspect of these models is that there is no limitation for the number of input variables in these models. In other words, there is less concern about the curse of dimensionality for them. The general steps for building a model and making interpretation using these techniques are as follow: first, data is divided into two sets, one set is the training data which typically includes the larger portion of data on which the model is built, and the other set is test data by which the model is validated. After creating the most effective model based on the training and testing data, we will reach to a model that can predict the observed behavior in data. This model now gives us the knowledge to identify the variables that have the most contribution to the prediction of the target variable. This process in data analysis is referred to as feature importance. For example, if we have hundreds of input data regarding a massive amount of observations, including individual and context related variables, by using this methodology we can find out what variables are critical in leading people to show different behaviors. These models also provide the possibility of local interpretation. In other words, we can understand the top input variables that are most important in each individual prediction. For example, two persons may show the same behavior for entirely different reasons. Therefore, looking at each individual prediction for one specific behavior and grouping the top variables that mostly explain this behavior will help us to understand what features are more influential in driving that behavior. This possibility is due to the nature of decision-tree based models, in which there is a specific path from branches to each leave representing the target value for each observation (figure 1). These powerful abilities of machine learning, thus, not only resolve the challenges of data scale but also enable us to match the right policy for a person in a population of thousands or millions of people. In other words, machine learning makes possible targeting the right nudges to the right people, in the right context. LinkedIn:  https://www.linkedin.com/in/atanehkar/ 180 2 180 180 2 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ashok Chilakapati Apr 25, 2019 Concept Drift and Model Decay in Machine Learning Concept drift is a drift of labels with time for the essentially the same data. It leads to the divergence of decision boundary for new data from that of a model built from earlier data/labels. Scoring randomly sampled new data can detect the drift allowing us to trigger the expensive 7 min read 7 min read Share your ideas with millions of readers. Ren Bremer Apr 25, 2019 How to use Python features in your data analytics project Python tutorial in Azure using OO, NumPy, pandas, SQL, PySpark 1. Introduction A lot of companies are moving to cloud and consider what tooling shall be used for data analytics. On-premises, companies mostly use propriety software for advanced analytics, BI and reporting. However, this tooling may not be the most logical choice in a cloud environment. 9 min read 9 min read Ryan Burge Apr 25, 2019 Member-only The Religious Composition of the Two Major Parties I make a lot of data visualizations. Most of the time I have a pretty good idea of what the results of the calculations are going to look like before I write the code. In fact, some of the results are so obvious that I dont even post them anywhere 4 min read 4 min read Ashutosh Singh Apr 25, 2019 Member-only Classifying Products as Banned Or Approved using Text Mining- Part II In this part, we will explain how to optimize the existing Machine Learning model in Part I and the deployment of this ML model using Flask. In the previous article of this series, We have discussed the business problem, shown how to train the model using fastText and classification of Banned or Approved products based on information like(Product Name, product description, and specifications). 5 min read 5 min read Favio Vzquez Apr 25, 2019 The Jungle of Koalas, Pandas, Optimus and Spark What to expect from the newest library from Databricks (Koalas), the Optimus framework and Apache Spark 3.x If you are as excited about data science as me, you probably know that the Spark+AI latest summit started yesterday (April 24th 2019). And there are great things to talk about. But I will do it with a spin-off. If youve been following me you now that I co-created a 8 min read 8 min read Ahmad Tanehkar Behavioral Economics and Consumer Psychology Researcher More from Medium Samuele Mazzanti in Towards Data Science Using Causal ML Instead of A/B Testing Anil Tilbe in Towards AI Bayesian Inference: The Best 5 Models and 10 Best Practices for Machine Learning Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 6362,\n",
       "  'url': 'https://towardsdatascience.com/the-easiest-github-tutorial-ever-4a3aa0396039',\n",
       "  'title': 'The easiest GitHub tutorial\\xa0ever',\n",
       "  'subtitle': '-',\n",
       "  'claps': 117,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 3,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-12-05',\n",
       "  'clap_prop': 5.779253271082049e-05,\n",
       "  'text': 'Towards Data Science Dec 5 2019 Member-only Listen Save easiest GitHub tutorial ever ease Git GitHub made super simple tutorial get started learn advantage Git GitHub tutorial five super simple step basic fundamentally important understanding GitHub practical tutorial want in-depth read GitHub check article need know getting started GitHub 1 Make repository GitHub Go GitHub.com click Create Repository Give repository name description Check box Initialize repository README readme simple text file put information current project click Create repository 2 Clone repository local pc ITowork code need get repository onto pc done cloning easiest way get done GitHub Desktop course GitHub Desktop installed see following screen Choose correct location clone repository able find computer 3 Add code file repository step ignore GitHub browse pc location cloned repository create test file Name file testfile Put text save file 4 Commit push code repository ha changed update new version GitHub Go back GitHub Desktop see ha already noticed changed something Two step undertaken upload code First type small comment like Create testfile.txt click Commit master Second click Push origin 5 Verify code ha changed GitHub Finished already verify really worked see change GitHub go GitHub page repository youll see code ha changed taken first step world GitHub course much know stay tuned Thanks reading 180 180 180 Towards Data Science home data science Medium publication sharing concept idea code Joos Korstanje Data Scientist Machine Learning R Python AWS SQL Medium Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Jason Chong Geek Culture Struggling Land Data Science Job Try Virtual Internships Free Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Dec 5, 2019 Member-only Listen Save The easiest GitHub tutorial ever For those who are not at ease with Git and GitHub, I made this super simple tutorial to get started and learn what the advantage of Git and GitHub can be. After this tutorial in five super simple steps, you will have a very basic, but fundamentally important, understanding of GitHub. This is a practical tutorial. If you want a more in-depth read about GitHub, check out my other article here:   All you need to know before getting started with GitHub  . 1. Make a repository on GitHub Go to GitHub.com and click on Create Repository. Give your repository a name and a description. Check the box at Initialize this repository with a README (the readme is a simple text file in which you can put information about the current project) Then click Create repository. 2. Clone the repository to your local pc ITowork on your code, you need to get this repository onto your pc. This is done by cloning it. The easiest way to get this done is through GitHub Desktop. When you do this (of course you should have GitHub Desktop installed) you see the following screen: Choose the correct location and clone your repository. You will now be able to find it on your computer. 3. Add a code file to your repository For this step, ignore GitHub. Just browse on your pc to the location where you have cloned the repository and create a test file: Name the file testfile: Put some text in it and save the file: 4. Commit and push the code Our repository has now changed and we should update the new version to GitHub. Go back to GitHub Desktop and see that it has already noticed that you changed something: Two steps have to be undertaken to upload your code: First, type a small comment like Create testfile.txt and click on Commit to master. Second, click on Push origin: 5. Verify that the code has changed on GitHub Finished already! To verify that it really worked, you can now see your change on GitHub. Just go to the GitHub page of your repository and youll see that the code has changed! You have just taken your first step in the world of GitHub. Of course, there is much more to know, so stay tuned. Thanks for reading! 180 180 180 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Joos Korstanje Data Scientist Machine Learning R, Python, AWS, SQL More from Medium Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Jason Chong in Geek Culture Struggling to Land a Data Science Job? Try These Virtual Internships For Free Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 2674,\n",
       "  'url': 'https://towardsdatascience.com/deploy-ml-models-at-scale-151204549f41',\n",
       "  'title': 'Deploy ML models at\\xa0scale',\n",
       "  'subtitle': 'Part 1: API service for ML\\xa0models',\n",
       "  'claps': 107,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 4,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-05-27',\n",
       "  'clap_prop': 5.285300000049395e-05,\n",
       "  'text': 'Towards Data Science May 27 2019 Listen Save Deploy ML model scale Part 1 API service ML model Lets assume built ML model happy performance next step deploy model production blog series cover deploy model large scale consumption scalable Infrastructure using AWS using docker container service blog start first step building API framework ML model running local machine purpose blog let consider Sentiment classification model built order deploy model follow step Convert model .hdf5 file .pkl file case model built sklearn would best save .pkl file Alternatively deep learning model recommended save model HDF file main difference .pkl .hdf pickle requires large amount memory save data structure disk HDF designed efficiently store large data set Save model Pickle .pkl Save model HDF .hdf5 trained deep learning model kera tensorflow save model architecture weight using .hdf5 file system Implement Flask API Step 1 Load saved model per previous section one following method depending type file i.e hdf5 pkl HDF5 PKL Step 2 Import flask create flask application object shown Step 3 next step build test API function return API working string used ensure health API deployed production use app.route python decorator decorator function take another function extends behaviour latter function without explicitly modifying Step 4 next step build POST request api processing request sentiment model using path name /sentiment function read json input convert panda dataframe extract relevant field json call get_sentiment_DL function processing get_sentiment_DL function contains trained model ha loaded via hdf5 file finally return back result model form json result Step 5 detailed model processing step performed get_sentiment_DL function case deep learning sentiment model passing 2. text_data Input text sentiment classification 3. word_idx Word index GloVe file detail model Step 6 Add section run app Host set 0.0.0.0 hosting local server However configure network setting Debug set True time building API functionality Port set 5005 however configured per requirement Run API run API open command line window go directory code stored Run python script running command sentiment.py name file API implementation running command able see result command line window API running test API going browser typing 0.0.0.0:5005/apitest get result browser pas data API using python shown address case http //0.0.0.0:5005/sentiment result model returned stored response field Conclusion conclusion covered step deploy model api service local computer next step deploy cloud server micro service following blog cover use container service docker deploy AWS 121 2 121 121 2 Towards Data Science home data science Medium publication sharing concept idea code Joseph Magiya May 26 2019 Member-only Pearson Coefficient Correlation Explained Ive come realize lot confusion different type co-relation perform data set Let clear smoke starting Pearson Coefficient Correlation correlation Correlation bi-variate analysis measure strength association 4 min read 4 min read Share idea million reader Mai Nguyen May 26 2019 Member-only Exploratory Data Analysis Python B2B Marketing deep dive B2B Marketing using Data Visualization project focus conducting Exploratory Data Analysis EDA B2B Marketing using Python use data Olist e-commerce platform connects small medium business top Marketplaces Brazil example Besides providing method code also want discus 10 min read 10 min read Vincent Tatan May 26 2019 Member-only 12 minute Stocks Analysis Pandas Scikit-Learn Analyse Visualize Predict stock price quickly Python One day friend mine told key financial freedom investing stock greatly true market boom still remains attractive option today trade stock part time Given easy access online trading platform many 12 min read 12 min read Gilbert Tanner May 26 2019 Member-only Google Coral USB Accelerator Introduction Speeding machine learning model small form factor Last year Google Next conference Google announced building two new hardware product around Edge TPUs purpose allow edge device like Raspberry Pi microcontrollers exploit power artificial intelligence application image classification object detection 8 min read 8 min read Alexander Osipenko May 26 2019 GaleShapley algorithm simply explained article learn stable pairing stable marriage problem learn solve problem using Game Theory Gale-Shapley algorithm particular use Python create solution using theorem original paper 1962 stable marriage pairing problem real 4 min read 4 min read Prajwal Shreyas Data Scientist/ ML Engineer Experienced building deploying large scale ML model enhance business value Medium Dharmaraj Deploying Deep Learning Model using Flask API Nate DiRenzo Towards Data Science Deploying ML Models using Streamlit Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Maria Gusarova Build Beautiful Machine Learning Web App Streamlit Python Tutorial Python code included Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science May 27, 2019 Listen Save Deploy ML models at scale Part 1: API service for ML models Lets assume that you have built a ML model and that you are happy with its performance. Then the next step is to deploy the model into production. In this blog series I will cover how you can deploy your model for large scale consumption with in a scalable Infrastructure using AWS using docker container service. In this blog I will start with the first step of building an API framework for the ML model and running it in you local machine. For the purpose of this blog, lets consider the Sentiment classification model built here . In order to deploy this model we will follow the below steps: Convert the model into  .hdf5  file or .pkl file In case the model is a built on sklearn, it would be best to save it as a .pkl file. Alternatively if it is a deep learning model then it is recommended to save the model as a HDF file. The main difference between .pkl and .hdf is, pickle requires a large amount of memory to save a data structure to disk, where as HDF is designed to efficiently store large data sets. Save model in Pickle(.pkl): Save model in HDF(.hdf5): Once you have trained your deep learning model in keras or tensorflow you can save the model architecture and its weights using a .hdf5 file system. Implement a Flask API Step 1: Load the saved model (as per previous section) by one of the following methods depending on the type of file i.e. hdf5 or pkl. HDF5: PKL: Step 2: Import flask and create a flask application object as shown below: Step 3: The next step is to build a test API function which returns the API working string. This can be used to ensure the health of the API when it is deployed in production. Here we use @app.route, which is a python decorator ( a decorator is a function that takes another function and extends the behaviour of the latter function without explicitly modifying it) Step 4: The next step is to build a POST request api for processing requests to our sentiment model. We are using the path name /sentiment. The function reads the json input and converts it into pandas dataframe. It extracts the relevant fields from the json and calls the get_sentiment_DL function for processing. get_sentiment_DL function contains the trained model which has been loaded via hdf5 file. It finally will return back the results of the model in the form of json result. Step 5 : The detailed model processing steps will be performed by get_sentiment_DL function. In the case of our deep learning sentiment model we are passing: 2. text_data: Input text for sentiment classification 3. word_idx: Word index from the GloVe file (details of the model here ). Step 6: Add the below section to run the app. Here Host is set as 0.0.0.0 as we are hosting in our local server. However you can configure it to your network settings. Debug can be set to True at the time of building the API functionality. The Port is set to 5005, however this can be configured as per your requirement. Run the API To run the API, open a command line window and go to the directory where the code is stored. Run the python script by running the below command (sentiment.py is the name of the file with the above API implementation). On running the above command you will be able to see the below result in your command line window: Once the API is running you can test the API by going to your browser and typing 0.0.0.0:5005/apitest. You will get the below result in you browser. You can now pass any data to the API using python as shown below. The address in our case is http://0.0.0.0:5005/sentiment . The results of the model will be returned and stored in response field. Conclusion In conclusion, we have covered steps to deploy the model into an api service in your local computer. The next step is to deploy this in a cloud server as a micro service. In following blogs I will cover the use of container service such as docker and deploy it in AWS. 121 2 121 121 2 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Joseph Magiya May 26, 2019 Member-only Pearson Coefficient of Correlation Explained. Ive come to realize there is a lot of confusion about the different types of co-relation that you can perform on a data set. Let me clear up the smoke on this starting with the Pearson Coefficient of Correlation. What is correlation? Correlation is a bi-variate analysis that measures the strength of association 4 min read 4 min read Share your ideas with millions of readers. Mai Nguyen May 26, 2019 Member-only Exploratory Data Analysis with Python in B2B Marketing A deep dive into B2B Marketing using Data Visualization This project focuses on conducting Exploratory Data Analysis (EDA) for B2B Marketing using Python. We will use data from Olist, an e-commerce platform that connects small and medium business with top Marketplaces in Brazil, as an example. Besides providing the method and the code, I also want to discuss the 10 min read 10 min read Vincent Tatan May 26, 2019 Member-only In 12 minutes: Stocks Analysis with Pandas and Scikit-Learn Analyse, Visualize and Predict stocks prices quickly with Python One day, a friend of mine told me that the key to financial freedom is investing in stocks. While it is greatly true during the market boom, it still remains an attractive options today to trade stocks part time. Given the easy access to online trading platform, there are many 12 min read 12 min read Gilbert Tanner May 26, 2019 Member-only Google Coral USB Accelerator Introduction Speeding up machine learning models in a small form factor Last year at the Google Next conference Google announced that they are building two new hardware products around their Edge TPUs. Their purpose is to allow edge devices like the Raspberry Pi or other microcontrollers to exploit the power of artificial intelligence applications such as image classification and object detection 8 min read 8 min read Alexander Osipenko May 26, 2019 GaleShapley algorithm simply explained From this article, you will learn about stable pairing or stable marriage problem. You will learn how to solve that problem using Game Theory and the Gale-Shapley algorithm in particular. We will use Python to create our own solution using theorem from the original paper from 1962. What is a stable marriage or pairing problem? In the real 4 min read 4 min read Prajwal Shreyas Data Scientist/ ML Engineer Experienced in building and deploying large scale ML models to enhance business value. More from Medium Dharmaraj Deploying Deep Learning Model using Flask API Nate DiRenzo in Towards Data Science Deploying ML Models using Streamlit Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Maria Gusarova Build A Beautiful Machine Learning Web App With Streamlit | Python Tutorial (Python code included) Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 377,\n",
       "  'url': 'https://towardsdatascience.com/applied-ai-going-from-concept-to-ml-components-7ae9c5d823d3',\n",
       "  'title': '<strong class=\"markup--strong markup--h3-strong\">Applied AI: Going From Concept to ML Components</strong>',\n",
       "  'subtitle': '-',\n",
       "  'claps': 107,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 5.285300000049395e-05,\n",
       "  'text': 'Towards Data Science Mar 30 2019 Member-only Listen Save Applied AI Going Concept ML Components Opening mind different way applying machine learning real world Abraham Kang special thanks Kunal Patel Jae Duk Seo sounding board providing input article Executive Summary Candidate Problem Many people interested automating redundant process within organization using AI Lets start concrete problem noticed lawyer typically gather fact client something bad happens fact form basis cause action negligence battery assault intentional infliction emotional distress individual sue cause action determined based legal justification fact complaint written submitted court commencement legal action complaint legal document set fact giving rise legal basis taking action another party Manually creating document time consuming similar fact result similar cause action example someone hit another person usually battery someone accidentally hurt someone else someone slip fall within store could action negligence Based problem customer would like use AI learn write complaint fact paragraph describing happened Understanding Problem Trying get AI/ML read fact figure way AI/ML write whole complaint might biting model chew may effort would take year solve However take time understand think underlying problem find existing technique slight modification could used solve different piece puzzle example look complaint start description party position plaintiff v defendant well counsel representing may class action section justification jurisdiction doe court power party description party justification venue proper court location listing cause action description fact look section think data going build individual section going come certain case answer look carefully see pattern correlation different section complaint allow think input neural network candidate output Getting Inputs Neural Network dont data per se may way parse fact existing complaint use input neural network Every complaint submitted court becomes public information plenty data solution require attorney write fact inserting directly complaint minor inconvenience able machine learning provide generated complaint Generating complete complaint may difficult let break problem Breaking Problem Logically would break generation document smaller piece Well need look one example http //www.heise.de/downloads/18/1/8/9/1/3/4/6/NP-v-Standard-Innovation-Complaint.pdf make interesting picked maker adult toy might peak curiosity Basically want eventually generate complaint pdf fact provided lawyer look document complaint find similar pattern structure think would best way break thing dont scroll time think .Really think .. Well said break thing section using templating would route would probably best break complaint cause action listed complaint cause action violation Federal Wiretap Act Illinois Eavesdropping Statute Intrusion upon Seclusion Unjust Enrichment Fraud Deceptive Business Practice Act etc ha supporting rule justification based fact two problem come cause action fact text generate supporting text cause action Finding Causes Action look fact case need find cause action law broken could sue direct solution finding cause action text think fundamentally existing technique think use look text infer meaning description text said multi-label text classification multi-label sentiment analysis ahead game http //paperswithcode.com/task/text-classification http //paperswithcode.com/task/sentiment-analysis Analyzing text determine associated cause action similar process classifying text finding sentiment related text associated problem like fact cause action need updated law introduced may alternate way create embedding fact tie cause action fact based triplet http //arxiv.org/pdf/1503.03832.pdf quadruplet loss http //arxiv.org/pdf/1704.01719.pdf push cause action sharing similar word together embedding space unrelated cause action apart use clustering technique find cause action close determinative word embeddings used supporting argument associated word individual cause action section complaint Generating Text Supporting Arguments Section Individual Causes Action figured get high level cause action text generate supporting argument text individual cause action section violation Federal Wiretap Act Illinois Eavesdropping Statute Intrusion upon Seclusion Unjust Enrichment Fraud Deceptive Business Practice Act etc. one straight forward Think neural network architecture generate text Dont scroll idea .Open mind.Use Force Text generation algorithm http //paperswithcode.com/task/data-to-text-generation http //paperswithcode.com/area/nlp/text-generation might option even best one create gibberish often better alternative might use architecture like neural network involved translation http //paperswithcode.com/task/machine-translation http //paperswithcode.com/task/unsupervised-machine-translation http //paperswithcode.com/paper/unsupervised-clinical-language-translation addition might good idea separate translation neural network cause action help neural network focus identifying key fact used generating supporting argument cause action Clean probably going good idea run candidate text supporting argument text cause action grammar checker/fixer http //paperswithcode.com/task/grammatical-error-correction way blatant mess ups fixed Conclusion hope learned apply machine learning solution broadly Let know get stuck would definitely interested hearing problem people trying solve machine learning 107 2 107 107 2 Towards Data Science home data science Medium publication sharing concept idea code Michel Kana Ph.D Mar 30 2019 Wild Wide AI responsible data science shoot first new race human Data Science good thing u improves life make thing efficient effective lead better experience however miss-steps data-driven analysis ha already exhibited example data science tool intentionally unintentionally misused 2012 9 min read 9 min read Share idea million reader Kevin Luk Mar 30 2019 Member-only library load image Python difference Summarization Comparison .imread face computer vision project first need load image preprocessing various library perform imread want consolidate popular library loading image difference article go Libraries loading 3 min read 3 min read Abhishek Mukherjee Mar 30 2019 Trust interpretability machine learning machine learning model always need interpretable Given choice interpretable model inaccurate non-interpretable model accurate wouldnt rather choose non-interpretable accurate model word reason sacrificing accuracy altar interpretability 9 min read 9 min read Christine Calo Mar 30 2019 Microsoft Introduction AI Part 1 Machine Learning bit like wanted learn Artificial Intelligence although felt little intimidated math involved Maybe thought concept difficult understand would depth recently completed Microsoft Introduction AI course 17 min read 17 min read Okoh Anita Mar 30 2019 Repetition Songs Python Tutorial One Ed Sheeran song case study Everyone ha heard song know song sound like carelessly say everyone define song word benefit doubt song according Wikipedia single work music typically intended sung 8 min read 8 min read Abraham Kang Abraham Kang fascinated nuanced detail security associated machine learning algorithm programming language associated APIs Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Anmol Tomar CodeX Say Goodbye Loops Python Welcome Vectorization Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Mar 30, 2019 Member-only Listen Save Applied AI: Going From Concept to ML Components Opening your mind to different ways of applying machine learning to the real world. By Abraham Kang with special thanks to Kunal Patel and Jae Duk Seo for being a sounding board and providing input for this article. Executive Summary Candidate Problem Many people are interested in automating redundant processes within the organization using AI. Lets start with a concrete problem, what I noticed is that lawyers typically gather facts from clients when something bad happens. These facts form the basis of causes of action (negligence, battery, assault, intentional infliction of emotional distress) that an individual can sue on. Once the causes of action have been determined based on legal justification and the facts, a complaint is written up and submitted to the court for commencement of the legal action. The complaint is a legal document which sets out the facts giving rise to a legal basis for taking action against another party. Manually creating this document can be time consuming and similar facts result in similar causes of action. For example, if someone hits another person there is usually a battery. If someone accidentally hurts someone else or someone slips and falls within a store there could be an action for negligence. Based in this problem we have a customer who would like to use AI to learn how to write a complaint from a fact paragraph describing what happened. Understanding the Problem Trying to get AI/ML to read facts and figure out a way for AI/ML to write a whole complaint might be biting off more than the model can chew and may be an effort that would take years to solve. However, if you take the time to understand and think about the underlying problem, you can find existing techniques (with some slight modifications) that could be used to solve different pieces of the puzzle. For example, when you look at a complaint it starts with a description of the parties and their positions (plaintiff vs defendant) as well as counsel representing them. There may be a class action section, a justification of jurisdiction (does court have power over parties), description of the parties, a justification of venue (are we in the proper court location), a listing of the causes of action, and description of the facts. When you look at the sections you have to think about where the data that is going to build the individual sections is going to come from. In certain cases you will not have an answer but if you look carefully you will see patterns and correlations between different sections of the complaint. This will allow you to think about what your inputs to the neural network will be and the candidate outputs. Getting Inputs for the Neural Network We dont have any data per se but there may be a way to parse the facts out of all existing complaints and use them as the input for our neural network. Every complaint that is submitted to the court becomes public information so there will be plenty of data. This solution will require attorneys to write their facts as if they were inserting them directly into the complaint, but this is a minor inconvenience to be able to have machine learning provide generated complaints. Generating a complete complaint may be difficult. So lets break the problem down. Breaking the Problem Down Logically how would you break the generation of a document down into smaller pieces? Well you need to look at one so here is an example: https://www.heise.de/downloads/18/1/8/9/1/3/4/6/NP-v-Standard-Innovation-Complaint.pdf . To make it interesting I picked a maker of adult toys so it might peak your curiosity. Basically, we want to eventually generate a complaint (above pdf) from the facts provided by a lawyer. So if you look at the document and at other complaints you will find similar patterns as to structure. So what do you think would be the best way to break things down dont scroll down until you have had time to think about it. .Really think about it.. Well if you said to break things down by section using templating, then this would be the route that would probably be best. When you break down a complaint there are causes of action listed in the complaint. Each cause of action (violation of Federal Wiretap Act, Illinois Eavesdropping Statute, Intrusion upon Seclusion, Unjust Enrichment, Fraud and Deceptive Business Practice Act, etc.) has supporting rules and justification based on the facts. So now there are two problems. How do you come up with the causes of action from the facts text and how do you generate the supporting text under each cause of action? Finding the Causes of Action When we look at the facts of the case we need to find all of the causes of action (laws that were broken) that we could sue on. There are no direct solutions for finding causes of action from text so we will have to think more fundamentally. What existing techniques do you think we can use to look at text and infer meaning or a description of the text. If you said multi-label text classification or multi-label sentiment analysis, then you are ahead of the game ( https://paperswithcode.com/task/text-classification , https://paperswithcode.com/task/sentiment-analysis). Analyzing text to determine its associated causes of action is a similar process to classifying text or finding the sentiment of related text. There are associated problems like the fact that causes of action will need to be updated as laws are introduced. There may be an alternate way to create an embedding for the facts and then tie the causes of action to the facts based on triplet (https://arxiv.org/pdf/1503.03832.pdf) or quadruplet loss (https://arxiv.org/pdf/1704.01719.pdf) to push causes of action sharing similar words together in the embedding space and unrelated causes of action further apart. Then use a clustering technique to find causes of action close to determinative word embeddings used in the supporting argument associated with the words in the individual cause of action sections of the complaint. Generating the Text in the Supporting Arguments Section of Individual Causes of Action Now that you have figured out how to get the high level causes of action from the text, how can you generate the supporting argument text for each of the individual cause of action sections (violation of Federal Wiretap Act, Illinois Eavesdropping Statute, Intrusion upon Seclusion, Unjust Enrichment, Fraud and Deceptive Business Practice Act, etc.)? This one is not so straight forward. Think about a what neural network architectures which generate text (Dont scroll down until you have some ideas). .Open your mind.Use the Force. Text generation algorithms ( https://paperswithcode.com/task/data-to-text-generation , https://paperswithcode.com/area/nlp/text-generation ) might be an option but even the best ones create gibberish often. The better alternative might be to use an architecture like neural networks involved in translation ( https://paperswithcode.com/task/machine-translation , https://paperswithcode.com/task/unsupervised-machine-translation , https://paperswithcode.com/paper/unsupervised-clinical-language-translation ). In addition, it might be a good idea to have a separate translation neural network for each cause of action to help each neural network focus on identifying the key facts used in generating a supporting argument for each cause of action. Clean Up It is probably going to be a good idea to run the candidate text for the supporting argument text for each cause of action through a grammar checker/fixer ( https://paperswithcode.com/task/grammatical-error-correction ). This way any blatant mess ups are fixed. Conclusion I hope you learned how to apply the machine learning solutions more broadly. Let me know if you get stuck as I would definitely be interested in hearing about problems that people are trying to solve with machine learning. 107 2 107 107 2 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Michel Kana, Ph.D Mar 30, 2019 Wild Wide AI: responsible data science Who shoots first the new race or the human? Data Science can do good things for us: it improves life, it makes things more efficient, more effective and leads to a better experience. There are however some miss-steps that data-driven analysis has already exhibited. Here are few examples where data science tools were intentionally or unintentionally misused: In 2012 9 min read 9 min read Share your ideas with millions of readers. Kevin Luk Mar 30, 2019 Member-only What libraries can load image in Python and what are their difference? Summarization & Comparison of .imread() When we face computer vision project, first of all we need to load the images before any preprocessing. There are various libraries out there to perform imread() . Here I want to consolidate the popular libraries for loading image and their difference. This article will go through: Libraries for loading 3 min read 3 min read Abhishek Mukherjee Mar 30, 2019 Trust and interpretability in machine learning Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldnt you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability? 9 min read 9 min read Christine Calo Mar 30, 2019 Microsoft Introduction to AI Part 1 Machine Learning Are you a bit like me and have wanted to learn about Artificial Intelligence although felt a little intimidated by the maths involved? Maybe you thought the concepts were too difficult to understand and you would be out of your depth. I recently completed the Microsoft Introduction to AI course 17 min read 17 min read Okoh Anita Mar 30, 2019 Repetition in Songs: A Python Tutorial One of Ed Sheeran songs as a case study Everyone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung 8 min read 8 min read Abraham Kang Abraham Kang is fascinated with the nuanced details and security associated with machine learning algorithms, programming languages and their associated APIs. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Anmol Tomar in CodeX Say Goodbye to Loops in Python, and Welcome Vectorization! Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4686,\n",
       "  'url': 'https://towardsdatascience.com/serverless-ml-3184c9c45f93',\n",
       "  'title': 'Serverless ML',\n",
       "  'subtitle': 'Bring your models to life with AWS\\xa0Lambdas.',\n",
       "  'claps': 105,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 10,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-12-20',\n",
       "  'clap_prop': 5.1865093458428644e-05,\n",
       "  'text': \"Towards Data Science Dec 19 2019 Listen Save Serverless ML Bring model life AWS Lambdas Introduction Working day day wrangling data set engineering interesting feature testing training heap heap model leave feeling bit disconnected real-world system Well rate Recently Ive spent time software engineer whove helped understand ML model deployed cloud leveraging serverless architecture little cost result model ensemble model fit larger system via simple API call Using AWS Lambdas dont need keep costly EC2 instance running 24/7 Lambdas spin really fast required Google Cloud similar offering called Cloud Functions Ill focusing AWS offering background course running EC2 server Lambdas abstracted away dont need bother EC2s many good reason think go serverless dont want run advert read Lambda essentially small fast container preconfigured one number different runtimes include data scientist work principally Python day tutorial Ill walk deploying simple ML model AWS Lambda accompanying github repo includes code youll need along model data training data Model exercise training great model use case used historical weather data Cape Town train model predict likelihood rain tomorrow based Cape Towns history feed city world predict whether rain tomorrow based Cape Towns history Thats ok really point Point unfamiliar Lambdas disillusioned cost housing Docker Containers EC2 instance hopefully give idea cheap easy think larger complex system model code luck creative juice stimulated Requirements look little tedious worth figuring Much need Ill gloss thing like AWS execution role key management Additionally tutorial use AWS web console CLI youre unfamiliar CLI hopefully easy introduction Python 3.6 use Python 3.6 Lambda runtime environment runtime come preconfigured layer containing scipy/numpy package OS dependency present Lambda environment default layer critical u use low level math function SKLearn see Python 3.7 runtime also ha scipy/numpy layer Docker order build layer need install Python package locally environment match Lambdas remote environment Solution Docker install youre Ubuntu get going Github repo github repo available tutorial Directory structure important Ill assume youre running command project root Youll need free account order query weather data Sign http //darksky.net/dev signup login home page show secret key Store layers/03_rain_model/keys.csv LocationIQ Sign free account Location IQ http //locationiq.com geocoding Create Access Token store layers/03_rain_model/keys.csv AWS Setup Account havent already set free AWS account Install AWS CLI AWS CLI provides command line access AWS need install typically pip configure use AWS account TLDR Ubuntu version check instruction system troubleshooting Configure Well need use console configure AWS CLI Logged AWS console Back terminal detailed instruction Create S3 bucket project Back web console S3 section create empty bucket project Bucket name unique across bucket call something remember course also get done CLI Ill call mine severless-ml-tutorial US West Oregon match config 'll need adjust code match name think place could potentially incur cost S3 storage free although scale cheap building layer delete bucket save potential cost couple bucket MB inside havent incurred cost yet little confusing Still well done Ok let build first simple Lambda expand later Create Lambda Lambda created property screen open Add API Gateway trigger configure http gateway interact new Lambda Add test event add example query test Lambda respond API call sending Lambda name city let set test event Add code last add code extremely simple assigns incoming string variable print return formatted HTML string containing string Designer panel Scroll Function code panel got simple Lambda accessible via URL send payload process return payload case HTTP response Extending Lambda layer add code interesting thing let make Lambda architecture little powerful adding layer Layers simply additional resource youre including Lambda container size constraint deal dont go crazy section get back code extend Lambda github repo youll see file required build layer nested within layer directory Typically need collate content layer upload S3 compile content Lambda layer attach layer Lambda 01 Scipy/Numpy layer layer precompiled AWS includes python library also system library need use really difficult layer build thankfully AWS done heavy lifting u dont need build layer 02 Dependencies layer layer include required Python library betond installed default collate content layer local machine using docker container resembles Lambda environment namely lambci within lambci container use pip install library local directory outside container use directory build new layer terminal project root worked expected see python directory inside 02_dependencies layer directory Zip layer content need compress layer content preparation uploading S3 Upload layer S3 Using AWS CLI instead web Console sync content zip directory location S3 good idea keep web console open see youre desired effect use uploaded zip file update create Lambda layer used Python 3.6 Lambda Remember change name bucket next S3Bucket= 03 Rain model layer file layer already exist layers/03_rain_model/ directory Docker hijinks needed need put api key file called keys.csv.template rename keys.csv Despite name main purpose layer Ill include API key addition model object NB correct/secure way manage key cloud DONT IMPORTANT KEYS Zip layer content need compress layer content preparation uploading S3 Upload layer S3 upload layer content S3 Build/update layer 03_rain_model build layer Remember change bucket name Check layer successfully created Use AWS web console make sure layer exist S3 section Lambdas section Add new Layers Lambda add required layer Lambda done set-up web console open Lambda Designer pane click Layers Dont forget click Save Thats set-up youve gotten far well done Writing seems long thats left build code Build code following step expands code previous step follow along see functionality grows add block code bottom previous overwriting previous code return brace updating saving step reload API endpoint URL submit different city youd like Alternatively find full code severless-ml-lambda.py Add geolocation block load API key us LoationIQ key geocode city submitted result printed returned via http Add weather query code take DarkSky key LocationIQ GPS co-ordinate return current weather condition city submitted Add model prediction Add code block load model pushed layer 3 make prediction weather might rain tomorrow NB run memory error test Scroll Basic setting panel increase Lambdas memory 1024 Mb youre increase timeout 10 sec Add writing S3 last step writes result query S3 really little extra show simple interact AWS infrastructure Python Lambdas Conclusion Admittedly wa quite long tutorial Hopefully ha illustrated trained model incorporated larger codebase using serverless function Enjoy Easier read http //philmassie.github.io/post/20191220/serverless_ml/ 152 1 152 152 1 Towards Data Science home data science Medium publication sharing concept idea code Ryan Burn Dec 19 2019 Member-only Form Cross-Validation Use Optimize right proxy out-of-sample prediction error Cross-validation partition dataset train validates model complementary subset average prediction error way datapoint validated out-of-sample prediction averaging error out-of-sample prediction across whole dataset hope cross-validation error act proxy 8 min read 8 min read Share idea million reader Bhanu Yerra Dec 19 2019 Car Image Classification Using Features Extracted Pre-trained Neural Networks Corvette Introduction According 2018 Used Car Market Report Outlook published Cox Automotive 40 million used vehicle sold US last year represents 70 total vehicle sold good portion sale already use online resource along various stage purchasing searching 6 min read 6 min read Shengyu Huang Dec 19 2019 Simple Python Script Document SQLite Databases Autogenerate markdown file document SQLite database issue constantly harasses work relational database documentation Entity relationship diagram standard far useful column name self-explanatory schema simply becomes large handle SchemaSpy open-source tool autogenerate ER 2 min read 2 min read Dilyan Kovachev Dec 19 2019 Member-only EPL Fantasy GW17 Recap GW18 Algorithm Picks Moneyball approach Fantasy EPL team_id 2057677 first time land one Fantasy EPL Blogs might want check original EPL blog Medium Profile get familiar overall approach improvement weve made time partner crime 7 min read 7 min read Nathan Rosidi Dec 19 2019 Member-only Technical Interview 6 Red Flags Watch Interviews Today going focus one important part recruitment process one probably feared technical interview first interview screening call recruiter usually technical interview sometimes done 5 min read 5 min read Phil Massie Data Scientist Medium Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Maria Gusarova Call Amazon SageMaker model endpoint using Amazon API Gateway AWS Lambda Barr Moses Towards Data Science Whats Next Data Engineering 2023 7 Predictions Emily Webber trained 10TB Stable Diffusion SageMaker Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Towards Data Science Dec 19, 2019 Listen Save Serverless ML Bring your models to life with AWS Lambdas. Introduction Working day in and day out wrangling data sets, engineering interesting features and testing and training heaps and heaps of models can leave you feeling a bit disconnected from real-world systems. Well, it can for me at any rate. Recently Ive spent time with software engineers whove helped me understand how ML models can be deployed in the cloud by leveraging serverless architectures, at little or no cost. As a result, models or ensembles of models can fit into larger systems via simple API calls. Using AWS Lambdas, we dont need to keep costly EC2 instances up and running 24/7. Lambdas spin up really fast as required. (Google Cloud have a similar offering called Cloud Functions but here Ill be focusing on the AWS offering.) In the background of course they are all running on EC2 servers but Lambdas are so abstracted away from them that you dont need to bother about the EC2s. There are many good reasons to think go serverless but I dont want to run an advert. You can read more here A Lambda is essentially a small fast container preconfigured with one of a number of different runtimes. These include: As a data scientist I work principally with Python these days and in this tutorial Ill walk you through deploying a simple ML model in an AWS Lambda. There is an accompanying github repo here which includes all the code youll need along with the model, data and training data. The Model This is not an exercise in training a great model. As a use case I used historical weather data for Cape Town to train a model to predict the likelihood of rain tomorrow based on Cape Towns history. You can feed it any city in the world, and it will predict whether it will rain there tomorrow based on Cape Towns history. Thats ok, its not really the point. The Point If you are unfamiliar with Lambdas or disillusioned with the costs of housing your Docker Containers on EC2 instances, hopefully this will give you an idea of how cheap and easy it can be to think about larger more complex systems of models and code. With luck your creative juices will be stimulated. Requirements This looks a little tedious but is worth figuring out. Much of it you will only need to do once. Ill gloss over some things here like AWS execution roles and key management. Additionally the tutorial will use both the AWS web console and the CLI. If youre unfamiliar with the CLI, hopefully this will be an easy introduction to it. Python 3.6 We will use the Python 3.6 Lambda runtime environment as this runtime comes with a preconfigured layer containing scipy/numpy. Because these packages have OS dependencies not present in the Lambda environment by default, this layer is critical for us to use any of the low level math functions such as those in SKLearn. I see that the Python 3.7 runtime also has a scipy/numpy layer now! Docker In order to build layers we need to install Python packages locally in an environment that matches the Lambdas remote environment. Solution: Docker ( how to install ) If youre in Ubuntu, this should get you going: Github repo A github repo is available for this tutorial. Directory structure is important and Ill assume youre running commands from the project root. Youll need a free account here in order to query weather data. Sign up at https://darksky.net/dev . After signup and login, your home page should show you your secret key. Store this in layers/03_rain_model/keys.csv . LocationIQ Sign up for a free account at Location IQ https://locationiq.com for geocoding. Create an Access Token and store this in layers/03_rain_model/keys.csv . AWS Setup Account If you havent already, set up your free AWS account. Install AWS CLI AWS CLI provides command line access to AWS. You need to install it, typically with pip, and configure it for use with your AWS account. TLDR Ubuntu version check these instructions for other systems and troubleshooting : Configure Well need to use the console to configure AWS CLI. Logged in to the AWS console: Back to your terminal ( detailed instructions ): Create an S3 bucket for the project Back to the web console, S3 section, create an empty bucket for this project. Bucket names have to be unique across all buckets so call this something you can remember. You can of course also get this done with the CLI. Ill call mine severless-ml-tutorial in US West Oregon (to match our config). you'll need to adjust the code below to match this name. I think this is the only place you could potentially incur some costs. S3 storage is not free, although at this scale it is very cheap. After building your layers delete this bucket to save potential costs. I have a couple buckets with a few MB inside and I havent incurred a cost yet. Its a little confusing. Still there? well done! Ok lets build our first simple Lambda. We will expand on it later. Create a Lambda Your Lambda will be created and its properties screen will open. Add an API Gateway trigger Now we will configure an http gateway where we can interact with our new Lambda. Add a test event Now we add an example query so that we can test how our Lambda will respond to API calls. We will be sending our Lambda the name of a city so lets set up a test event to do that. Add some code Now at last we can add some code. This is extremely simple, It assigns the incoming string to a variable, prints it out and returns a formatted HTML string containing the string. Designer panel Scroll down to Function code panel You have now got a simple Lambda, accessible via a URL, you can send it a payload, and it can process and return the payload, in this case as an HTTP response. Extending the Lambda layers Before we add more code to do more interesting things, lets make our Lambda architecture a little more powerful. We can do this by adding layers. Layers are simply additional resources youre including in your Lambda container There are size constraints to deal with so dont go too crazy. After this section we can get back to the code and extend the Lambda. If you have the github repo youll see that I have the files required to build each layer nested within a layers directory. Typically we need to collate the contents of each layer, upload them to S3, compile the contents into Lambda layers and then attach the layers to our Lambda. 01 Scipy/Numpy layer This layer is precompiled by AWS and includes not only python libraries but also system libraries that we need to use. This is a really difficult layer to build for yourself but thankfully AWS have done the heavy lifting for us. We dont need to build this layer. 02 Dependencies layer This layer will include all of the required Python libraries betond those installed by default. We will collate the contents of this layer on our local machine using a docker container that resembles the Lambda environment, namely lambci. From within lambci container, we can use pip to install the libraries to a local directory, outside of the container. We then use this directory to build our new layer. In the terminal, from the project root: If that worked as expected you should now see a python directory inside your 02_dependencies layer directory. Zip the layer contents We need to compress the layer contents in preparation for uploading to S3. Upload the layer to S3 Using the AWS CLI instead of the web Console, sync the contents of your zips directory with a location in S3. Its a good idea to keep the web console open to see that what youre doing is having the desired effect. Now we can use the uploaded zip file to update or create a Lambda layer that can be used in any Python 3.6 Lambda. Remember to change the name of your bucket next to S3Bucket=. 03 Rain model layer The files for this layer already exist on the layers/03_rain_model/ directory, so no Docker hijinks needed here. You need to put your api keys into the file called keys.csv.template and rename it to keys.csv . Despite the name and main purpose of the layer, Ill include the API keys in addition to the model object. NB This is NOT the correct/secure way to manage keys in the cloud. DONT DO THIS WITH IMPORTANT KEYS! Zip the layer contents We need to compress the layer contents in preparation for uploading to S3. Upload the layer to S3 As before, upload the layer contents to S3 Build/update layer 03_rain_model and build the layer. Remember to change your bucket name here. Check that layers have been successfully created Use the AWS web console to make sure layers exist. S3 section Lambdas section Add the new Layers to your Lambda We will add all the required layers to our Lambda and then were done with the set-up. In the web console, open your Lambda In the Designer pane, click Layers. Dont forget to click Save! Thats it for the set-up. If youve gotten this far well done! Writing this out, it seems very long. All thats left now is to build out the code! Build out the code Each of the following steps expands on the code from the previous step. To follow along and see how the functionality grows, add each block of code to the bottom of the previous, overwriting the previous codes return braces. After updating and saving at each step, reload the API endpoint URL and submit a different city if youd like. Alternatively, find the full code in severless-ml-lambda.py. Add geolocation This block loads up the API keys and uses the LoationIQ key to geocode the city you submitted. These results are printed and returned via http. Add weather query This code takes the DarkSky key and the LocationIQ GPS co-ordinates and returns the current weather conditions for the city you submitted. Add model prediction Add this code block to load up the model you pushed in layer 3 and make a prediction about weather it might rain tomorrow. NB you will run into memory errors when you test this. Scroll down to Basic settings panel and increase your Lambdas memory to 1024 Mb. While youre there increase your timeout to 10 sec. Add writing to S3 This last step writes the results of each query out to S3. Its really just a little extra to show how simple it can be to interact with the AWS infrastructure from Python and Lambdas. Conclusion Admittedly that was quite a long tutorial. Hopefully it has illustrated how a trained model can be incorporated into a larger codebase by using serverless functions. Enjoy. Easier to read at  https://philmassie.github.io/post/20191220/serverless_ml/ 152 1 152 152 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ryan Burn Dec 19, 2019 Member-only What Form of Cross-Validation Should You Use? Optimize the right proxy for out-of-sample prediction error Cross-validation partitions a dataset, trains and validates models on complementary subsets, and averages prediction errors in such a way that each datapoint is validated once as an out-of-sample prediction. By averaging errors of out-of-sample predictions across the whole dataset, we hope that the cross-validation error acts as a proxy for 8 min read 8 min read Share your ideas with millions of readers. Bhanu Yerra Dec 19, 2019 Car Image Classification Using Features Extracted from Pre-trained Neural Networks Is that a Corvette? Introduction According to the 2018 Used Car Market Report & Outlook published by Cox Automotive, 40 million used vehicles were sold in the US last year. This represents about 70% of the total vehicles sold. A good portion of these sales already use online resources along various stages of purchasing: searching 6 min read 6 min read Shengyu Huang Dec 19, 2019 A Simple Python Script to Document SQLite Databases Autogenerate a markdown file to document the SQLite databases This issue constantly harasses me when I work with relational databases: documentation. Entity relationship diagram is the standard, but it is far from useful when the column names are not self-explanatory or the schema simply becomes too large to handle. SchemaSpy is an open-source tool that can autogenerate an ER 2 min read 2 min read Dilyan Kovachev Dec 19, 2019 Member-only EPL Fantasy GW17 Recap and GW18 Algorithm Picks Our Moneyball approach to the Fantasy EPL (team_id: 2057677) If this is the first time you land on one of my Fantasy EPL Blogs, you might want to check out some of my original EPL blogs in my Medium Profile to get familiar with our overall approach and the improvements weve made over time. My partner in crime for 7 min read 7 min read Nathan Rosidi Dec 19, 2019 Member-only What Not To Do During a Technical Interview 6 Red Flags to Watch Out For In Interviews Today were going to focus on one of the most important parts of the recruitment process and the one that is probably the most feared: the technical interview. Your first interview (other than the screening call with the recruiter) is usually the technical interview, which is sometimes done over the 5 min read 5 min read Phil Massie Data Scientist More from Medium Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Maria Gusarova Call an Amazon SageMaker model endpoint using Amazon API Gateway and AWS Lambda. Barr Moses in Towards Data Science Whats Next for Data Engineering in 2023? 7 Predictions Emily Webber How I trained 10TB for Stable Diffusion on SageMaker Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 4098,\n",
       "  'url': 'https://towardsdatascience.com/exploration-and-visualization-on-each-presidential-candidate-supporters-tweets-in-indonesia-a2b26c180f7e',\n",
       "  'title': 'Exploration and Visualization on each Presidential Candidate Supporter’s Tweets in Indonesia',\n",
       "  'subtitle': '-',\n",
       "  'claps': 98,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 18,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-01-26',\n",
       "  'clap_prop': 4.840742056120007e-05,\n",
       "  'text': \"Towards Data Science Jan 26 2019 Listen Save Exploration Visualization Presidential Candidate Supporters Tweets Indonesia Hand Practice using Plotly visualize finding data following Data Science Process Introduction Hello Welcome article time want write something different Ive written far article ha published implementing Artificial Intelligence especially Deep Learning article want write different topic think topic also important want develop model step done modelling Exploratory Data Analysis EDA Sorry Ive busy preparing new job Even busy still want write medium share knowledge Whenever free time write article little little article also want write telling code also implement real study case data used twitter scraped library case Analysis presidential candidate supporter Indonesia 2018 Even text Indonesian Language try make understand meaning text word data used article based tweet crawled Maybe tweet crawler ha skipped Even say data sample population representative data Ive crawled around 700.000 tweet fun reading statistic go feel free skip technical detail article Oh also include conclusion insight weve found data write analysis explanation visualization create conclusion mind want dont also EDA course write Outline EDA Taken KDNuggets step Data Science creating model Gather Data Process Data Clean Data finally EDA Actually EDA help u better making model quality improves knowing data feel learning algorithm learns data well feel think impossible instance predicted neither learner process EDA done modelling every task Data Science Machine Learning except Pure Reinforcement Learning doesnt data train example Natural Language Processing NLP Sentiment Analysis maybe make better model removing word found EDA Another example Computer Vision CV maybe found model cant learn inability model predict instance data unique noise anomaly hasnt cleaned previously data cleaning step many others benefit EDA modelling Maybe get intuition make model better step read previous article generating Deep Learning Lyric skipped part Sometimes generating lyric text dont want appear REFF example well didnt notice noticed EDA first need know step go Steps First must define Research Questions mean define want search Questions guide next step also need answered next step example Research Question List anything need search question form Typically start pin 5W 1H opinion since gain new insight later increase number question depending finding focusing main Research Question Second need collect data Make sure search dataset according research question already defined several way step search structured data set data set repository Kaggle UCL Repository also search unstructured data scrapping social medium Facebook Twitter using API possible depends research question Often find data need structured format Third Preprocess Clean Data Data Wrangling painful annoying step let head explode step step painful need know data Find pest need cleaned Example pest NLP Typo Informal Word Emoticon Elongated Word Many Hidden Bomb usually remove make visualization model better data structured format need convert structured format sake making cleaning data easier Let tell often go back step many time since often find pest next step Finally EDA Exploratory Data Analysis answer Research Question defined First Steps Usually Visualization communicate result visualize plot data see data printing result Remember result analysis seen result seen others sake UX User Experience need present data visualizing make sure reader become comfortable see result think article seen many people present result cropping screenshot printed result CLI Command Line Interface article EDA Jupyter Notebook since ha beautiful UX also used present data Technology use Step One Research Question Okay formulate question let tell 2019 presidential election Indonesia 2 candidate Joko Widodo Jokowi Maaruf Amin 01 Prabowo Subianto Sandiaga Uno Sandi 02 focus article supporter candidate social medium article focus Twitter Social Media researching googling Ive found hashtags used Jokowis Prabowos Supporter declare support simplicity Ive narrowed hashtags limit year tweet 2018 Thats Lets create research question research question done Actually many question ask sake make article become 1 hour read time cut question want ask twelve Step Two Collect Data step use Twint library article scope data follow scraped 706.208 tweet step Step Three Preprocess Data Okay since article focus EDA make step short possible First read csv panda tweet pd.read_csv tweet.csv encoding=utf-8 make simple formalize word remove stopwords formalize two way first regex substitute known slang word formal word later need dictionary slang word formal word formalized word better way cleaning formalize_rule function use nltk TweetTokenizer Well want try regex thats also implement removing stopword formalize_word apply DataFrame Note step reality come back step several EDAs Sometimes find pest EDA step done move onto last step Step Four Exploratory Data Analysis Since Research Questions want answer answered end Data Science process step Without ado let answer question Wait define function used multiple time defined DataFrame filter use often later also create function output statistic DataFrame statistic plotted Plotly 0 many instance data 1 frequency supporter tweet dataset take attribute shape supporter tweet need show plot Jupyter Lets set plot many way visualize data Since comparable use Pie chart visualize data two minimal component needed plot plotly First data data set data type chart want visualize combine multi kind type chart example visualize Bar chart Pie Chart visualization Second layout layout container visualization customize title legend axis many combine container chart putting go.Figure figure figure ready plotted Plot Analysis Prabowos supporter tweet frequency higher Jokowis supporter tweet 2 top-30 frequency supporter tweet month Since data want plot sequential plot line chart First filter loop month Reverse list Dec Jan Jan Dec plot Plot Analysis Prabowos supporter tweet usually frequency Jokowis supporter tweet tweet peak September 3 seeing largest frequency tweet month word top-30 frequency month supporter tweet First set largest frequency tweet September Sep find highest frequency word using function take tweet posted September plot limit TOP 30 highest frequency Since data sequential Bar chart right choice Word cloud also good visualize word frequency dont want know frequency word Plot Analysis jokowi ha highest frequency September ha around 35k 40k frequency followed Indonesia Orang Person Presiden President Rakyat Citizen Dukung Support Allah God Prabowo others 4 seeing largest frequency tweet month Top-30 word frequency month Jokowi supporter tweet similar RQ Research Question 3 difference need filter Jokowis supporter tweet Plot Analysis jokowi word also highest difference word big ha positive word berbagi sharing tulu sincere bergerak move also ha Allah word 5 Top-30 word frequency month Prabowo supporter tweet similar RQ 4 filter Prabowo supporter tweet Plot Analysis unexpected jokowi frequency higher prabowo highest one indonesia difference word frequency big word notice ulama Muslims Scholar Cleric rezim regime cebong tadpole bad alias jokowis supporter prabowos supporter emak group mother bangsa nation also ha Allah word 6 Top-30 frequency Token accompany prabowo word Jokowi Supporters Tweet month since often plot writing code many time create function reusable filter dataframe according need Plot Analysis Jokowi highest frequency notice word interesting uang money thecebongers tadpole prestasinya achievment survei survey asing foreign country 7 Top-30 frequency Token accompany jokowi word Prabowo Supporters Tweet month filter dataframe according need Plot Analysis Prabowo highest frequency notice word interesting gerakan movement ulama mahasiswa college student rupiah Indonesia currency rezim regime 8 Top-30 frequency Token accompany prabowo word Prabowo Supporters Tweet month filter dataframe according need Analysis sandi ha highest frequency ha big gap word word got attention ulama allah emak gerakan ijtima ulamas/muslim schoolarss decision jokowi also second highest frequency 9 top-30 frequency Token accompany jokowi word Jokowi Supporters Tweet month filter dataframe according need Plot Analysis prabowo 20 highest frequency different Anyway word got attention blokir blocked pembangunan construction kepemimpinan leadership allah hebat great bergerak move 10 top-30 frequency Hashtags Prabowo Supporters Tweet Since hashtags column string format need cast type list using eval join content list call previous function see statistic data limited September Plot Analysis Hashtags eye set 2019tetapantipki 2019 Stay Anti-communism mahasiswabergerak College student move rupiahlongsor jokowilengser Rupiah Fall Jokowi stepped jokowi2periode Jokowi two Periods last hashtags hashtags Jokowis supporter hashtags mostly talk changing president negative thing Jokowi 11 Top-30 frequency Hashtags Jokowi Supporters Tweet really similar RQ 10 Plot Analysis Hashtags eye set indonesiamaju Advanced Indonesia jokowimembangunindonesia Jokowi Build Indonesia kerjanyata Visible Work diasibukkerja Hes busy working Mostly hashtags keeping Jokowi president positive thing Jokowi 2019gantipresiden hashtags Prabowos supporter 12 mean length char Jokowi Prabowo Supporters Tweet Line chart Since data comparable visualize one figure make default line chart also need new function done let plot make list contains 2 line chart show one figure :-1 Means reverse month default start December January Plot Analysis Jokowis mean char length tend rise peak November Wheras Prabowos mean char length tend rise August keep tend fall month 13 mean length word Jokowi Prabowo Supporters Tweet last RQ need new function Thats let plot Plot Analysis expected ha almost got similar result answer RQ 12 Conclusion answered Research Question defined many interesting point answer kind word top-30 frequency word president supporter tweet supporter talk president candidate president candidate opponent wont dive deeper statistic make article longer EDA notice thing cleaned make data better example tweet contains hashtag Jokowis support Prabowos supporter one tweet tweet removed dataset move back cleaning step EDA Afterwords Thats folk article mostly EDA Actually RQs Ive answered sake shorting article select must wondering result finding need dive deeper exploring data share dataset many reader want many task done dataset Topic Modelling Sentiment Analysis Detecting Anomaly detecting buzzer many interesting task anyone want write think writing welcome feedback improve article Im process learning writing really need feedback become better make sure give feedback proper manner several next article Ill go back NLP Computer Vision maybe topic Repository TBD Source Data Science Process Springboard data student often ask u question like `` doe Data Scientist '' `` doe day www.kdnuggets.com 101 101 101 Get email whenever Haryo Akbarianto Wibowo publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Henry Feng Jan 25 2019 User guide First Data Product Medium Post Metric Displayer Know Medium Post Better Data Origin regular writer Medium well data geek busy year 2018 Id like reflect achieved Medium blog Furthermore based performance 2018 plan make aggressive writing plan year 2019 14 min read 14 min read Share idea million reader Md Kamaruzzaman Jan 25 2019 Member-only Back metal Top 3 Programming language develop Big Data framework C++ Rust Go Java Data Intensive framework previous blog post Programming language rule Data Intensive Big Data Fast Data framework briefly discussed popular Big Data framework showed Java de-facto programming language Data Intensive framework Java significant advantage e.g 12 min read 12 min read Brayden Gerrard Jan 25 2019 Member-only Evolution US Electric Grid past 15 year US electric grid ha undergone massive shift mostly better Renewable energy natural gas coal GHG emission see US grid ha evolved recent year 4 min read 4 min read zgr Gen Jan 25 2019 Member-only Notes Artificial Intelligence Machine Learning Deep Learning curious people AI ha intriguing topic 2018 according McKinsey mentioned key enabler 1 3 spot Gartner Top 10 Strategic Technology Trends 2019 AI became catch-all term refers computer program automatically doe something 22 min read 22 min read Felix Kuestahler Jan 25 2019 Member-only Python Tutorial Twitter Account Retrieval Swiss Government Members Start journey social medium analysis politician Click link Title list tutorial article first article tutorial show extract list tweeting Swiss Government Members via Twitter API extracted data put Panda Dataframe 7 min read 7 min read Haryo Akbarianto Wibowo Mad AI Enthusiast write mostly Artificial Intelligence Self Development also love read Engineering Psychology Startup Love share Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Jan 26, 2019 Listen Save Exploration and Visualization on each Presidential Candidate Supporters Tweets in Indonesia Hand on Practice by using Plotly to visualize the finding on the data by following Data Science Process. Introduction Hello! Welcome to this article. This time, I want to write about something different from what Ive written so far. My articles that has been published are about implementing an Artificial Intelligence, especially about Deep Learning. In this article, I want to write about different topic. I think this topic is also important if we want to develop a model. This step should be done before doing modelling. Which is Exploratory Data Analysis (EDA) . Sorry, Ive been busy on preparing my new job . Even I am busy, I still want to write medium and share my knowledge to all of you . Whenever I have a free time, I write this article little by little. In this article, I also want to write by not only telling how to code it, but also implement it to a real study case. The data that I used is from twitter which I scraped with library. The case is about Analysis on each presidential candidates supporter in Indonesia on 2018 . Even the text is in Indonesian Language, I will try to make you understand what is the meaning of the text and word. The data that is used in this article is based on the tweet that I crawled . Maybe there are some tweets that the crawler has skipped. Even so, you can say that the data sample of the population which should be representative. The data that Ive crawled is around 700.000 tweets. If you are here to have fun on reading the statistic, go on and feel free to skip the technical detail on this article. Oh, I also will not include any conclusion on the insight that weve found in the data. I will only write the analysis and the explanation of the visualization. You can create your conclusion on your mind and If you want more, why dont you also do the EDA ? And of course write it . Outline Why do EDA? Taken from KDNuggets, the step on doing a Data Science before creating a model are Gather Data, Process the Data, Clean Data, and finally EDA. Actually, doing EDA can help us better on making the models quality improves. By knowing the data, we can feel how our learning algorithm learns our data. And well, if you feel that you think its impossible that some instance can be predicted, neither do our learner. The process EDA should be done before modelling in every tasks of Data Science and Machine Learning (except Pure Reinforcement Learning which doesnt have any data to train). For example, in Natural Language Processing (NLP), when doing Sentiment Analysis, maybe we can make a better model by removing some words which can be found when doing EDA. Another example, in Computer Vision (CV), maybe we can found that the model cant learn because of the inability of our model to predict some instances because our data have an unique noise or some anomalies, which hasnt been cleaned previously in the data cleaning step. There are many others benefit when doing EDA before modelling. Maybe you can get an intuition that can make your model better. which is why we should do this step. If you read my previous article on generating Deep Learning Lyric , I skipped this part. Sometimes in generating the lyric, there are some text that I dont want it to appear (REFF for example). And well, I didnt notice and should have noticed it if I did the EDA first. Before we do it, we need to know what are the steps. Here we go! Steps First , we must define what are the Research Questions ? That means to define what we want to search. These Questions will be the guide on what we will do on the next step. They will also need to be answered in the next step. There are some examples for a Research Question: List anything you need to search in question form. Typically, you can start to pin down to 5W 1H (What, When, Where, Why, Who, How). In my opinion, since we can gain new insights later, we can increase the number of the questions depending on our finding while focusing on our main Research Question. Second , We need to collect the data. Make sure that we search the dataset according to the research question that we have already defined. There are several ways to do this steps. We can search any structured data set in a data set repository such as Kaggle and UCL Repository. Or we can also search any unstructured data by scrapping some social media such as Facebook and Twitter using their API if possible. It depends on your research question. Often, you will not find the data that you need in a structured format out there . Third , Preprocess, Clean the Data, and Data Wrangling ! This will be the painful and annoying steps to do . Do not let your head explode on doing this step. Why this step is painful? You need to know your data. Find if there are any pests out there need to be cleaned. Example of pests in NLP are Typo, Informal Word, Emoticon, Elongated Word, any Many Hidden Bomb There that usually if we remove it, it will make our visualization and model better ! Before doing any of that, if the data is not the structured format, we need to convert it to the structured format for the sake of making cleaning data easier. Let me tell you, we will often go back to this steps many times since we often find the pests in the next step. Finally , we do EDA (Exploratory Data Analysis) . This is where we will answer some of the Research Question defined in the First Steps. Usually we will do some Visualization here to communicate our result. Why we visualize or plot our data when we can see the data by printing the result ? Remember that the result of the analysis will not be seen only for you. The result will be seen by others. For the sake of UX (User Experience) , We need to present the data by visualizing it to make sure the readers become comfortable to see the results. I do not think my article will be seen by many people if I present the result here by cropping the screenshot of the printed result in CLI (Command Line Interface). In this article, we will do EDA in Jupyter Notebook since it has beautiful UX and also can be used to present our data. Technology We will use: Step One : Research Question Okay, before we formulate the questions, let me tell you that in 2019, there will be a presidential election in Indonesia. There are 2 candidates. They are Joko Widodo (Jokowi) Maaruf Amin (NO 01) and Prabowo Subianto Sandiaga Uno (Sandi) (NO 02). What we will focus on in this article is how are the supporter of each candidates in social media. In this article, we will focus on Twitter Social Media. By researching and googling, Ive found some hashtags used by Jokowis and Prabowos Supporter to declare their support. For the simplicity, Ive narrowed down their hashtags by: We will limit the year of the tweet only on 2018. Thats it, Lets create the research question. Here are my research questions : Were done ! Actually there are too many questions that we can ask. For the sake not to make this article become 1 hour read time, we will cut the question we want to ask to twelve. Step Two : Collect the Data For this steps, we will use Twint as our library. For this article, we will scope our data as follow: I scraped 706.208 tweets in this step Step Three : Preprocess Data Okay, since this article will focus for EDA, we will make this step as short as possible. First, we will read the csv with pandas tweet = pd.read_csv(tweet.csv, encoding=utf-8\\') To make it simple, we will only formalize word and remove the stopwords We will formalize in two ways, first is with regex and then substitute known slang words to formal words. The later will need a dictionary of slang word and its formal word. This is how I formalized the word: There should be a better way to do the cleaning on the formalize_rule function such as use nltk TweetTokenizer . Well, I want to try regex and thats it. I also implement on removing stopword on the formalize_word . We apply it into our DataFrame: Note that in this step, in reality, we will come back to this step after doing several EDAs. Sometimes, we will find the pests in EDA step. We are done, move onto last step! Step Four: Exploratory Data Analysis Since all of the Research Questions that we want to answer can be answered here, we will end the Data Science process in this step. Without further ado, lets answer all of the questions! Wait, before we do that. We should define some functions that will be used multiple times. We have defined the DataFrame filter that we will use often later on. We also create some functions which output the statistic in the DataFrame. The statistic will be plotted with Plotly. 0. How many instances in the data? 1. How is the frequency of each supporters tweets in the dataset? How to do it? We take the attribute shape of each supporters tweets. We need to do this to show the plot in Jupyter: Lets set up the plot There are many ways to visualize this data. Since each of them is comparable, we can use Pie chart to visualize the data. There are two minimal components needed to plot on plotly. First is the data. The data is a set of data with the type of chart that we want to visualize. We can combine multi kinds type of chart here. For example, you can visualize a Bar chart with a Pie Chart in the visualization. Second is the layout. The layout is the container of the visualization. This is where we can customize the title, legend, axis, and many more. Then we combine the container and the charts by putting it into go.Figure (a figure). The figure is ready to be plotted. Plot It! Analysis Prabowos supporter tweets frequency is higher than Jokowis supporter tweets 2. How is the top-30 frequency of each supporters tweets each months? How to do it? Since the data that we want to plot is sequential. We can plot in line chart. First, we filter and loop for each months Reverse the list (from Dec Jan to Jan Dec) Then plot it Plot It! Analysis The Prabowos supporter tweets usually have more frequency than Jokowis supporter tweets. Their tweets are at its peak in September. 3. By seeing the largest frequency tweet by month, How is the words top-30 frequency in that month on each supporters tweet? How to do it? First, we will set that the largest frequency tweet is in September. i = Sep Then, we find the highest frequency word by using above functions. We will take only tweets posted in September and plot it. We will limit it into TOP 30 highest frequency. Since the data is not sequential, Bar chart is the right choice here. Word cloud also good in how we visualize the word frequency if we dont want to know the frequency of each words. Plot It! Analysis jokowi has the highest frequency in September. It has around 35k 40k frequency. It is followed by Indonesia, Orang (Person), Presiden (President), Rakyat (Citizen), Dukung (Support), Allah (God), Prabowo, and the others. 4. By seeing the largest frequency tweet by month, How is the Top-30 words frequency in that month on Jokowi supporters tweet? How to do it? Its similar on how we do it on RQ (Research Question) 3. The difference is that we need to filter the Jokowis supporter tweet. Plot it! Analysis The jokowi word is also the highest here and the difference with other words is big. It has positive words such as berbagi (sharing), tulus (sincere) and bergerak (move). It also has Allah word there. 5. How is the Top-30 words frequency in that month on Prabowo supporters tweet How to do it? Again, it is similar on doing RQ 4. We will filter to Prabowo supporters tweet. Plot it! Analysis Its unexpected that the jokowi frequency is higher than prabowo. The highest one is indonesia. The difference of each words frequency is not too big. The words that we should notice are ulama (Muslims Scholar or Cleric), rezim (regime), cebong (tadpole, the bad alias for jokowis supporter by prabowos supporter) , emak (group of mothers) and bangsa (nation). It also has Allah word there. 6. How is the Top-30 frequency of Token accompany prabowo word in Jokowi Supporters Tweet on that month? Before we do it, since we often plot by writing the code many times, we should create a function that is reusable. After that, we will filter the dataframe according to what we need. Plot it! Analysis Jokowi is the highest frequency here. We will notice some words that is interesting, which is uang (money), thecebongers (the tadpole), prestasinya (the achievment), survei (survey), and asing (foreign countries) 7. How is the Top-30 frequency of Token that accompany jokowi word in Prabowo Supporters Tweet on that month We will filter the dataframe according to what we need. Plot it! Analysis Prabowo is the highest frequency here. We will notice some words that is interesting, which is gerakan (movement), ulama, mahasiswa (college student), rupiah (Indonesia currency), and rezim (regime). 8. How is the Top-30 frequency of Token accompany prabowo word in Prabowo Supporters Tweet on that month? We will filter the dataframe according to what we need. Analysis sandi has the highest frequency here. It has big gap to other words. the words that got my attention are ulama, allah, emak, gerakan, and ijtima (ulamas/muslim schoolarss decision). jokowi is also the second highest frequency here. 9. How is the top-30 frequency of Token accompany jokowi word in Jokowi Supporters Tweet on that month? How we do it? We will filter the dataframe according to what we need: Plot it! Analysis prabowo is not in the 20 highest frequency here. Its different from the above. Anyway, word that got my attention are blokir (blocked), pembangunan (construction), kepemimpinan (leadership), allah, hebat (great), and bergerak (move) 10. How is the top-30 frequency of Hashtags in Prabowo Supporters Tweet? How to do it? Since the hashtags column is in string format, we need to cast the type into list by using eval . After that, we join the content of the list by and call our previous function. We will see the statistic of the data not limited on September. Plot it! Analysis Hashtags that my eyes are set on are 2019tetapantipki (2019 Will Stay Anti-communism) , mahasiswabergerak (College student move), rupiahlongsor jokowilengser (Rupiah Fall Jokowi stepped down), and jokowi2periode (Jokowi two Periods). The last hashtags should be the hashtags for Jokowis supporter. The hashtags mostly talks about changing the president and negative things about Jokowi. 11. How is the Top-30 frequency of Hashtags in Jokowi Supporters Tweet? How to do it? Its really similar to RQ 10. Plot it! Analysis Hashtags that my eyes are set on are indonesiamaju (Advanced Indonesia), jokowimembangunindonesia (Jokowi Build Indonesia), kerjanyata (Visible Work), diasibukkerja (Hes busy working). Mostly, the hashtags are about keeping Jokowi as the president and positive things about Jokowi. And again, there is a 2019gantipresiden that should be the hashtags for Prabowos supporter. 12. How is the mean of length char in Jokowi and Prabowo Supporters Tweet? How we do it? We will do it in Line chart. Since these data are comparable, we will visualize them in one figure. We will make our default line chart And we also need a new functions: We are done, lets plot them: We will make a list that contains 2 line charts and show it in one figure. The [::-1] Means that we will reverse the month. The default will start from December to January. Plot it! Analysis  Jokowis mean of chars length is tend to rise and its at its peak at November. Wheras Prabowos mean of chars length tend to rise until August and keep tend to fall after that month 13. How is the mean of length word in Jokowi and Prabowo Supporters Tweet? How to do it? Our last RQ. Its the same as above but we need new function: Thats all lets plot it: Plot it! Analysis As expected, it has almost got the similar result with the answer of RQ 12. Conclusion We have answered all the Research Question that we have defined. There are many interesting points from the answers. Such as the kinds of word in the top-30 frequency word of each presidents supporter tweets and how each supporter talk about their president candidate or their president candidates opponent. I wont dive deeper on the statistic here as it will make this article longer. After we do EDA, we should notice that there are some thing that should be cleaned to make the data better. For example, there are some tweets that contains the hashtag of Jokowis support and Prabowos supporter in one tweet. These tweets should be removed from the dataset. We should move back to the cleaning step and do EDA again. Afterwords Thats it folks for my article mostly about EDA. Actually, I have more RQs that Ive answered. But for the sake of shorting this article, I select a few of them. You must be wondering about some of the result of our finding. For that, you need to dive deeper on exploring the data. I will share the dataset if there are many readers who want it. There are many tasks that can be done for that dataset such as Topic Modelling, Sentiment Analysis, Detecting Anomaly (Such as detecting buzzer), And many interesting tasks. If anyone want me to write about it, I will think about writing it. I welcome any feedback that can improve myself and this article. Im in the process of learning on writing. I really need a feedback to become better. Just make sure to give feedback in a proper manner . For my several next articles, Ill go back to NLP or Computer Vision (maybe) topics. Repository TBD Source The Data Science Process At Springboard, our data students often ask us questions like \"what does a Data Scientist do?\". Or \"what does a day in www.kdnuggets.com 101 101 101 Get an email whenever Haryo Akbarianto Wibowo publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Henry Feng Jan 25, 2019 User guide to My First Data Product: Medium Post Metric Displayer Know Your Medium Post Better with Data Origin As a regular writer on Medium as well as a data geek, after the busy year of 2018, Id like to reflect what I have achieved on my Medium blog. Furthermore, based on the performance in 2018, I plan to make more aggressive writing plan in the year 2019. I 14 min read 14 min read Share your ideas with millions of readers. Md Kamaruzzaman Jan 25, 2019 Member-only Back to the metal: Top 3 Programming language to develop Big Data frameworks C++, Rust, Go over Java for Data Intensive frameworks In a previous blog post: Programming language that rules the Data Intensive (Big Data, Fast Data) frameworks, I have briefly discussed some of the most popular Big Data frameworks and showed that Java is the de-facto programming language in Data Intensive frameworks. Java had significant advantages (e.g. 12 min read 12 min read Brayden Gerrard Jan 25, 2019 Member-only The Evolution Of The US Electric Grid Over the past 15 years, the US electric grid has undergone some massive shifts (mostly for the better). Renewable energy and natural gas are up, while coal and GHG emissions are down. Here we can see how the US grid has evolved in recent years: 4 min read 4 min read zgr Gen Jan 25, 2019 Member-only Notes on Artificial Intelligence, Machine Learning and Deep Learning for curious people AI has been the most intriguing topic of 2018 according to McKinsey. It is mentioned as the key enabler now at the #1 and #3 spot of Gartner Top 10 Strategic Technology Trends for 2019. AI became a catch-all term that refers to any computer program that automatically does something 22 min read 22 min read Felix Kuestahler Jan 25, 2019 Member-only Python Tutorial: Twitter Account Retrieval of Swiss Government Members Start your journey into social media analysis of politicians Click on the link above the Title for a list of all tutorial articles. This is the first article. The tutorial will show you how to extract a list of tweeting Swiss Government Members via the Twitter API. The extracted data will be put into a Panda Dataframe and then 7 min read 7 min read Haryo Akbarianto Wibowo Mad AI Enthusiast. I write mostly about Artificial Intelligence and Self Development. I also love to read Engineering, Psychology and Startup. Love to share! More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5458,\n",
       "  'url': 'https://towardsdatascience.com/you-dont-need-a-masters-to-be-a-data-scientist-9ab690c7bddd',\n",
       "  'title': 'You Don’t Need a Masters To Be a Data Scientist',\n",
       "  'subtitle': 'How I did\\xa0it',\n",
       "  'claps': 82,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 4.050416822467761e-05,\n",
       "  'text': 'Towards Data Science Apr 1 2019 Member-only Listen Save Dont Need Masters Data Scientist Heres weird story became data scientist without pursuing master degree paid resource list simply sharing done hands-on video tutorial machine learning deep learning artificial intelligence checkout YouTube channel University enrolled chemical engineering McGill University one top university Canada ha quite good reputation worldwide chose chemical engineering wa still passionate clean energy technology wa motivated contributing cleaner sustainable way producing energy Unfortunately program fell short meeting expectation class addressed topic career perspective mostly big oil company least thats saw time Simply put wa enjoying degree Feeling wa going anywhere pursuing chemical engineering tried finding new field friend mine wa web developer even started web development company seemed successful passion inspired check Learning web development university set learn web development completing bachelor chemical engineering first started reading MDNs web development guide immediately fell love wa dry learned lot reading documentation trying thing However wanted push skill forward took Colt Steeles Web Developer Bootcamp read many great review course thought wa worth paying dollar try turned amazing course absolutely enjoyed learned great deal used course build simple portfolio small project presented recruiter university career fair landed first job web developer wa ecstatic managed set foot tech world interviewer asked transitioned chemical engineering programming prove required skill whereas computer science student software engineering student simply show CV December 2017 completed bachelor assured job start July 2018 end university new job January 2018 July 2018 wa free school work 6 month relaxing waiting start new job web developer However simply cant nothing Thats learned craze data scientist Everywhere wa reading wa sexiest job jazz figured check started learning data science Dataquest approach fitted learning style since lot free time knew could complete curriculum fast fact crammed entire data scientist data engineer path 2 month experience wa amazing opinion learned lot get complete many project build solid data science portfolio However main drawback wa understand wa applying worked lacked theory behind algorithm Therefore took Andrew Ngs Machine Learning course heard course think probably one highest rated course Coursera absolutely loved learned much math behind machine learning However like course wa taught Matlab/Octave wa hard translate learned Python learned great deal fundamental machine learning still felt wa lacking knowledge Learning data science working July 2018 started new job wa excited wa juggling learning web development data science started data science spare time work decided read book Introduction Statistical Learning friend recommended book saying wa best introduction data science gave shot wa pleasantly surprised Reading book wa actually enjoyable wa serious study wa taking note forced apply every algorithm Python building small project really helped master traditional machine learning algorithm importantly gained confidence skill something weird happened wa browsing Facebook work yes work admit saw ad data scientist position one largest bank Canada ad simply said looking data scientist Take quiz thought nothing lose took quiz got 11/13 Even wa impressed score week later get phone interview one hour telling interviewer transitioned chemical engineering programming recruiter decided make go final step final step consisted completing data science project presenting senior data scientist decided rework project completed studying Dataquest presented week later got job offer accepted January 2019 started new job data scientist Since January 2019 Since learning work opportunity multiplied get collaborate incredibly smart motivated people company encourages u learn explore experiment innovate feel lucky type organization proud managed learn data science acquire relevant skill land job deep learning natural next step already working right right attitude mindset yes firmly believe anyone accomplish far exception way fitted personality learning style could However many drawback well hard passionate disciplined subject accomplish Yet found way wa definitely rewarding need master data scientist think company looking diploma anymore looking skilled people instead end really matter set skill Whether decide pursue master realize different way reaching result case make sure gain following skill aspiring data scientist wish best luck absolutely love field think class took online really helped love understand field end key passionate subject willing share passion many people possible opportunity come wish lot success 102 1 102 102 1 Get email whenever publish Get freebie course announcement VIP invitation event straight inbox Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Shaw Lu Apr 1 2019 Member-only Visualizing Beta Distribution Bayesian Updating Seeing believing build intuition simulating visualizing inspecting every step Beta distribution one esoteric distribution compared Bernoulli Binomial Geometric distribution also rare practice doe readily available real-world analogy help intuition make matter worse online tutorial tend intimidate reader complex formula beta 7 min read 7 min read Share idea million reader Rudradeb Mitra Apr 1 2019 Member-only Successful Adoption AI Need Female Leaders woman posse right quality lead AI project successfully enable environment collaboration inclusion build real-world AI product exciting breakthrough twenty-first century occur technology expanding concept mean human John Naisbitt Part Differences Men Women story woman men work differently 8 min read 8 min read Manu Siddharth Jha Mar 31 2019 Member-only Artificial Intelligence next big thing Hollywood Movies captured imagination people ever since came limelight Right first motion picture late 1880s upcoming latest sci-fi blockbuster cinema ha become medium love joy passion movie lover Almost every country across globe 5 min read 5 min read Guy Tsror Mar 31 2019 Member-only romantic side data science Analyzing relationship year worth text message wa supposed private project one thought would upload private Google Colab notebook share whomever relevant one person referred OJ lightbulb clean share 11 min read 11 min read Salma Ghoneim Mar 31 2019 Member-only Object detection via color-based image segmentation using python tutorial contouring using python OpenCV Getting started already jupyter notebook IDE run python OpenCV installed skip Execution Tools hero today Anaconda free open-source distribution help installing different package sort mess isolated environment Wikipedias telling u 7 min read 7 min read Marco Peixeiro Senior data scientist Author Instructor write hands-on article focus practical skill Medium Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Akshad Singi Better Humans 4 Unsexy One-Minute Habits Save 30+ Hours Every Week Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 1, 2019 Member-only Listen Save You Dont Need a Masters To Be a Data Scientist How I did it Heres my (weird) story on how I became a data scientist without pursuing a masters degree. I am not paid for any of the resource I will list below. I am simply sharing what I have done. For hands-on video tutorials on machine learning, deep learning, and artificial intelligence, checkout my YouTube channel . University I enrolled in chemical engineering at McGill University. It is one of the top universities in Canada, and it has quite a good reputation worldwide. I chose chemical engineering because I was (and still am) very passionate about clean energy technologies. I was motivated in contributing to a cleaner and more sustainable way of producing energy. Unfortunately, the program fell short of meeting my expectations. Very few classes addressed this topic, and the career perspectives were mostly with big oil companies (or at least, thats what I saw at that time). Simply put, I was not enjoying my degree at all. Feeling I was not going anywhere pursuing chemical engineering, I tried finding a new field. A friend of mine was a web developer and he even had started his own web development company. He seemed successful and his passion for it inspired me to check it out. Learning web development while university So I set myself to learn web development while completing my bachelors in chemical engineering. I first started reading MDNs web development guide . I immediately fell in love with it. It was very dry, but I learned a lot while reading the documentation and trying things on my own. However, I wanted to push my skills forward, so I took Colt Steeles  The Web Developer Bootcamp  . I had read many great reviews about this course, and I thought it was worth paying a few dollars to try it out. It turned out to be an amazing course! I absolutely enjoyed it and learned a great deal. I used the course to build a simple portfolio of small projects and presented them to recruiters at my universitys career fair. This is how I landed my first job as a web developer. I was ecstatic! I managed to set my foot in the tech world on my own. All interviewers asked me why I transitioned from chemical engineering to programming, and I had to prove them that I had the required skills, whereas computer science students or software engineering students simply had to show a CV. In December 2017, I completed my bachelors and I had an assured job to start in July 2018. Between the end of university and my new job From January 2018 to July 2018, I was free! No school, no work, just 6 months of relaxing and waiting to start my new job as a web developer. However, I simply cant do nothing. Thats when I learned about the craze for data scientists. Everywhere, I was reading it was the sexiest job , and all that jazz. I figured I had to check it out. I started learning data science on Dataquest . Their approach fitted more my learning style and since I had a lot of free time, I knew I could complete the curriculum fast. In fact, I crammed the entire data scientist and data engineer paths in 2 months. The experience was amazing in my opinion. I learned a lot, and you get to complete many projects, which will build a very solid data science portfolio. However, the main drawback for me was that I did not understand what I was applying. It worked, but I lacked the theory behind the algorithms. Therefore, I took Andrew Ngs Machine Learning course. Again, I had heard about this course and I think it is probably one of the highest rated course on Coursera. I absolutely loved it and learned much more about the math behind machine learning. However, I did not like that the course was taught in Matlab/Octave, so it was hard to translate what I learned in Python. I learned a great deal about the fundamentals of machine learning, but I still felt I was lacking some knowledge. Learning data science while working In July 2018, I started my new job. I was very excited, but I was juggling between learning more about web development and data science. So I started doing data science on my spare time (after work). I decided to read the book  An Introduction to Statistical Learning  . A friend recommended this book, saying it was the best introduction to data science. I gave it a shot, and I was pleasantly surprised! Reading this book was actually enjoyable. I was very serious in my study. I was taking notes and I forced myself to apply every algorithm in Python by building very small projects. This really helped master most of the traditional machine learning algorithms and most importantly, I gained confidence in my skills. Then, something weird happened I was browsing Facebook at work (yes, at work, I admit it), and I saw an ad for a data scientist position at one of the largest banks in Canada. The ad simply said: Were looking for data scientists! Take the quiz! I thought I had nothing to lose. I took the quiz and got 11/13. Even I was impressed with my score! A week later, I get a phone interview. After one hour (and after telling the interviewer how I transitioned from chemical engineering to programming), the recruiter decided to make go through the final step. The final step consisted in completing a data science project and presenting it to senior data scientists. I decided to rework a project I completed while studying with Dataquest, and I presented it. A week later, I got a job offer and I accepted it. In January 2019, I started my new job as a data scientist. Since January 2019 Since then, the learning and work opportunities have just multiplied. I get to collaborate with incredibly smart and motivated people, and the company encourages us to learn, explore, experiment and innovate. I feel very lucky to be in that type of organization, and I am very proud that I managed to learn data science and acquire the relevant skills by myself to land a job. Now, deep learning is a natural next step, which I am already working on right now. Can you do the same? With the right attitude and mindset, yes. I firmly believe that anyone can accomplish the same. I am far from being an exception. I did it this way because it fitted my personality and my learning style. I could: However, there were many drawbacks as well: What I did is hard. You have to be passionate and disciplined about a subject to accomplish what I did. Yet, I found this way was definitely the most rewarding. So, do I need a masters to be a data scientist? No. I think that companies are not looking at diplomas anymore, and are looking for skilled people instead. In the end, what really matters, is your set of skills. Whether you decide to pursue a masters or not, realize that it is only a different way of reaching the same result. In any case, make sure you gain the following skills: If you are an aspiring data scientist, I wish you best of luck! I absolutely love the field, and I think that the classes I took online really helped love and understand this field. In the end, the key is being passionate about a subject and to be willing to share your passion with as many people as possible. Then, the opportunities will come. I wish you a lot of success! 102 1 102 102 1 Get an email whenever I publish! Get freebies, course announcement, VIP invitations to events and more straight into your inbox! Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Shaw Lu Apr 1, 2019 Member-only Visualizing Beta Distribution and Bayesian Updating Seeing is believing: build intuition by simulating, visualizing, and inspecting every step Beta distribution is one of the more esoteric distributions compared to Bernoulli, Binomial and Geometric distributions. It is also rare in practice because it does not have a readily available real-world analogy that helps intuition. To make matters worse, online tutorials tend to intimidate readers with complex formula (beta & 7 min read 7 min read Share your ideas with millions of readers. Rudradeb Mitra Apr 1, 2019 Member-only For the Successful Adoption of AI, We Need More Female Leaders Why women possess the right qualities to lead AI projects more successfully and enable an environment for collaboration and inclusion to build real-world AI products. The most exciting breakthroughs of the twenty-first century will not occur because of technology, but because of an expanding concept of what it means to be human John Naisbitt Part I: Differences between Men and Women A story of how women and men can work differently 8 min read 8 min read Manu Siddharth Jha Mar 31, 2019 Member-only Is Artificial Intelligence the next big thing in Hollywood? Movies have captured the imagination of people ever since they came into the limelight. Right from the first motion picture in the late 1880s to the upcoming latest sci-fi blockbuster, cinema has become a medium of love, joy, and passion for all movie lovers. Almost every country across the globe 5 min read 5 min read Guy Tsror Mar 31, 2019 Member-only The romantic side of data science: Analyzing a relationship through a year worth of text messages This was supposed to be a private project, one I thought I would upload to a private Google Colab notebook, and share with just whomever it is relevant for (which is, one other person, referred to here as OJ). But then, a lightbulb why not clean it up and share 11 min read 11 min read Salma Ghoneim Mar 31, 2019 Member-only Object detection via color-based image segmentation using python A tutorial on contouring using python & OpenCV. Getting started If you already have jupyter notebook or an IDE with which you can run python & OpenCV installed, just skip to Execution. Tools Our hero today is Anaconda. a free open-source distribution that helps with installing different packages & sorts out their messes into isolated environments. What Wikipedias telling us about 7 min read 7 min read Marco Peixeiro Senior data scientist | Author | Instructor. I write hands-on articles with a focus on practical skills. More from Medium Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Akshad Singi in Better Humans 4 Unsexy One-Minute Habits That Save Me 30+ Hours Every Week Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5461,\n",
       "  'url': 'https://towardsdatascience.com/the-romantic-side-of-data-science-analyzing-a-relationship-through-a-year-worth-of-text-messages-be7e32d81fa9',\n",
       "  'title': 'The romantic side of data science: Analyzing a relationship through a year worth of text\\xa0messages',\n",
       "  'subtitle': '-',\n",
       "  'claps': 80,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 11,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 3.95162616826123e-05,\n",
       "  'text': 'Towards Data Science Mar 31 2019 Member-only Listen Save romantic side data science Analyzing relationship year worth text message wa supposed private project one thought would upload private Google Colab notebook share whomever relevant one person referred OJ lightbulb clean share others well Basically celebrate first anniversary dating decided explore relationship communication point view look messaging behavior saying saying respond would include call log actually never tracked shame let go especially since mostly two phone call week making bit le interesting go thought process interested towards end share link tool came handy process working improving Python skill go could also find Python-related snippet handy please note story much better read desktop since infographics interactive Methodology key communication channel use WhatsApp SMS-text occasionally negligible occasional Messenger link YouTube message rare chose stick WhatsApp simplify process Story met April 2018 right-swipe Tinder spontaneous setup ever day swipe date wa first rainy walk bunch bar Montreals Plateau neighbourhood found semi-decent table get know pisco sour Fast forward one year wa trying think creative way mark occasion working data-heavy quantified-self project thought oh maybe geek one analyzing year worth text message Overall Messaging History Overall period approximately 11 month wanted ready time sent 17,408 message average 60 message per day 59 OJ ha sent 22,589 message average 80 messages85 guess know chatter timeline show peak mostly one super peak one two dead-zones little going peak usually correspond trip one u Hawaii July Toronto November NYC December Toronto March low point often traveling together Israel June Colombia February didnt need much message communication except maybe occasional bring toilet paper roll plz oreo get big shock overall OJ sends message interesting consider whether come actually word simply breaking what-would-be long message shorter one table show slightly detailed breakdown fact sending message OJs message tend shorter mine tad longer although statistically significant doe give general feel messaging behavior one fond multiple short message row prefers single longer message Interaction may ask bit interaction converse using message doe interaction look like initiating long doe usually take side respond Lets see Culture reflected habit Looking figure showcasing breakdown messaging statistic per day box plot explanation need seems like work day Monday Thursday behavior generally similar slight drop Wednesdays perhaps worth crossing data day spent together actually collect data might look future post happening Friday-Saturday interesting make lot sense know people question one u OJ observing jew meaning Shabbat Friday dark Saturday dark usually electronic device use read therefore communicate much le Friday-Saturday outlier case much closer core day Sundays back normal limitation Outliers often correlated period travel oddly keep even higher communication rate daytime creature see figure conversation happen expected daytime hour peak around 34pm towards end workday 5pm~ish Canada looking delta u 3pm also seems one gap biggest perhaps related fact peak hour productivity work thats discussion another post lack attention communication around Homework show initiative let look initiative need improve show initiative come message Well guess title paragraph might given one inspect well look first interaction day anything thats happening 7am let consider fact rarely long night therefore ruling see initiated conversation hard tell improvement seems like lag behind need step game interesting next question ask doe change time start good gradually slow merely Im big morning person Well seems like periodical ups least seems like mostly improving slowly balancing ask different maybe bit scary question first message actually look like might know negative-Nancy OJ positive-Patricia dynamic pretty easy capture first message day Im much inclined use term ugh horrible OJs vocabulary far positive anyone slow morning expect looking great moment OJ getting high mark relationship-maintenance negative morning lower initiative wonder trail behind also come responsiveness age connectivity Something keep mind diving section millennials yes despite born 80 See diagram Ive added hyper-connected phone another organ body actually hating actively trying disconnect keep phone near thats also material different post Looking reply-time distribution seems actually quite similar responsive sigh Considering bin distribution approximately 2-minute delay lag tad behind seems insignificant could interesting investigate whether responsive specific time day might valuable general distribution response time Lets look excuse lack subplots plotly sure make impossible generate using heatmaps First like saw earlier communication happens daytime around mid-afternoon expected fastest respond late night message seemingly different come delay answering daytime message pretty fast case interesting look month hopefully connected could useful measure even talking Content interesting statistic weve explored far also personal matter keep section minimal Negative Nancy Positive Patricia reading carefully might noticed traditionally relationship considered negative among u doe actually reflect say versus OJ say Seems like definite fact comparing percentage-wise actually seems bit le negative OJ shocker difference minor alternative viewpoint might reflect situation better negativity messaging rather lack positivity OJ clearly positive whole 28.9 message said positive tone stand whole 5.7 lower wa expected Looking happens throughout day excuse inserting photo wa impossible insert decent subplot seems dont demonstrate anything significantly odd perhaps except slight higher presence negative content end early morning hour sigh Nicknames anyone Finally looking word cloud visualizing content rather first message content first conclusion cant really tell much arent word stand unusual pick habit like Brits OJ frequent use word mister sir u overusing lol haha hehe Millennials cant even detect nickname side strong indicator phasing nickname useful practice understand content analyzing within specific context example doe conversation change trip saying negative context complaining bottom line wa interesting journey went thinking back couldve looked plenty aspect us word love often spot long began dating start using love long average text conversation many many question could asked seeing learn practice Happy first anniversary u OJ technical info data enthusiast wrote code project Python mentioned earlier pandas-heavy plotting wa done almost entirely plotly always best mate process stackoverflow documentation page different library used Since inquired Ive made code available GitHub Feel free fork clone play ask anything VADER great nltk tool analyzing text data sentiment Python havent got training set label available us term scoring wide lexicon word easily add see reviewing sample data export actually pretty damn accurate Word cloud generated Pythons WordCloud library decent havent worked often look better resolution customization tool feel free leave message suggestion link found useful might Hope wa insightful ask away question 98 1 98 98 1 Towards Data Science home data science Medium publication sharing concept idea code Salma Ghoneim Mar 31 2019 Member-only Object detection via color-based image segmentation using python tutorial contouring using python OpenCV Getting started already jupyter notebook IDE run python OpenCV installed skip Execution Tools hero today Anaconda free open-source distribution help installing different package sort mess isolated environment Wikipedias telling u 7 min read 7 min read Share idea million reader Sahil Dhankhad Mar 31 2019 Member-only Forget APIs Python Scraping Using Beautiful Soup Import Data File web Part 2 APIs always every website Beautiful Soup going stay forever collect kind data website Today Data play critical role every industry data coming internet company 8 min read 8 min read Andrew Donaldson Mar 31 2019 Member-only Dont Fear Robots Impressive tech important remember Boston Dynamics video trailer movie isnt yet made Every time one Boston Dynamics video hit met mix amazement prediction doom hand new robot overlord impressive also well choreographed staged Impressive tech important 4 min read 4 min read Jasmine Vasandani Mar 31 2019 Member-only 10 Steps Teaching Data Science Well resource data science instructor increase demand data scientist ha parallel growth training program within company educational institution teach people data science addition focusing meeting job demand need resource instructor facilitating data science classroom Heres 4 min read 4 min read Karan Bhanot Mar 31 2019 Member-only income 50K/yr Machine Learning tell Machine learning breaking ground numerous field including Finance could use Machine Learning model identify income individual found right dataset called Census Income Dataset used information dataset predict someone would earn income 7 min read 7 min read Guy Tsror data-nerd trying adult way life Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Mark Vassilevskiy 5 Unique Passive Income IdeasHow Make 4,580/Month Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Mar 31, 2019 Member-only Listen Save The romantic side of data science: Analyzing a relationship through a year worth of text messages This was supposed to be a private project, one I thought I would upload to a private Google Colab notebook, and share with just whomever it is relevant for (which is, one other person, referred to here as OJ). But then, a lightbulb why not clean it up and share with others as well? Basically, to celebrate our first anniversary of dating, I decided to explore our relationship from the communication point of view, and look at our messaging behaviors: what are we saying to each other, when are we saying it, how do we respond to each other, and so on. I would include call logs in it, but I actually never tracked that (shame on me!) so I had to let it go, especially since we mostly have up to two phone calls a week, making it a bit less interesting. So I will go through my thought process to those who are interested in what I did and why, and towards the end, I will share some links and tools that came in handy during the process. I am working on improving my Python skills as I go, so you could also find there some Python-related snippets that were handy. (please note: this story is much better read on desktop, since infographics are interactive) Methodology The key communication channel we use is WhatsApp. We do SMS-text occasionally, but its negligible, and the occasional Messenger link or YouTube message are very rare, so I chose to stick to WhatsApp to simplify the process. The Story We met in April of 2018, after a right-swipe on Tinder and the most spontaneous setup ever same day swipe and date! It was a first for me, and after a rainy walk between a bunch of bars in Montreals Plateau neighbourhood, we found a semi-decent table to get to know each other over pisco sour. Fast forward one year, and I was trying to think of a creative way to mark this occasion. As I am working on some other data-heavy and quantified-self projects, I thought oh, maybe I can geek this one out? and so, here we are, analyzing a year worth of text messages. Overall Messaging History Overall, in a period of approximately 11 months (wanted this to be ready in time! ), I have sent 17,408 messages (an average of 60 messages per day 59) while OJ has sent 22,589 messages (an average of 80 messages85) I guess we know whos the chatter here! The timeline shows some peaks (mostly one super peak) and one or two dead-zones with very little going on. The peaks usually correspond with trips that one of us had (Hawaii in July, Toronto in November, NYC in December and Toronto in March), and the low points are often when we both were traveling together (Israel in June, Colombia in February) and didnt need much message communications (except for maybe the occasional bring me a toilet paper roll plz or which oreos should I get?). So no big shocks here. So, overall OJ sends more messages, but it is interesting to consider whether that comes down to actually more words, or simply breaking down what-would-be long messages to shorter ones. The table below shows a slightly more detailed breakdown: in fact, while sending more messages, OJs messages tend to be shorter, while mine are a tad longer (although not statistically significant). It does give a general feel to our messaging behavior one more fond of multiple short messages in a row, while the other prefers a single longer message. Interaction: when and how? We may now ask a bit more about the interaction itself: when do we converse using messages? How does this interaction look like who is initiating, when and how long does it usually take for the other side to respond? Lets see! Culture reflected in habits Looking at the figure below, showcasing a breakdown of messaging statistics per day (box plot explanation here , if you need it! ), it seems like on most work days (Monday through Thursday) the behavior is generally similar, with a slight drop on Wednesdays (perhaps worth crossing it with data on days we spent together? I actually collect that data, and might look into it in a future post!). What is happening on Friday-Saturday is interesting, and makes a lot of sense once you know the people in question: one of us (OJ) is an observing jew (meaning, on Shabbat, which is Friday after dark until Saturday after dark, theres usually no electronic devices in use read more on this here ) therefore we communicate much less on Friday-Saturday, and the outliers in these cases are much closer to the core than on most other days. Sundays are back to normal, as there are no limitations again! Outliers are often correlated with periods of travel, in which we (oddly) keep an even higher communication rate. We are daytime creatures As we see in the figure below, most of our conversations happen in expected daytime hours with peaks around 34pm towards the end of the workday (5pm~ish) here in Canada. When looking at the delta between us, 3pm also seems to be the one where the gap is the biggest perhaps this is related to the fact that its peak hour in my productivity at work (thats a discussion for another post!) and my lack of attention to communications around then. Homework for me: show more initiative! Now lets look at initiative: who needs to improve here and show more initiative when it comes to messages? Well, I guess the title of this paragraph might have given that one out. But to inspect that, well look at the first interaction of each day, for anything thats happening after 7am (lets consider the fact that we do rarely have long nights out, therefore were ruling these out), and see who initiated the conversation! Its not hard to tell: I have some improvement to do it seems like I lag behind, and need to step my game up. An interesting next question to ask is how does this change over time? Did I start good and gradually slow down, or is it merely because Im not a big morning person? Well, seems like its periodical! We both have our ups and downs, but at least it seems like I am mostly improving, and it slowly is balancing. Now we can ask a different, maybe a bit scary question: how do these first messages actually look like? You might not know this, but I am the negative-Nancy and OJ is the positive-Patricia in our dynamics, and its pretty easy to capture just from the first messages of each day: Im much more inclined to use terms such as ugh and horrible, while OJs vocabulary is far more positive than anyone with slow mornings can expect. I am not looking so great at the moment, while OJ is getting high marks in relationship-maintenance! With my negative mornings and lower initiative, but I wonder, do I trail behind also when it comes to responsiveness? The age of connectivity Something to keep in mind before diving into this section is that we are both millennials (yes, despite me being born in the 80s See the diagram Ive added here), and as such, can be hyper-connected and have our phone as another organ of our body. I actually am hating that and actively trying to disconnect and not keep my phone near me, but thats also material for a different post! Looking at our reply-time distribution it seems that we are actually quite similar, and very responsive (sigh). Considering each bin in this distribution is approximately a 2-minute delay, I do lag a tad behind, but it seems insignificant. What could be interesting to investigate is whether we are more responsive during a specific time of day this might be more valuable than just a general distribution of our response time! Lets look below (excuse me for the lack of subplots here, but plotly sure makes it impossible to generate when using heatmaps! ): First off, like we saw earlier, most of the communication happens in daytime, around mid-afternoon. As expected, we both are not the fastest to respond to late night messages, and seemingly not too different when it comes to our delay answering any daytime messages: were pretty fast in most cases! This will be interesting to look at in a few months, hopefully I will not be as connected as before and this could be a useful measure. What am I even talking about? Content is just as interesting as the statistics weve explored so far. BUT its also a more personal matter, so I will keep this section minimal :) Negative Nancy or Positive Patricia? If you have been reading carefully, you might have noticed that traditionally in this relationship, I am considered the more negative among us. But does this actually reflect in what I say versus what OJ says? Seems like its a definite no! In fact, when comparing percentage-wise, it actually seems I am a bit less negative than OJ! This is a shocker but the differences are minor. An alternative viewpoint that might reflect the situation better is not my negativity in messaging, but rather lack of positivity. OJ is clearly more positive, with a whole 28.9% of messages said in positive tone, while I stand a whole 5.7% lower. But this was expected. Looking at what happens throughout the day (excuse me for inserting this as a photo, it was impossible to insert as a decent subplot! ), it seems we dont demonstrate anything significantly odd, perhaps except for a slight higher presence of negative content from my end in the early morning hours (sigh). Nicknames anyone? Finally, by looking at a word cloud visualizing all of the content (rather than just first messages content), the first conclusion is: we cant really tell much. There arent any words that stand out as unusual; we can pick on a few habits in there, like the Brits (OJ) frequent use of the words mister or sir, or both us overusing lol,haha and hehe. Millennials after all. We cant even detect any nickname in here, on any side, a strong indicator of our phasing in and out of nicknames. No, a more useful practice to understand content will be analyzing within specific contexts: for example, how does our conversation change during trips? What are we saying in a negative context? What are we complaining about? And so on. The bottom line This was an interesting journey for me as I went through it, and thinking back, I couldve looked at plenty other aspects: who uses the word love more often? Can we spot how long after we began dating did we start using love with each other? How long is our average text conversation? And many many more questions that could be asked. But seeing what we have, what can we learn from this practice? Happy first anniversary to us, OJ :) Some technical info for the data enthusiasts I wrote the code for this project in Python, as I mentioned earlier. Its pandas-heavy, and the plotting was done almost entirely with plotly. As always, some of my best mates in the process were stackoverflow and documentation pages of the different libraries I used. Since some of you inquired, Ive made the code available on GitHub . Feel free to fork, clone, play and ask anything about it. VADER is a great nltk tool for analyzing text data sentiment in Python, if you havent got a training set with labels available. It uses term scoring for a wide lexicon of words (that you can easily add to see below) and after reviewing a sample data from my export it actually is pretty damn accurate! Word clouds were generated with Pythons WordCloud library, and were decent. I havent worked with these very often, but am on the look out for a better resolution and customization tool, so feel free to leave a message if you have suggestions for that! Here are some links that I found useful and you might too! Hope this was insightful, ask away if you have any questions! 98 1 98 98 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Salma Ghoneim Mar 31, 2019 Member-only Object detection via color-based image segmentation using python A tutorial on contouring using python & OpenCV. Getting started If you already have jupyter notebook or an IDE with which you can run python & OpenCV installed, just skip to Execution. Tools Our hero today is Anaconda. a free open-source distribution that helps with installing different packages & sorts out their messes into isolated environments. What Wikipedias telling us about 7 min read 7 min read Share your ideas with millions of readers. Sahil Dhankhad Mar 31, 2019 Member-only Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2 APIs are not always there for you for every website, but Beautiful Soup is going to stay with you forever to collect any kind of data from any website. Today, Data play a critical role in every industry. And most of this data is coming from the internet. Most company 8 min read 8 min read Andrew Donaldson Mar 31, 2019 Member-only Dont Fear the Robots Impressive as the tech here is, it is important to remember that the Boston Dynamics videos are trailers for a movie that isnt yet made. Every time one of these Boston Dynamics videos hit, they are met with a mix of amazement and predictions of doom at the hands of our new robot overlords. It is impressive. It is also very well choreographed and staged. Impressive as the tech here is, it is important 4 min read 4 min read Jasmine Vasandani Mar 31, 2019 Member-only 10 Steps to Teaching Data Science Well A resource for data science instructors. With the increase in demand for data scientists, there has been a parallel growth in training programs within companies and educational institutions that teach people data science. In addition to focusing on meeting the job demand, there need to be more resources for instructors facilitating data science classrooms. Heres a 4 min read 4 min read Karan Bhanot Mar 31, 2019 Member-only Will your income be more than $50K/yr? Machine Learning can tell Machine learning is breaking grounds in numerous fields including Finance. What if we could use Machine Learning models to identify incomes of individuals? I found just the right dataset for this, called Census Income Dataset. I used the information in the dataset to predict if someone would earn an income 7 min read 7 min read Guy Tsror A data-nerd trying to adult my way through life. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Mark Vassilevskiy 5 Unique Passive Income IdeasHow I Make $4,580/Month Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 2290,\n",
       "  'url': 'https://medium.com/datadriveninvestor/advanced-git-commands-that-can-boost-your-productivity-707476a2a06',\n",
       "  'title': 'Advanced Git Commands that can Boost Your Productivity',\n",
       "  'subtitle': '-',\n",
       "  'claps': 71,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Data Driven Investor',\n",
       "  'date': '2019-12-27',\n",
       "  'clap_prop': 3.507068224331842e-05,\n",
       "  'text': \"Sign DataDrivenInvestor Dec 27 2019 Listen Save Advanced Git Commands Boost Productivity else learn besides git add git commit struggling git point know FEELING lost progress messed Git know bad feel always need ask coworkers Git help Today day change post cover basic advanced git command use work Four main git command talked cherry-pick reset revert reflog post also talk squash split undo change AI forecast 'Disruption productivity Data Driven Investor growing concern white-collar job disappear spread machine learning www.datadriveninvestor.com post meant someone started learning Git requires familiarity Git said let get started BACKGROUND dive command need understand one simple concept Git Git made commit tree One branch sequence commits One easy way think git tree think n-ary tree treenode ha pointer parent node path root leaf-node one branch leaf-node latest commit within branch tracing parent node pointer iteratively get sequence commits branch said start git journey GIT COMMANDS Git Cherry-pick command add selected commit current branch creates new commit copy doe delete original commit cherry-pick Examples Git Reset command move branch pointer point different commit within branch git reset Difference git reset git reset -- hard Git reset use command branch pointer point previous commit reverted commit change still example Git reset -- hard use command branch pointer point past commits reverted commit change gone Thus use hard reset commit squash split accidentally used git reset -- hard lost change use git reflog go back previous state detail talked reflog section Git Rebase command used rebase current branch onto new branch process rebase find new base cherry-pick unduplicated change onto base reassign branch pointer Visual explanation git rebase Examples Let 's say want combine d-e-f one commit git rebase HEAD~3 -i see something similar following choose squash commit-84b30b8 commit-e487eb1 commit-f1d3f6c one single commit f1d3f6c following Git Reflog command return list recent used commit sha-hash operation ha done Combine git reset one useful command need undo something use git reflog use git reflog typically something like Examples Lets say messed branch accidentally using git reset -- hard need find back change following allows u go back commit operation Useful Tips Squashing Splitting three way commit squash one way splitting Undo change two way undo change RECAP post learned Thanks Ankita De 71 71 71 DataDrivenInvestor empowerment data knowledge expertise subscribe DDIntel http //ddintel.datadriveninvestor.com Serhii Biloshkurskyi Dec 27 2019 Documentation software development come mind hear project documentation people really ideal procedure fully completed important manage Really interesting question Ill take question bringing mind documentation 4 min read 4 min read Share idea million reader Alex Mitrani Dec 26 2019 Time Series Trend Analysis Time-dependent trend unique feature time series analysis sequence event matter need analyze possible trend trend ultimately used creating model predict future value recently published article working time series data creating OHLC 4 min read 4 min read Robert Reinold Dec 26 2019 Path Maximized IoT Introduction Internet Things IoT movement embed sensor controller asset enable application create safer efficient local globalized system 3 min read 3 min read Freda Xin Dec 26 2019 List Comprehensions Python Set Builder Notation Set Theory post discus similarity connection list comprehension Python set builder notation Set Theory also briefly compare list comprehension Haskell List Comprehensions Python list comprehension Python let construct new list iterating element 5 min read 5 min read Fabiansyah Cahyo Dec 26 2019 Extreme Learning Machine Simple Classification last week friend college asked help implementing code extreme learning machine time didnt quite understand algorithm decided help anyway learning understanding 4 min read 4 min read Recommended Medium Toro Cloud microservices evolutionary revolutionary TechGenyz TechGenyz WooCommerce Magento eCommerce Project platform choose Paige McFarlain smucs Comparing Performance Community Detection Algorithms Pythons NetworkX C++s Boost Stanley Masinde Idea Ingenious Piece Setting proper server permission Piyush Mehta Getting started Ansible Cliff Weitzman Never Procrastinate Bullet Proof Way Tonya Nguyen Lab 1 Physical Computing ProjectAgoraEng Project Agora Engineering Catch glimpse Project Agoras Header Bidding Solution Help Terms Privacy Get Medium app Sign Jay Shi Software Engineer Google write Python tutorial stuff help become better software engineer Medium Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Jacob Bennett Level Coding Use Git like senior engineer Frank Andrade Geek Culture Top 5 Paid Subscriptions Ill Never Cancel Programmer Dr. Derek Austin Better Programming Prefer Regular Merge Commits Squash Commits Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Sign In DataDrivenInvestor Dec 27, 2019 Listen Save Advanced Git Commands that can Boost Your Productivity What else can we learn besides git add and git commit ? We have all been struggling with git at some point. We all know that FEELING when we lost all the progress because we messed up Git. We all know how bad we feel when we always need to ask coworkers for some Git help. Today is the day to change that ;). This post will cover up some basic advanced git commands that you can use during work. Four main git commands will be talked about cherry-pick, reset, revert, reflog. This post will also talk about squash, split and undo changes. AI forecast: 'Disruption, then productivity' | Data Driven Investor There is growing concern about all the white-collar jobs that will disappear with the spread of machine learning and www.datadriveninvestor.com This post is not meant for someone who just started learning Git. It requires that you have some familiarity with Git. With all that being said, lets get started! BACKGROUND Before we dive into the commands, we just need to understand one simple concept of Git. Git is made of commit trees. One branch is a sequence of commits. One easy way to think of the git trees is to think of a n-ary tree. Each treenode has a pointer to its parent node. Each path from root to any leaf-node is one branch. The leaf-node here is the latest commit within that branch, and by tracing its parent node pointer iteratively, we can get a sequence of all commits for that branch. With that being said, we can now start on the git journey. GIT COMMANDS Git Cherry-pick This command adds the selected commit to the current branch. It creates a new commit copy and does not delete the original commit. Why and when to cherry-pick? Examples: Git Reset This command moves the branch pointer to point to a different commit within that branch. Why and when git reset Difference between git reset and git reset --hard Git reset: When we use this command, the branch pointer points to the previous commit, but the reverted commit changes are still here. For example: Git reset --hard: When we use this command, the branch pointer points to the past commits, and the reverted commit changes are gone. Thus, we cannot use hard reset to do commit squash or split. When you accidentally used git reset --hard and you lost the changes, you can use git reflog to go back to the previous state. More details will be talked about in the reflog section. Git Rebase This command is used to rebase the current branch onto a new branch The process of rebase: find the new base cherry-pick all the unduplicated changes onto that base reassign the branch pointer. Visual explanation: Why and when git rebase: Examples: Let's say now we want to combine d-e-f into one commit we can do: git rebase HEAD~3 -i And we will see something similar to the following: Then we can choose to squash commit-84b30b8, commit-e487eb1 and commit-f1d3f6c into one single commit f1d3f6c by doing the following: Git Reflog This command returns a list most recent used commit sha-hash and what operation has been done on them. Combine with git reset , this is one of the most useful command when we need to undo something. Why and when to use git reflog When we use git reflog, we typically something like this: Examples: Lets say we have messed up the branch by accidentally using git reset --hard, and we need to find back those changes, we can do the following: This allows us to go back to the commit we do the operation before. Some Useful Tips Squashing and Splitting There are three ways to do commit squash There is one way to do splitting: Undo changes: There are two ways to undo changes RECAP In this post, we learned about: Thanks to Ankita De 71 71 71 More from DataDrivenInvestor empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com Serhii Biloshkurskyi Dec 27, 2019 Documentation of software development What comes to mind when you hear about project documentation? Are there people who are really into it? Is it ideal? Is the procedure fully completed, and how important is it? How do you manage it? Really interesting questions! Ill take you through these questions bringing to mind what documentation 4 min read 4 min read Share your ideas with millions of readers. Alex Mitrani Dec 26, 2019 Time Series and Trend Analysis Time-dependent trends are a unique feature of time series analysis. If the sequence of events matters, then you need to analyze possible trends. These trends can ultimately be used for creating models that predict future values. I recently published articles about working with time series data and creating OHLC 4 min read 4 min read Robert Reinold Dec 26, 2019 The Path to Maximized IoT Introduction The Internet of Things (IoT) is the movement to embed sensors and controllers into assets to enable applications that create safer and more efficient local and globalized systems. 3 min read 3 min read Freda Xin Dec 26, 2019 List Comprehensions in Python and Set Builder Notation in Set Theory In this post, I will discuss the similarities and connections between list comprehensions in Python and set builder notation in Set Theory. I will also briefly compare them with list comprehensions in Haskell. List Comprehensions in Python A list comprehension in Python will let you construct a new list by iterating through each element 5 min read 5 min read Fabiansyah Cahyo Dec 26, 2019 Extreme Learning Machine for Simple Classification So last week, my friend in college asked my help about implementing the code of extreme learning machine. At that time, I didnt quite understand about the algorithm but I decided to help anyway while learning and understanding it. 4 min read 4 min read Recommended from Medium Toro Cloud Are microservices evolutionary or revolutionary? TechGenyz in TechGenyz WooCommerce or Magento for an eCommerce Project: Which platform to choose? Paige McFarlain in smucs Comparing the Performance of Community Detection Algorithms in Pythons NetworkX and C++s Boost Stanley Masinde in An Idea (by Ingenious Piece) Setting proper server permissions Piyush Mehta Getting started with Ansible Cliff Weitzman Never Procrastinate: My Bullet Proof Way Tonya Nguyen Lab 1: Physical Computing ProjectAgoraEng in Project Agora Engineering Catch a glimpse of Project Agoras Header Bidding Solution About  Help  Terms  Privacy Get the Medium app Sign In Jay Shi Software Engineer @ Google. I write about Python tutorials and stuff that can help you become a better software engineer. More from Medium Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Jacob Bennett in Level Up Coding Use Git like a senior engineer Frank Andrade in Geek Culture My Top 5 Paid Subscriptions Ill Never Cancel as a Programmer Dr. Derek Austin in Better Programming Why I Prefer Regular Merge Commits Over Squash Commits Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 4328,\n",
       "  'url': 'https://medium.com/datadriveninvestor/web-hosting-using-python-part-2-ec081e48631e',\n",
       "  'title': 'Web Hosting Using Python Part\\xa02',\n",
       "  'subtitle': '-',\n",
       "  'claps': 67,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 3,\n",
       "  'publication': 'Data Driven Investor',\n",
       "  'date': '2019-10-23',\n",
       "  'clap_prop': 3.3094869159187803e-05,\n",
       "  'text': \"DataDrivenInvestor Oct 22 2019 Listen Save Web Hosting Using Python Part 2 previous part tutorial saw run basic web page server using Flask tutorial found take look render already present HTML page also include CSS Creating HTML File basic HTML file also contains CSS Style Tags JavaScript script tag dynamic page output change button click output code shown HTML file run smoothly machine without server run Flask server need add entire HTML code file return statement really feasible Lets see easier way Flask Data Driven Investor Microsoft 'Edge Chrome Brief History wa never fan browser well exact wa fan one Chrome ha www.datadriveninvestor.com File Structure Flask Server HTML file stored folder named template directory Flask application Hence include HTML file application create folder directory Flask application name template need add Flask file template folder HTML file included template folder Changes Flask Application Making change Flask application fairly simple need specify name HTML file want render HTML code Flask default look template folder search HTML template change made Flask application line 1 line 6 line 1 import render template module line 6 include return statement passing name HTML file process run application local server remains discussed first tutorial Lets see output application local server see Flask application run perfectly fine server Thanks Reading upcoming Publications see render CSS JavaScript present separate file included HTML document Please comment view publication comment box 67 67 67 DataDrivenInvestor empowerment data knowledge expertise subscribe DDIntel http //ddintel.datadriveninvestor.com Joseph Brown Oct 22 2019 Member-only Future Mindset Key Lifetime Success know life look like five year imagined want live ten twenty thirty year time Entrepreneurial Trek Embrace Learning Data Driven Investor building multimillion-dollar company wa n't hard enough entrepreneur take extra care theirwww.datadriveninvestor.com never Well used imagine life might like fantasize good life could stress 5 min read 5 min read Share idea million reader Michael Trigg Oct 22 2019 Member-only Many Crypto Currencies Global Economy Handle must limit number cryptocurrencies first cryptocurrency enter global economy wa BitCoin people major market know first actual Bitcoin came 3rd January 2009 Satoshi Nakamoto mined genesis block bitcoin block number 0 reaped reward 4 min read 4 min read Hamza Ergder Oct 22 2019 Artificial Intelligence develops much near future robot may get ahead human development artificial intelligence idea people passed education system emphasizes human emotion-based feature rather rote education avoid passed might way 3 min read 3 min read Michael Trigg Oct 22 2019 Member-only Certain Humans Crying Climate Change Wolf Kind like shouting fire crowded theatre climate Planet Earth changing without doubt people around world tend agree matter people live hotter drier region planet know climate warming 4 min read 4 min read Daniel G. Jennings Oct 22 2019 Member-only Trucking Survive JB Hunt Market Mad House Trucking receiving lot scrutiny criticism day Many observer believe trucking industry verge major disruption Elon Musk example belief electric-powered semi soon become norm industry Consequently Musks Tesla Motors NASDAQ TSLA manufacturing marketing 5 min read 5 min read Ayush Kalla Data Scientist interest Python JavaScript Web Development free time like read Blockchain bake go hike Medium Dennis Niggl Python Plain English Creating Awesome Web App Python Streamlit Avi Chawla Towards Data Science Building All-In-One Audio Analysis Toolkit Python Yang Zhou TechToFreedom 9 Fabulous Python Tricks Make Code Elegant Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"DataDrivenInvestor Oct 22, 2019 Listen Save Web Hosting Using Python Part 2 In the previous part of this tutorial we saw how to run a basic web page on a server using Flask. The tutorial can be found here . Now we can take a look at how to render an already present HTML page which can also include CSS in itself. Creating the HTML File Below is a basic HTML file that also contains CSS (between the Style Tags) and some JavaScript (between the script tag). This is a dynamic page as its output changes on button click. The output of the above code is as shown below This HTML file runs smoothly on the machine (without any server), but to run this on a Flask server we will need to add the entire HTML code for the file in the return statement, which is not really feasible. Lets see an easier way to do it in Flask. Data Driven Investor | Microsoft Having An 'Edge' Over Chrome A Brief History I was never a fan of browsers, well to be exact I was only a fan of one, Chrome. It has been my www.datadriveninvestor.com File Structure of a Flask Server All the HTML files are stored in a folder named templates which can be in the same directory as the Flask application. Hence, to include the HTML file in the application, we create a folder in the same directory as our Flask application and name it as templates. Now we need to add our Flask file to this templates folder. The HTML file is included in the templates folder. Changes to the Flask Application Making changes to the Flask application is fairly simple. We just need to specify the name of the HTML file from which we want to render the HTML code. Flask by default looks at the templates folder to search for HTML templates. The only changes made in this Flask application is on lines 1 and line 6. On line 1 we import the render template module and on line 6 we include it in the return statement, by passing the name of the HTML file. The process to run the application on the local server remains the same as discussed in the first tutorial. Lets see the output of the application on the local server. As we can see, our Flask application runs perfectly fine now on the server. Thanks for Reading In the upcoming Publications we will see how to render CSS and JavaScript that are present as separate files and not included in the same HTML document. Please comment your views about the publication in the comment box. 67 67 67 More from DataDrivenInvestor empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com Joseph Brown Oct 22, 2019 Member-only The Future Mindset The Key for a Lifetime of Success Do you know what your life will look like in five years? Have you imagined how you want to live in ten, twenty, thirty years time? On the Entrepreneurial Trek, Embrace the Learning | Data Driven Investor As if building a multimillion-dollar company wasn't hard enough, entrepreneurs have to take extra care of theirwww.datadriveninvestor.com I never did. Well, I used to imagine what life might be like, and fantasize about how good life could be, and stress about 5 min read 5 min read Share your ideas with millions of readers. Michael Trigg Oct 22, 2019 Member-only How Many Crypto Currencies Can The Global Economy Handle? There must be a limit to the number of cryptocurrencies. The first cryptocurrency to enter the global economy was BitCoin, as most people in the major markets know. The first actual Bitcoin came into being on the 3rd of January 2009 when Satoshi Nakamoto mined the genesis block of bitcoin - block number 0 - and reaped a reward of 4 min read 4 min read Hamza Ergder Oct 22, 2019 What do we do if Artificial Intelligence develops too much? In the near future, robots may get ahead of humans with the development of artificial intelligence. There is an idea that people should be passed through an education system that emphasizes human emotion-based features rather than rote education to avoid being passed. This might be the only way we can 3 min read 3 min read Michael Trigg Oct 22, 2019 Member-only Are Certain Humans Crying Climate Change Wolf? Kind of like shouting fire in a crowded theatre. The climate on Planet Earth is changing without a doubt. Most people around the world tend to agree on that matter. Most people who live in the hotter and drier regions of the planet know the climate is warming. 4 min read 4 min read Daniel G. Jennings Oct 22, 2019 Member-only Can Trucking Survive at JB Hunt? Market Mad House Trucking is receiving a lot of scrutiny and criticism these days. Many observers believe the trucking industry is on the verge of major disruption. Elon Musk, for example, believes electric-powered semis will soon become the norm in the industry. Consequently, Musks Tesla Motors (NASDAQ: TSLA) is manufacturing and marketing an 5 min read 5 min read Ayush Kalla Data Scientist with interests in Python, JavaScript and Web Development. In my free time I like to read about Blockchain, bake and go on hikes. More from Medium Dennis Niggl in Python in Plain English Creating an Awesome Web App With Python and Streamlit Avi Chawla in Towards Data Science Building an All-In-One Audio Analysis Toolkit in Python Yang Zhou in TechToFreedom 9 Fabulous Python Tricks That Make Your Code More Elegant Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 4351,\n",
       "  'url': 'https://towardsdatascience.com/getting-started-with-geographic-data-science-in-python-part-2-f9e2b1d8abd7',\n",
       "  'title': 'Getting started with Geographic Data Science in Python\\u200a—\\u200aPart\\xa02',\n",
       "  'subtitle': 'Tutorials, Real World projects\\xa0&…',\n",
       "  'claps': 56,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-05-22',\n",
       "  'clap_prop': 2.766138317782861e-05,\n",
       "  'text': 'Towards Data Science May 22 2019 Member-only Listen Save Getting started Geographic Data Science Python Part 2 Tutorials Real World project Exercises Second article three-part series article Getting started Geographic Data Science Python learn reading manipulating analysing Geographic data Python article series designed sequential first article lay foundation second one get intermediate advanced level Geographic data science topic third part cover relevant real-world project wrapping cement learning first article accessed Master Geographic Data Science Real World project Exercises Real World project Exercises Real World project Exercisestowardsdatascience.com Learning Objectives tutorial 1 Understand GeodataFrames Geoseries 2 Perform Table join Spatial Join 3 Carry buffer overlay analysis 1 GeodataFrame Geoseries Let u read country cite dataset load data get table geographic geometry geographic geometry allow u perform spatial operation addition typical tabular data analysis panda simple excel DataFrame vs. GeoDataFrame GeoDataFrame tabular data structure contains GeoSeries.The important property GeoDataFrame always ha one GeoSeries column hold special status GeoSeries referred GeoDataFrames geometry spatial method applied GeoDataFrame spatial attribute like Area called command always act geometry column one column either dataFrame GeodataFrame One column Geometry Column called GeoeDataFrame Otherwise DataFrame column geometry column Similarly One column mean either Series Geoseries data type column Geometry column called Geoseries Let u see example data type start Dataframe two column none Geometry column therefore type data dataframe output type function pandas.core.frame.DataFrame happen geometry column table Geodatframe Similarly Geoseries single Geometry column Series datatype one column geometry column shown yield pandas.core.series.Series geopandas.geoseries.GeoSeries respectively GeoDataFrame/GeoSeries carry geographic processing task far seen including .plot Another example getting centriods polygon Let u get country centroid plot plot look like point represents country center Exercise 1.1 Create union polygon geometry Countries Hint use .unary_union Exercise 1.2 calculate area country Hint use .area 2 Table Join vs. Spatial join Table join classical query operation two separate table example sharing one column case perform table join two table joined using shared column hand spatial join relates geographic operation example joining location city country see example join/merge two table based shared column NAME pure panda operation doe entail geographic operation However spatial join merging entail geographic operation perform example spatial join want join following two table based location example country doe contain city city within country use Geopandas function .sjoin spatial join show sample 5 row see table city matched corresponding country based location used op=within take city point within country polygon could also use intersect Also could use op=contain find country contain city point 3 Buffer Analysis Buffer analysis important geoprocessing task used widely many domain get distance around point example first get city Sweden buffer around One tricky thing need know CRS/projection using get correct output want data projected projection meter used output meter classical error world Geodata used resource find CRS Sweden ha meter SWEREF99 TM EPSG Projection -- Spatial Reference Home Upload List user-contributed reference List reference spatialreference.org use 3 different buffer distance 100 200 500 single point Stockholm city plot result show concept buffering Exercise 3.1 Create buffer city Try different projection different distance Overlay sometimes need create new feature different data type like Points Lines Polygons Set operation Overlays play important role using dataset instead reading unzipped folder use built-in dataset reading mechanism Geopandas example come Geopandas documentation subset data select Africa illustrate overlay function consider following case one wish identify core portion country defined area within 500km capital using GeoDataFrame Africa GeoDataFrame capital select portion country within 500km capital specify option intersect creates new set polygon two layer overlap Changing option allows different type overlay operation example interested portion country far capital periphery would compute difference two Conclusion tutorial covered geoprocessing task Geographic data using Geopandas First studied difference dataframe Geodataframe followed exploring spatial join also done buffer analysis well Overlay analysis next tutorial apply learned preceding part project code available GitHub repository shakasom/GDS Geographic data science tutorial series Contribute shakasom/GDS development creating account GitHub github.com also go directly run Google Collaboraty Jupyter Notebooks directly link shakasom/GDS Geographic data science tutorial series Contribute shakasom/GDS development creating account GitHub github.com 56 1 56 56 1 Enjoy read Reward writer Beta tip go Abdishakur third-party platform choice letting know appreciate story Get email whenever Abdishakur publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Marie Stephen Leo May 22 2019 Member-only New York Taxi data set analysis Predicting taxi fare using Regression model Recently opportunity play New York taxi public data set hosted Google cloud Big Query platform decided apply machine learning technique data set try build predictive model using Python post Ill attempt predict 7 min read 7 min read Share idea million reader Joshua J Luo May 21 2019 Member-only Exploration Neural Networks Playing Video Games Thank team member Benjamin Guo Christian Han Cooper Shearer Justin Qu Kylar Osborne Introduction Video game arent fun provide platform neural network learn interact dynamic environment solve complex problem like real life Video game 13 min read 13 min read George Seif May 21 2019 Member-only use Pandas RIGHT way speed code Want inspired Come join Super Quotes newsletter Pandas library ha heavenly gift Data Science community Ask Data Scientist like handle datasets Python theyll undoubtedly talk Pandas Pandas epitome 4 min read 4 min read Jiawei Wang May 21 2019 Member-only Representing Human Mobility Patterns Social Network Data Using Hidden Markov Models Jiawei Wang Seth Lee Hyunsu Chae Fei Github http //github.com/sethlee0111/MobilityHMM Introduction Understanding knowing utilize human mobility helpful various application modern day example knowing people move around city city planner developer design city efficiently However 10 min read 10 min read Marco Cerliani May 21 2019 Member-only Extreme Event Forecasting LSTM Autoencoders Improve forecasting performance developing strong Neural Network architecture Dealing extreme event prediction frequent nightmare every Data Scientist Looking around found interesting resource deal problem Personally literally fall love approach released Uber Researchers paper two version available developed 7 min read 7 min read Abdishakur Writing Geospatial Data Science AI ML DL Python SQL GIS Top writer 1m view Medium Nik Piepenbreier Better Programming Make Awesome Maps Python Geopandas Maurcio Cordeiro Towards Data Science Artificial Intelligence Geospatial Analysis Pytorchs TorchGeo Part 1 Abdishakur Spatial Data Science Explore Open-source Python GIS Earth Observation library Interactively Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science May 22, 2019 Member-only Listen Save Getting started with Geographic Data Science in Python Part 2 Tutorials, Real World projects & Exercises This is the Second article of a three-part series of articles in Getting started Geographic Data Science with Python. You will learn about reading, manipulating and analysing Geographic data in Python. The articles in this series are designed to be sequential where the first article lays the foundation and the second one gets into intermediate and advanced level Geographic data science topics. The third part covers a relevant and real-world project wrapping up to cement your learning. The first article can be accessed here. Master Geographic Data Science with Real World projects & Exercises Real World projects & Exercises Real World projects & Exercisestowardsdatascience.com Learning Objectives for this tutorial are: 1. Understand GeodataFrames and Geoseries 2. Perform Table join and Spatial Join 3. Carry out the buffer and overlay analysis 1. GeodataFrame & Geoseries Let us read the countries and cites dataset. Once you load the data, what we get is a table with geographic geometries. The geographic geometries allow us to perform spatial operations in addition to the typical tabular data analysis in pandas or simple excel. DataFrame vs. GeoDataFrame. A GeoDataFrame is a tabular data structure that contains a GeoSeries.The most important property of a GeoDataFrame is that it always has one GeoSeries column that holds a special status. This GeoSeries is referred to as the GeoDataFrames geometry. When a spatial method is applied to a GeoDataFrame (or a spatial attribute like Area is called), this commands will always act on the geometry column. If you have more than one column, you have either a dataFrame or GeodataFrame. If One of the columns is a Geometry Column, then it is called a GeoeDataFrame . Otherwise, it is a DataFrame if any of the columns is not a geometry column. Similarly, One column means you have either a Series or Geoseries data type. If the only column is the Geometry column, then it is called Geoseries . Let us see an example of each data type. We start with Dataframe. We have only two columns here and none of them is a Geometry column, therefore, the type of this data will be a dataframe and the output of the type function is pandas.core.frame.DataFrame . If we happen to have any geometry column in our table, then it will be a Geodatframe as below. Similarly, a Geoseries is when we have a single Geometry column and Series datatype will be when this one column is not a geometry column as shown below. This will yield pandas.core.series.Series and geopandas.geoseries.GeoSeries respectively. With GeoDataFrame/GeoSeries you can carry out geographic processing tasks. So far we have seen few including .plot() . Another example is getting centriods of polygons. Let us get each countrys centroid and plot it. And this is how the plot looks like, each point represents the countrys center. Exercise 1.1: Create a union of all polygon geometries (Countries). Hint use (.unary_union) Exercise 1.2: calculate the area of each country. Hint use (.area) 2. Table Join vs. Spatial join Table joins is classical query operation where we have two separate tables, for example, sharing one column. In that case, you can perform a table join where the two tables are joined using the shared column. On the other hand, spatial join relates to geographic operations, for example, joining by location each city and its country. We will see both examples below. We can join/merge the two tables based on their shared column NAME. This is pure pandas operation and does not entail any geographic operations. However, in spatial join, the merging entails a geographic operation. We will perform an example of a spatial join. We want to join the following two tables based on their locations. For example, which country does contain which city or which city is within which country. We will use Geopandas function .sjoin() to do the spatial join and show a sample of 5 rows. As you can see from the below table, each city is matched with its corresponding country based on the location. We have used op=within which takes city points that are within a countries polygon. Here we could also use intersect. Also, we could use op=contain and find out which countries contain the city points. 3. Buffer Analysis Buffer analysis is an important geoprocessing task. It is used widely in many domains to get a distance around a point. In this example, we will first get a city in Sweden and then do a buffer around it. One tricky thing here is you need to know which CRS/projection you are using to get the correct output you want. If your data is not projected into projection where meters are used, then the output will not be in meters. This is a classical error in the world of Geodata. I have used this resource to find out which CRS Sweden has in meters. SWEREF99 TM: EPSG Projection -- Spatial Reference Home | Upload Your Own | List user-contributed references | List all references spatialreference.org We use here 3 different buffer distances, 100, 200, and 500 on a single point, Stockholm city. Then we plot the result to show the concept of buffering. Exercise 3.1: Create a buffer of all cities. Try different projections and different distances. Overlay We sometimes need to create new features out of different data types like Points, Lines and Polygons. Set operations or Overlays play an important role here. We will be using the same dataset but instead of reading it from our unzipped folder we can use built-in dataset reading mechanism in Geopandas. This example comes from Geopandas documentation. We can subset data to select only Africa. To illustrate the overlay function, consider the following case in which one wishes to identify the core portion of each country defined as areas within 500km of a capital using a GeoDataFrame of Africa and a GeoDataFrame of capitals. To select only the portion of countries within 500km of a capital, we specify the how option to be intersect, which creates a new set of polygons where these two layers overlap: Changing the how option allows for different types of overlay operations. For example, if we were interested in the portions of countries far from capitals (the peripheries), we would compute the difference between the two. Conclusion This tutorial covered some geoprocessing task in Geographic data using Geopandas. First, we studied differences between dataframe and Geodataframe followed by exploring spatial join. We have also done buffer analysis as well as Overlay analysis. In the next tutorial, we will apply what we have learned in this and preceding part in a project. The code is available in this GitHub repository: shakasom/GDS Geographic data science tutorials series. Contribute to shakasom/GDS development by creating an account on GitHub. github.com You can also go directly and run Google Collaboraty Jupyter Notebooks directly from this link: shakasom/GDS Geographic data science tutorials series. Contribute to shakasom/GDS development by creating an account on GitHub. github.com 56 1 56 56 1 Enjoy the read? Reward the writer. Beta Your tip will go to Abdishakur through a third-party platform of their choice, letting them know you appreciate their story. Get an email whenever Abdishakur publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Marie Stephen Leo May 22, 2019 Member-only New York Taxi data set analysis Predicting taxi fare using Regression models Recently I had the opportunity to play with the New York taxi public data set hosted by Google clouds Big Query platform. I decided to apply machine learning techniques on the data set to try and build some predictive models using Python. For this post, Ill attempt to predict the 7 min read 7 min read Share your ideas with millions of readers. Joshua J Luo May 21, 2019 Member-only An Exploration of Neural Networks Playing Video Games Thank you to my team members: Benjamin Guo, Christian Han, Cooper Shearer, Justin Qu, and Kylar Osborne. Introduction Video games arent just fun. They provide a platform for neural networks to learn how to interact with dynamic environments and solve complex problems, just like in real life. Video games have been 13 min read 13 min read George Seif May 21, 2019 Member-only How to use Pandas the RIGHT way to speed up your code Want to be inspired? Come join my Super Quotes newsletter. The Pandas library has been a heavenly gift to the Data Science community. Ask any Data Scientist how they like to handle their datasets in Python and theyll undoubtedly talk about Pandas. Pandas is the epitome of what a 4 min read 4 min read Jiawei Wang May 21, 2019 Member-only Representing Human Mobility Patterns with Social Network Data Using Hidden Markov Models By: Jiawei Wang, Seth Lee, Hyunsu Chae, Fei He Github: https://github.com/sethlee0111/MobilityHMM Introduction Understanding and knowing how to utilize human mobility can be very helpful for various applications in modern days. For example, by knowing how people move around the city, city planners and developers can design the city more efficiently. However 10 min read 10 min read Marco Cerliani May 21, 2019 Member-only Extreme Event Forecasting with LSTM Autoencoders Improve forecasting performance developing a strong Neural Network architecture Dealing with extreme event prediction is a frequent nightmare for every Data Scientist. Looking around I found very interesting resources that deal with this problem. Personally, I literally fall in love with the approach released by Uber Researchers. In their papers (two versions are available here and here) they developed 7 min read 7 min read Abdishakur Writing about Geospatial Data Science, AI, ML, DL, Python, SQL, GIS | Top writer | 1m views. More from Medium Nik Piepenbreier in Better Programming Make Awesome Maps in Python and Geopandas Maurcio Cordeiro in Towards Data Science Artificial Intelligence for Geospatial Analysis with Pytorchs TorchGeo (Part 1) Abdishakur in Spatial Data Science Explore Open-source Python GIS and Earth Observation libraries Interactively Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5731,\n",
       "  'url': 'https://towardsdatascience.com/using-fips-to-visualize-in-plotly-14fa7a6ddcf0',\n",
       "  'title': 'Using FIPS to Visualize in\\xa0Plotly',\n",
       "  'subtitle': '-',\n",
       "  'claps': 40,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-06-08',\n",
       "  'clap_prop': 1.975813084130615e-05,\n",
       "  'text': 'Towards Data Science Jun 7 2019 Member-only Listen Save Using FIPS Visualize Plotly Recently done two visualization project Plotly visualizing average rent per square foot California average data scientist salary across United States used two different approach visualize data map Scatter plot Map Chloropleth Map Plotly Plotly one powerful visualization package available Python One advantage using Plotly may use Python generate d3 graph Plotly built top d3 take long time learn d3 Plotly help eliminate annoying moment focus understanding data Plotly ha rich library map visualization easy use Map one type Plotly could ease frustration Scatter Plot Map good idea make scatter plot map data unit city chose visualize average data scientist salary cross United States approach average salary based city process making visualization almost making scatter plot Plotly except background map mean data plotting map based longitude latitude corresponds x value respectively data frame average salary city ready obtain longitude latitude city store data frame next step assign colour want differentiate level salary final step define plot layout visualization Plotly nice may go plotly.graph_objs may find Scattergeo US map visualization referenced example official Plotly documentation may find link bottom post would like look code may also find link bottom post Scatterplot map best visualizing data based city easy interrupting ha problem try visualize data US map plenty non-US map available However doe work well city data set close instance many data point Bay Area point stacked viewer may find difficult find difference area Choropleth Map alternative approach visualize data map choropleth map Choropleth map county-based map filled colour county map look like One advantage choropleth map data point stack may think hard plot use longitude latitude plot map instead use FIPS locate county FIPS county code FIPS county code stand Federal Information Processing Standard United States federal government assigns number county country nice feature Plotlys choropleth map Plotly take FIPS county code parameter FIPS county code ha 5 digit first 2 digit represent state last 3 digit represent county example FIPS county code San Francisco County 06075 06 mean California 075 represent San Francisco Since FIPS county code designated county would plot data wrong data Plotly may find list FIPS county code federal government website included link bottom post Choropleth Map Example one project graduate school professor gave data set rent across California sourced Craigslist decided find median rent per square foot across California data set contains small amount data outside California nice thing FIPS exclude observation FIPS start 06 FIPS start value California data ready may import create_choropleth plotly.figure_factory pas FIPS value colour create choropleth final visualization look like may also find code visualization downside Plotly making choropleth map found useful US map One time attempted plot choropleth map UK map find package option support current version great visualizing US outside US Thought mentioned two type map visualization powered Plotly scatter plot map choropleth map map server different purpose map depends whether want plot city county want plot data city go scatter plot map expect longitude latitude ready verse want plot data county choropleth map good way FIPS county code ready data set coming federal government likely FIPS county code ha paired data already Therefore choropleth map Plotly handy package visualize data US national data federal government Reference Plotly Scatterplot Map http //plot.ly/python/scatter-plots-on-maps/ FIPS county source http //www.census.gov/geographies/reference-files/2013/demo/popest/2013-geocodes-all.html http //www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/ cid=nrcs143_013697 Github http //github.com/jacquessham Data Salary across US Scatterplot Map http //github.com/jacquessham/ds_salary_opt Median Rent per Square foot county across California Choropleth Map http //github.com/jacquessham/california_rent 43 43 43 Towards Data Science home data science Medium publication sharing concept idea code Dipen Chawla Jun 7 2019 Member-only Machine Learning help identify Effectiveness Adverseness Drug Building system processing text review neurological drug employing ML algorithm provide overview effectiveness adverse reaction form insightful visually informative representation ever looked common cold medicine Internet horrified number side-effects listed gone review section verify got overwhelmed sheer number review listed probably couldnt read 10 min read 10 min read Share idea million reader Robert Dargavel Smith Jun 7 2019 Member-only discover new music Spotify Artificial Intelligence Find similar sounding music artist may never heard UPDATE Try Deej-A.I Apps Google Play Discover new music using Artificial Intelligence automatically generate playlist based music soundsplay.google.com http //apps.apple.com/us/app/deej-a-i/id1529860910 mt=8 DJ question hate got anything insert totally inappropriate artist kind music listen opinion soon put music genre box becomes constrained limit 5 min read 5 min read Sachin Mehta Jun 7 2019 Member-only ESPNetv2 Semantic Segmentation Nowadays number real-world application autonomous vehicle involves visual scene understanding Semantic segmentation one main task open way visual scene understanding However one computationally expensive task computer vision 4 min read 4 min read Aldo von Wangenheim Jun 7 2019 Member-only Artificial Intelligence Paleontology Use Deep Learning search Microfossils analyze micro-tomographies sedimentary rock obtained drilling rig probe semantic segmentation posting show Deep Learning-based method fully automated microfossil identification extraction bore core sample acquired via MicroCT identification developed Deep Learning approach resulted high rate correct microfossil identification 98 IoU validate use ground 12 min read 12 min read Edoardo Barp Jun 7 2019 Implementing Simple Auto-Encoder Tensorflow Generative Adversarial Networks GAN recently risen popularity display capability initially imitating famous painter art style recently DeepFake allows seamlessly replace facial expression video keeping high output quality One 7 min read 7 min read Jacques Sham data engineer BI company Czech heritage whisky-lover aviation enthusiast gamer Concern use data science answer question Medium Nik Piepenbreier Better Programming Make Awesome Maps Python Geopandas Norris Towards Data Science Plotting Heat Maps Python using Bokeh Folium hvPlot Vinod Dhole JovianData Science Machine Learning Interesting Heatmaps Using Python Folium MecSimCalc Level Coding Python tutorial use Folium publish interactive map Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Jun 7, 2019 Member-only Listen Save Using FIPS to Visualize in Plotly Recently I have done two visualization projects with Plotly, visualizing average rent per square foot in California and average data scientist salary across the United States. I used two different approaches to visualize data on the map Scatter plot on Map and Chloropleth Map. Why Plotly? Plotly is one of the powerful visualization packages available in Python. One of the advantages of using Plotly is that you may use Python to generate d3 graphs because Plotly is built on top of d3. It takes a very long time to learn d3 but Plotly can help to eliminate the annoying moment and focus more on understanding the data. Plotly has a rich library on map visualization and easy to use. Map is one of the types in Plotly that could ease your frustration. Scatter Plot on Map  It is a good idea to make scatter plot on map when the data unit is city, so I chose to visualize average data scientist salary cross the United States with this approach because the average salary is based on city. The process of making this visualization is almost the same as making scatter plot in Plotly, except the background is a map. It means the data is plotting on the map based on longitude and latitude, which corresponds to x and y values, respectively. Once you have the data frame with average salary and cities ready, then obtain the longitude and latitude of each city and store in the same data frame. The next step is to assign colour you want to differentiate the level of salary. The final step is to define the plot and the layout of the visualization. Plotly is nice because you may go to plotly.graph_objs which you may find Scattergeo for US map. This visualization is referenced from the example from the official Plotly documentation, you may find the link at the bottom of the post. If you would like to look at my code, you may also find the link at the bottom of the post too. Scatterplot on the map is best for visualizing data based on city and very easy interrupting. It has no problem if you try to visualize data no on the US map as there is plenty of non-US maps available. However, it does not work too well if the cities in the data set are very close to each other. For instance, if you have too many data points in the Bay Area, some points stacked on each other that viewers may find it difficult to find the difference in that area. Choropleth Map  The alternative approach to visualize data on maps is choropleth map. Choropleth map is a county-based map which filled colour the county on the map. It looks like this: One advantage of a choropleth map is that data points do not stack on each other. You may think it is hard to plot as you cannot use longitude and latitude to plot on the map, but instead, you use FIPS to locate the counties. FIPS county code  FIPS county code stands for Federal Information Processing Standard which the United States federal government assigns a number on each county in the country. A nice feature of Plotlys choropleth map is that Plotly takes FIPS county code as a parameter. FIPS county code has 5 digits, the first 2 digits represent the state and the last 3 digits represent the county. For example, the FIPS county code of San Francisco County is 06075. 06 means California, and 075 represent San Francisco. Since the FIPS county code is designated to each county, you would not plot the data on the wrong data in Plotly. You may find the list of FIPS county codes the federal government website and I have included the link at the bottom of this post. Choropleth Map Example  In one of my projects in graduate school, my professor gave me a data set on rent across California sourced from Craigslist and I decided to find out the median rent per square foot across California. The data set contains a small amount of data outside of California, the nice thing about FIPS is that I can exclude the observations that do not have a FIPS start with 06 because FIPS start with other values are not California. Once the data is ready, you may import create_choropleth from plotly.figure_factory and pass FIPS, values and colour to create a choropleth. My final visualization looks like this: You may also find my codes on this visualization. The downside about Plotly making choropleth map is that I only found that useful for US map. One time I attempted to plot a choropleth map on an UK map but I cannot find any package or option support that. The current version is great for visualizing in the US but not outside the US. Thought  I have mentioned two types of map visualization powered by Plotly scatter plot on map and choropleth map. Both maps server different purpose of map, depends on whether if you want to plot by city or county. If you want to plot data by city, you should go with scatter plot on map and expect to have longitude and latitude ready. By verse, if you want to plot data by county, choropleth map is a good way and you should have FIPS county code ready. If the data set is coming from the federal government, it is very likely that the FIPS county code has been paired with data already. Therefore, choropleth map from Plotly is a handy package to visualize data on US national data from the federal government. Reference  Plotly Scatterplot on Map:  https://plot.ly/python/scatter-plots-on-maps/ FIPS county source:  https://www.census.gov/geographies/reference-files/2013/demo/popest/2013-geocodes-all.html https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697 My Github:  https://github.com/jacquessham Data Salary across the US (Scatterplot on Map):  https://github.com/jacquessham/ds_salary_opt Median Rent per Square foot by county across California (Choropleth Map):  https://github.com/jacquessham/california_rent 43 43 43 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Dipen Chawla Jun 7, 2019 Member-only How Machine Learning can help identify Effectiveness and Adverseness of a Drug Building a system for processing text reviews of neurological drugs by employing ML algorithms to provide an overview of the effectiveness or adverse reactions in the form of an insightful and visually informative representation. Have you ever looked up your common cold medicine on the Internet only to be horrified by the number of side-effects listed under it? Or gone through the review section to verify them but got overwhelmed by the sheer number of reviews listed you probably couldnt read if you had 10 min read 10 min read Share your ideas with millions of readers. Robert Dargavel Smith Jun 7, 2019 Member-only How to discover new music on Spotify with Artificial Intelligence Find similar sounding music by artists you may never have heard of before UPDATE: Try it out for yourself here, here Deej-A.I. - Apps on Google Play Discover new music using Artificial Intelligence to automatically generate playlists based on how the music soundsplay.google.com or here: https://apps.apple.com/us/app/deej-a-i/id1529860910?mt=8 As a DJ, the question I hate most after Have you got anything by [insert totally inappropriate artist here]? is What kind of music do you listen to?. In my opinion, as soon as you put music in a genre box, it becomes constrained by its limits 5 min read 5 min read Sachin Mehta Jun 7, 2019 Member-only ESPNetv2 for Semantic Segmentation Nowadays, a number of real-world applications, such as autonomous vehicles, involves visual scene understanding. Semantic segmentation is one of the main tasks that opens the way for visual scene understanding. However, it is one of the most computationally expensive tasks in computer vision. 4 min read 4 min read Aldo von Wangenheim Jun 7, 2019 Member-only Artificial Intelligence & Paleontology: Use Deep Learning to search for Microfossils How to analyze micro-tomographies of sedimentary rocks obtained from drilling rig probes with semantic segmentation In this posting we show a Deep Learning-based method for fully automated microfossil identification and extraction in bore core samples acquired via MicroCT. For the identification we developed a Deep Learning approach which resulted in a high rate of correct microfossil identification (98% IoU). To validate it we use ground 12 min read 12 min read Edoardo Barp Jun 7, 2019 Implementing a Simple Auto-Encoder in Tensorflow Generative Adversarial Networks (GAN) have recently risen in popularity through the display of some of their capabilities, initially by imitating famous painters art styles, but more recently through DeepFake, which allows to seamlessly replace facial expression in videos, while keeping a high output quality. One of the 7 min read 7 min read Jacques Sham A data engineer in a BI company with Czech heritage, a whisky-lover, an aviation enthusiast, and a gamer. Concern how to use data science to answer questions. More from Medium Nik Piepenbreier in Better Programming Make Awesome Maps in Python and Geopandas Will Norris in Towards Data Science Plotting Heat Maps in Python using Bokeh, Folium, and hvPlot Vinod Dhole in JovianData Science and Machine Learning Interesting Heatmaps Using Python Folium MecSimCalc in Level Up Coding Python tutorial on how to use Folium to publish an interactive map Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 2033,\n",
       "  'url': 'https://towardsdatascience.com/automatically-analyzing-laboratory-test-data-32c27e4e3075',\n",
       "  'title': 'Automatically Analyzing Laboratory Test\\xa0Data',\n",
       "  'subtitle': 'How to write Python programs that perform your\\xa0data…',\n",
       "  'claps': 31,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 12,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-09-17',\n",
       "  'clap_prop': 1.5312551402012268e-05,\n",
       "  'text': 'Towards Data Science Sep 17 2019 Member-only Listen Save Tutorial Automatically Analyzing Laboratory Data Create Performance Map Automatically Analyzing Laboratory Test Data write Python program perform data analysis common scientist find large data set Sometimes come form gigabyte worth data single file time hundred file containing small amount data Either way hard manage Hard make sense Hard computer process need way simplify process make data set manageable help keep track everything Thats tutorial process writing Python script automatically analyze data store data meaningful intuitive file name using example taken actual research know skill youre developing practical useful first post tutorial introduced concept tutorial term heat pump water heater coefficient performance COP performance map dont mean anything might want read second post introduced companion data set split data set multiple file user-friendly name companion data set valuable part tutorial process allows follow along write exact code Ill present run code see result compare result present way ensure youre right end second part tutorial three data file containing test result specified ambient temperature file see electricity consumption heat pump temperature water storage tank air temperature surrounding water heater next step process result need write code automatically make sense data calculates COP heat pump plot data visually understand Without ado let get started Python programming need first import package package need import several package important step Bokeh used script written portion tutorial however script basis future portion worth importing bokeh dont worry moving next portion later project well need import entirety glob panda numpy importing certain function bokeh called using following code Note panda wa imported assigned pd numpy wa assigned np mean reference package writing pd np instead panda numpy package imported next step create glob list loop sequentially iterate file prepared write code analyzing data file iterate file first step configure glob need provide glob path filetype file type optional data .csv format want specify avoid including extraneous file call glob create list file type specified folder done using following line code Path variable tell glob look specified folder relevant file Note folder specified folder saved file first part tutorial second line call glob function glob package create list specified file Notice code reference Path variable specified state folder glob search file followed code stating glob include .csv file nothing else code completed glob create list .csv file folder form 1.csv 2. csv 3.csv n.csv downloaded companion data set following tutorial full path PerformanceMap_HPWH_55.csv PerformanceMap_HPWH_70.csv PerformanceMap_HPWH_95.csv list file next step create loop iterates file need open file perform data analysis need following code code automatically iterates every entry Filenames list Note way written lead Filename holding actual filename entry list instance first time loop Filename contain full path PerformanceMap_HPWH_55.csv second line us panda read file memory save Data later analysis file located sequentially opened next step add code analyzing file next step write code automatically analyze file requires fair amount knowledge equipment studied top knowledge Python programming Since assume arent reader HPWHs Ill make sure write relevant information Filtering data contain important portion process care data heat pump HPWH active heat pump device typically draw 400600 W depending ambient water temperature Meanwhile on-board electronics consume electricity filter data segment care need remove data electricity consumption le 300 W chosen significantly higher power draw on-board electronics minimum draw heat pump following line line reset data frame data device consuming 300 W. impact index data frame need reset keep data frame clean use following code Identifying change time measurement timestamp data data set easy work Fortunately know collaborating lab testing partner measurement taken every 10 second create new column data frame state long test ha active using following code Calculating change energy stored water One key parameter impacting COP HPWHs temperature water storage tank water wont mixed well enough hold single constant temperature Typically cold water bottom tank hot water top sake exercise good enough calculate average temperature tank Since lab tester wa kind enough inform u used 8 temperature measurement evenly spaced throughout tank calculate average water temperature using really care much average temperature tank change one measurement time another way identify change energy stored tank thus energy added water heat pump using following two line code first line us .shift command panda data frame create new column data frame containing Average Tank Temperature deg F data shifted one row data frame creates empty cell first row Index 0 cause error performing calculation second line code rectifies using .loc fill cell 72.0 friendly lab tester told u test started precisely 72.0 deg F every single time calculate change energy stored water every two time stamp need know constant equation put together calculate change stored energy using following line Calculating COP next step analyzing data set calculating COP function water temperature tank goal tutorial identify COP function water temperature ambient temperature provide understanding COP function water temperature specified ambient temperature lead right direction calculate COP heat pump need perform unit conversion electricity consumption currently expressed W energy added water currently expressed Btu/timestep make unit conversion use ratio 1 W 3.412142 Btu/hr convert Btu/hr Btu/s multiply 10 second per timestamp give code COP definition amount heat added water divided amount electricity consumed Thus calculated Generating Regressions table showing COP function water temperature three specified COPs better Wouldnt nice function use calculate COP enter water temperature identify COP accordingly Numpy provides tool make easy use numpy function polyfit identify coefficient refression describing COP function water temperature flexible function allowing control shape curve specifying order function end Since COP heat pump function temperature parabolic curve need second order regression example Thus coefficient identified line numpy poly1d function used create regression using coefficient done identify COP heat pump specified water temperature using regression Remember regression generated specific air temperature estimate COP using regression correct air temperature Creating 2-d performance map ultimate goal tutorial arent yet COP specified water temperature identifying calling function using water temperature input instance want find COP water temperature 72 F enter save result data saved using technique always use described Automatically Storing Results Analyzed Data Sets need 1 Ensure folder analyzed result available 2 Create new filename clearly state file contains 3 Save data case want save data new file called Analyzed data folder used show result analysis following code first line creates path new folder add \\\\Analyzed currently existing path stating looking folder called Analyzed within current folder second line determines whether folder already exists doesnt third line creates need set filename data set coefficient done combining already subsection string use string index identify portion filename want keep instance section filename first file say PerformanceMap_HPWH_50 state quite clearly file contains Since know last four character filename .csv isolate section string using index -26 -4 word want character string running 26th last 4th last including 4th last Following customize filename bit Namely state want data filename state contains analyzed data want coefficient filename state contains coefficient write filename file using following line simply save file analyzed data stored panda .to_csv function coefficient saved numpy .tofile function follows Note line saving data set index False mean index data frame saved saving table Also note numpy .tofile function requires specify separator case using comma specified sep code know worked right enormous number thing could gone wrong point process Maybe lab tester made mistake running experiment Maybe instrument broke Maybe typo code incorrect unit conversion imperative ensure none problem others occurred process Therefore next step process checking data set error cover next phase tutorial First Ill discus check data error manually way youll get firm understanding potential error checking identify Ill discus add code script check error automatically warns something go wrong Tutorial Table Contents part series article teaching skill needed automatically analyze laboratory data develop performance map heat pump water heater article series found using following link Introduction Splitting Data Sets Checking Analyzed Laboratory Data Errors Write Scripts Check Data Quality Automatically Generate Regressions Python 38 38 38 Towards Data Science home data science Medium publication sharing concept idea code Peter Grant Scientist Lawrence Berkeley National Laboratory also teach skill need build fulfilling career Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Yang Zhou TechToFreedom 9 Fabulous Python Tricks Make Code Elegant Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Sep 17, 2019 Member-only Listen Save Tutorial: Automatically Analyzing Laboratory Data to Create a Performance Map Automatically Analyzing Laboratory Test Data How to write Python programs that perform your data analysis for you Its very common that scientists find themselves with large data sets. Sometimes it comes in the form of gigabytes worth of data in a single file. Other times its hundreds of files, each containing a small amount of data. Either way, its hard to manage. Hard to make sense of. Hard for your computer to process. You need a way to simplify the process, to make the data set more manageable, and to help you keep track of everything. Thats what this tutorial is all about. Were in the process of writing Python scripts that will automatically analyze all of the data for you and store the data with meaningful, intuitive file names. All while using an example taken from actual research, so you know that the skills youre developing are practical and useful. The first post in this tutorial introduced the concepts of the tutorial. If the terms heat pump water heater, coefficient of performance (COP), and performance map dont mean anything to you, you might want to read it . The second post introduced the companion data set , and split the data set into multiple files with user-friendly names. The companion data set is a valuable part of the tutorial process, as it allows you to follow along. You can write the exact same code that Ill present, run the code, see the results, and compare it to results I present. In that way you can ensure that youre doing it right. At the end of the second part of the tutorial we now have three data files, each containing test results at a specified ambient temperature. In each of those files we see electricity consumption of the heat pump, the temperature of water in the storage tank, and the air temperature surrounding the water heater. The next step is to process those results. We need to write some code that automatically makes sense of the data, calculates the COP of the heat pump, and plots the data so that we can visually understand it. Without further ado, lets get started. As with all Python programming, we need to first import packages. What packages do I need to import? T here are several packages which will be very important for this step. They are: Bokeh will not be used in the script written during this portion of the tutorial; however, this same script will be the basis of future portions. Its worth importing bokeh now so you dont have to worry about it when moving on to the next portion later. For this project well need to import the entirety of glob, pandas, os and numpy while only importing certain functions from bokeh. These can all be called in using the following code: Note that pandas was imported and assigned to pd, and numpy was assigned to np. This means that we can now reference these packages by writing pd and np instead of pandas and numpy. Now that all of our packages are imported, the next step is to create our glob list and for loop to sequentially iterate through all of your files. When that is prepared, you can write code analyzing each of our data files. How do I iterate through all my files? T he first step is to configure glob. To do this, you need to provide glob with a path and a filetype (The file type is optional, but all of our data is in .csv format so we want to specify that to avoid including extraneous files). Then when you call glob it will create a list of all files of that type in the specified folder. This can be done using the following lines of code: The Path variable tells glob to look in the specified folder for relevant files. Note that the folder specified above is the same folder that you saved files to in the first part of this tutorial . The second line calls the glob function from the glob package to create a list of all the specified files. Notice that the code references the Path variable you specified to state what folder glob should search for files. This is then followed up by code stating that glob should include all .csv files, and nothing else. After this code is completed glob will create a list of all .csv files in the folder. It will have the form [1.csv, 2. csv, 3.csv, , n.csv]. For those who have downloaded the companion data set and are following the tutorial, it will be the full path for PerformanceMap_HPWH_55.csv, PerformanceMap_HPWH_70.csv, and PerformanceMap_HPWH_95.csv. Now that you have a list of files, the next step is to create a for loop that iterates through each of these files. You then need to open the files so you can perform data analysis on each of them. To do so, you need the following code: This code automatically iterates through every entry in the Filenames list . Note that the way this is written leads to Filename holding the actual filename of each entry in the list. For instance, on the first time through the for loop Filename will contain the full path for PerformanceMap_HPWH_55.csv. The second line uses pandas to read the file into memory, and saves it to Data for later analysis. Now that the files are located and being sequentially opened, the next step is to add code analyzing each file. And that will be our next step. How do I write code to automatically analyze each file? T his requires a fair amount of knowledge about the equipment being studied on top of the knowledge of Python programming. Since I assume you arent a reader on HPWHs, Ill make sure to write out the relevant information. Filtering the data to only contain the important portion For this process, we only care about data when the heat pump in the HPWH is active. The heat pumps in these devices typically draw 400600 W depending on the ambient and water temperatures. Meanwhile, they have on-board electronics that consume some electricity. To filter the data to the segment we care about we need to remove all data with electricity consumption less than 300 W, chosen to be significantly higher than the power draw of the on-board electronics but under the minimum draw of the heat pump. We can do this with the following line: That line resets our data frame to have only the data where the device is consuming over 300 W. But that did impact the index of the data frame, so we need to reset that to keep our data frame clean. We can use the following code for that: Identifying the change in time between measurements Now, the timestamp data in this data set is not easy to work with. Fortunately, we know from collaborating with our lab testing partner that measurements were taken every 10 seconds. So we can create a new column in our data frame that states how long the test has been active using the following code: Calculating the change in energy stored in the water One of the key parameters impacting the COP of HPWHs is the temperature of water in the storage tank. The water wont be mixed well enough to hold a single, constant temperature. Typically there will be cold water on the bottom of the tank, and hot water at the top. For the sake of this exercise, its good enough to calculate the average temperature of the tank. Since our lab tester was kind enough to inform us that (s)he used 8 temperature measurements evenly spaced throughout the tank, we can calculate the average water temperature using: Now, what we really care about here is how much the average temperature of the tank changes from one measurement time to another. This way we can identify the change in energy stored in the tank, and thus the energy added to the water by the heat pump. We can do this using the following two lines of code: The first line uses the .shift command of a pandas data frame to create a new column in the data frame containing the Average Tank Temperature (deg F) data, but shifted down one row in the data frame. This creates an empty cell in the first row (Index 0) which causes errors when performing calculations. The second line of code rectifies this by using .loc to fill this cell with 72.0. We can do this because our friendly lab tester told us that the tests started precisely at 72.0 deg F every single time. Now we can calculate the change in energy stored in the water between every two time stamps. To do this, we need to know a few constants and equations: We can put it all together and calculate the change in stored energy using the following line: Calculating the COP The next step in analyzing each data set is calculating the COP as a function of the water temperature in the tank. The goal of this tutorial is to identify the COP as a function of both water temperature and ambient temperature, and this will provide an understanding of the COP as a function of water temperature at each specified ambient temperature. It leads in the right direction. To calculate the COP of the heat pump we need to perform some unit conversions. The electricity consumption is currently expressed in W while the energy added to the water is currently expressed in Btu/timestep. To make this unit conversion we use the ratio 1 W = 3.412142 Btu/hr, then convert Btu/hr to Btu/s and multiply by the 10 seconds per timestamp. This gives the code: The COP is by definition the amount of heat added to the water divided by amount of electricity consumed. Thus, it can be calculated with: Generating Regressions Now we have a table showing the COP as a function of the water temperature at each of the three specified COPs. But we can do better than that. Wouldnt it be nice to have a function we can use to calculate the COP? Just enter the water temperature, and identify the COP accordingly? Numpy provides the tools to make this easy. We can use the numpy function polyfit to identify the coefficients of a refression describing the COP as a function of the water temperature. Its a flexible function, allowing you to control the shape of the curve by specifying the order of the function at the end. Since the COP of a heat pump as a function of temperature is a parabolic curve, we need a second order regression for this example. Thus, the coefficients can be identified with the line: The numpy poly1d function can be used to create a regression using those coefficients. This is done with: Now you can identify the COP of the heat pump at a specified water temperature using this regression. Remember that the regression is only generated for a specific air temperature, so only estimate the COP using the regression for the correct air temperature. Creating a 2-d performance map is the ultimate goal of this tutorial, but we arent there yet. The COP at a specified water temperature can be identifying by calling the function and using the water temperature as an input. For instance, if you want to find the COP when the water temperature is 72 F, you can enter: How do I save these results? The data can be saved using the same techniques we always use, as described in Automatically Storing Results from Analyzed Data Sets . We need to 1) Ensure that a folder for analyzed results is available, 2) Create a new filename that clearly states what the file contains, and 3) Save the data. In this case we want to save the data to a new file called Analyzed. It should be in the same data folder, and used to show the results of the analysis. We can do this with the following code: The first line creates the path for the new folder. It adds \\\\Analyzed to the currently existing path, stating that its looking for a folder called Analyzed within the current folder. The second line determines whether or not that folder already exists. If it doesnt, the third line creates it. After that, we need to set the filenames for both the data set and the coefficients. This can be done by combining what we already have with a subsection of the strings. We can use string indexes to identify the portion of the filename that we want to keep. For instance, the section of the filename for the first file that says PerformanceMap_HPWH_50 states quite clearly what the file contains. Since we know that the last four characters of the filenames are .csv we can isolate that section of the string by using the indices [-26:-4]. In other words, we want the characters of the string running from 26th to last to 4th to last not including the 4th to last. Following that we can customize the filenames a bit. Namely, we can state that we want the data filename to state that it contains analyzed data, and we want the coefficients filename to state that it contains coefficients. We can write the filename for both files using the following lines: Then we simply save the files. The analyzed data can be stored with the pandas .to_csv function, and the coefficients can be saved with the numpy .tofile function as follows: Note that the line saving the data sets index = False. This means that the index of the data frame will not be saved when saving the table. Also note that the numpy .tofile function requires you to specify a separator. In this case were using a comma, as specified with the sep = ,  code. How do I know that it worked right? T here are an enormous number of things that could have gone wrong by this point in the process. Maybe the lab tester made some mistakes while running the experiments. Maybe an instrument broke. Maybe theres a typo in the code, or an incorrect unit conversion. Its imperative to ensure that none of these problems, or any others, occurred during the process. Therefore the next step in the process is checking the data set for errors. We will cover this in the next phase of the tutorial. First Ill discuss how to check the data for errors manually. That way youll get a firm understanding of the potential errors were checking for and how to identify them. Then Ill discuss how to add code to this script that checks for those errors automatically, and warns you when something goes wrong. Tutorial Table of Contents This is a part of a series of articles teaching you all of the skills needed to automatically analyze laboratory data and develop a performance map of heat pump water heaters. The other articles in the series can be found using the following links: Introduction Splitting Data Sets Checking Analyzed Laboratory Data for Errors How to Write Scripts that Check Data Quality for You How to Automatically Generate Regressions in Python 38 38 38 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Peter Grant Scientist at Lawrence Berkeley National Laboratory who also teaches skills you need to build a fulfilling career. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Yang Zhou in TechToFreedom 9 Fabulous Python Tricks That Make Your Code More Elegant Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5990,\n",
       "  'url': 'https://towardsdatascience.com/a-beginners-guide-to-reinforcement-learning-using-rock-paper-scissors-and-tensorflow-js-37d42b6197b5',\n",
       "  'title': 'A Beginner’s Guide to Reinforcement Learning using Rock-Paper-Scissors and Tensorflow.js',\n",
       "  'subtitle': '-',\n",
       "  'claps': 28,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-10-21',\n",
       "  'clap_prop': 1.3830691588914305e-05,\n",
       "  'text': 'Towards Data Science Oct 20 2019 Listen Save Beginners Guide Reinforcement Learning using Rock-Paper-Scissors Tensorflow.js Tensorflow.js released attended lecture use case browser experience using Tensorflow extensively Python wa curious difficult would build small reinforcement learning example browser could see agent learning time chose Rock-Paper-Scissors referred RPS brevity laziness sake simple game rule knew learning would happen example tutorial Anyone interested intuitive non-math based tutorial Reinforcement Learning RL Neural Networks basic knowledge JavaScript html understand code required understand conceptual portion goal build agent able learn rule RPS using reinforcement learning neural network mean want agent able choose Rock given user chooses Scissors Reinforcement learning intuitively described following loop something based belief get positive negative reward update belief based reward framing reinforcement learning problem keep cycle mind break RPS game manner Keeping mind tutorial broken two section start script.js start learning cycle something based Agents belief context mean choosing move given Users move line 612 evaluation phase want see well Agent ha learned Therefore move chosen one highest value i.e. move Agent ha confidence done help Neural Network Aside Neural Networks neural network function tuned take input doe something input output result context RL RPS neural network used represent belief agent input network Users move output confidence Agent ha choosing Rock Paper Scissors seen Users move Rock Agent 0.7 confident choose Scissors 0.1 confident Rock 0.2 confident Paper Evaluation phase Agent choose move ha highest value/the confidence i.e. Scissors line 812 Convert move format Neural Network understand ask network output choose move highest value line 1416 training phase instead choosing move based belief move chosen randomly allows Agent explore move instead potentially stuck always choosing move next phase reinforcement cycle get reward user handled index.html reward received Agent update belief line 57 Handles getting confidence Agent ha move choose given Users move line 1012 Update belief Agent reward done using Neural Network Aside Updating Neural Network scenario user chose Rock Agent ha chosen Scissors user ha given reward -100 since move wrong move -100 added 0.7 sent back network tell network value 0.7 wa high lowered amount process called Backpropogation act updating belief Agent Agent le confident choosing Scissors scenario confident choosing either Rock Paper line 13 network updated new belief plotted method go move User choose plot belief Agent Users move line 10 Applies function make output neural network sum 1 allows User able understand output neural network easier line 25 Plots data set div index.html using Plotly.js piece complete reinforcement cycle visualization portion Next describe index.html required thing following excerpt index.html reflect point line 13 three button user move clicked call chooseMove function pas value button agent select return move line 56 button call train function user determines whether move Agent chose wa good bad tell agent update belief positively negatively line 811 Toggle whether agent learning evaluating ha learnt line 1317 Contain divs used plot belief Agent move come end tutorial see code visit repository Feel free clone play around code cloned open index.html browser see setup shown image top button Users move select move Agent choose move randomly User click Positive Reward Negative Reward button belief updated real time Click new move repeat process satisfied learned behavior switch toggle Train Evaluate move chosen Agent highest value histogram hope tutorial ha helpful provided intuitive understanding frame reinforcement learning problem neural network used help Stay tuned tutorial basic neural network reinforcement learning Thanks reading 36 1 36 36 1 Towards Data Science home data science Medium publication sharing concept idea code Suradech Kongkiatpaiboon Oct 20 2019 Member-only use Python without administrative right workplace use portable mode Python beautiful language experience work fast easy learn adapted variety field started using Python learn Data Science found lately also expand web scraping automated BOT programming Usually 5 min read 5 min read Share idea million reader Sam Mourad Oct 20 2019 Member-only Prophet v DeepAR Forecasting Food Demand collaboration Christian Aalby Svalesen global food industry face significant sustainability challenge UN estimate approximately one-third global food production wasted lost annually 66 loss food group freshness important criterion consumption addition 8 min read 8 min read Ishtiak Mahmud Oct 20 2019 Self Driving Car Localization doe self-driving car know given time One project artificial intelligence ha always fascinated self-driving car major motivation behind learning deep learning artificial intelligence One vital thing self-driving car ha accomplish perform function localization 7 min read 7 min read Alex Albano Oct 20 2019 Member-only Autonomous Distributed Networks unfulfilled libertarian dream breaking free regulation Keywords Blockchain Distributed Ledger Technology Decentralised Governance Regulatory Compliance Autonomous Distributed Networks Faultless Responsibility Intelligent Smart Contracts Summary Decentralisation embodies libertarian dream breaking free regulatory influence government bypassing via trustless network need central authority decision making Techno-libertarian crypto-anarchists maintain 17 min read 17 min read Georgi Tancev Oct 20 2019 Member-only Detecting Lesions Multiple Sclerosis Patients Deep Learning Introduction convolutional neural network medical image analysis Introduction Applications Deep Learning Deep learning part broader family machine learning method based artificial neural network One promising application deep learning image analysis e.g image segmentation classification Whereas segmentation yield probability distribution per pixel also known mask class 10 min read 10 min read Sacha Gunaratne Im thing visualization analytics predictive modelling also enjoy delving deep reinforcement learning optimization Medium Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Frank Andrade Geek Culture Top 5 Paid Subscriptions Ill Never Cancel Programmer Dennis Bakhuis Towards Data Science Python 3.14 faster C++ Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Oct 20, 2019 Listen Save A Beginners Guide to Reinforcement Learning using Rock-Paper-Scissors and Tensorflow.js Tensorflow.js had just been released and I had just attended a lecture on its use cases in the browser. I had experience using Tensorflow extensively in Python but was curious how difficult it would be to build a small reinforcement learning example in the browser in which you could see an agent learning over time. I chose Rock-Paper-Scissors which will be referred to as RPS for brevity and laziness sake from now on because of its simple game rules and because I knew the learning would happen with very few examples. Who is this tutorial for: Anyone who is interested in an intuitive non-math based tutorial on Reinforcement Learning (RL) and Neural Networks. They should have basic knowledge of JavaScript and html to understand the code but this is not required to understand the conceptual portions. The goal: To build an agent that is able to learn the rules of RPS using reinforcement learning and neural networks. This means that we want the agent to be able to choose Rock given that the user chooses Scissors. Reinforcement learning intuitively can be described as the following: loop: do something based on beliefs get a positive or negative reward update beliefs based on reward So when framing a reinforcement learning problem you have to keep the above cycle in mind. So we can break down the RPS game in the same manner. Keeping this in mind, the tutorial is broken down into two sections: We will start with the script.js. The start of the learning cycle is do something based on the Agents beliefs. In this context that means choosing a move given the Users move. lines 612: In the evaluation phase, we want to see how well the Agent has learned. Therefore, the move that is chosen is the one with the highest value. i.e., the move that the Agent has the most confidence in. This is done with the help of a Neural Network. Aside: Neural Networks A neural network is a function that can be tuned. It takes some input, does something to the input and outputs some result. In the context of RL and RPS, a neural network is used to represent the beliefs of the agent. The input into the network is the Users move and the output is the confidence the Agent has in choosing Rock, Paper and Scissors. As can be seen above, the Users move is Rock. The Agent is 0.7 confident that it should choose Scissors, 0.1 confident about Rock, and 0.2 confident about Paper. If we are in the Evaluation phase, the Agent will choose the move which has the highest value/the most confidence in. i.e., Scissors. lines 812: Convert the move into a format that the Neural Network can understand, ask the network for its output, and then choose the move with the highest value. lines 1416: In the training phase instead of choosing the move based on a belief, the move is chosen randomly. This allows the Agent to explore all moves instead of potentially being stuck always choosing the same move. The next phase of the reinforcement cycle is to get the reward from the user which will be handled in index.html. After the reward is received the Agent can update its beliefs. lines 57: Handles getting the confidence the Agent has about the moves it should choose given the Users move lines 1012: Update the beliefs of the Agent with the reward. This is done using the Neural Network. Aside: Updating a Neural Network In this scenario, the user chose Rock and the Agent has chosen Scissors. The user has given a reward of -100 since this move is the wrong move. The -100 is then added to the 0.7 and sent back into the network. This will tell the network that the value of 0.7 was too high and that it should be lowered by some amount. This process is called Backpropogation. This is the act of updating the beliefs of the Agent. The Agent should be less confident about choosing Scissors in this scenario and more confident about choosing either Rock or Paper. line 13: Once the network is updated, then the new beliefs are plotted. This method goes through all the moves that the User can choose and plots the beliefs of the Agent for each of the Users moves. line 10: Applies a function to make the outputs of the neural network sum up to 1. This allows the User to be able to understand the output of the neural network easier. line 25: Plots the data set in each div in index.html using Plotly.js Now we have all the pieces complete for the reinforcement cycle and visualization portion. Next we will describe index.html. This is required to do a few things: The following is an excerpt from index.html which reflect points above. lines 13: There are three buttons which are the users moves which when clicked will call the chooseMove function and pass it the value of the button. This will have the agent select and return a move. lines 56: These buttons will call the train function when the user determines whether the move that the Agent chose was good or bad. This will tell the agent to update its beliefs positively or negatively. lines 811: Toggle whether the agent is learning or evaluating what it has learnt lines 1317: Contain the divs that will be used to plot the beliefs of the Agent for each move. And with that we come to the end of the tutorial. To see all the code visit this repository . Feel free to clone it and play around with the code. Once you have cloned it, open index.html in a browser. You will see the setup shown in the image below. The top buttons are the Users moves. Once you select a move, the Agent will choose a move randomly. Then the User can click on the Positive Reward or Negative Reward buttons and the beliefs will be updated in real time. Click a new move and repeat the process. Once you are satisfied with the learned behavior, switch the toggle from Train to Evaluate and then the moves chosen by the Agent will be those that are highest value in each histogram. I hope that this tutorial has been helpful and provided an intuitive understanding of how to frame a reinforcement learning problem and how a neural network can be used to help with that. Stay tuned for further tutorials on the basics of neural networks and reinforcement learning. Thanks for reading. 36 1 36 36 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Suradech Kongkiatpaiboon Oct 20, 2019 Member-only How to use Python without administrative right at your workplace or use it in portable mode Python is a beautiful language from my experience. It works so fast, easy to learn and can be adapted in a variety of fields. I started using Python to learn Data Science but found out lately that it can also expand to web scraping or automated BOT programming. Usually, most 5 min read 5 min read Share your ideas with millions of readers. Sam Mourad Oct 20, 2019 Member-only Prophet vs DeepAR: Forecasting Food Demand In collaboration with Christian Aalby Svalesen The global food industry faces significant sustainability challenges, and the UN estimates that approximately one-third of the global food production is wasted or lost annually. About 66% of the losses are in food groups where freshness is an important criterion for consumption. In addition 8 min read 8 min read Ishtiak Mahmud Oct 20, 2019 Self Driving Car Localization How does a self-driving car know where it is at any given time? One of the projects of artificial intelligence which has always fascinated me is the self-driving car. It is my major motivation behind learning more about deep learning and artificial intelligence. One of the most vital things that a self-driving car has to accomplish to perform its other functions is localization 7 min read 7 min read Alex Albano Oct 20, 2019 Member-only Autonomous Distributed Networks: The unfulfilled libertarian dream of breaking free from regulations Keywords: Blockchain, Distributed Ledger Technology, Decentralised Governance, Regulatory Compliance, Autonomous Distributed Networks, Faultless Responsibility, Intelligent Smart Contracts. Summary Decentralisation embodies the libertarian dream of breaking free from the regulatory influence of governments and of bypassing via trustless networks the need for a central authority for decision making. Techno-libertarian and crypto-anarchists maintain 17 min read 17 min read Georgi Tancev Oct 20, 2019 Member-only Detecting Lesions in Multiple Sclerosis Patients with Deep Learning Introduction to convolutional neural networks in medical image analysis. Introduction Applications of Deep Learning Deep learning is part of a broader family of machine learning methods based on artificial neural networks. One of the most promising applications of deep learning is image analysis, e.g. for image segmentation or classification. Whereas segmentation yields a probability distribution per pixel (also known as mask) for each class 10 min read 10 min read Sacha Gunaratne Im into all things visualization, analytics and predictive modelling. I also enjoy delving deep into reinforcement learning and optimization. More from Medium Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Frank Andrade in Geek Culture My Top 5 Paid Subscriptions Ill Never Cancel as a Programmer Dennis Bakhuis in Towards Data Science Python 3.14 will be faster than C++ Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5085,\n",
       "  'url': 'https://towardsdatascience.com/how-to-capture-weather-data-on-your-home-400716bde645',\n",
       "  'title': 'How to Capture Weather Data with your own IoT Home\\xa0Station',\n",
       "  'subtitle': 'Capturing weather data, and logging\\xa0them…',\n",
       "  'claps': 28,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 22,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-06-04',\n",
       "  'clap_prop': 1.3830691588914305e-05,\n",
       "  'text': \"Towards Data Science Jun 4 2019 Listen Save Capture Weather Data IoT Home Station Capturing weather data logging locally IoT service via MQTT protocol Introduction talking physical variable temperature pressure etc. Data Scientist usually start working dataset wa created somewhere else thought capture data tutorial learn get data several different sensor sending IoT service ThingSpeak.com mobile App Thingsview log play data also explore several different communication way connecting sensor Raspberry Pi short data captured saved locally CSV file send IoT service ThingSpeak.com via MQTT protocol see block diagram complete real Weather Station final step also learn measure wind speed direction following Mauricio Pinto tutorial Supplies 1 Development Environment Jupyter Notebook Jupyter Notebook fantastic tool better open-source web application allows create share document contain live code equation visualization narrative text Jupyter notebook largely used Data Science cleaning transforming data numerical simulation statistical modeling data visualization machine learning much tutorial use Jupyter Notebook interact Raspberry Pi GPIOs directly reading sensor sending data internet Installation may skip step already Jupyter Notebook installed RPi install Jupyter Raspberry run Python 3 open Terminal enter following command terminal run command thats Amazing simple easy Jupyter Notebook running server Note default browser automatically opened address running Home Page shown picture stop server close kernel Jupyter notebook must use Ctrl C keyboard one time start Pi want use Jupyter Notebook type command Jupyter notebook terminal keep running time important need use terminal another task run program example open new Terminal window follow tutorial step step creating Notebook download final one GitHub Rpi_Weather_Station.ipynb 2 DHT22 Temperature Humidity Sensor first sensor installed DHT22 capturing air temperature relative humidity data ADAFRUIT site provides great information sensor Bellow information retrieved Overview low-cost DHT temperature humidity sensor basic slow great hobbyist want basic data logging DHT sensor made two part capacitive humidity sensor thermistor also basic chip inside doe analog digital conversion spit digital signal temperature humidity digital signal fairly easy read using microcontroller DHT22 Main characteristic Good 0100 humidity reading 25 accuracy Good -40 125C temperature reading 0.5C accuracy 0.5 Hz sampling rate every 2 second usually use sensor distance le 20m 4K7 ohm resistor connected Data VCC pin DHT22 output data pin connected Raspberry GPIO 16 Check electrical diagram connecting sensor RPi pin forget Install 4K7 ohm resistor Vcc Data pin sensor connected must also install library RPi Installing DHT Library Raspberry starting /home go /Documents Create directory install library move browser go Adafruit GitHub adafruit/Adafruit_Python_DHT Python library read DHT series humidity temperature sensor Raspberry Pi Beaglebone Black github.com Download library clicking download zip link right unzip archive Raspberry Pi recently created folder go directory library subfolder automatically created unzipped file execute command Jupyter Notebook Import Adafrut DHT Library define digital pin connect DHT RPi run code capture temperature humidity Run Cell print result portion Jupyter Notebook showing result 3 DS18B20 Temperature Sensor Sensor Overview use tutorial waterproofed version DS18B20 sensor useful capturing temperature wet condition example humid soil sensor isolated take measurement 125oC Adafrut doe recommend use 100oC due cable PVC jacket DS18B20 digital sensor make good use even long distance 1-wire digital temperature sensor fairly precise 0.5C much range give 12 bit precision onboard digital-to-analog converter work great RPi using single digital pin even connect multiple one pin one ha unique 64-bit ID burned factory differentiate sensor work 3.0 5.0V mean powered directly 3.3V provided one Raspberry pin 1 17 sensor ha 3 wire find full data DS18B20 Datasheet Sensor Installation Follow diagram make connection Installing Python Library Next let install Python library handle sensor running script test sensor check 1-Wire interface enabled RPi see print screen Enable Interfaces forget restart RPi changeing configuration Testing sensor testing sensor simple python code used portion Jupyter Notebook showing result 4 BMP180 Temperature Pressure Sensor Sensor Overview BMP180 successor BMP085 new generation high precision digital pressure sensor consumer application ultra-low power low voltage electronics BMP180 optimized use mobile phone PDAs GPS navigation device outdoor equipment low altitude noise merely 0.25m fast conversion time BMP180 offer superior performance I2C interface allows easy system integration microcontroller BMP180 based piezo-resistive technology EMC robustness high accuracy linearity well long-term stability complete BMP datasheet found BMP180 Digital Pressure Sensor Sensor Installation Follow diagram make connection Enabling I2C Interface Go RPi Configuration confirm I2C interface enabled enable restart RPi Using BMP180 everything ha installed connected okay ready turn Pi start seeing BMP180 telling world around first thing check Pi see BMP180 Try following terminal window command worked see something similar Terminal Printscreen showing BMP180 channel 77 Installing BMP180 Library Create directory install library go browser go Adafruit GITHub adafruit/Adafruit_Python_BMP Python library accessing BMP series pressure temperature sensor like BMP085/BMP180 Raspberry Pi github.com Download library clicking download zip link right unzip archive Raspberry Pi created folder go created subfolder execute following command directory library Jupyter write following code Check variable read sensor bellow code portion Jupyter Notebook showing result Note sensor pressure presented Pa Pascals See next step better understand unit 5 Measuring Weather Altitude BMP180 Sea Level Pressure Lets take time understand little bit get BMP reading skip part tutorial return later want know Sensor reading please go great tutorial http //learn.sparkfun.com/tutorials/bmp180-barome ... BMP180 wa designed accurately measure atmospheric pressure Atmospheric pressure varies weather altitude Atmospheric Pressure definition atmospheric pressure force air around exerting everything weight gas atmosphere creates atmospheric pressure common unit pressure pound per square inch psi use international notation newton per square meter called pascal Pa took 1 cm wide column air would weigh 1 kg weight pressing footprint column creates atmospheric pressure measure sensor like BMP180 cm-wide column air weighs 1Kg follows average sea level pressure 101325 pascal better 1013.25 hPa 1 hPa also known milibar mbar drop 4 every 300 meter ascend higher get le pressure youll see column top atmosphere much shorter therefore weighs le useful know measuring pressure math determine altitude air pressure 3 810 meter half sea level BMP180 output absolute pressure pascal Pa One pascal small amount pressure approximately amount sheet paper exert resting table often see measurement hectopascals 1 hPa 100 Pa library used provides output floating-point value hPa also happens equal one millibar mbar conversion pressure unit Temperature Effects temperature affect density gas density affect mass gas mass affect pressure whew atmospheric pressure change dramatically temperature Pilots know density altitude make easier take cold day hot one air denser ha greater aerodynamic effect compensate temperature BMP180 includes rather good temperature sensor well pressure sensor perform pressure reading first take temperature reading combine raw pressure reading come final temperature-compensated pressure measurement library make easy Measuring Absolute Pressure application requires measuring absolute pressure get temperature reading perform pressure reading see example sketch detail final pressure reading hPa mbar wish convert different unit using conversion factor Note absolute pressure atmosphere vary altitude current weather pattern useful thing measure Weather Observations atmospheric pressure given location earth anywhere atmosphere isnt constant complex interaction earth spin axis tilt many factor result moving area higher lower pressure turn cause variation weather see every day watching change pressure predict short-term change weather example dropping pressure usually mean wet weather storm approaching low-pressure system moving Rising pressure usually mean clear weather approaching high-pressure system moving remember atmospheric pressure also varies altitude absolute pressure house Lo Barnechea Chile altitude 950m always lower absolute pressure San Francisco example le 2 meter almost sea level weather station reported absolute pressure would difficult directly compare pressure measurement one location another large-scale weather prediction depend measurement many station possible solve problem weather station always remove effect altitude reported pressure reading mathematically adding equivalent fixed pressure make appear reading wa taken sea level higher reading San Francisco Lo Barnechea always weather pattern altitude function library called sea level P take absolute pressure P hPa station current altitude meter remove effect altitude pressure use output function directly compare weather reading station around world Determining Altitude Since pressure varies altitude use pressure sensor measure altitude caveat average pressure atmosphere sea level 1013.25 hPa mbar drop zero climb towards vacuum space curve drop-off well understood compute altitude difference two pressure measurement p p0 using specific equation use sea level pressure 1013.25 hPa baseline pressure p0 output equation current altitude sea level Theres function library called altitude P P0 let get calculated altitude explanation wa extracted BMP 180 Sparkfun tutorial 6 Sea Level Pressure Measurement could learn previous step important hand Sea Level pressure calculated real altitude measuring absolute pressure function help u case BMP180 installed real measured altitude 957 meter following updated data sensor 7 Using ADC Analog Digital Converter next step discus get UV data simple good analog sensor problem Raspberry Pi doe analog input pin Arduino NodeMCU overcome problem using analog digital A/D converter help interfacing analog sensor Raspberry Pi A/D converter use project popular MCP3008 MCP3008 10bit 8-channel ADC Analog Digital Converter use SPI bus protocol interfacing Raspberry Pi cheap doesnt require additional component give 8 analog input us four GPIOs Raspberry Pi plus power ground pin MCP3008 output range 01,023 0 mean 0V 1,023 mean 3.3V MCP3008 Pinout pin numbering MCP3008 start top/left Pin 1 CH0 half circle top see pinout diagram MCP3008 ADC ha total 16 pin 8 pin taking analog input analog input pin CH0-CH7 Pins 18 side pin 916 different function follows project use Channel 0 Pin 1 analog input SPI Raspberry Pi equipped one SPI bus ha 2 chip selects SPI master driver disabled default Raspbian enable use raspi-config confirm SPI bus enabled procedure wa done 1-Wire start import spidev Linux driver access SPI bus open configure bus access analog channel ADC testing write function connect Channel 0 MCP3008 pin 1 3.3V run function result see 1023 8 Analog UV Sensor UV sensor generates analog output proportional Ultra-Violet radiation found light-sensing spectrum us UV photodiode based Gallium Nitride detect 240370nm range light cover UVB UVA spectrum signal level photodiode small nano-ampere level module ha embedded operational amplifier amplify signal readable volt-level 0 1V sensor op-amp powered connecting VCC 3.3VDC GND power ground analog signal gotten pin output millivolt read Analog Input CH0 ADC connected RPi Using code shown last step see raw data generated UV sensor case 43 raw sensor data convert map value better handled code function readSensorUV function read UV sensor 3 time taking average converting measured value mV example raw measurement 43 fact equivalent 128mV look table curve see 128mV related radiation index 0 1 Lets create function calculate index common measurement UV radiation consider range Vout shown table start point range 110mV example UV measurement 227mV 337mv considered Index 1 previous measurement 128mV index 0 9 Complete HW SW point sensor installed tested Lets develop function capture data Note defined sensor variable global keep local returning value function better practice 10 Logging Data Locally point tool capture lot data sensor simple answer create single loop function capture data regular base saving local file code open file named rpi_weather_station.csv root directory Every 30 second timestamp plus data sensor append file see 11 IoT Sending Data Cloud Service point learned capture data sensor saving local CSV file time see send data IoT platform tutorial use ThingSpeak.com ThingSpeak open source Internet Things IoT application store retrieve data thing using REST MQTT APIs ThingSpeak enables creation sensor logging application location tracking application social network thing status update First must account ThinkSpeak.com Next follow instruction create Channel taking note Channel ID Write API Key creating channel must also define info uploaded one 8 field shown 12 MQTT Protocol ThingSpeak Connection MQTT publish/subscribe architecture wa developed primarily connect bandwidth power-constrained device wireless network simple lightweight protocol run TCP/IP socket WebSockets MQTT WebSockets secured SSL publish/subscribe architecture enables message pushed client device without device needing continuously poll server MQTT broker central point communication charge dispatching message sender rightful receiver client device connects broker publish subscribe topic access information topic contains routing information broker client want send message publishes certain topic client want receive message subscribes certain topic broker delivers message matching topic appropriate client ThingSpeak ha MQTT broker URL mqtt.thingspeak.com port 1883 ThingSpeak broker support MQTT publish MQTT subscribe case use MQTT Publish MQTT Publish starting let install Eclipse Paho MQTT Python client library implement version 3.1 3.1.1 MQTT protocol Next let import paho library initiate Thingspeak channel MQTT protocol connection method simplest requires least system resource must define topic payload tPayload '' upload IoT service send everything OK get Echo data sent ThingSpeak channel page see data 13 Logging Sensor Data IoT Service ThingSpeak Channel uploaded data know line code possible upload data IoT service let create loop function automatically regular interval time similar done Logging Data Locally simple code continuously capture data logging channel would Looking ThingSpeak channel page observe data loaded continuously field channel automatically log data future analysis complete CSV file data could also downloaded site included function save_Log also log data locally CSV file complete Jupyter notebook wa used development found Rpi_Weather_Station.ipynb 14 ThingsView ThingSpeak App logged data viewed directly local saved CSV file ThingSpeak.com site via APP example ThingsView ThingView APP developed CINETICA enables visualize ThingSpeak channel easy way enter channel ID ready go public channel application respect window setting color timescale chart type number result current version support line column chart spline chart displayed line chart private channel data displayed using default setting way read private window setting API key ThingView APP download ANDROID IPHONE 15 Measuring Wind Speed Direction Weather Station tutorial part joint project developed friend Mauricio Pinto learned capture several important data related weather Air Temperature Humidity Pressure UV Another important data added Weather Station Wind Speed Direction Mauricio great job writing detailed tutorial explained construct Anemometer mostly recycled material find project 2 part tutorial Part 1 Construction device Anemometer Wind Vane Direction Part 2 sketch using Arduino IDE Esp8266 Nodemcu transmission ThingSpeak Mauricio explained tutorial anemometer device capable measuring wind speed direction Using Hall Effect sensor wa able count many rotation cup give period time intensity wind proportional speed rotation axis simple physic equation could determine linear velocity wind moment wind direction wa measured windshield neodymium magnet reed switch see anemometer installed house located around 400 meter far Weather Station wind speed direction also sent Thingspeak.com 16 Conclusion always hope project help others find way exciting world Electronics Data Science detail final code please visit GitHub depository RPi-Weather-Station project please visit blog MJRoBot.org Saludos south world See next article Thank Marcelo 54 54 54 Towards Data Science home data science Medium publication sharing concept idea code Jan Teichmann Jun 4 2019 Member-only make success story data science team Data science resounds throughout every industry ha reached mainstream medium longer explain living long call AI peak data science hype consequence company looking towards data science 12 min read 12 min read Share idea million reader Nikolay Dimolarov Jun 4 2019 state Deep Learning outside CUDAs walled garden Deep Learning researcher afficionando happen love using Macs privately professionally every year get latest greatest disappointing AMD upgrade GPU disappointing get latest greatest Vega GPU course doe 5 min read 5 min read Rohit Agrawal Jun 4 2019 Analyzing Text Classification Techniques Youtube Data Text Classification classic problem Natural Language Processing NLP aim solve refers analyzing content raw text deciding category belongs similar someone reading Robin Sharma book classifying garbage 9 min read 9 min read Chitta Ranjan Jun 4 2019 Step-by-step understanding LSTM Autoencoder layer break LSTM autoencoder network understand layer-by-layer go input output flow layer also compare LSTM Autoencoder regular LSTM network Download free book Understanding Deep Learning learn previous post LSTM Autoencoder Extreme Rare Event Classification 1 learned build LSTM autoencoder multivariate time-series data 7 min read 7 min read Jo Stichbury Jun 4 2019 Kedro New Tool Data Science new Python library production-ready data pipeline post introduce Kedro new open source tool data scientist data engineer brief description likely become standard part every professional toolchain describe use tutorial 10 min read 10 min read Marcelo Rovai Engineer MBA Master Data Science Passionate share knowledge Data Science Electronics focus Physical Computing IoT Robotics Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Ishan Getting started MicroPython Raspberry Pi Pico Black_Raven James Ng Geek Culture Face Recognition 46 line code Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Jun 4, 2019 Listen Save How to Capture Weather Data with your own IoT Home Station Capturing weather data, and logging them locally and on an IoT service, via MQTT protocol. Introduction When we are talking about physical variables, as temperature, pressure, etc., as a Data Scientist, usually you start working from a dataset that was created somewhere else. But have you thought about how to capture those data yourself? On this tutorial we will learn how to get data from several different sensors, sending them to an IoT service, ThingSpeak.com and to a mobile App (Thingsview), where we can log and play with data. We will also explore several different communication ways of connecting sensors to a Raspberry Pi, as: In short, all data will be captured, saved locally on a CSV file and send to an IoT service (ThingSpeak.com), via MQTT protocol, as you can see on below block diagram: To complete a real Weather Station, on the final step you will also learn how to measure wind speed and direction, following Mauricio Pinto s tutorial. Supplies: 1. Development Environment Jupyter Notebook Jupyter Notebook is a fantastic tool, or better, an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. Jupyter notebook is largely used in Data Science, for cleaning and transforming data, doing numerical simulation, statistical modeling, data visualization, machine learning, and much more! On this tutorial, we will use Jupyter Notebook to interact with Raspberry Pi GPIOs, directly reading sensors and sending data to the internet. Installation You may skip this step if already have Jupyter Notebook installed on your RPi To install Jupyter on your Raspberry (that will run with Python 3), open Terminal and enter with following commands: Now on your terminal, run the command: And thats it!!!! Amazing! very simple and easy. The Jupyter Notebook will be running as a server on: Note that your default browser will be automatically opened on the above address, running as a Home Page, as shown at above pictures. To stop the server and close the kernels (the Jupyter notebooks), you must use [Ctrl] + [C] from your keyboard. From now one, any time that you start your Pi and want to use Jupyter Notebook, just type the command: Jupyter notebook on your terminal and keep it running all the time. This is very important! If you need to use the terminal for another task as run a program, for example, open a new Terminal window. You can follow this tutorial step by step, creating your own Notebook, or download the final one from my GitHub: Rpi_Weather_Station.ipynb . 2. DHT22 Temperature & Humidity Sensor The first sensor to be installed will be the DHT22 for capturing air temperature and relative humidity data. The ADAFRUIT site provides great information about those sensors. Bellow, some information retrieved from there: Overview The low-cost DHT temperature & humidity sensors are very basic and slow but are great for hobbyists who want to do some basic data logging. The DHT sensors are made of two parts, a capacitive humidity sensor, and a thermistor. There is also a very basic chip inside that does some analog to digital conversion and spits out a digital signal with the temperature and humidity. The digital signal is fairly easy to be read using any microcontroller. DHT22 Main characteristics: Good for 0100% humidity readings with 25% accuracy Good for -40 to 125C temperature readings 0.5C accuracy No more than 0.5 Hz sampling rate (once every 2 seconds) Once usually you will use the sensor on distances less than 20m, a 4K7 ohm resistor should be connected between Data and VCC pins. The DHT22 output data pin will be connected to Raspberry GPIO 16. Check the above electrical diagram, connecting the sensor to RPi pins as below: Do not forget to Install the 4K7 ohm resistor between Vcc and Data pins Once the sensor is connected, we must also install its library on our RPi. Installing DHT Library: On your Raspberry, starting on /home, go to /Documents Create a directory to install the library and move to there: On your browser, go to Adafruit GitHub: adafruit/Adafruit_Python_DHT Python library to read the DHT series of humidity and temperature sensors on a Raspberry Pi or Beaglebone Black. github.com Download the library by clicking the download zip link to the right and unzip the archive on your Raspberry Pi recently created folder. Then go to the directory of the library (subfolder that is automatically created when you unzipped the file), and execute the command: On The Jupyter Notebook, Import Adafrut DHT Library, define the digital pin to connect the DHT with RPi and run the code to capture temperature and humidity: Run the Cell and print the results: Below, the portion of Jupyter Notebook showing the result: 3. DS18B20 Temperature Sensor Sensor Overview: We will use in this tutorial a waterproofed version of the DS18B20 sensor. It is very useful for for capturing temperature in wet conditions, for example on humid soil. The sensor is isolated and can take measurements until 125oC (Adafrut does not recommend to use it over 100oC due to its cable PVC jacket). The DS18B20 is a digital sensor what makes it good to use even over long distances! These 1-wire digital temperature sensors are fairly precise (0.5C over much of the range) and can give up to 12 bits of precision from the onboard digital-to-analog converter. They work great with the RPi using a single digital pin, and you can even connect multiple ones to the same pin, each one has a unique 64-bit ID burned in at the factory to differentiate them. The sensor works from 3.0 to 5.0V, which means that it can be powered directly from the 3.3V provided by one of the Raspberry pins (1 or 17). The sensor has 3 wires: Here, you can find the full data: DS18B20 Datasheet Sensor Installation: Follow the above diagram and make the connections: Installing the Python Library: Next, lets install the Python library that will handle the sensor: Before running the script to test the sensor, check if the 1-Wire interface is enabled in your RPi (see below print screen) Enable Interfaces Do not forget to restart your RPi, after changeing its configuration Testing the sensor: For testing the sensor a simple python code can be used: Below, the portion of Jupyter Notebook showing the result: 4. BMP180 Temperature & Pressure Sensor Sensor Overview: The BMP180 is the successor of the BMP085, a new generation of high precision digital pressure sensors for consumer applications. The ultra-low power, low voltage electronics of the BMP180 is optimized for use in mobile phones, PDAs, GPS navigation devices and outdoor equipment. With a low altitude noise of merely 0.25m at fast conversion time, the BMP180 offers superior performance. The I2C interface allows for easy system integration with a microcontroller. The BMP180 is based on piezo-resistive technology for EMC robustness, high accuracy, and linearity as well as long-term stability. The complete BMP datasheet can be found here: BMP180 Digital Pressure Sensor Sensor Installation: Follow the above diagram and make the connections: Enabling I2C Interface Go to RPi Configuration and confirm that I2C interface is enabled. If not, enable it and restart the RPi. Using the BMP180 If everything has been installed and connected okay, you are now ready to turn on your Pi and start seeing what the BMP180 is telling you about the world around you. The first thing to do is to check if the Pi sees your BMP180. Try the following in a terminal window: If the command worked, you should see something similar to the below Terminal Printscreen, showing that the BMP180 is on channel 77. Installing the BMP180 Library: Create a directory to install the library and go there: On your browser, go to Adafruit GITHub: adafruit/Adafruit_Python_BMP Python library for accessing the BMP series pressure and temperature sensors like the BMP085/BMP180 on a Raspberry Pi github.com Download the library by clicking the download zip link to the right and unzip the archive on your Raspberry Pi created folder. Then go to the created subfolder and execute the following command in the directory of the library: On Jupyter, write the following code: Check the variables read by the sensor with bellow code: Below, the portion of Jupyter Notebook showing the result Note that the sensor pressure is presented in Pa (Pascals). See next step to better understand about this unit. 5. Measuring Weather and Altitude With BMP180 Sea Level Pressure Lets take a time to understand a little bit more about what we will get, with the BMP readings. You can skip this part of the tutorial, or return later, and if you want to know more about Sensor readings, please go to this great tutorial: https://learn.sparkfun.com/tutorials/bmp180-barome... The BMP180 was designed to accurately measure atmospheric pressure. Atmospheric pressure varies with both weather and altitude. What is Atmospheric Pressure? The definition of atmospheric pressure is a force that the air around you is exerting on everything. The weight of the gasses in the atmosphere creates atmospheric pressure. A common unit of pressure is pounds per square inch or psi. We will use here the international notation, that is newtons per square meter, which are called pascals (Pa). If you took 1 cm wide column of air would weigh about 1 kg This weight, pressing down on the footprint of that column, creates the atmospheric pressure that we can measure with sensors like the BMP180. Because that cm-wide column of air weighs about 1Kg, it follows that the average sea level pressure is about 101325 pascals, or better, 1013.25 hPa (1 hPa is also known as milibar mbar). This will drop by about 4% for every 300 meters you ascend. The higher you get, the less pressure youll see, because the column to the top of the atmosphere is that much shorter and therefore weighs less. This is useful to know because by measuring the pressure and doing some math, you can determine your altitude. The air pressure at 3, 810 meters is only half of that at sea level. The BMP180 outputs absolute pressure in pascals (Pa). One pascal is a very small amount of pressure, approximately the amount that a sheet of paper will exert resting on a table. You will more often see measurements in hectopascals (1 hPa = 100 Pa). The library used here provides outputs floating-point values in hPa, which also happens to equal one millibar (mbar). Here are some conversions to other pressure units: Temperature Effects Because temperature affects the density of a gas, and density affects the mass of a gas, and mass affects the pressure (whew), atmospheric pressure will change dramatically with temperature. Pilots know this as density altitude, which makes it easier to take off on a cold day than a hot one because the air is denser and has a greater aerodynamic effect. To compensate for temperature, the BMP180 includes a rather good temperature sensor as well as a pressure sensor. To perform a pressure reading, you first take a temperature reading, then combine that with a raw pressure reading to come up with a final temperature-compensated pressure measurement. (The library makes all of this very easy.) Measuring Absolute Pressure If your application requires measuring absolute pressure, all you have to do is get a temperature reading, then perform a pressure reading (see the example sketch for details). The final pressure reading will be in hPa = mbar. If you wish, you can convert this to a different unit using the above conversion factors. Note that the absolute pressure of the atmosphere will vary with both your altitude and the current weather patterns, both of which are useful things to measure. Weather Observations The atmospheric pressure at any given location on earth (or anywhere with an atmosphere) isnt constant. The complex interaction between the earths spin, axis tilt, and many other factors result in moving areas of higher and lower pressure, which in turn cause the variations in weather we see every day. By watching for changes in pressure, you can predict short-term changes in the weather. For example, dropping pressure usually means wet weather or a storm is approaching (a low-pressure system is moving in). Rising pressure usually means that clear weather is approaching (a high-pressure system is moving through). But remember that atmospheric pressure also varies with altitude. The absolute pressure in my house, Lo Barnechea in Chile (altitude 950m) will always be lower than the absolute pressure in San Francisco for example (less than 2 meters, almost sea level). If weather stations just reported their absolute pressure, it would be difficult to directly compare pressure measurements from one location to another (and large-scale weather predictions depend on measurements from as many stations as possible). To solve this problem, weather stations always remove the effects of altitude from their reported pressure readings by mathematically adding the equivalent fixed pressure to make it appear as if the reading was taken at sea level. When you do this, a higher reading in San Francisco than Lo Barnechea will always be because of weather patterns, and not because of altitude. To do this, there is a function in the library called  sea level(P, A)  . This takes the absolute pressure (P) in hPa, and the stations current altitude (A) in meters, and removes the effects of the altitude from the pressure. You can use the output of this function to directly compare your weather readings to other stations around the world. Determining Altitude Since pressure varies with altitude, you can use a pressure sensor to measure altitude (with a few caveats). The average pressure of the atmosphere at sea level is 1013.25 hPa (or mbar). This drops off to zero as you climb towards the vacuum of space. Because the curve of this drop-off is well understood, you can compute the altitude difference between two pressure measurements (p and p0) by using a specific equation. If you use sea level pressure (1013.25 hPa) as the baseline pressure (p0), the output of the equation will be your current altitude above sea level. Theres a function in the library called  altitude(P, P0)  that lets you get the calculated altitude. The above explanation was extracted from BMP 180 Sparkfun tutorial. 6. Sea Level Pressure Measurement As we could learn on the previous step, it is important to have on hand the Sea Level pressure, that is calculated once we have the real altitude where we are measuring the absolute pressure. The below function will help us with that: On my case, I have the BMP180 installed on a real measured altitude of 957 meters, so we can have the following updated data from sensors: 7. Using the ADC (Analog to Digital Converter) On the next step, we will discuss how to get UV data from a very simple, but good analog sensor. The problem here is that the Raspberry Pi does not have analog input pins as an Arduino or NodeMCU, but we can overcome this problem by using an analog to digital (A/D) converter which will help in interfacing the analog sensors with the Raspberry Pi. The A/D converter that we will use on this project is the popular MCP3008. MCP3008 is a 10bit 8-channel ADC (Analog to Digital Converter) which use the SPI bus protocol for interfacing with Raspberry Pi. It is cheap and doesnt require any additional components with it. It gives you 8 analog inputs and it uses just four GPIOs of Raspberry Pi, plus power and ground pins. MCP3008 output will be a range from 01,023 where 0 means 0V and 1,023 means 3.3V. MCP3008 Pinout The pins numbering of the MCP3008 starts from the top/left (Pin 1: CH0), having the half circle on top as you can see in the above pinout diagram. MCP3008 ADC has a total of 16 pins out of which 8 pins are for taking the analog input. The analog input pins are from CH0-CH7 (Pins 18). On the other side (pins 916), we have different functions as follows: On this project, we will use Channel 0 (Pin 1) as the analog input. SPI The Raspberry Pi is equipped with one SPI bus that has 2 chip selects. The SPI master driver is disabled by default on Raspbian. To enable it, use raspi-config to confirm that SPI bus is enabled (the same procedure that was done before with 1-Wire). As a start, import spidev, a Linux driver to access the SPI bus: And open and configure the bus: From there, you can access any of the analog channels of our ADC. For testing write the below function: and, connect Channel 0 (MCP3008 pin 1) to 3.3V and run the function: As a result, you should see: 1023 8. The Analog UV Sensor This UV sensor generates an analog output proportional to Ultra-Violet radiation found on the light-sensing spectrum. It uses a UV photodiode (based on Gallium Nitride), which can detect the 240370nm range of light (which covers UVB and most of UVA spectrum). The signal level from the photodiode is very small, in the nano-ampere level, so the module has embedded an operational amplifier to amplify the signal to a more readable volt-level (0 to 1V). The sensor and op-amp can be powered, by connecting VCC to 3.3VDC and GND to power ground. The analog signal can be gotten from the OUT pin. Its output will be in millivolts and will be read by Analog Input (CH0) of ADC connected to our RPi. Using the same code shown in the last step, we can see the raw data generated by our UV sensor (in this case 43): Having the raw sensor data, we should convert (or map) it for values to be better handled by the code. We can do it with the function readSensorUV(). This function reads the UV sensor 3 times, taking the average and converting the measured value to mV: For example, a raw measurement of 43 is, in fact, equivalent to 128mV: If we look at the table and curve below: we will see that 128mV should be related to radiation between index 0 and 1. Lets create a function to calculate this index that is the most common measurement of UV radiation. What we will do is consider a range, having the Vout shown at the above table as the start point, with a range of 110mV. For example, UV measurements between 227mV and 337mv will be considered Index 1. So, for the previous measurement (128mV), the index should be 0. 9. The Complete HW & SW At this point, we have all the sensors installed and tested. Lets now develop a function to capture all data at once: Note that I have defined all sensors variables as global. You can keep them local, returning the values from the function (This is a better practice). 10. Logging Data Locally At this point, you have all the tools to capture a lot of data from sensors. But what to do with them? The most simple answer is to create a single loop function to capture the data at regular bases, saving them on a local file. The above code opens a file named rpi_weather_station.csv on your root directory. Every 30 seconds, the timestamp plus the data from all sensors will be append to this file, as you can see above. 11. IoT Sending Data to a Cloud Service At this point, we have learned how to capture data from sensors, saving them on a local CSV file. Now, it is time to see how to send those data to an IoT platform. On this tutorial, we will use ThingSpeak.com . ThingSpeak is an open source Internet of Things (IoT) application to store and retrieve data from things, using REST and MQTT APIs. ThingSpeak enables the creation of sensor logging applications, location tracking applications, and a social network of things with status updates. First, you must have an account at ThinkSpeak.com. Next, follow the instructions to create a Channel, taking note of its Channel ID and Write API Key . When creating the channel, you must also define what info will be uploaded to each one of the 8 fields, as shown above. 12. MQTT Protocol and ThingSpeak Connection MQTT is a publish/subscribe architecture that was developed primarily to connect bandwidth and power-constrained devices over wireless networks. It is a simple and lightweight protocol that runs over TCP/IP sockets or WebSockets. MQTT over WebSockets can be secured with SSL. The publish/subscribe architecture enables messages to be pushed to the client devices without the device needing to continuously poll the server. The MQTT broker is the central point of communication, and it is in charge of dispatching all messages between the senders and the rightful receivers. A client is any device that connects to the broker and can publish or subscribe to topics to access the information. A topic contains routing information for the broker. Each client that wants to send messages publishes them to a certain topic, and each client that wants to receive messages subscribes to a certain topic. The broker delivers all messages with the matching topic to the appropriate clients. ThingSpeak has an MQTT broker at the URL mqtt.thingspeak.com and port 1883 . The ThingSpeak broker supports both MQTT publish and MQTT subscribe. In our case, we will use the MQTT Publish. MQTT Publish For starting, lets install the Eclipse Paho MQTT Python client library , that implements versions 3.1 and 3.1.1 of the MQTT protocol. Next, lets import the paho library: and initiate the Thingspeak channel and MQTT protocol. This connection method is the simplest and requires the least system resources: Now, you must define the topic payload (tPayload\") to be upload to your IoT service: And send it: If everything is OK you will get an Echo of the data sent and on ThingSpeak channel page, you can see the data. 13. Logging Sensor Data on IoT Service ThingSpeak Channel uploaded data Now, that we know that with only a few lines of code it is possible to upload data to an IoT service, lets create a loop function to do it automatically at a regular interval of time (similar to what we have done with Logging Data Locally). A simple code to continuously capture data, logging them on our channel would be: Looking for your ThingSpeak channel page, you will observe that the data will be loaded continuously to each field. The channel will automatically log those data for future analysis. A complete CSV file of the data could be also be downloaded from the site. We have included a function (save_Log()) to also log data locally on a CSV file: The complete Jupyter notebook that was used for development can be found here: Rpi_Weather_Station.ipynb . 14. ThingsView the ThingSpeak App The logged data can be viewed directly on local saved CSV file, on ThingSpeak.com site or via an APP, for example,  ThingsView  ! ThingView is an APP developed by CINETICA , that enables you to visualize your ThingSpeak channels in an easy way. Just enter the channel ID and you are ready to go. For public channels, the application will respect your windows settings: color, timescale, chart type and the number of results. The current version supports line and column charts, the spline charts are displayed as line charts. For private channels, the data will be displayed using the default settings, as there is no way to read the private windows settings with the API key only. The ThingView APP can be download for ANDROID and IPHONE . 15. Measuring Wind Speed and Direction This Weather Station tutorial is part of a joint project developed with my friend Mauricio Pinto . Here, we learned how to capture several important data, related to weather, as Air Temperature and Humidity, Pressure and UV. Another very important data to be added to a Weather Station are Wind Speed and Direction. Mauricio did a great job, writing a very detailed tutorial, explained how to construct an Anemometer, mostly with recycled material. You can find his project on this 2 part tutorial: Part 1 Construction of the devices Anemometer and Wind Vane Direction. Part 2 The sketch using Arduino IDE for Esp8266 Nodemcu and transmission to ThingSpeak As Mauricio explained in his tutorial, the anemometer is a device capable of measuring the wind speed and its direction. Using a Hall Effect sensor he was able to count how many rotations the cups give on a period of time, being the intensity of the wind, proportional to the speed of rotation of the axis. With some simple physics equations, he could determine the linear velocity of the wind, at that moment. The wind direction was measured through a windshield with a neodymium magnet and reed switches. Here, you can see the anemometer installed in his house (that is located around 400 meters far from my Weather Station): The wind speed and direction are also sent to Thingspeak.com. 16. Conclusion As always, I hope this project can help others find their way into the exciting world of Electronics and Data Science! For details and final code, please visit my GitHub depository: RPi-Weather-Station For more projects, please visit my blog: MJRoBot.org Saludos from the south of the world! See you in my next article! Thank you, Marcelo 54 54 54 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Jan Teichmann Jun 4, 2019 Member-only How to make a success story of your data science team. Data science resounds throughout every industry and has reached the mainstream media. I no longer have to explain what I do for a living as long as I call it AI we are peak data science hype! As a consequence, more and more companies are looking towards data science 12 min read 12 min read Share your ideas with millions of readers. Nikolay Dimolarov Jun 4, 2019 On the state of Deep Learning outside of CUDAs walled garden If you are a Deep Learning researcher or afficionando and you happen to love using Macs privately or professionally, every year you get the latest and greatest disappointing AMD upgrade for your GPU. Why is it disappointing? Because you get the latest and greatest Vega GPU that of course does 5 min read 5 min read Rohit Agrawal Jun 4, 2019 Analyzing Text Classification Techniques on Youtube Data Text Classification is a classic problem that Natural Language Processing (NLP) aims to solve which refers to analyzing the contents of raw text and deciding which category it belongs to. It is similar to someone reading a Robin Sharma book and classifying it as garbage. 9 min read 9 min read Chitta Ranjan Jun 4, 2019 Step-by-step understanding LSTM Autoencoder layers Here we will break down an LSTM autoencoder network to understand them layer-by-layer. We will go over the input and output flow between the layers, and also, compare the LSTM Autoencoder with a regular LSTM network. <<Download the free book, Understanding Deep Learning, to learn more>> In my previous post, LSTM Autoencoder for Extreme Rare Event Classification [1], we learned how to build an LSTM autoencoder for a multivariate time-series data. 7 min read 7 min read Jo Stichbury Jun 4, 2019 Kedro: A New Tool For Data Science A new Python library for production-ready data pipelines In this post, I will introduce Kedro, a new open source tool for data scientists and data engineers. After a brief description of what it is and why it is likely to become a standard part of every professionals toolchain, I will describe how to use it in a tutorial 10 min read 10 min read Marcelo Rovai Engineer, MBA, Master in Data Science. Passionate to share knowledge about Data Science and Electronics with focus on Physical Computing, IoT and Robotics. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Ishan Getting started with MicroPython on Raspberry Pi Pico Black_Raven (James Ng) in Geek Culture Face Recognition in 46 lines of code Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 1380,\n",
       "  'url': 'https://towardsdatascience.com/web-scrape-twitter-by-python-selenium-part-1-b3e2db29051d',\n",
       "  'title': 'Web Scrape Twitter by Python Selenium (Part\\xa01)',\n",
       "  'subtitle': '-',\n",
       "  'claps': 24,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-11-09',\n",
       "  'clap_prop': 1.185487850478369e-05,\n",
       "  'text': 'Nov 9 2019 Member-only Listen Save Web Scrape Twitter Python Selenium Part 1 Introduction One popular application Python web scraping Web scraping mean extracting data website software programming ton data Internet used data analytics However impractical impossible manually copy store data moment web scraping need famous Python package perform web scraping like request inbuilt package BeautifulSoup Scrapy Selenium Today demonstrate use Selenium web scrape objective web scrape tweet traffic data science twitter account link PS direct way connecting Twitter using Tweepy package use Twitter example article many different application using web scraping Moreover far know function checking tweet analytics information Selenium found via Selenium Python Selenium Python Bindings 2 documentation Note official documentation would like contribute documentation fork selenium-python.readthedocs.io Content Prerequisites Begin tutorial Ending Prerequisites 2 Download Driver include path PATH http //selenium-python.readthedocs.io/installation.html driver Driver used control browser using Python program Depending browser select suitable driver either place path driver PATH environment declare path driver every time Python script would recommend include PATH environment directly dont need set path every time 3 Basic HTML language dont need in-depth knowledge HTML dont need experience web design However much easier basic knowledge structure syntax HTML dont understand anything said want know basic first recommend go w3schools HTML Tutorial HTML standard markup language Web page HTML create Website HTML easy learn www.w3schools.com Begin tutorial PS new beginner would suggest work Jupyter Notebook first face error anytime using Jupyter Notebook run script step step know problem first step open browser navigate twitter page Depending browser select correct driver webdriver Finally visit twitter page get method moment see new browser prompt directing twitter page 2 Locate input box username password time look html either Chrome Firefox right-click page select Inspect Chrome function locate html script object webpage Click email box Chrome jump particular part script see div tag class called LoginForm-input LoginForm-username particular email box click small arrow expand part see input tag inside div tag indicates pact designed user input use information input tag locate email box see multiple element like type class name better use specific element since common one element element name Like case either class name recommended since element name represent email input use class name example actually three class name class element text-input email-input js-signin-email safest way use js-signin-email locate task find element class name equal js-signin-email multiple method Selenium locate element class name even partial text link find information 4 Locating Elements Selenium Python Bindings 2 documentation various strategy locate element page use appropriate one case Selenium selenium-python.readthedocs.io Since locate email box class name use find_element_by_class_name find email box Simple right really One common problem web scraping program search element right accessing webpage Maybe Internet connection speed element loading bit later mainframe element ha correctly located webpage yet Thus Selenium prompt NoSuchElementException error message tell find element Therefore necessary allow webpage load second element found worst case done using time.sleep function inefficient Luckily Selenium provides wait look element two type wait Selenium implicit explicit introduce explicit wait want confirm presence element use explicit wait moment Explicit wait try find element found nothing happens found instead immediately prompt error message wait certain time period time continues finding element certain time Selenium still find element finally prompt TimeoutException error many method explicit wait find information 5 Waits Selenium Python Bindings 2 documentation day web apps using AJAX technique page loaded browser element within selenium-python.readthedocs.io doc provides script use wait However since frequent use explicit wait write function reuse multiple time define variable attribute wait presence element element found exit program locate email box script simplifies Since locate email box define variable indicate email box use find_element_by_class_name pas name class next step input email address rather easy use send_keys email variable input email address successful able see email box filled email address already similar process done also password box Except pas one argument send_keys Keys.ENTER passing Keys.ENTER simulates pressing Enter filling password password box skip locating login button page till everything smooth log twitter account already Congratulations made big step web scraping Ending Web scraping interesting area help automate many task normally take u hour finish However also challenging complexity website nowadays Therefore trial error inevitable finally getting expected result end part 1 next part introduce access tweet love reading face problem feel free leave comment reply comment without Pythons help Stay tuned next article Edit Part 2 article already Web Scrape Twitter Python Selenium Part 2 Continue scraping journey Selenium towardsdatascience.com 46 46 46 WY Fok Amazonian Former data science intern Amazon Germany Bachelor Statistics Master Operation Research Love working number Python SQL SAS Love podcasts audiobooks Learn go new app WY Fok Amazonian Former data science intern Amazon Germany Bachelor Statistics Master Operation Research Love working number Python SQL SAS Medium HKN MZ Python Plain English Scrape Everything Twitter Using Python Guilherme Guidolin Dynamic Web Scraping Python Obalanatosin Web Scraping Football Matches EPL Python Lilian Ugwu Sentiment Analysis Nigerians Primary Election Tweets Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Nov 9, 2019 Member-only Listen Save Web Scrape Twitter by Python Selenium (Part 1) Introduction One popular application of Python is web scraping. Web scraping means extracting data from a website by software or programming. There are tons of data on the Internet that can be used for data analytics. However, it is impractical, if not impossible, to manually copy and store data. At this moment, web scraping is what we need. There are some famous Python packages that perform web scraping like requests (an inbuilt package), BeautifulSoup, Scrapy, and Selenium. Today I will demonstrate how to use Selenium to web scrape. And the objective is to web scrape all tweet traffics of my data science twitter account link . (PS: There is a more direct way connecting Twitter by using Tweepy package. I use Twitter as an example only in this article. There are many different applications by using web scraping. Moreover, as far as I know, there is no function of checking the tweet analytics.) More information about Selenium can be found via: Selenium with Python - Selenium Python Bindings 2 documentation Note This is not an official documentation. If you would like to contribute to this documentation, you can fork this selenium-python.readthedocs.io Content Prerequisites Begin of tutorial Ending Prerequisites 2. Download a Driver and include the path in PATH https://selenium-python.readthedocs.io/installation.html#drivers A Driver is used to control your browser by using the Python program. Depending on your browser, select a suitable driver. Then you either place the path of this driver in PATH environment or you declare the path of the driver every time in your Python script. I would recommend to include in your PATH environment directly so you dont need to set the path every time. 3. Basic HTML language You dont need in-depth knowledge of HTML nor you dont need any experience in web design. However, it is much easier if you have some basic knowledge about the structure and syntax of HTML. If you dont understand anything I said or you want to know some basic first then I recommend you go to w3schools. HTML Tutorial HTML is the standard markup language for Web pages. With HTML you can create your own Website. HTML is easy to learn www.w3schools.com Begin of tutorial PS: For a new beginner, I would suggest you work in Jupyter Notebook first because you will face more errors than anytime before. By using Jupyter Notebook you can run the script step by step so that you know where the problem is. The first step is to open a browser and navigate the twitter page. Depending on your browser and select the correct driver from webdriver. Finally, visit the twitter page by get method. At this moment you should see a new browser prompts up and directing to the twitter page. 2. Locate input boxes for username and password Now its time to look at html. In either Chrome or Firefox, right-click the page and select Inspect. In Chrome, there is a function to locate the html script of any object on the webpage. Click the email box and then Chrome will jump to that particular part of the script. So here we can see a div tag with the class called LoginForm-input LoginForm-username for this particular email box. Then click the small arrow to expand this part. We can see there is an input tag inside the div tag. This indicates that this pact is designed for a user to input. So we will use information from the input tag to locate this email box. As you can see there are multiple elements here, like type, class, name,. It is better to use a more specific element since it is common that there is more than one element with the same element name. Like in this case, either class or name is recommended since the element names represent email input. Here I use class name as an example. There are actually three class names here in the class element, text-input, email-input, and js-signin-email. The safest way is to use js-signin-email to locate. So the task here is to find an element with a class name equal to js-signin-email. There are multiple methods in Selenium to locate an element, from class name to even the partial text of the link. You can find more information by 4. Locating Elements - Selenium Python Bindings 2 documentation There are various strategies to locate elements in a page. You can use the most appropriate one for your case. Selenium selenium-python.readthedocs.io Since we locate this email box by class name, so we will use find_element_by_class_name and then find the email box. Simple right? Not really One common problem for web scraping is that the program searches for the element right after accessing the webpage. Maybe because of your Internet connection speed or that element loading a bit later than the mainframe, the element has not been correctly located on the webpage yet. Thus Selenium will prompt a NoSuchElementException error message and tell you cannot find that element. Therefore, it is necessary to allow the webpage to load for a few seconds until that element is found. In the worst case, this can be done by using time.sleep function but it is inefficient. Luckily Selenium provides waits and look for the element. There are two types of waits in Selenium, implicit and explicit. Here I will introduce explicit wait. When you want to confirm the presence of an element, you can use explicit wait at this moment. Explicit wait will try and find the element. If it is found then nothing happens. But if it is not found, instead of immediately prompts an error message, it will wait for a certain time period while at the same time it continues finding the element. After a certain time if Selenium still cannot find the element then it will finally prompt an TimeoutException error. There are many methods for explicit waits. You can find further information by 5. Waits - Selenium Python Bindings 2 documentation These days most of the web apps are using AJAX techniques. When a page is loaded by the browser, the elements within selenium-python.readthedocs.io In the doc, it provides a script of how to use wait. However, since it is frequent to use this explicit wait, I write a function so I can reuse it multiple times. Here you can define the variable and the attribute of and then wait until the presence of the element. If the element is not found then it will exit the program. So to locate the email box, the script simplifies as Since we can now locate the email box. We then define a variable to indicate this email box. We can use find_element_by_class_name and then pass the name of the class. The next step is to input the email address. This is rather easy because we can use send_keys on email variable to input the email address. If successful, you should be able to see the email box is filled with the email address already. A similar process is done also for the password box. Except we pass one more argument in send_keys, Keys.ENTER By passing Keys.ENTER, this simulates pressing an Enter after filling in the password in the password box. Then we can skip locating the login button on the page. Up till now, if everything is smooth, you should log in to your twitter account already. Congratulations, you made a big step in web scraping. Ending Web scraping is an interesting area because this can help automate many tasks that normally take us hours to finish. However, it is also challenging because of the complexity of websites nowadays. Therefore trial and error are inevitable before finally getting your expected results. Here is the end of part 1. In the next part, I will introduce how to access all your tweets. If you love reading this or if you face any problem, feel free to leave your comment and I will reply to your comments without any Pythons help. Stay tuned for my next article. Edit: Part 2 of this article is already here: Web Scrape Twitter by Python Selenium (Part 2) Continue the scraping journey with Selenium towardsdatascience.com 46 46 46 More from WY Fok Amazonian. Former data science intern in Amazon Germany. Bachelor in Statistics and Master in Operation Research. Love working with number. Python / SQL / SAS Love podcasts or audiobooks? Learn on the go with our new app. WY Fok Amazonian. Former data science intern in Amazon Germany. Bachelor in Statistics and Master in Operation Research. Love working with number. Python / SQL / SAS More from Medium HKN MZ in Python in Plain English How to Scrape Everything from Twitter Using Python Guilherme Guidolin Dynamic Web Scraping with Python Obalanatosin Web Scraping Football Matches [EPL] With Python Lilian Ugwu Sentiment Analysis: Nigerians Primary Election Tweets Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 3187,\n",
       "  'url': 'https://towardsdatascience.com/how-to-use-python-features-in-your-data-analytics-project-e8032374d6fc',\n",
       "  'title': 'How to use Python features in your data analytics project',\n",
       "  'subtitle': 'Python tutorial in Azure using OO,\\xa0NumPy…',\n",
       "  'claps': 22,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 9,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-25',\n",
       "  'clap_prop': 1.0866971962718383e-05,\n",
       "  'text': 'Towards Data Science Apr 25 2019 Listen Save use Python feature data analytics project Python tutorial Azure using OO NumPy panda SQL PySpark 1 Introduction lot company moving cloud consider tooling shall used data analytics On-premises company mostly use propriety software advanced analytics BI reporting However tooling may logical choice cloud environment Reasons 1 lack integration cloud provider 2 lack big data support 3 lack support new use case machine learning deep learning Python general-purpose programming language widely used data analytics Almost cloud data platform offer Python support often new feature become available Python first Python seen Swiss Army knife data analytics 2 Objective tutorial two project created take account important feature Python Projects described follows Therefore following step executed standalone tutorial focus learn different aspect Python focus le deep dive separate aspect case interested deep learning see devops AI refer previous blog focus security see 3 Prerequisites following resource need created 4 OO NumPy panda sql Jupyter DSVM part sample Python project three class Using class football player data registered following step executed 4a Get started Log Windows Data Science Virtual Machine DSVM desktop overview icon preinstalled component found Click Jupyter short cut start Jupyter session Subsequently open Jupyter command line session found taskbar Copy URL open Firefox session download following notebook desktop DSVM Finally select upload notebook jupyter session click run button menu run code cell important part notebook also discussed remaining chapter 4b Object-oriented OO programming part tutorial inspired following tutorial Theophano Mitsa part three class created keep track football player data snippet first class Player found following seen class Subsequently class FirstTeamPlayer found following seen class Finally snippet training class found following seen class example three class instantiated used found final snippet 4c Matrix analytics using NumPy NumPy fundamental package scientific computing Python tutorial used matrix analytics Notice attribute _rawData already encapsulated Player class NumPy array NumPy often used Matplotlib visualize data snippet data taken player class matrix operation done basic advanced Full example found github project 4d Statistical analytics using panda Pandas package high-performance easy-to-use data structure data analysis tool Python hood panda us NumPy array structure tutorial used calculate basic statistic snippet data taken player class statistic operation done Full example found github project 4e Read/write database Finally data written SQL database tutorial MSSQL database used part DSVM Look Microsoft SQL Server Management Studio SSMS icon found taskbar start new session Log using Windows Authentication see also Look New Query menu start new query session execute following script Finally data written read database Pandas dataframes used see also snippet 5 PySpark Azure Databricks Spark cluster previous chapter code wa run single machine case data generated advanced calculation need done e.g deep learning possibility take heavier machine thus scale execute code compute distributed VMs Spark analytics framework distribute compute VMs thus scale adding VMs work time efficient supercomputer work Python used Spark often referred PySpark tutorial Azure Databricks used Apache Spark-based analytics platform optimized Azure following step executed 5a Get started Start Azure Databricks workspace go Cluster Create new cluster following setting Subsequenly select Workspace right-click select import radio button select import following notebook using URL See also picture Select notebook imported 4b attach notebook cluster created 4a Make sure cluster running otherwise start Walk notebook cell cell using shortcut SHIFT+ENTER Finally want keep track model create HTTP endpoint model and/or create DevOps pipeline project see advanced DevOps AI tutorial focus security see 5b Setup project project machine learning model created predicts income class person using feature age hour week working education following step executed Notice pyspark.ml library used build model also possible run scikit-learn library Azure Databricks however work would done driver master node compute distributed See snippet pyspark package used 6 Conclusion tutorial two Python project created follows lot company consider tooling use cloud data analytics almost cloud data analytics platform Python support therefore Python seen Swiss Army knife data analytics tutorial may helped explore possibilites Python 23 1 23 23 1 Towards Data Science home data science Medium publication sharing concept idea code Ryan Burge Apr 25 2019 Member-only Religious Composition Two Major Parties make lot data visualization time pretty good idea result calculation going look like write code fact result obvious dont even post anywhere 4 min read 4 min read Share idea million reader Ashutosh Singh Apr 25 2019 Member-only Classifying Products Banned Approved using Text Mining- Part II part explain optimize existing Machine Learning model Part deployment ML model using Flask previous article series discussed business problem shown train model using fastText classification Banned Approved product based information like Product Name product description specification 5 min read 5 min read Favio Vzquez Apr 25 2019 Jungle Koalas Pandas Optimus Spark expect newest library Databricks Koalas Optimus framework Apache Spark 3.x excited data science probably know Spark+AI latest summit started yesterday April 24th 2019 great thing talk spin-off youve following co-created 8 min read 8 min read Rinu Gour Apr 25 2019 Member-only Complete Guide Learn R R Programming Technology open source programming language Also R programming language latest cutting-edge tool R Basics hottest trend Moreover R command line interface C.L.I consists prompt usually character History R John Chambers colleague developed R Bell Laboratories Basically 8 min read 8 min read Sriram Parthasarathy Apr 25 2019 Member-only forecast sale revenue Compare various forecasting approach 100 method forecast sale question becomes one choose article briefly cover popular way forecast sale compare method key metric Depending use case customer may ok simple 5 min read 5 min read Ren Bremer Data Solution Architect Microsoft working Azure service ADFv2 ADLSgen2 Azure DevOps Databricks Function Apps SQL Opinions mine Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 25, 2019 Listen Save How to use Python features in your data analytics project Python tutorial in Azure using OO, NumPy, pandas, SQL, PySpark 1. Introduction A lot of companies are moving to cloud and consider what tooling shall be used for data analytics. On-premises, companies mostly use propriety software for advanced analytics, BI and reporting. However, this tooling may not be the most logical choice in a cloud environment. Reasons can be 1) lack of integration with cloud provider, 2) lack of big data support or 3) lack of support for new use cases such as machine learning and deep learning. Python is a general-purpose programming language and is widely used for data analytics. Almost all cloud data platforms offer Python support and often new features become available in Python first. In this, Python can be seen as the Swiss Army knife of data analytics. 2. Objective In this tutorial, two projects are created that take into account important features of Python. Projects are described as follows: Therefore, the following steps are executed: It is a standalone tutorial in which the focus is to learn the different aspects of Python. The focus is less to deep dive in the separate aspects. In case you more interested in deep learning, see here or in devops for AI, refer to my previous blogs, here and with focus on security, see here . 3. Prerequisites The following resources need to be created: 4. OO, NumPy, pandas and sql with Jupyter on DSVM In this part, a sample Python project with three classes. Using these classes, football players data is registered. The following steps are executed: 4a. Get started Log in to your Windows Data Science Virtual Machine (DSVM). On the desktop, an overview of icons of preinstalled components can be found. Click on Jupyter short cut to start a Jupyter session. Subsequently, open the Jupyter command line session that can be found in the taskbar. Copy the URL and open this in a Firefox session. Then download the following notebook to the desktop of your DSVM: Finally, select to upload the notebook in your jupyter session. Then click on the run button in the menu to run code in a cell. The most important parts of the notebook are also discussed in the remaining of the chapter. 4b. Object-oriented (OO) programming This part of the tutorial is inspired by the following tutorial by Theophano Mitsa. In this part, three classes are created to keep track of football players data. A snippet of the first class Player can be found below: The following can be seen in this class: Subsequently, class FirstTeamPlayer can be found below: The following can be seen in this class: Finally, a snippet of the training class can be found below: The following can be seen in this class: An example how the three classes are instantiated and are used can be found in the final snippet below: 4c. Matrix analytics using NumPy NumPy is the fundamental package for scientific computing with Python. In this tutorial it will be used to do matrix analytics. Notice that attribute _rawData is already encapsulated in the Player class as a NumPy array. NumPy is often used with Matplotlib to visualize data. In the snippet below, the data is taken from player class and then some matrix operations are done, from basic to more advanced. Full example can be found in the github project. 4d. Statistical analytics using pandas Pandas is the package for high-performance, easy-to-use data structures and data analysis tools in Python. Under the hood, pandas uses NumPy for its array structure. In this tutorial it will be used to calculate some basic statistics. In the snippet below, the data is taken from player class and then some statistics operations are done. Full example can be found in the github project. 4e. Read/write to database Finally, the data will be written to a SQL database. In this tutorial, the MSSQL database is used that is part of the DSVM. Look for the Microsoft SQL Server Management Studio (SSMS) icon that can be found in taskbar and start a new session. Log in using Windows Authentication, see also below. Look for New Query in the menu and start a new query session. Then execute the following script: Finally, the data can be written to and read from the database. Pandas dataframes will be used for this, see also the snippet below. 5. PySpark with Azure Databricks on Spark cluster In the previous chapter, all code was run on a single machine. In case more data is generated or more advanced calculations need to be done (e.g. deep learning), the only possibility is to take a heavier machine an thus to scale up to execute the code. That is, compute cannot be distributed to other VMs. Spark is an analytics framework that can distribute compute to other VMs and thus can scale out by adding more VMs to do work. This is most of times more efficient than having a supercomputer doing all the work. Python can be used in Spark and is often referred to as PySpark. In this tutorial, Azure Databricks will be used that is an Apache Spark-based analytics platform optimized for the Azure. In this, the following steps are executed. 5a. Get started Start your Azure Databricks workspace and go to Cluster. Create a new cluster with the following settings: Subsequenly, select Workspace, right-click and then select import. In the radio button, select to import the following notebook using URL: See also picture below: Select the notebook you imported in 4b and attach the notebook to the cluster you created in 4a. Make sure that the cluster is running and otherwise start it. Walk through the notebook cell by cell by using shortcut SHIFT+ENTER. Finally, if you want to keep track of the model, create an HTTP endpoint of the model and/or create a DevOps pipeline of the project, see my advanced DevOps for AI tutorial here , and with focus on security, see here . 5b. Setup of project In this project, a machine learning model is created that predicts the income class of a person using features as age, hours of week working, education. In this, the following steps are executed: Notice that pyspark.ml libraries are used to build the model. It is also possible to run scikit-learn libraries in Azure Databricks, however, then work would only be done be the driver (master) node and the compute is not distributed. See below a snippet of what pyspark packages are used. 6. Conclusion In this tutorial, two Python projects were created as follows: A lot of companies consider what tooling to use in the cloud for data analytics. In this, almost all cloud data analytics platforms have Python support and therefore, Python can be seen as the Swiss Army knife of data analytics. This tutorial may have helped you to explore the possibilites of Python. 23 1 23 23 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ryan Burge Apr 25, 2019 Member-only The Religious Composition of the Two Major Parties I make a lot of data visualizations. Most of the time I have a pretty good idea of what the results of the calculations are going to look like before I write the code. In fact, some of the results are so obvious that I dont even post them anywhere 4 min read 4 min read Share your ideas with millions of readers. Ashutosh Singh Apr 25, 2019 Member-only Classifying Products as Banned Or Approved using Text Mining- Part II In this part, we will explain how to optimize the existing Machine Learning model in Part I and the deployment of this ML model using Flask. In the previous article of this series, We have discussed the business problem, shown how to train the model using fastText and classification of Banned or Approved products based on information like(Product Name, product description, and specifications). 5 min read 5 min read Favio Vzquez Apr 25, 2019 The Jungle of Koalas, Pandas, Optimus and Spark What to expect from the newest library from Databricks (Koalas), the Optimus framework and Apache Spark 3.x If you are as excited about data science as me, you probably know that the Spark+AI latest summit started yesterday (April 24th 2019). And there are great things to talk about. But I will do it with a spin-off. If youve been following me you now that I co-created a 8 min read 8 min read Rinu Gour Apr 25, 2019 Member-only A Complete Guide to Learn R R Programming Technology is an open source programming language. Also, the R programming language is the latest cutting-edge tool. R Basics is the hottest trend. Moreover, the R command line interface (C.L.I) consists of a prompt, usually the > character. History of R John Chambers and colleagues developed R at Bell Laboratories. Basically 8 min read 8 min read Sriram Parthasarathy Apr 25, 2019 Member-only How to forecast sales revenue: Compare various forecasting approaches There are 100s of methods to forecast sales. The question becomes which ones to choose. In this article, I will briefly cover the popular ways to forecast sales and how to compare the methods with key metrics. Depending on the use case, a customer may be ok with a simple 5 min read 5 min read Ren Bremer Data Solution Architect @ Microsoft, working with Azure services as ADFv2, ADLSgen2, Azure DevOps, Databricks, Function Apps and SQL. Opinions here are mine. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4102,\n",
       "  'url': 'https://towardsdatascience.com/user-guide-to-my-first-data-product-medium-post-metric-displayer-e99e74e52b3a',\n",
       "  'title': 'User guide to My First Data Product: Medium Post Metric Displayer',\n",
       "  'subtitle': 'Origin',\n",
       "  'claps': 19,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 14,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-01-26',\n",
       "  'clap_prop': 9.385112149620422e-06,\n",
       "  'text': 'Towards Data Science Jan 25 2019 Listen Save User guide First Data Product Medium Post Metric Displayer Know Medium Post Better Data Origin regular writer Medium well data geek busy year 2018 Id like reflect achieved Medium blog Furthermore based performance 2018 plan make aggressive writing plan year 2019 think Medium writer know anti-data-centered interface design analytics platform Medium simple design quite discouraged really looking data make decision Thats main reason decided develop first data product following article show product code use creating product reader use data Medium analytics platform well showcase product run whole thinking execution process also detailed documented Anyone keen knowing Medium post performance feel free use code give sense data Problem Statement Pain Point Problem statement annoying know article performs well robust visualized manner Also article cant probably grouped classified Among metric metric may distinguishable one different group article Pain point Medium user might find difficult make analytics platform Stat Tab provided Medium data cant downloaded hardly tell much information next Constraint constraint product obvious still capable scraping output performance data every article Medium automated way Therefore user need document data manually excel column name mine Users need regularly put metric seasonal basis quarterly semi-annually product deal excel file thoroughly specific data file ready go Skills Tools Tool Python Jupyter Notebook Skills Data processing munging Pandas visualization Matplotlib clustering plus PCA Sklearn numerous function creation repetitive operation Product Outlook product simple Ive created layout interface instead introduce operational logic behind product Medium Post Metric Displayer consists numerous small handy function user pass excel file use documenting data function result automatically generated contains two feature product first one Dashboard Visualization Product second one Clustering Product first phase product roadmap sure keep expanding feature However think beta version Displayer well-rounded enough provide insight writer make decision Thats reason decided soft release product point Feel free fetch Github code use product lot fun yunhanfeng/medium_metric_data_product first data product providing data solution Medium writer track post performance github.com Part 0 Data import pre-check introducing product let take look data set first sure needed package python imported Pandas Numpy Matplotlib familiar friend used Translator Goolgetrans time reason behind sometimes write article Chinese reader Taiwan might also write post Chinese Translating Chinese title English fit Matplotlib constraint Matplotlib cant show Chinese later read excel file display first row checking column type well shape data extracted documented Medium basic seven feature Medium website wrote 45 article far Part 1 Dashboard Visualization Product Within product designed eight plus one feature function enable user return similar chart graph different metric simple excel file input Based chart graph writer able get sense good bad article perform make next-step strategy user engagement funnel Medium post basically follow funnel design product Function 0 function mainly deal translated issue Chinese title help translated title English thanks powerful Google Translate Function 1 function return static value total view basic number Medium post showing generic description performance passed file function return total view around 15K Function 2 Function 2 return top 5 article view result displayed table format well bar chart format may also get glimpse top 5 contributes total view Operate function result top 5 article view shown seems article related MSBA application draw view Function 3 Similar function 2 function return table bar chart top 5 article additional view Additional view quite strange metric definition Medium view RSS reader FB instant article AMP dont factor read ratio every article writer posted additional view experience article curated publication website Medium may additional view increase give insight good outbound article perform Operate function Function 4 Read even deeper metric regarding engagement Read metric user scroll way bottom indicates user read post function take argument file name return table bar graph Operate function result slightly different top 5 article view top 5 read Chinese post multiple possible reason behind One reader Taiwan Chinese relatively familiar language majority user tend finish reading article Second might indicate English writing long convoluted user read may need polish post concise Three apart language issue top 5 post read related information-related topic prepare application ask question review MSBA course Maybe kind topic trigger interest reading post Function 5 Read ratio another tricky metric Medium writer Read ratio read view click said Medium algorism reward article higher read ratio higher rank priority show front user function work returning top 5 article highest read ratio form displayed table bar graph Operate function interpretation result tricky certain way One shorter article possible achieve higher read ratio top one article among post link course get highest read ratio Second article get fewer view may also lead higher read ratio like article named Creation conclusion endogeneity affecting metric suggest user cross-validating metric function Function 6 function return average read ratio article Since read ratio key metric toward Medium algorism suggested writer use function track progress article performance occasionally Operate function also set based characteristic well category content light easy-to-digest article might higher average read ratio vice versa article average read ratio 0.528 bad try set goal increasing read ratio Medium website Writing structured readable laconic post good approach Function 7 next function showing top 5 article top fan Fans metric metric presenting deeper engagement toward article Readers clapped post showed like last step metric funnel achieved harder Operate function Function 8 last function first part product show average view day elapsed create new metric capture daily additional view post formula Total view Days today day article published calculated daily flow-in view toward specific article Operate function think metric important show article still bring traffic everyday basis article drive user writer Medium website help user identify cash cow Medium blog knowing article regularly promoting social medium making referred affliates good strategy keep fresh traffic streaming Medium Part 2 Clustering Product like mentioned function 8 identifying article cash cow important Thats intention proposed created clustering product unsupervised learning provided feature article segmented different group group ha different trait play different role regarding user acquisition retention engagement using clustering product user need pas excel file put number cluster want create result automatically generated several function related visualizing cluster introduce four plus one function following article Function 9 function helper function munging data create total_veiw view_per_day orginal data frame Function 10 11 two function used clustering used K-means clustering article took read read ratio fan total view view_per_day segmentation based five feature function started user need pas excel file cluster number function Function 10 return data frame cluster label Users may see helper function product function data frame return easily seen played around Function 11 return summary cluster grant user clear overview cluster performance quality think super helpful user finally design suitable promotion strategy towards different cluster Whenever want re-share revitalize traffic using product function really justify choice also suggest using many cluster argument enter different number see number grant interpretable result Operate function 10 Operate function 11 Function 12 function following previous function return ideal cluster overview may want visualize Therefore utilize principal component analysis PCA turn five-feature dimension two dimension article different group labeled 2D plane pas file cluster number get result also return explained ratio principal component 1 2 try n 3 n 5 Operate code Function 13 last function also related clustering function creates so-called Parallel Coordinates Graph showcase different cluster extent feature contributes Simply put reveals logic behind clustering writer able make decision based result try cluster 3 5 example Operate function n 3 may conclude cluster 1 top two performing article excel total view read fan view per day relatively poor read ratio distinction cluster 0 2 mainly based read ratio mean metric play great portion influence article may focus writing strategy boosting read ratio year strategy made using data product Conclusion many constrain Medium analytics platform Stat tab However creating data product hope benefit Medium community equip writer data-driven mindset better writing strategy based result generated product Happy analyzing 27 1 27 27 1 Towards Data Science home data science Medium publication sharing concept idea code Md Kamaruzzaman Jan 25 2019 Member-only Back metal Top 3 Programming language develop Big Data framework C++ Rust Go Java Data Intensive framework previous blog post Programming language rule Data Intensive Big Data Fast Data framework briefly discussed popular Big Data framework showed Java de-facto programming language Data Intensive framework Java significant advantage e.g 12 min read 12 min read Share idea million reader Brayden Gerrard Jan 25 2019 Member-only Evolution US Electric Grid past 15 year US electric grid ha undergone massive shift mostly better Renewable energy natural gas coal GHG emission see US grid ha evolved recent year 4 min read 4 min read zgr Gen Jan 25 2019 Member-only Notes Artificial Intelligence Machine Learning Deep Learning curious people AI ha intriguing topic 2018 according McKinsey mentioned key enabler 1 3 spot Gartner Top 10 Strategic Technology Trends 2019 AI became catch-all term refers computer program automatically doe something 22 min read 22 min read Felix Kuestahler Jan 25 2019 Member-only Python Tutorial Twitter Account Retrieval Swiss Government Members Start journey social medium analysis politician Click link Title list tutorial article first article tutorial show extract list tweeting Swiss Government Members via Twitter API 7 min read 7 min read Tobias Hill Jan 25 2019 got 1 better accuracy data augmentation hard let go thing put love yes really thought wa done 5 th article series accuracy/error metric MNIST example started haunting wa quite sure could improve 4 min read 4 min read Henry Feng Sr. BI Engineer Product Analytic DS UMN MSBA Tech Education Medium List http //pse.is/SGEXZ Podcast Medium Samuele Mazzanti Towards Data Science Using Causal ML Instead A/B Testing Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Antonio Blago ILLUMINATION Become Data Analyst 2023 Justin Brooke Made 3,483,510.77 Membership Site Weird Niche Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Jan 25, 2019 Listen Save User guide to My First Data Product: Medium Post Metric Displayer Know Your Medium Post Better with Data Origin As a regular writer on Medium as well as a data geek, after the busy year of 2018, Id like to reflect what I have achieved on my Medium blog. Furthermore, based on the performance in 2018, I plan to make more aggressive writing plan in the year 2019. I think most Medium writers know the anti-data-centered interface design of analytics platform of Medium. It is too simple for me. And the design quite discouraged me from really looking into data and make the decision. Thats the main reason I decided to develop my very first data product. In the following article, I will show the product and the codes I use for creating the product to readers. I will use my own data at Medium analytics platform as well to showcase how my product run. The whole thinking and execution process will also be detailed documented. Anyone who is keen about knowing more about his or her own Medium posts performance, feel free to use my code to give some sense of data. Problem Statement and Pain Point Problem statement : It is annoying to know which article performs well in a more robust and visualized manner. Also, the articles cant be probably grouped or classified. Among all the metrics, which metrics may be the most distinguishable ones for different groups of articles? Pain point : Medium users might find it difficult to make the most of the analytics platform (Stat Tab) provided by Medium. The data cant be downloaded and it hardly tells much information about what to do next. Constraint The constraint of this product is very obvious. For me, I am still not capable of scraping or output the performance data of every article from Medium in a more automated way. Therefore, users need to document those data manually just as I did in this excel. The column name should be the same as mine. Users just need to regularly put down those metrics on a seasonal basis (quarterly or semi-annually) My product can deals with those excel file thoroughly. Once you have that specific data file. We are ready to go. Skills and Tools Tool: Python Jupyter Notebook Skills: Data processing and munging with Pandas, visualization with Matplotlib, clustering plus PCA with Sklearn, numerous function creation for repetitive operation Product Outlook This product is very simple. Ive not created the layout or interface of it, but instead, I will introduce the operational logic behind this product. Medium Post Metric Displayer consists of numerous small and handy functions. Once the user passes the excel file (we use for documenting data) to those functions, the result will automatically be generated. It contains two features for this product. The first one is the Dashboard & Visualization Product. The second one is Clustering Product. It is the first phase of my product roadmap. (I am not sure if I will keep expanding its features). However, I think this beta version of Displayer is well-rounded enough to provide some insights to the writers to make decisions. Thats the reason I decided to soft release my product at this point. Feel free to fetch my Github code and use this product with lots of fun! yunhanfeng/medium_metric_data_product My first data product, providing data solution for Medium writer to track post performance github.com Part 0: Data import for pre-check Before introducing the product itself, lets take a look at our data set first. It is for sure that the needed package in python is imported. Pandas, Numpy, and Matplotlib is my familiar friend. I used Translator from Goolgetrans this time. The reason behind is that I sometimes write articles in Chinese and most of my readers are from Taiwan. They might also write posts in Chinese. Translating the Chinese title into English will be more fit into Matplotlib constraint, where Matplotlib cant show Chinese. I later read into the excel file and display the first few rows of it, further checking the column type as well as the shape. From the data extracted and documented from Medium, there are basic seven features. And from my Medium website, I wrote 45 articles so far. Part 1: Dashboard and Visualization Product Within this product, I designed eight plus one features functions which enable users to return similar charts and graphs on different metrics with a simple excel file input. Based on those charts and graphs, writers are able to get some sense of how good or bad their articles perform and further make the next-step strategy. Below is the user engagement funnel on Medium post. And basically, I just follow the funnel to design my product. Function 0 This function mainly deals with the translated issue of Chinese title. It helps translated those titles into English, thanks to powerful Google Translate. Function 1 This function returns the static value of total views. It is the basic number of all Medium posts, showing the generic description of your performance. I passed my file into this function, and it returns my total views is around 15K. Function 2 Function 2 returns the top 5 articles with the most views. The result will be displayed in both table format as well as bar chart format. You may also get a glimpse of how these top 5 contributes to your total views. Operate the function The result of my top 5 articles with most views is shown below. It seems my articles related to MSBA application draw most of views. Function 3 Similar to function 2, this function returns the table and bar chart of the top 5 articles with additional view. Additional view, for me, is quite a strange metrics. The definition from Medium is the views from RSS readers, FB instant articles and AMP, and they dont factor into read ratio. Not every article a writer posted will have an additional view. In my experience, only when your article is curated by other publication websites on Medium may the additional views increase. It gives some insights that how good those outbound articles perform. Operate the function. Function 4 Read is an even deeper metrics regarding engagement. Read is the metric if the user scrolls all the way to bottom, which indicates a user reads through the posts. The function takes the argument of the file name and returns table and bar graph. Operate the function. The result is slightly different from the top 5 articles with most views. The top 5 with most reads are all Chinese posts. There are multiple possible reasons behind. One, most of my readers are from Taiwan, Chinese is a relatively familiar language to the majority of my users. They tend to finish reading the articles more. Second, it might indicate that my English writing is too long or convoluted for users to read through. I may need to polish my post to be more concise. Three, apart from language issue, the top 5 posts with most reads are related to information-related topics, such as how to prepare for application, how to ask questions and the review of MSBA course. Maybe this kind of topics can trigger more interests in reading all through the posts. Function 5 Read ratio is another tricky metric for Medium writer. Read ratio = reads / views (clicks). It is said that Medium algorism will reward articles of higher read ratio with higher rank and priority to show in front of users. The function works the same as those above, returning top 5 articles with the highest read ratio, in the form of displayed table and bar graph. Operate the function. The interpretation of results is tricky in certain ways. One, the shorter the article, the more possible it achieve higher read ratio. The top one article among all my post is just a link. Of course, it will get the highest read ratio. Second, the article which gets fewer views may also lead to higher read ratio, just like my article named [Creation]. The conclusion is that there is some endogeneity affecting this metric. I suggest users cross-validating this metric with functions above. Function 6 The function returns the average read ratio of all the articles. Since read ratio is a key metrics toward Medium algorism. I suggested writers use this function to track the progress of article performance occasionally. Operate the function. It should also be set based on the characteristics as well as the categories of your content. Some light and easy-to-digest articles might have a higher average read ratio, vice versa. For my article, the average read ratio is 0.528, which for me, is not bad. I will try to set this as my goal of increasing read ratio for my Medium website. Writing structured, readable and laconic post is a good approach to that. Function 7 The next function is about showing the top 5 articles with top fans. Fans metric is the metric presenting deeper engagement toward articles. Readers clapped the post and showed their likes. It is the last step of metric funnel, which can be achieved harder. Operate the function. Function 8 The last function in the first part of the product is to show average view through the days elapsed. I create this new metric to capture the daily additional views for posts. The formula is: Total views / Days between today and the day that article published. For me, it calculated the daily flow-in of view toward that specific article. Operate the function I think this metric is very important. It shows that the articles still bring in traffic on an everyday basis. Those articles drive the user to the writers Medium website. It helps users to identify the cash cow of your Medium blog. After knowing these articles, regularly promoting them on social media or making them referred by affliates is a good strategy to keep fresh traffic streaming into your Medium. Part 2: Clustering Product Just like what I mentioned in function 8, identifying which articles are cash cows is important. Thats the very intention I proposed and created this clustering product. With unsupervised learning on provided features, the articles can be segmented into different groups. Each group has different traits. They can play different roles regarding user acquisition, retention, and engagement. By using this clustering product, users just need to pass the excel file and further put in number of clusters he wants to create, and the result is automatically generated. And there are several functions is related to visualizing the cluster too. I will introduce four plus one functions in the following article. Function 9 This is function is just a helper function for munging the data. I create total_veiw and view_per_day out of the orginal data frame. Function 10 & 11 These two functions are used for clustering. I used K-means for clustering articles. I took in reads, read ratio, fans, total views, view_per_day. And the segmentation is based on these five features. From the functions started here, users need to pass both excel file and cluster number to the function. Function 10 returns the data frame with cluster label. Users may see it as a helper function or a product function. The data frame it returns can be easily seen and played around. Function 11 returns the summary of each cluster. It grants users a clear overview of each cluster performance and quality. I think it is super helpful, for users can finally design suitable promotion strategy towards different cluster. Whenever you want to re-share or revitalize the traffic, using this product function can really justify your choice. And I also suggest not using too many clusters in the argument, you can just enter different numbers and see which number grant the most interpretable result. Operate the function 10 Operate function 11 Function 12 This function is following the previous function. Once you return the ideal cluster overview. You may want to visualize it. Therefore, I utilize principal component analysis (PCA) to turn five-feature dimension into two dimensions. And all the articles in different groups can be labeled on this 2D plane. Just pass the file and cluster number, you will get the result. It will also return the explained ratio of principal component 1 and 2. I will try n = 3 and n = 5 here. Operate the code Function 13 The last function is also related to clustering. This function creates the so-called Parallel Coordinates Graph. It showcases for different clusters, the extent each feature contributes. Simply put, it reveals the logic behind clustering. And writers are able to make the further decision based on the result. I will try cluster = 3 and 5 for example. Operate the function If n = 3, I may conclude that cluster 1, the top two performing articles excel at total views, reads, fans and view per day, but they have relatively poor read ratio. And the distinction between cluster 0 and 2 is mainly based on read ratio, which means this metric play a great portion of influence in most of my article. I may focus my writing strategy on boosting read ratio this year. Some strategies made by using my data product Conclusion There are many constrain in Medium analytics platform (Stat tab). However, by creating this data product, I hope I can benefit the Medium community to equip writers with some data-driven mindset and better the writing strategy based on the result generated from this product. Happy analyzing!!! 27 1 27 27 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Md Kamaruzzaman Jan 25, 2019 Member-only Back to the metal: Top 3 Programming language to develop Big Data frameworks C++, Rust, Go over Java for Data Intensive frameworks In a previous blog post: Programming language that rules the Data Intensive (Big Data, Fast Data) frameworks, I have briefly discussed some of the most popular Big Data frameworks and showed that Java is the de-facto programming language in Data Intensive frameworks. Java had significant advantages (e.g. 12 min read 12 min read Share your ideas with millions of readers. Brayden Gerrard Jan 25, 2019 Member-only The Evolution Of The US Electric Grid Over the past 15 years, the US electric grid has undergone some massive shifts (mostly for the better). Renewable energy and natural gas are up, while coal and GHG emissions are down. Here we can see how the US grid has evolved in recent years: 4 min read 4 min read zgr Gen Jan 25, 2019 Member-only Notes on Artificial Intelligence, Machine Learning and Deep Learning for curious people AI has been the most intriguing topic of 2018 according to McKinsey. It is mentioned as the key enabler now at the #1 and #3 spot of Gartner Top 10 Strategic Technology Trends for 2019. AI became a catch-all term that refers to any computer program that automatically does something 22 min read 22 min read Felix Kuestahler Jan 25, 2019 Member-only Python Tutorial: Twitter Account Retrieval of Swiss Government Members Start your journey into social media analysis of politicians Click on the link above the Title for a list of all tutorial articles. This is the first article. The tutorial will show you how to extract a list of tweeting Swiss Government Members via the Twitter API. 7 min read 7 min read Tobias Hill Jan 25, 2019 How I got 1% better accuracy by data augmentation It is hard to let go of things you put love into. And yes, I really thought I was done after the 5:th article in this series but then the accuracy/error metrics from the MNIST example started haunting me and I was quite sure that I could improve on 4 min read 4 min read Henry Feng Sr. BI Engineer | Product Analytic & DS | UMN MSBA | #Tech #Education | Medium List: https://pse.is/SGEXZ | Podcast: More from Medium Samuele Mazzanti in Towards Data Science Using Causal ML Instead of A/B Testing Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Antonio Blago in ILLUMINATION Why You Should (not) Become a Data Analyst in 2023! Justin Brooke How I Made $3,483,510.77 With a Membership Site in a Weird Niche Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4139,\n",
       "  'url': 'https://medium.com/datadriveninvestor/part-i-data-visualization-using-python-7b08799f820c',\n",
       "  'title': 'Part I\\u200a—\\u200aData visualization using\\xa0Python',\n",
       "  'subtitle': '-',\n",
       "  'claps': 17,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 4,\n",
       "  'publication': 'Data Driven Investor',\n",
       "  'date': '2019-01-26',\n",
       "  'clap_prop': 8.397205607555114e-06,\n",
       "  'text': \"DataDrivenInvestor Jan 25 2019 Listen Save Part Data visualization using Python Data visualization given data set presented graphical format help detection pattern trend correlation might go undetected text-based data first part data visualization tutorial series showing get started python graph Let 's get First need import pyplot matplotlib library python documentation found matplotlib library Python 2D plotting library allows generate plot scatter plot histogram bar chart etc line code matplotlib.pyplot collection command style function make matplotlib function like MATLAB article focusing scatter plot line graph diving code let 's look basic property using plotting Scatter plot scatter plot present data point individually used want show relationship two variable plt.scatter allows generation scatter plot X represents data used x-axis graph data used y-axis plot customized change color shape size opacity etc data point size plot etc covered next tutorial tutorial series Multiple data set also presented using single scatter plot done simply adding another plt.scatter statement Line Graph Line graph present data using single line connecting data point usually used visualize time series data detect trend data period time plt.plot used generate line graph multiple data set presented graph adding plt.plot statement required also create graph combining different type plot code shown generates graph show one data set scatter plot line graph Pie Chart pie chart used show percentage proportional data used trying compare part whole code size label data represented pie chart corresponding label color specify color section pie chart instance section 22.6 presented green Doughnut Chart chart petty much serf purpose pie chart However doughnut chart contain multiple data series unlike pie chart whcih contain single data series change pie chart doughnut chart adding circle origin shown line 6 brings u end article Hope article helped get basic understanding get started data visualization using python next time .. Adios 19 19 19 DataDrivenInvestor empowerment data knowledge expertise subscribe DDIntel http //ddintel.datadriveninvestor.com Jens Erik Gould Jan 25 2019 Mexico Planning Imminent Change Auctions article wa first published 2010 Bloomberg News 3 min read 3 min read Share idea million reader Gyst Audio Jan 25 2019 Surround Sound Next Wave Audio Ubiquity audio streaming platform proliferate appliance start talking online publisher eyeing speaker well screen Hayley Grgurich Gyst Audio 4 min read 4 min read Henrique Tom Jan 25 2019 Technical Analysis Limitations best market strategy use Academics argue fundamental everything else retail trader say technical run market fact Ive already written previous article opinion correct approach 80 fundamental 3 min read 3 min read Aleksey Aleks Weyman Jan 25 2019 Member-only 5 Ingenious Apps Making World Better 2019 Technology ha inarguably revolutionized world live sign slowing 5 min read 5 min read Brennan Baraban Jan 25 2019 Learn Machines Learn doe A.I work software capable predicting interpreting human behavior Quite simply machine learn 14 min read 14 min read Krishni Software Engineer ML enthusiast Medium Demetrio 8 easy plotting categorical variable seaborn Pandas Dataframe Okan Yenign Towards Dev plot Histogram Python Matplotlib Seaborn Plotly Avi Chawla Towards Data Science Building All-In-One Audio Analysis Toolkit Python Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"DataDrivenInvestor Jan 25, 2019 Listen Save Part I Data visualization using Python Data visualization is where a given data set is presented in a graphical format. It helps the detection of patterns, trends and correlations that might go undetected in text-based data. This is the first part of the data visualization tutorial series where we will be showing you how to get started with python graphs. Let's get to it! First of all, you need to import pyplot of the matplotlib library in python(documentation can be found here ). The matplotlib library is a Python 2D plotting library which allows you to generate plots, scatter plots, histograms, bar charts etc. with just a few lines of code. matplotlib.pyplot is a collection of command style functions which makes matplotlib function like MATLAB. In this article, we will be focusing on scatter plots and line graphs. Before diving into the code, let's look at some of the basic properties we will be using when plotting. Scatter plot A scatter plot presents each data point individually. They are used when you want to show the relationship between two variables. plt.scatter allows the generation of the scatter plot. X represents the data used for the x-axis of the graph and y the data used for the y-axis. These plots can be customized to change the color, shape, size, opacity etc. of the data points, size of the plot etc. which will be covered in the next tutorials of this tutorial series. Multiple data sets can also be presented using a single scatter plot. This can be done by simply adding another plt.scatter statement. Line Graph Line graphs present data using a single line connecting all the data points. They are usually used to visualize time series data to detect trends in data over a period of time. plt.plot is used to generate a line graph and multiple data sets can be presented in the same graph by adding plt.plot statements as required. We can also create graphs combining different types of plots. The code shown below generates a graph which shows one data set as a scatter plot and the other as a line graph. Pie Chart A pie chart is used to show percentage or proportional data. It is used when trying to compare parts of a whole. In the above code, sizes and labels are the data represented in the pie chart and its corresponding label. colors specify the color of each section of the pie chart. For instance, section  A  is  22.6%  and is presented in  green  . Doughnut Chart This chart petty much serves the same purpose as a pie chart. However, doughnut charts can contain multiple data series unlike pie charts, whcih can contain only a single data series. We can change a pie chart to a doughnut chart by adding a circle to its origin as shown in line 6 . This brings us to the end of this article. Hope this article helped you to get a basic understanding of how to get started with data visualization using python. Until next time..Adios! 19 19 19 More from DataDrivenInvestor empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com Jens Erik Gould Jan 25, 2019 Mexico Not Planning `Imminent Change in Auctions This article was first published in 2010 by Bloomberg News. 3 min read 3 min read Share your ideas with millions of readers. Gyst Audio Jan 25, 2019 Surround Sound: The Next Wave in Audio is Ubiquity As audio streaming platforms proliferate and our appliances start talking, online publishers are eyeing speakers as well as screens By Hayley Grgurich for Gyst Audio 4 min read 4 min read Henrique Tom Jan 25, 2019 Technical Analysis Limitations What are the best market strategies to use? Academics argue that it is fundamentals above everything else and most of the retail traders says that is technical that runs the market. In fact, as Ive already written on previous articles, in my opinion, the correct approach is 80% fundamental which 3 min read 3 min read Aleksey (Aleks) Weyman Jan 25, 2019 Member-only 5 Ingenious Apps Making Our World Better in 2019 Technology has inarguably revolutionized the world we live in, and theres no sign of it slowing down 5 min read 5 min read Brennan D Baraban Jan 25, 2019 Learn How Machines Learn How does A.I. work? How is software capable of predicting and interpreting human behavior? Quite simply, how do machines learn? 14 min read 14 min read Krishni Software Engineer | ML enthusiast More from Medium Demetrio 8 easy plotting categorical variables with seaborn for Pandas Dataframe Okan Yenign in Towards Dev How to plot Histogram in Python? (Matplotlib, Seaborn, Plotly) Avi Chawla in Towards Data Science Building an All-In-One Audio Analysis Toolkit in Python Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 378,\n",
       "  'url': 'https://towardsdatascience.com/wild-wide-ai-responsible-data-science-16b860e1efe9',\n",
       "  'title': 'Wild Wide AI: responsible data\\xa0science',\n",
       "  'subtitle': 'Who shoots first\\u200a—\\u200athe new race or the\\xa0human?',\n",
       "  'claps': 12,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 9,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 5.927439252391845e-06,\n",
       "  'text': \"Towards Data Science Mar 30 2019 Listen Save Wild Wide AI responsible data science shoot first new race human Data Science good thing u improves life make thing efficient effective lead better experience however miss-steps data-driven analysis ha already exhibited example data science tool intentionally unintentionally misused Data Science impartial often claimed data science algorithmic therefore biased yet saw example traditional evil discrimination exhibit data science ecosystem Bias inherited data process propelled amplified Transparency idea mindset set mechanism help prevent discrimination enable public debate establish trust make data science interact society way decision ha environment trust participant public Technology alone wont solve issue User engagement policy effort important Data responsibility Aspects responsibility data science ecosystem include fairness transparency diversity data protection area responsible data science new already edge top machine learning conference difficult interesting relevant problem Fairness Philosophers lawyer sociologist asking question many year data science context usually solve task predictive analytics predicting future performance behavior based past present observation dataset Statistical bias occurs model used solve task fit data well biased model somehow imprecise doe summarize data correctly Societal bias happens data model doe represent world correctly example occurs data representative case used data police going neighborhood use information crime particular neighborhood Societal bias also caused define world world trying impact predictive analytics world determine world like discrimination legal system two concept defining discrimination talk discrimination using term could uncomfortable racism gender sexual orientation Political correctness extreme sense ha place debate responsible data science able name concept able talk talk concept take corrective action Technical definition fairness Lets consider vendor assigning outcome member population basic case binary classification Positive outcome may offered employment accepted school offered loan offered discount Negative outcome may denied employment rejected school denied loan offered discount worry fairness outcome assigned member population Lets assume 40 got positive outcome sub-population however may treated differently process Lets assume know ahead time sub-population example red haired people Thus divide population two group people red hair people without red hair example observe 40 population got positive outcome 20 red haired received positive outcome 60 received positive outcome according definition observe disparate impact group red haired individual Another way denote situation statistical parity fails baseline definition fairness without conditioning quite sophistication using assessment real-life example court basic definition fairness written many law around globe dictate demographic individual receiving outcome demographic underlying population Assessing disparate impact vendor could say actually intend look hair color happens sensitive attribute dataset Instead vendor would say decided give positive outcome people whose hair long vendor denying accusation saying discriminating based hair color thing vendor ha adversely impacted red haired people intention care effect sub-population word blinding legal ethical excuse Removing hair color vendor process outcome assignment doe prevent discrimination occurring Disparate impact legally assessed impact intention Mitigating disparate impact detect violation statistical parity may want mitigate environment number positive outcome assign swap outcome take positive outcome somebody not-red haired group give someone else red haired group everyone agree swapping outcome individual used get positive outcome would stop getting would lead individual fairness stipulates two individual similar within particular task receive similar outcome tension group individual fairness easy resolve Individual v group fairness example individual fairness group fairness wa taken supreme court appears Ricci v. DeStefano case 2009 Firefighters took test promotion department threw test result none black firefighter scored high enough promoted fire department wa afraid could sued discrimination disparate impact admit result promote black firefighter lawsuit wa brought firefighter would eligible promotion werent promoted result wa individual fairness argument disparate treatment argument argued race wa used negatively impact case wa ruled favor white firefighter favor individual fairness Individual fairness equality everybody get box reach tree Group fairness equity view everybody get many box need able reach tree Equity cost society ha invest two intrinsically different world view logically decide one better two different point view isnt better one go back believe world world truth going somewhere middle important understand kind mitigation consistent kind belief system Formal definition fairness Friedler et al tease difference belief fairness mechanism logically follow belief paper 2016 construct space intrinsically state world made thing directly measure intelligence grit propensity commit crime risk-adverseness however want measure intelligence grit decide admit college want know propensity person recommit crime risk-adverseness justice raw property exhibited directly accessible Instead look observed space proxy greater lesser degree aligned property want measure intelligence proxy would SAT score grit would measured high-school GPA propensity commit crime family history risk-adverseness age decision space made would like decide performance college recidivism Fairness defined mapping construct space decision space via observe space Individual fairness equality belief observed space faithfully represents construct space example high-school GPA good measure grit Therefore mapping construct decision space ha low distortion Group fairness equity however say systematic distortion caused structural bias society bias going construct space observed space Furthermore distortion aligns group structure membership protected group society word society systematically discriminates continued References lecture responsible data science Harvard University Prof. Julia Stoyanovich New York University selected chapter Age Surveillance Capitalism book Shoshana Zuboff thought worry AI post Franois Chollet 10 10 10 Get email whenever Michel publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Kevin Luk Mar 30 2019 Member-only library load image Python difference Summarization Comparison .imread face computer vision project first need load image preprocessing various library perform imread want consolidate popular library loading image difference article go Libraries loading 3 min read 3 min read Share idea million reader Abhishek Mukherjee Mar 30 2019 Trust interpretability machine learning machine learning model always need interpretable Given choice interpretable model inaccurate non-interpretable model accurate wouldnt rather choose non-interpretable accurate model word reason sacrificing accuracy altar interpretability 9 min read 9 min read Christine Calo Mar 30 2019 Microsoft Introduction AI Part 1 Machine Learning bit like wanted learn Artificial Intelligence although felt little intimidated math involved Maybe thought concept difficult understand would depth recently completed Microsoft Introduction AI course 17 min read 17 min read Okoh Anita Mar 30 2019 Repetition Songs Python Tutorial One Ed Sheeran song case study Everyone ha heard song know song sound like carelessly say everyone define song word benefit doubt song according Wikipedia single work music typically intended sung 8 min read 8 min read Alex Blyakhman Mar 30 2019 Member-only Getting Started Google BigQuerys Machine Learning Titanic Dataset still Beta BigQuery ML ha available since mid last year however didnt get around working Google cloud-based Machine Learning offering recently non-data scientist first impression whats like ability run ML model 12 min read 12 min read Michel Kana Ph.D Husband Dad Founder Immersively.care Top Medium Writer 20 year AI Expert Harvard Empowering human-centered organization high-tech Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Akshad Singi Better Humans 4 Unsexy One-Minute Habits Save 30+ Hours Every Week Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Luay Rahil ILLUMINATION Elon Musk Eliminated Remote Work Working Home `` n't Work '' Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Mar 30, 2019 Listen Save Wild Wide AI: responsible data science Who shoots first the new race or the human? Data Science can do good things for us: it improves life, it makes things more efficient, more effective and leads to a better experience. There are however some miss-steps that data-driven analysis has already exhibited. Here are few examples where data science tools were intentionally or unintentionally misused: Is Data Science impartial? It is often claimed that data science is algorithmic and therefore cannot be biased. And yet, we saw examples above where all traditional evils of discrimination exhibit themselves in the data science ecosystem. Bias is inherited both in the data and in the process, is propelled and amplified. Transparency is an idea, a mindset, a set of mechanisms that can help prevent discrimination, enable public debate and establish trust. When we make data science, we interact with society. The way we do decisions has to be in an environment where we have trust from the participants, from the public. Technology alone wont solve the issue. User engagement, policy efforts are important. Data responsibility Aspects of responsibility in the data science ecosystem include: fairness, transparency, diversity and data protection. The area of responsible data science is very new but is already at the edge of all the top machine learning conferences because these are difficult but interesting and relevant problems. What is Fairness? Philosophers, lawyers, sociologists have been asking this question for many years. In the data science context we usually solve the task of predictive analytics, predicting future performance or behavior based on some past or present observation (dataset). Statistical bias occurs when models used to solve such tasks do not fit the data very well. A biased model is somehow imprecise and does not summarize the data correctly. Societal bias happens when the data or the model does not represent the world correctly. An example occurs when the data is not representative. This is the case if we only used the data for police going to the SAME neighborhood over and over, and we use this information only about crime from those particular neighborhoods. Societal bias can also be caused by how we define world. Is it the world as it is that we are trying to impact with predictive analytics or the world as it should be? Who should determine what the world should be like? What is discrimination? In most legal systems, there are two concepts defining discrimination: When we talk about discrimination, we are using terms which could be uncomfortable, such as racism, gender, sexual orientation. Political correctness in the extreme sense has no place in these debates about responsible data science. We have to be able to name concepts to be able to talk about them. Once we can talk about those concepts, we can take corrective action. Technical definition of fairness Lets consider vendors who are assigning outcomes to members of a population. This is the most basic case, a binary classification. Positive outcomes may be: offered employment, accepted to school, offered a loan, offered a discount. Negative outcomes may be: denied employment, rejected from school, denied a loan, not offered a discount. What we worry about in fairness is how outcome is assigned to members of a population. Lets assume that 40% got the positive outcome. Some sub-population however may be treated differently by this process. Lets assume that we know ahead of time what the sub-population is, for example red haired people. Thus we can divide our population into two groups: people with red hair, and people without red hair. In our example we observe that while 40% of the population got the positive outcome, only 20% of red haired received the positive outcome. 60% of the other received the positive outcome. Here, according to some definition, we observe disparate impact on the group of red haired individuals. Another way to denote this situation is that statistical parity fails. This is a baseline definition of fairness without conditioning. There is quite some sophistication about using such assessment in real-life, for example in courts. This basic definition of fairness, written into many laws around the globe, dictates that demographics of the individuals receiving any outcome are the same as demographics of the underlying population. Assessing disparate impact The vendor could say that he actually did not intend or did not look at all at hair color, which happens to be the sensitive attribute in the dataset. Instead the vendor would say that he decided to give the positive outcome to people whose hair is long. The vendor is denying the accusation and saying that he is not discriminating based on hair color. The thing is that the vendor has adversely impacted red haired people. It is not the intention that we care about, but the effect on the sub-population. In other words, blinding is not a legal or ethical excuse. Removing hair color from vendors process on outcome assignment does not prevent discrimination from occurring. Disparate impact is legally assessed on the impact, not on the intention. Mitigating disparate impact If we detect a violation of statistical parity, we may want to mitigate. In an environment in which we have a number of positive outcomes which we can assign, we have to swap some outcomes. We have to take a positive outcome from somebody in the not-red haired group and give it to someone else in the red haired group. Not everyone will agree with swapping outcomes. An individual who used to get the positive outcome would stop getting it any more. This would lead to individual fairness. It stipulates that any two individuals who are similar within a particular task should receive similar outcomes. There is a tension between group and individual fairness that is not easy to resolve. Individual vs group fairness An example in which individual fairness and group fairness was taken to the supreme court appears in the Ricci v. DeStefano case in 2009 . Firefighters took a test for promotion, and the department threw out the test results because none of the black firefighters scored high enough to be promoted. The fire department was afraid that they could be sued for discrimination and disparate impact if they were to admit results and not promote any black firefighter. But then the lawsuit was brought by the firefighters who would have been eligible for promotion but who werent promoted as a result of this. There was an individual fairness argument, a disparate treatment argument. They argued that race was used to negatively impact them. This case was ruled in favor of white firefighters, in favor of individual fairness. Individual fairness is equality, everybody gets the same box to reach the tree. Group fairness is the equity view, everybody gets as many boxes as they need to be able to reach the tree. Equity costs more because society has to invest more. These are two intrinsically different world views that we cannot logically decide which one is better. These are just two different points of view, there isnt a better one. They go back to what we believe a world as it is, is a world as it should be. The truth is going to be somewhere in the middle. It is important to understand which kinds of mitigation are consistent with which kinds of belief systems. Formal definition of fairness Friedler et. al. tease out the difference between beliefs about fairness and mechanisms that logically follow from those beliefs in their paper from 2016. The construct space is intrinsically the state of the world. It is made of things we cannot directly measure such as intelligence, grit, propensity to commit crime and risk-adverseness. We however want to measure intelligence and grit when we decide who to admit to college. We want to know the propensity of a person to recommit crime and his risk-adverseness in justice. These are raw properties which are exhibited and not directly accessible. Instead we look at the observed space where there are proxies, which are to a greater or lesser degree aligned with the properties that we want to measure. For intelligence the proxy would be SAT score, grit would be measured by high-school GPA, propensity to commit crime by family history and risk-adverseness by age. The decision space is then made of what we would like to decide: performance in college and recidivism. Fairness is defined here as a mapping from the construct space to the decision space, via the observe space. Individual fairness (equality) believes that the observed space faithfully represents the construct space. For example high-school GPA is a good measure of grit. Therefore the mapping from construct to decision space has low distortion. Group fairness (equity) however says that there is a systematic distortion caused by structural bias, society bias when going from the construct space to observed space. Furthermore this distortion aligns with groups structure, with membership in protected groups in our society. In other words the society systematically discriminates. to be continued References : lecture on responsible data science at Harvard University by Prof. Julia Stoyanovich (New York University) selected chapters from The Age of Surveillance Capitalism book by Shoshana Zuboff thoughts from What worries me about AI post by Franois Chollet. 10 10 10 Get an email whenever Michel publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Kevin Luk Mar 30, 2019 Member-only What libraries can load image in Python and what are their difference? Summarization & Comparison of .imread() When we face computer vision project, first of all we need to load the images before any preprocessing. There are various libraries out there to perform imread() . Here I want to consolidate the popular libraries for loading image and their difference. This article will go through: Libraries for loading 3 min read 3 min read Share your ideas with millions of readers. Abhishek Mukherjee Mar 30, 2019 Trust and interpretability in machine learning Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldnt you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability? 9 min read 9 min read Christine Calo Mar 30, 2019 Microsoft Introduction to AI Part 1 Machine Learning Are you a bit like me and have wanted to learn about Artificial Intelligence although felt a little intimidated by the maths involved? Maybe you thought the concepts were too difficult to understand and you would be out of your depth. I recently completed the Microsoft Introduction to AI course 17 min read 17 min read Okoh Anita Mar 30, 2019 Repetition in Songs: A Python Tutorial One of Ed Sheeran songs as a case study Everyone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung 8 min read 8 min read Alex Blyakhman Mar 30, 2019 Member-only Getting Started with Google BigQuerys Machine Learning Titanic Dataset While still in Beta, BigQuery ML has been available since mid last year; however, I didnt get around to working with this Google cloud-based Machine Learning offering until recently. As a non-data scientist, my first impression whats not to like? After all, the ability to run ML models from 12 min read 12 min read Michel Kana, Ph.D Husband & Dad. Founder @Immersively.care. Top Medium Writer. 20 years in IT. AI Expert @Harvard. Empowering human-centered organizations with high-tech. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Akshad Singi in Better Humans 4 Unsexy One-Minute Habits That Save Me 30+ Hours Every Week Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Luay Rahil in ILLUMINATION Elon Musk Eliminated Remote Work Because Working From Home \"Doesn\\'t Work\" Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4101,\n",
       "  'url': 'https://towardsdatascience.com/python-tutorial-retrieve-a-list-of-swiss-government-members-from-twitter-d5f999555f98',\n",
       "  'title': 'Python Tutorial: Retrieve A List Of Swiss Government Members From\\xa0Twitter',\n",
       "  'subtitle': 'The tutorial will show\\xa0you…',\n",
       "  'claps': 2,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-01-26',\n",
       "  'clap_prop': 9.879065420653076e-07,\n",
       "  'text': 'Towards Data Science Jan 25 2019 Member-only Listen Save Python Statistic Tutorial Series Python Tutorial Twitter Account Retrieval Swiss Government Members Start journey social medium analysis politician Click link Title list tutorial article first article tutorial show extract list tweeting Swiss Government Members via Twitter API extracted data put Panda Dataframe visualized via powerful Plotly package result look follows get data twitter account SoMePolis maintains list Swiss government member Twitter account goal program fetch list member government member tweeting extract key figure follower friend Create Twitter Developer Account App confirmation create first App developer account done Twitter generate API key token use program communicate Twitter API time write program create class TwitterClient offer two method get_government_members create_plotly_table program use following python package Thats using four library ready go __init__ Within Class initialization method establish connection Twitter API use key token loaded yaml file located outside current directory secret directory common approach locate sensitive data directory life local computer never checked remote source system Github Yaml human-readable data serialization language commonly used configuration file could used many application data stored ha straightforward syntax case four key value pair security token loaded side information enhance program require additional non-sensitive configuration data would introduce second yaml file public information check-in normal source code next step use OAuth authenticate Twitter gain access API OAuth open standard access delegation commonly used way Internet user grant website application access information website without giving password I.e. provide secret access token key create private class variable _api hold reference Twitter API Via reference object interact Twitter API method available described use private variable API want offer API directly consumer class rather provide higher level method getgovernment_members Defining scope class variable public protected private important design decision designing class used within program module get_government_members method fetch twitter politician account described list BundesparlamentarierInnen twitter account SoMePolis first step get Twitters list SomePolis account search required one found use list.id retrieve member twitter account list retrieve member list use so-called Cursor position 7 important concept API interface manage data retrieval arbitrary large list i.e. list may million entry use program following code sequence would return first 25 member list strategy Twitter API ensure deliver huge data amount single request client program case want member call multiple time list_members request packet 25 list member account returned concept called pagination used throughout public APIs various provider find information Tweeply Cursor Tutorial list ha 110 member mean list_members method called five time end result variable 110 Twitter Accounts account create_plotly_table Finally extract list twitter account certain data processing prepare Panda Dataframe two-dimensional size-mutable potentially heterogeneous tabular data structure labeled ax row column list Twitter account extract create column data frame achieved via following command sequence iterate list extract record screen_name Finally create plotly Table two main module need generate Plotly graph create account http //plot.ly first run application generated table uploaded result get table similar like one account Account Creation Plotly first time run program asked sign create account plotly created account retrieve access token creating new one necessary store file .credentials home directory folder .plotly Thats today exercise try add following code sequence Exercise Try identify party government person anaylzing screen_name description guiding help check following guide party Switzerland Main party CVP FDP SP SVP well Grne high chance abbreviation mentioned person see screenshot build new Panda Column add Data Frame visualizing source code lesson1.py found http //github.com/talfco/clb-sentiment solution exercise Originally published dev.cloudburo.net January 26 2019 4 4 4 Enjoy read Reward writer Beta tip go Felix Kuestahler third-party platform choice letting know appreciate story Towards Data Science home data science Medium publication sharing concept idea code Partha Deka Jan 25 2019 Empowering citizen data scientist hardware design manufacturing Improving productivity hardware design manufacturing professional advanced AI tool Authors Partha Deka Rohit Mittal citizen data scientist Expert data scientist rely custom coding make sense data use case could data cleansing data imputation creating segment finding pattern data building 4 min read 4 min read Share idea million reader Lak Lakshmanan Jan 25 2019 Bayesian hyper-parameter tuning blackbox model Optimization arbitrary function Cloud ML Engine Google Cloud ML Engine offer hyper-parameter tuning service us Bayesian method restricted TensorFlow scikit-learn fact even limited machine learning use Bayesian approach tune pretty much blackbox model demonstrate Ill tune traffic 3 min read 3 min read Aaron Frederick Jan 25 2019 Member-only Creating AI GameBoy Part 2 Collecting Data Screen Welcome part 2 Creating AI GameBoy missed Part 1 Coding Controller click catch edition going intelligently get information game various image processing classification technique important 5 min read 5 min read Semi Koen Jan 25 2019 Member-only Statistics Grammar Data Science Part 2/5 Statistics refresher kick start Data Science journey 2nd article Statistics Grammar Data Science series covering various type probability distribution plot Revision Bookmarks rest article easy access Article Series Part 1 Data Types Measures Central Tendency Measures 6 min read 6 min read Thomas Nield Jan 25 2019 Member-only Data Science Become Vague Lets Specialize Break would opposed downplaying term data science breaking specialized discipline misunderstand think global data science movement wa necessary positive impact curmudgeon corporate world campaign ha everybody bought 10 min read 10 min read Felix Kuestahler Exploring emerging world decentralization content opinion expressed context legal entity http //atnode.ch/ Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month ibrahim zahir 7 killer web apps start using 2023 Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Jan 25, 2019 Member-only Listen Save Python Statistic Tutorial Series Python Tutorial: Twitter Account Retrieval of Swiss Government Members Start your journey into social media analysis of politicians Click on the link above the Title for a list of all tutorial articles. This is the first article. The tutorial will show you how to extract a list of tweeting Swiss Government Members via the Twitter API. The extracted data will be put into a Panda Dataframe and then visualized via the powerful Plotly package. The result will look as follows: Where to get the data from? The twitter account SoMePolis maintains a list of Swiss government members, who have a Twitter account. The goal of the program is to fetch all list members (government members who are tweeting) and extract some key figures (followers and friends) Create a Twitter Developer Account and App After the confirmation, you then create your first App in your developer account. Having done that, Twitter will generate API keys and tokens which you will use in your program to communicate with the Twitter API. Its time to write the program. We will create a class TwitterClient which offers two methods get_government_members and create_plotly_table The program will use the following python packages Thats is by using these four libraries we are ready to go. __init__ Within the Class initialization method, we have to establish a connection with the Twitter API. For that, we use our keys and tokens which are loaded from a yaml file, which is located outside of the current directory in the secret directory. This is a common approach, to locate sensitive data in a directory which only lives on the local computer and never will be checked in a remote source system as Github. Yaml is a human-readable data serialization language. It is commonly used for configuration files but could be used in many applications where data is being stored. It has a straightforward syntax, for our case four <key>:<value> pairs for the security tokens, which are loaded: As a side information, when you enhance your program and require additional non-sensitive configuration data, you would introduce a second yaml file with public information, which you can check-in with the normal source code. In the next step, we use OAuth to authenticate with Twitter and gain access to their API. OAuth is an open standard for access delegation, commonly used as a way for Internet users to grant websites or applications access to their information on other websites but without giving them the passwords. I.e., you will provide them with your secret access token and keys. We create a private class variable _api which holds the reference to the Twitter API. Via this reference object you can now interact with the Twitter API, all methods available are described here. We use a private variable for the API because we don t want to offer the API directly to consumers of our classes but rather provide higher level methods as getgovernment_members . Defining the scope of your class variables public, protected, private is an important design decision, when you are designing classes which will be used within a program or module. get_government_members In this method, we will fetch all twitter politician accounts out of the above described list BundesparlamentarierInnen of the twitter account SoMePolis. In a first step, we get all Twitters lists of the SomePolis account and search for the required one. Having found it, we will use its list.id to retrieve all members (twitter accounts) of the list. To retrieve all members of a list we have to use a so-called Cursor ( position 7 ) Its an important concept in API interfaces to manage data retrieval of arbitrary large lists (i.e., a list may have millions of entries) If you use in the program the following code sequence, you would just return the first 25 members of the list. With this strategy, the Twitter API will ensure that it will not have to deliver huge data amount in a single request to a client program. In case you want all members you have to call multiple times the list_members. In each request, a packet of 25 list member accounts will be returned. This concept is called pagination and is used throughout public APIs of various providers (you can find more information here: Tweeply Cursor Tutorial ) The list has about 110 members, that means the list_members method will be called five times. In the end, the result variable will have the 110 Twitter Accounts accounts. create_plotly_table Finally, we extract out of the list of twitter accounts certain data for further processing. We will prepare a Panda Dataframe, which is a two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). Of our list of Twitter accounts we will extract We have to create columns for the data frame which is achieved via the following command sequence We iterate through our lists and extract for each record the screen_name . Finally, we create a plotly Table: There are the two main modules that we will need to generate our Plotly graphs. You will have to create an account at https://plot.ly (when you first run the application), to which your generated table will be uploaded. As a result, you will get a table similar like this one in your account. Account Creation at Plotly The first time you run the program, you will be asked to sign up and create an account at plotly. Having created an account you will have to retrieve the access token (by creating a new one if necessary) And store it in the file .credentials in the home directory in the folder .plotly Thats it for today as an exercise try to add the following code sequence. Exercise Try to identify the party of each government person, by anaylzing its screen_name or description. As a guiding help, check out the following guide of parties in Switzerland . Main parties are CVP, FDP, SP, SVP as well as Grne. There is a high chance that the abbreviation is mentioned by a person (see screenshot below). So build up a new Panda Column and add it to the Data Frame for visualizing. The source code ( lesson1.py ) can be found here: https://github.com/talfco/clb-sentiment The solution to the exercise here Originally published at  dev.cloudburo.net  on January 26, 2019. 4 4 4 Enjoy the read? Reward the writer. Beta Your tip will go to Felix Kuestahler through a third-party platform of their choice, letting them know you appreciate their story. More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Partha Deka Jan 25, 2019 Empowering a citizen data scientist for hardware design & manufacturing Improving productivity of a hardware design and manufacturing professional with an advanced AI tool Authors: Partha Deka and Rohit Mittal What is a citizen data scientist? Expert data scientists rely on custom coding to make sense out of data. The use case could be data cleansing, data imputation, creating segments, finding patterns in the data, building 4 min read 4 min read Share your ideas with millions of readers. Lak Lakshmanan Jan 25, 2019 How to do Bayesian hyper-parameter tuning on a blackbox model Optimization of arbitrary functions on Cloud ML Engine Google Cloud ML Engine offers a hyper-parameter tuning service that uses Bayesian methods. It is not restricted to TensorFlow or scikit-learn. In fact, it is not even limited to machine learning. You can use the Bayesian approach to tune pretty much any blackbox model. To demonstrate, Ill tune a traffic 3 min read 3 min read Aaron Frederick Jan 25, 2019 Member-only Creating AI for GameBoy Part 2: Collecting Data From the Screen Welcome to part 2 of Creating an AI for GameBoy! If you missed Part 1: Coding a Controller, click here to catch up. In this edition, I will be going over how to intelligently get information out of the game through various image processing and classification techniques. This is important 5 min read 5 min read Semi Koen Jan 25, 2019 Member-only Statistics is the Grammar of Data Science Part 2/5 Statistics refresher to kick start your Data Science journey This is the 2nd article of the Statistics is the Grammar of Data Science series, covering the various types of probability distributions and how we plot them. Revision Bookmarks to the rest of the articles for easy access: Article Series Part 1: Data Types | Measures of Central Tendency | Measures of 6 min read 6 min read Thomas Nield Jan 25, 2019 Member-only Data Science Has Become Too Vague Lets Specialize and Break it Up! I would not be opposed to downplaying the term data science and breaking it up into specialized disciplines. Do not misunderstand, I think the global data science movement was necessary and had a positive impact on the curmudgeon corporate world. But the campaign has been won and everybody is bought 10 min read 10 min read Felix Kuestahler Exploring the emerging world of decentralization. All content and opinion expressed in the context of my legal entity https://atnode.ch/ More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month ibrahim zahir 7 killer web apps you should start using in 2023 Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4097,\n",
       "  'url': 'https://towardsdatascience.com/the-evolution-of-the-us-electric-grid-f18bce6473d5',\n",
       "  'title': 'The Evolution Of The US Electric\\xa0Grid',\n",
       "  'subtitle': '-',\n",
       "  'claps': 1,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 4,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-01-26',\n",
       "  'clap_prop': 4.939532710326538e-07,\n",
       "  'text': \"Towards Data Science Jan 25 2019 Member-only Listen Save Evolution US Electric Grid past 15 year US electric grid ha undergone massive shift mostly better Renewable energy natural gas coal GHG emission see US grid ha evolved recent year Lets also put table form ease access time-span every energy source declined except natural gas non-hydro renewable energy good news environment emission intensity electric sector ha dropped new low chart show emission intensity US power sector mean 1 MWh electricity result 35 le emission Q2 2018 recent data point available 2003 gain offset increased electricity production total US power sector emission still substantially lower 2003 Total electric-sector emission still saw 26 reduction time-frame Due electricity sector may finally lose mantle largest emitting sector US transportation sector Natural Gas Coal Since 2003 profound shift occurred coal natural gas natural gas ha steady rise coal ha seen continual decline isnt likely stop soon renewable energy natural gas still expanding rapid pace future look increasingly grim coal taking look coal capacity change peak future coal electric grid Preliminary estimate showing 2018 continued trend large coal retirement possibly even breaking record coal addition coming complete halt decline look terminal Renewable Energy renewable energy break see energy source producing gain rise wind solar energy evident 2003 hydroelectricity made almost 78 renewable energy production 2018 share dropped 39.7 36 renewable energy wind 13.6 solar Wind energy likely overcome hydro near future According EIA 2017 wind generated 6.3 electricity solar generated 1.3 figure likely higher 2018 full year data available natural gas ha outpaced renewable growth date EIA predicts lead charge Meanwhile coal share generation expected continue decline next year Causes large shift past 15 year result shifting economics Natural gas solar wind energy experienced significant decline cost recent year 2 2 2 Get email whenever Brayden Gerrard publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code zgr Gen Jan 25 2019 Member-only Notes Artificial Intelligence Machine Learning Deep Learning curious people AI ha intriguing topic 2018 according McKinsey mentioned key enabler 1 3 spot Gartner Top 10 Strategic Technology Trends 2019 AI became catch-all term refers computer program automatically doe something 22 min read 22 min read Share idea million reader Felix Kuestahler Jan 25 2019 Member-only Python Tutorial Twitter Account Retrieval Swiss Government Members Start journey social medium analysis politician Click link Title list tutorial article first article tutorial show extract list tweeting Swiss Government Members via Twitter API extracted data put Panda Dataframe 7 min read 7 min read Tobias Hill Jan 25 2019 got 1 better accuracy data augmentation hard let go thing put love yes really thought wa done 5 th article series accuracy/error metric MNIST example started haunting wa quite sure could improve 4 min read 4 min read Partha Deka Jan 25 2019 Empowering citizen data scientist hardware design manufacturing Improving productivity hardware design manufacturing professional advanced AI tool Authors Partha Deka Rohit Mittal citizen data scientist Expert data scientist rely custom coding make sense data use case could data cleansing data imputation creating segment finding pattern data building 4 min read 4 min read Lak Lakshmanan Jan 25 2019 Bayesian hyper-parameter tuning blackbox model Optimization arbitrary function Cloud ML Engine Google Cloud ML Engine offer hyper-parameter tuning service us Bayesian method restricted TensorFlow scikit-learn fact even limited machine learning use Bayesian approach tune pretty much blackbox model demonstrate Ill tune traffic 3 min read 3 min read Brayden Gerrard Electric Vehicles Green Energy Data Science Contact gerrard.brayden gmail dot com Medium Barry Gander Ancient Rome Fall Real Story Even Scarier America Connects Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Laurel B. Miller Sweary Mommy Naughty Snaps Observing Wannabe Model Attempts Hunt Clients Wild Luay Rahil ILLUMINATION Elon Musk Eliminated Remote Work Working Home `` n't Work '' Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Jan 25, 2019 Member-only Listen Save The Evolution Of The US Electric Grid Over the past 15 years, the US electric grid has undergone some massive shifts (mostly for the better). Renewable energy and natural gas are up, while coal and GHG emissions are down. Here we can see how the US grid has evolved in recent years: Lets also put it in table form for ease of access: So during this time-span, every energy source declined except for natural gas and non-hydro renewable energy. This is good news for the environment, as emissions intensity in our electric sector has dropped to new lows: This chart shows the emissions intensity of the US power sector. That means that 1 MWh of electricity results in over 35% less emissions in Q2 2018 (the most recent data point available) than it did in 2003. Some of these gains have been offset from increased electricity production, but total US power sector emissions are still substantially lower than in 2003: Total electric-sector emissions still saw a 26% reduction during that time-frame. Due to this, the electricity sector may finally lose its mantle as the largest emitting sector in the US to the transportation sector. Natural Gas And Coal Since 2003, the most profound shift occurred between coal and natural gas. While natural gas has been on a steady rise, coal has seen a continual decline that isnt likely to stop soon: With both renewable energy and natural gas still expanding at rapid pace, the future looks increasingly grim for coal. By taking a look at coal capacity changes, we can peak into the future of coal in the electric grid: Preliminary estimates are showing that 2018 continued the trend of large coal retirements possibly even breaking the record. With coal additions coming to a complete halt, the decline looks to be terminal from here. Renewable Energy From renewable energy, we can break it down further to see what energy sources are producing the gains. The rise of wind and solar energy is very evident. In 2003, hydroelectricity made up almost 78% of renewable energy production. By 2018, that share had dropped to 39.7%. 36% of renewable energy is now wind, and 13.6% is now solar. Wind energy is likely to overcome hydro in the near future. According to the EIA , in 2017 wind generated 6.3% of electricity while solar generated 1.3%. Both of these figures are likely to be higher in 2018 once the full year of data is available. While natural gas has outpaced renewable growth to date, the EIA predicts that they will lead the charge from here on out. Meanwhile, coals share of generation is expected to continue to decline in the next few years. Causes The large shifts over the past 15 years have been a result of shifting economics. Natural gas, solar, and wind energy all experienced significant declines in cost in recent years. 2 2 2 Get an email whenever Brayden Gerrard publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. zgr Gen Jan 25, 2019 Member-only Notes on Artificial Intelligence, Machine Learning and Deep Learning for curious people AI has been the most intriguing topic of 2018 according to McKinsey. It is mentioned as the key enabler now at the #1 and #3 spot of Gartner Top 10 Strategic Technology Trends for 2019. AI became a catch-all term that refers to any computer program that automatically does something 22 min read 22 min read Share your ideas with millions of readers. Felix Kuestahler Jan 25, 2019 Member-only Python Tutorial: Twitter Account Retrieval of Swiss Government Members Start your journey into social media analysis of politicians Click on the link above the Title for a list of all tutorial articles. This is the first article. The tutorial will show you how to extract a list of tweeting Swiss Government Members via the Twitter API. The extracted data will be put into a Panda Dataframe and then 7 min read 7 min read Tobias Hill Jan 25, 2019 How I got 1% better accuracy by data augmentation It is hard to let go of things you put love into. And yes, I really thought I was done after the 5:th article in this series but then the accuracy/error metrics from the MNIST example started haunting me and I was quite sure that I could improve on 4 min read 4 min read Partha Deka Jan 25, 2019 Empowering a citizen data scientist for hardware design & manufacturing Improving productivity of a hardware design and manufacturing professional with an advanced AI tool Authors: Partha Deka and Rohit Mittal What is a citizen data scientist? Expert data scientists rely on custom coding to make sense out of data. The use case could be data cleansing, data imputation, creating segments, finding patterns in the data, building 4 min read 4 min read Lak Lakshmanan Jan 25, 2019 How to do Bayesian hyper-parameter tuning on a blackbox model Optimization of arbitrary functions on Cloud ML Engine Google Cloud ML Engine offers a hyper-parameter tuning service that uses Bayesian methods. It is not restricted to TensorFlow or scikit-learn. In fact, it is not even limited to machine learning. You can use the Bayesian approach to tune pretty much any blackbox model. To demonstrate, Ill tune a traffic 3 min read 3 min read Brayden Gerrard Electric Vehicles | Green Energy | Data Science | Contact: gerrard.brayden@gmail dot com More from Medium Barry Gander Ancient Rome Did Not Fall: Why Real Story is Even Scarier for America and How It Connects to Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Laurel B. Miller in Sweary Mommy Naughty Snaps: Observing the Wannabe Model as She Attempts to Hunt Clients in the Wild Luay Rahil in ILLUMINATION Elon Musk Eliminated Remote Work Because Working From Home \"Doesn\\'t Work\" Help Status Writers Blog Careers Privacy Terms About Text to speech'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(results, key=lambda x: x[\"claps\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "396b557f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 3187,\n",
       "  'url': 'https://towardsdatascience.com/how-to-use-python-features-in-your-data-analytics-project-e8032374d6fc',\n",
       "  'title': 'How to use Python features in your data analytics project',\n",
       "  'subtitle': 'Python tutorial in Azure using OO,\\xa0NumPy…',\n",
       "  'claps': 22,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 9,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-25',\n",
       "  'clap_prop': 1.0866971962718383e-05,\n",
       "  'text': 'Towards Data Science Apr 25 2019 Listen Save use Python feature data analytics project Python tutorial Azure using OO NumPy panda SQL PySpark 1 Introduction lot company moving cloud consider tooling shall used data analytics On-premises company mostly use propriety software advanced analytics BI reporting However tooling may logical choice cloud environment Reasons 1 lack integration cloud provider 2 lack big data support 3 lack support new use case machine learning deep learning Python general-purpose programming language widely used data analytics Almost cloud data platform offer Python support often new feature become available Python first Python seen Swiss Army knife data analytics 2 Objective tutorial two project created take account important feature Python Projects described follows Therefore following step executed standalone tutorial focus learn different aspect Python focus le deep dive separate aspect case interested deep learning see devops AI refer previous blog focus security see 3 Prerequisites following resource need created 4 OO NumPy panda sql Jupyter DSVM part sample Python project three class Using class football player data registered following step executed 4a Get started Log Windows Data Science Virtual Machine DSVM desktop overview icon preinstalled component found Click Jupyter short cut start Jupyter session Subsequently open Jupyter command line session found taskbar Copy URL open Firefox session download following notebook desktop DSVM Finally select upload notebook jupyter session click run button menu run code cell important part notebook also discussed remaining chapter 4b Object-oriented OO programming part tutorial inspired following tutorial Theophano Mitsa part three class created keep track football player data snippet first class Player found following seen class Subsequently class FirstTeamPlayer found following seen class Finally snippet training class found following seen class example three class instantiated used found final snippet 4c Matrix analytics using NumPy NumPy fundamental package scientific computing Python tutorial used matrix analytics Notice attribute _rawData already encapsulated Player class NumPy array NumPy often used Matplotlib visualize data snippet data taken player class matrix operation done basic advanced Full example found github project 4d Statistical analytics using panda Pandas package high-performance easy-to-use data structure data analysis tool Python hood panda us NumPy array structure tutorial used calculate basic statistic snippet data taken player class statistic operation done Full example found github project 4e Read/write database Finally data written SQL database tutorial MSSQL database used part DSVM Look Microsoft SQL Server Management Studio SSMS icon found taskbar start new session Log using Windows Authentication see also Look New Query menu start new query session execute following script Finally data written read database Pandas dataframes used see also snippet 5 PySpark Azure Databricks Spark cluster previous chapter code wa run single machine case data generated advanced calculation need done e.g deep learning possibility take heavier machine thus scale execute code compute distributed VMs Spark analytics framework distribute compute VMs thus scale adding VMs work time efficient supercomputer work Python used Spark often referred PySpark tutorial Azure Databricks used Apache Spark-based analytics platform optimized Azure following step executed 5a Get started Start Azure Databricks workspace go Cluster Create new cluster following setting Subsequenly select Workspace right-click select import radio button select import following notebook using URL See also picture Select notebook imported 4b attach notebook cluster created 4a Make sure cluster running otherwise start Walk notebook cell cell using shortcut SHIFT+ENTER Finally want keep track model create HTTP endpoint model and/or create DevOps pipeline project see advanced DevOps AI tutorial focus security see 5b Setup project project machine learning model created predicts income class person using feature age hour week working education following step executed Notice pyspark.ml library used build model also possible run scikit-learn library Azure Databricks however work would done driver master node compute distributed See snippet pyspark package used 6 Conclusion tutorial two Python project created follows lot company consider tooling use cloud data analytics almost cloud data analytics platform Python support therefore Python seen Swiss Army knife data analytics tutorial may helped explore possibilites Python 23 1 23 23 1 Towards Data Science home data science Medium publication sharing concept idea code Ryan Burge Apr 25 2019 Member-only Religious Composition Two Major Parties make lot data visualization time pretty good idea result calculation going look like write code fact result obvious dont even post anywhere 4 min read 4 min read Share idea million reader Ashutosh Singh Apr 25 2019 Member-only Classifying Products Banned Approved using Text Mining- Part II part explain optimize existing Machine Learning model Part deployment ML model using Flask previous article series discussed business problem shown train model using fastText classification Banned Approved product based information like Product Name product description specification 5 min read 5 min read Favio Vzquez Apr 25 2019 Jungle Koalas Pandas Optimus Spark expect newest library Databricks Koalas Optimus framework Apache Spark 3.x excited data science probably know Spark+AI latest summit started yesterday April 24th 2019 great thing talk spin-off youve following co-created 8 min read 8 min read Rinu Gour Apr 25 2019 Member-only Complete Guide Learn R R Programming Technology open source programming language Also R programming language latest cutting-edge tool R Basics hottest trend Moreover R command line interface C.L.I consists prompt usually character History R John Chambers colleague developed R Bell Laboratories Basically 8 min read 8 min read Sriram Parthasarathy Apr 25 2019 Member-only forecast sale revenue Compare various forecasting approach 100 method forecast sale question becomes one choose article briefly cover popular way forecast sale compare method key metric Depending use case customer may ok simple 5 min read 5 min read Ren Bremer Data Solution Architect Microsoft working Azure service ADFv2 ADLSgen2 Azure DevOps Databricks Function Apps SQL Opinions mine Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 25, 2019 Listen Save How to use Python features in your data analytics project Python tutorial in Azure using OO, NumPy, pandas, SQL, PySpark 1. Introduction A lot of companies are moving to cloud and consider what tooling shall be used for data analytics. On-premises, companies mostly use propriety software for advanced analytics, BI and reporting. However, this tooling may not be the most logical choice in a cloud environment. Reasons can be 1) lack of integration with cloud provider, 2) lack of big data support or 3) lack of support for new use cases such as machine learning and deep learning. Python is a general-purpose programming language and is widely used for data analytics. Almost all cloud data platforms offer Python support and often new features become available in Python first. In this, Python can be seen as the Swiss Army knife of data analytics. 2. Objective In this tutorial, two projects are created that take into account important features of Python. Projects are described as follows: Therefore, the following steps are executed: It is a standalone tutorial in which the focus is to learn the different aspects of Python. The focus is less to deep dive in the separate aspects. In case you more interested in deep learning, see here or in devops for AI, refer to my previous blogs, here and with focus on security, see here . 3. Prerequisites The following resources need to be created: 4. OO, NumPy, pandas and sql with Jupyter on DSVM In this part, a sample Python project with three classes. Using these classes, football players data is registered. The following steps are executed: 4a. Get started Log in to your Windows Data Science Virtual Machine (DSVM). On the desktop, an overview of icons of preinstalled components can be found. Click on Jupyter short cut to start a Jupyter session. Subsequently, open the Jupyter command line session that can be found in the taskbar. Copy the URL and open this in a Firefox session. Then download the following notebook to the desktop of your DSVM: Finally, select to upload the notebook in your jupyter session. Then click on the run button in the menu to run code in a cell. The most important parts of the notebook are also discussed in the remaining of the chapter. 4b. Object-oriented (OO) programming This part of the tutorial is inspired by the following tutorial by Theophano Mitsa. In this part, three classes are created to keep track of football players data. A snippet of the first class Player can be found below: The following can be seen in this class: Subsequently, class FirstTeamPlayer can be found below: The following can be seen in this class: Finally, a snippet of the training class can be found below: The following can be seen in this class: An example how the three classes are instantiated and are used can be found in the final snippet below: 4c. Matrix analytics using NumPy NumPy is the fundamental package for scientific computing with Python. In this tutorial it will be used to do matrix analytics. Notice that attribute _rawData is already encapsulated in the Player class as a NumPy array. NumPy is often used with Matplotlib to visualize data. In the snippet below, the data is taken from player class and then some matrix operations are done, from basic to more advanced. Full example can be found in the github project. 4d. Statistical analytics using pandas Pandas is the package for high-performance, easy-to-use data structures and data analysis tools in Python. Under the hood, pandas uses NumPy for its array structure. In this tutorial it will be used to calculate some basic statistics. In the snippet below, the data is taken from player class and then some statistics operations are done. Full example can be found in the github project. 4e. Read/write to database Finally, the data will be written to a SQL database. In this tutorial, the MSSQL database is used that is part of the DSVM. Look for the Microsoft SQL Server Management Studio (SSMS) icon that can be found in taskbar and start a new session. Log in using Windows Authentication, see also below. Look for New Query in the menu and start a new query session. Then execute the following script: Finally, the data can be written to and read from the database. Pandas dataframes will be used for this, see also the snippet below. 5. PySpark with Azure Databricks on Spark cluster In the previous chapter, all code was run on a single machine. In case more data is generated or more advanced calculations need to be done (e.g. deep learning), the only possibility is to take a heavier machine an thus to scale up to execute the code. That is, compute cannot be distributed to other VMs. Spark is an analytics framework that can distribute compute to other VMs and thus can scale out by adding more VMs to do work. This is most of times more efficient than having a supercomputer doing all the work. Python can be used in Spark and is often referred to as PySpark. In this tutorial, Azure Databricks will be used that is an Apache Spark-based analytics platform optimized for the Azure. In this, the following steps are executed. 5a. Get started Start your Azure Databricks workspace and go to Cluster. Create a new cluster with the following settings: Subsequenly, select Workspace, right-click and then select import. In the radio button, select to import the following notebook using URL: See also picture below: Select the notebook you imported in 4b and attach the notebook to the cluster you created in 4a. Make sure that the cluster is running and otherwise start it. Walk through the notebook cell by cell by using shortcut SHIFT+ENTER. Finally, if you want to keep track of the model, create an HTTP endpoint of the model and/or create a DevOps pipeline of the project, see my advanced DevOps for AI tutorial here , and with focus on security, see here . 5b. Setup of project In this project, a machine learning model is created that predicts the income class of a person using features as age, hours of week working, education. In this, the following steps are executed: Notice that pyspark.ml libraries are used to build the model. It is also possible to run scikit-learn libraries in Azure Databricks, however, then work would only be done be the driver (master) node and the compute is not distributed. See below a snippet of what pyspark packages are used. 6. Conclusion In this tutorial, two Python projects were created as follows: A lot of companies consider what tooling to use in the cloud for data analytics. In this, almost all cloud data analytics platforms have Python support and therefore, Python can be seen as the Swiss Army knife of data analytics. This tutorial may have helped you to explore the possibilites of Python. 23 1 23 23 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ryan Burge Apr 25, 2019 Member-only The Religious Composition of the Two Major Parties I make a lot of data visualizations. Most of the time I have a pretty good idea of what the results of the calculations are going to look like before I write the code. In fact, some of the results are so obvious that I dont even post them anywhere 4 min read 4 min read Share your ideas with millions of readers. Ashutosh Singh Apr 25, 2019 Member-only Classifying Products as Banned Or Approved using Text Mining- Part II In this part, we will explain how to optimize the existing Machine Learning model in Part I and the deployment of this ML model using Flask. In the previous article of this series, We have discussed the business problem, shown how to train the model using fastText and classification of Banned or Approved products based on information like(Product Name, product description, and specifications). 5 min read 5 min read Favio Vzquez Apr 25, 2019 The Jungle of Koalas, Pandas, Optimus and Spark What to expect from the newest library from Databricks (Koalas), the Optimus framework and Apache Spark 3.x If you are as excited about data science as me, you probably know that the Spark+AI latest summit started yesterday (April 24th 2019). And there are great things to talk about. But I will do it with a spin-off. If youve been following me you now that I co-created a 8 min read 8 min read Rinu Gour Apr 25, 2019 Member-only A Complete Guide to Learn R R Programming Technology is an open source programming language. Also, the R programming language is the latest cutting-edge tool. R Basics is the hottest trend. Moreover, the R command line interface (C.L.I) consists of a prompt, usually the > character. History of R John Chambers and colleagues developed R at Bell Laboratories. Basically 8 min read 8 min read Sriram Parthasarathy Apr 25, 2019 Member-only How to forecast sales revenue: Compare various forecasting approaches There are 100s of methods to forecast sales. The question becomes which ones to choose. In this article, I will briefly cover the popular ways to forecast sales and how to compare the methods with key metrics. Depending on the use case, a customer may be ok with a simple 5 min read 5 min read Ren Bremer Data Solution Architect @ Microsoft, working with Azure services as ADFv2, ADLSgen2, Azure DevOps, Databricks, Function Apps and SQL. Opinions here are mine. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 1155,\n",
       "  'url': 'https://towardsdatascience.com/overloading-operators-in-python-2e24da0d36d7',\n",
       "  'title': 'Overloading Operators in\\xa0Python',\n",
       "  'subtitle': 'And a bit on overloading methods as well (but I’ll try not to overload…',\n",
       "  'claps': 337,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 8,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-09-11',\n",
       "  'clap_prop': 0.0001664622523380043,\n",
       "  'text': \"Towards Data Science Sep 11 2019 Member-only Listen Save Overloading Operators Python bit overloading general Ill try overload u learning program Python run concept behind operator overloading relatively early course learning path like aspect Python language matter pretty much anything learning overloaded operator necessarily tie concept broadening scope topic somewhat obfuscating route individual learning curve mind try keep on-topic without pulling many area learning Python- Object-Oriented Programming naturally tie however Id like focus overloading operator broader complex topic overloading function also warrant least mention Overloading context programming refers ability function operator behave different way depending parameter passed function operand operator act 1 Python operator overloading also known operator ad-hoc polymorphism particular form syntactic sugar enables powerful convenient way prescribe operator er operation infix expression specified type Said another way operator overloading give extended meaning operator beyond pre-defined operational meaning classic operator-overloading example Python plus sign binary i.e. two operand operator add pair number also concatenates pair list string asterisk similarly overloaded multiplier number also repetition operator list string Comparison operator == exhibit similar behavior however overloaded operator Python user ought somewhat careful considering type checking MITs Professor John Guttag reminds u type checking Python strong programming language e.g. Java better Python 3 Python 2 example pretty clear mean used compare two string two number value 4 3 Rather arbitrarily designer Python 2 decided False numeric value le value type str designer Python 3 modern language decided since expression dont obvious meaning generate error message 2 well good operator used one user-defined data type i.e. created class operand case- say attempting add pair x coordinate shown here- compiler throw error since doesnt know add two object overloading done existing operator Python handful along corresponding magic method operator invoke using corresponding method create/access/edit inner working see end article Like nomenclature rapidly-evolving field doesnt seem consensus call them- theyre somewhat commonly referred magic methods- called magic since theyre invoked directly- seems closest standard perhaps since alternative special method sound well special herald colorful moniker- dunder method shorthand double-underscore method i.e. dunder-init-dunder Anyhow theyre special type method limited method associated operator __init__ __call__ example fact quite quick aside- printing ha associated magic method __str__ print plain Point class lone __init__ would get not-so-user-friendly output shown Adding __str__ method Point class remedy Interestingly format also invokes __str__ method print doe turn using x coordinate walk example overloading method Python concept somewhat common practice learning Python probably since get create class something mathematically familiar coordinate point one create number useful magic method one user-defined class use overload operator noteworthy aspect operator overloading position operand relation operator Take le operator example- call __lt__ method first left/preceding operand word expression x shorthand x.__lt__ first operand user-defined class need corresponding __lt__ method order able use may seem like nuisance actually add handy flexibility designing one class since could customize operator function doe class addition providing syntactic convenience writing infix expression use Professor Guttag point overloading provides automatic access polymorphic method defined using __lt__ built-in method sort one method 2 light distinction first second operand Python also provides u set reverse method __radd__ __rsub__ __rmul__ Keep mind reverse method called left operand doe support corresponding operation operand different type Pythonista redditor named Rhomboid explains way better ever could humbly defer take someone explain __radd__ simple term read documentation dont understand One final caveat- awesome flexibility ought keep original intention operator mind example len generally understood used return length sequence overloading method requires integer returned otherwise return TypeError brief toe-dip vaster choppier water overloading function According wikipedia pertains ability create multiple function name different implementation function may differ arity type parameter concept way useful language C++ Java doe really comport Pythonic way thing stackoverflow pointed use method overloading Python one helpful thread matter Clearly Python handle case different manner said reading method overloading helped discern important Pythonic concept polymorphism defined ability leverage interface different underlying form data type class Polymorphism signature feature class Python allows commonly-named method utilized across many class subclass furthermore enables function use object belonging one class way doe object different class without needing aware distinction across class 3 allows duck typing special case dynamic typing us characteristic polymorphism including late binding dynamic dispatch evaluate object type start single vs. multiple static vs. dynamic dispatch way beyond current level understanding Ill leave Sources 1 http //stackabuse.com/overloading-functions-and-operators-in-python/ 2 Guttag John V .. Introduction Computation Programming Using Python MIT Press MIT Press Kindle Edition 3 http //www.digitalocean.com/community/tutorials/how-to-apply-polymorphism-to-classes-in-python-3 4 title image http //www.osgpaintball.com/event/operation-overlord-scenario-paintball/ http //www.reddit.com/r/learnpython/comments/3cvgpi/can_someone_explain_radd_to_me_in_simple_terms_i/ List python class special method magic method Micropyramid List python class special method magic method magic function allow u override add default micropyramid.com Python Operator Overloading operator overloading Python Python operator work built-in class operator behaves www.programiz.com Python Tutorial Magic Methods so-called magic method nothing wizardry already seen previous chapter www.python-course.eu Overloading Functions Operators Python Overloading Overloading context programming refers ability function operator stackabuse.com Operator overloading computer programming operator overloading sometimes termed operator ad hoc polymorphism specific case en.wikipedia.org Python Operator Overloading Python Magic Methods DataFlair Python tutorial going discus Python Operator Overloading example operator overloading data-flair.training Operator Function Overloading Custom Python Classes Real Python might wondered built-in operator function show different behavior object different realpython.com use method overloading Python trying implement method overloading Python class def stackoverflow self print 'first method def stackoverflow.com Python function overloading know Python doe support method overloading 've run problem ca n't seem solve stackoverflow.com Function Overloading Python Recently one conversation Practo found guy complaining bad medium.com Disclaimer Mistakes misinterpretation abuse concept idea taken far mine mine 341 341 341 Towards Data Science home data science Medium publication sharing concept idea code Sergi Lehkyi Sep 11 2019 Football Winners Win Losers Lose Exploring 5 Years European Football Intro notebook explore modern metric football xG xGA xPTS influence sport analytics Expected Goals xG measure quality shot based several variable assist type shot angle distance goal whether wa headed shot 17 min read 17 min read Share idea million reader MILA JONES Sep 11 2019 Effective Ways Data Analytics Help Make Better Entrepreneur Check Business Intelligence BI data analytics remove uncertainty business provide insight help decision making forecasting Business Intelligence data analytics integral part successful business venture Business analytics ha dedicated market industry often sought-after method skip guesswork accelerate pace growth data analytics get invaluable insight business 4 min read 4 min read Mark Nagelberg Sep 11 2019 Member-only Take Python Skills Next Level Fluent Python intermediate programmer ticket advanced Python Youve programming Python although know way around dicts list tuples set function class feeling Python knowledge heard pythonic code fall short Youre intermediate Python programmer 7 min read 7 min read Javier Rodriguez Zaurin Sep 11 2019 RecoTour II neural recommendation algorithm second series post recommendation algorithm python first series wrote quite ago quickly went number algorithm implemented tried using Kaggles Ponpare dataset find related 15 min read 15 min read Vikash Kumar Sep 11 2019 Python Vs R Whats Best Machine Learning thinking build machine learning project stuck choosing right programming language project Well article going help clear doubt related characteristic Python R. Lets get started basic R Python 5 min read 5 min read Jay Kim Medium Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Herman Michaels Better Programming Setup Data Classes Python Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Yang Zhou TechToFreedom 5 Levels Using Context Managers Python Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Towards Data Science Sep 11, 2019 Member-only Listen Save Overloading Operators in Python and a bit on overloading in general (but Ill try not to overload you) Most of us learning to program in Python run into concepts behind operator overloading relatively early during the course of our learning path. But, like most aspects of Python (and other languages; and, for that matter, pretty much anything), learning about overloaded operators necessarily ties into other concepts, both broadening the scope of topic and somewhat obfuscating the route through our individual learning curve. With that in mind, I will try to keep on-topic without pulling in too many other areas of learning Python- some Object-Oriented Programming naturally ties in, however; and, while Id like to focus on overloading operators , the broader, more complex topic of overloading functions also warrants at least some mention. Overloading, in the context of programming, refers to the ability of a function or an operator to behave in different ways depending on the parameters that are passed to the function, or the operands that the operator acts on [1]. In Python, operator overloading (also known as operator ad-hoc polymorphism) in particular is a form of syntactic sugar that enables powerful and convenient ways to prescribe an operators, er, operations, as an infix expression of specified types. Said another way, operator overloading gives extended meaning to operators beyond their pre-defined operational meaning. The classic operator-overloading example in Python is the plus sign, a binary (i.e., two operands) operator that not only adds a pair of numbers, but also concatenates a pair of lists or strings. The asterisk is similarly overloaded as not only a multiplier for numbers, but also as a repetition operator for lists or strings. Comparison operators (such as >, ==, or !=) exhibit similar behavior; however, for all of these overloaded operators, we as Python users ought be somewhat careful when considering type checking. As MITs Professor John Guttag reminds us, type checking in Python is not as strong as in some other programming languages (e.g., Java), but it is better in Python 3 than in Python 2. For example, it is pretty clear what < should mean when it is used to compare two strings or two numbers. But what should the value of 4 < 3 be? Rather arbitrarily, the designers of Python 2 decided that it should be False because all numeric values should be less than all values of type str. The designers of Python 3 and most other modern languages decided that since such expressions dont have an obvious meaning, they should generate an error message. [2] This is all well and good, but what if an operator is being used on one or more user-defined data types (i.e., from a created class) as an operand? In such a case- say, attempting to add a pair of (x, y) coordinates, as shown here- the compiler will throw an error since it doesnt know how to add the two objects. And, while overloading can only be done on existing operators in Python, there are a handful of them, along with the corresponding magic method each of these operators invoke; using these corresponding methods, we can create/access/edit their inner workings (see end of article). Like other nomenclature in this rapidly-evolving field, there doesnt seem to be a consensus on what to call them- theyre somewhat commonly referred to as magic methods- called magic since theyre not invoked directly- and that seems to be closest to standard, perhaps since the alternative special method sounds, well, not so special. Some herald a more colorful moniker- dunder methods as a shorthand for double-underscore methods (i.e., dunder-init-dunder). Anyhow, theyre a special type of method, and are not only limited to the methods associated with operators ( __init__() or __call__() , as examples); in fact, there are quite a few of them. Just as a quick aside- printing has its own associated magic method, __str__() . If we were to print the plain Point class with just the lone __init__() , we would get the not-so-user-friendly output shown above. Adding the __str__() method into the Point class will remedy that. Interestingly, format() also invokes the same __str__() method that print() does. It turns out that using (x, y) coordinates to walk through examples of overloading, methods, and other Python concepts is a somewhat common practice when learning Python, probably since we get to create our own class with something as mathematically familiar as coordinate points. From there, one can create a number of useful magic methods for ones user-defined class and use them to overload operators. A noteworthy aspect of operator overloading is the position of each operand in relation to its operator. Take the less than operator < as an example- it calls the __lt__() method for the first (or left/preceding) operand. In other words, the expression x < y is shorthand for x.__lt__(y)  ; if the first operand is a user-defined class, it needs to have its own corresponding __lt__() method in order to be able to use <. That may seem like a nuisance, but it actually adds some handy flexibility for designing ones classes, since we could customize what any operators function does for a class. In addition to providing the syntactic convenience of writing infix expressions that use <, Professor Guttag points out, this overloading provides automatic access to any polymorphic method defined using __lt__() . The built-in method sort is one such method. [2] In light of this distinction between first and second operands, Python also provides us with a set of reverse methods, such as __radd__(), __rsub__(), __rmul__() , and so on. Keep in mind that these reverse methods are only called if the left operand does not support the corresponding operation and the operands are of different types. A Pythonista redditor named Rhomboid explains it way better than I ever could, so I humbly defer to his take: Can someone explain __radd__ to me in simple terms? I read the documentation and I dont understand it. One final caveat- while its awesome that we have this flexibility, we ought to keep the original intention of the operators in mind. For example, len() is generally understood to be used to return the length of a sequence; so overloading this method requires that an integer is returned (otherwise it will return a TypeError). and now for a brief toe-dip into the vaster, choppier waters of overloading functions . According to wikipedia, this pertains to the ability to create multiple functions of the same name with different implementations. The functions may differ by the arity or types of their parameters. This concept is way more useful in other languages (C++, Java), and does not really comport with the Pythonic way of doing things, as some on stackoverflow have pointed out: How do I use method overloading in Python? and one more helpful thread on the matter: Clearly, Python handles such cases in a different manner. With that said, reading up on method overloading helped me discern an important Pythonic concept: polymorphism. This is defined as the ability to leverage the same interface for different underlying forms such as data types or classes . Polymorphism is a signature feature of classes in Python, in that it allows commonly-named methods to be utilized across many classes or subclasses, which furthermore enables functions to use objects belonging to one class in the same way that it does for objects of a different class, all without needing to be aware of distinctions across classes. [3] . This allows for duck typing, a special case of dynamic typing that uses the characteristics of polymorphism (including late binding and dynamic dispatch ) to evaluate object types. From here, this all starts into a single vs. multiple, and static vs. dynamic dispatch, which is way beyond my current level of understanding; so Ill leave that be for now. Sources: [1] https://stackabuse.com/overloading-functions-and-operators-in-python/ [2] Guttag, John V.. Introduction to Computation and Programming Using Python (The MIT Press) . The MIT Press. Kindle Edition. [3] https://www.digitalocean.com/community/tutorials/how-to-apply-polymorphism-to-classes-in-python-3 [4] (title image)  https://www.osgpaintball.com/event/operation-overlord-scenario-paintball/ https://www.reddit.com/r/learnpython/comments/3cvgpi/can_someone_explain_radd_to_me_in_simple_terms_i/ List of python class special methods or magic methods - Micropyramid List of python class special methods or magic methods. magic functions allow us to override or add the default micropyramid.com Python Operator Overloading What is operator overloading in Python? Python operators work for built-in classes. But same operator behaves www.programiz.com Python Tutorial: Magic Methods The so-called magic methods have nothing to do with wizardry. You have already seen them in previous chapters of our www.python-course.eu Overloading Functions and Operators in Python What is Overloading? Overloading, in the context of programming, refers to the ability of a function or an operator to stackabuse.com Operator overloading In computer programming, operator overloading, sometimes termed operator ad hoc polymorphism , is a specific case of en.wikipedia.org Python Operator Overloading and Python Magic Methods - DataFlair In this Python tutorial, we are going to discuss Python Operator Overloading, examples of operator overloading in data-flair.training Operator and Function Overloading in Custom Python Classes - Real Python You might have wondered how the same built-in operator or function shows different behavior for objects of different realpython.com How do I use method overloading in Python? I am trying to implement method overloading in Python: class A: def stackoverflow(self): print 'first method' def stackoverflow.com Python function overloading I know that Python does not support method overloading, but I've run into a problem that I can't seem to solve in a stackoverflow.com Function Overloading in Python Recently in one of the conversations at Practo, I found some guys complaining that its so bad that we do not have medium.com Disclaimer: Mistakes, misinterpretation, abuses of concepts, and ideas taken too far are mine and only mine. 341 341 341 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Sergi Lehkyi Sep 11, 2019 Football: Why Winners Win and Losers Lose Exploring 5 Years of European Football Intro In this notebook we will explore modern metrics in football (xG, xGA and xPTS) and its influence in sport analytics. Expected Goals (xG) measures the quality of a shot based on several variables such as assist type, shot angle and distance from goal, whether it was a headed shot 17 min read 17 min read Share your ideas with millions of readers. MILA JONES Sep 11, 2019 Effective Ways How Data Analytics Help to Make a Better Entrepreneur Check out how Business Intelligence (BI) and data analytics remove uncertainty in business and provide insights that help in decision making and forecasting. Business Intelligence and data analytics are an integral part of any successful business venture. Business analytics has its dedicated market in the industry and is often a sought-after method to skip the guesswork and accelerate the pace of growth. With data analytics, you get invaluable insights into your business. 4 min read 4 min read Mark Nagelberg Sep 11, 2019 Member-only Take your Python Skills to the Next Level With Fluent Python The intermediate programmers ticket to advanced Python Youve been programming in Python for a while, and although you know your way around dicts, lists, tuples, sets, functions, and classes, you have a feeling your Python knowledge is not where it should be. You have heard about pythonic code and yours falls short. Youre an intermediate Python programmer 7 min read 7 min read Javier Rodriguez Zaurin Sep 11, 2019 RecoTour II: neural recommendation algorithms This is the second of a series of posts on recommendation algorithms in python. In the first of the series, that I wrote quite a while ago, I quickly went through a number of algorithms that I implemented and tried using Kaggles Ponpare dataset. You can find all the related 15 min read 15 min read Vikash Kumar Sep 11, 2019 Python Vs R: Whats Best for Machine Learning Are you thinking to build a machine learning project and stuck between choosing the right programming language for your project? Well, then this article is going to help you clear the doubts related to the characteristics of Python and R. Lets get started with the basics. R and Python both 5 min read 5 min read Jay Kim More from Medium Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Herman Michaels in Better Programming How to Setup Data Classes in Python Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Yang Zhou in TechToFreedom 5 Levels of Using Context Managers in Python Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 4101,\n",
       "  'url': 'https://towardsdatascience.com/python-tutorial-retrieve-a-list-of-swiss-government-members-from-twitter-d5f999555f98',\n",
       "  'title': 'Python Tutorial: Retrieve A List Of Swiss Government Members From\\xa0Twitter',\n",
       "  'subtitle': 'The tutorial will show\\xa0you…',\n",
       "  'claps': 2,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-01-26',\n",
       "  'clap_prop': 9.879065420653076e-07,\n",
       "  'text': 'Towards Data Science Jan 25 2019 Member-only Listen Save Python Statistic Tutorial Series Python Tutorial Twitter Account Retrieval Swiss Government Members Start journey social medium analysis politician Click link Title list tutorial article first article tutorial show extract list tweeting Swiss Government Members via Twitter API extracted data put Panda Dataframe visualized via powerful Plotly package result look follows get data twitter account SoMePolis maintains list Swiss government member Twitter account goal program fetch list member government member tweeting extract key figure follower friend Create Twitter Developer Account App confirmation create first App developer account done Twitter generate API key token use program communicate Twitter API time write program create class TwitterClient offer two method get_government_members create_plotly_table program use following python package Thats using four library ready go __init__ Within Class initialization method establish connection Twitter API use key token loaded yaml file located outside current directory secret directory common approach locate sensitive data directory life local computer never checked remote source system Github Yaml human-readable data serialization language commonly used configuration file could used many application data stored ha straightforward syntax case four key value pair security token loaded side information enhance program require additional non-sensitive configuration data would introduce second yaml file public information check-in normal source code next step use OAuth authenticate Twitter gain access API OAuth open standard access delegation commonly used way Internet user grant website application access information website without giving password I.e. provide secret access token key create private class variable _api hold reference Twitter API Via reference object interact Twitter API method available described use private variable API want offer API directly consumer class rather provide higher level method getgovernment_members Defining scope class variable public protected private important design decision designing class used within program module get_government_members method fetch twitter politician account described list BundesparlamentarierInnen twitter account SoMePolis first step get Twitters list SomePolis account search required one found use list.id retrieve member twitter account list retrieve member list use so-called Cursor position 7 important concept API interface manage data retrieval arbitrary large list i.e. list may million entry use program following code sequence would return first 25 member list strategy Twitter API ensure deliver huge data amount single request client program case want member call multiple time list_members request packet 25 list member account returned concept called pagination used throughout public APIs various provider find information Tweeply Cursor Tutorial list ha 110 member mean list_members method called five time end result variable 110 Twitter Accounts account create_plotly_table Finally extract list twitter account certain data processing prepare Panda Dataframe two-dimensional size-mutable potentially heterogeneous tabular data structure labeled ax row column list Twitter account extract create column data frame achieved via following command sequence iterate list extract record screen_name Finally create plotly Table two main module need generate Plotly graph create account http //plot.ly first run application generated table uploaded result get table similar like one account Account Creation Plotly first time run program asked sign create account plotly created account retrieve access token creating new one necessary store file .credentials home directory folder .plotly Thats today exercise try add following code sequence Exercise Try identify party government person anaylzing screen_name description guiding help check following guide party Switzerland Main party CVP FDP SP SVP well Grne high chance abbreviation mentioned person see screenshot build new Panda Column add Data Frame visualizing source code lesson1.py found http //github.com/talfco/clb-sentiment solution exercise Originally published dev.cloudburo.net January 26 2019 4 4 4 Enjoy read Reward writer Beta tip go Felix Kuestahler third-party platform choice letting know appreciate story Towards Data Science home data science Medium publication sharing concept idea code Partha Deka Jan 25 2019 Empowering citizen data scientist hardware design manufacturing Improving productivity hardware design manufacturing professional advanced AI tool Authors Partha Deka Rohit Mittal citizen data scientist Expert data scientist rely custom coding make sense data use case could data cleansing data imputation creating segment finding pattern data building 4 min read 4 min read Share idea million reader Lak Lakshmanan Jan 25 2019 Bayesian hyper-parameter tuning blackbox model Optimization arbitrary function Cloud ML Engine Google Cloud ML Engine offer hyper-parameter tuning service us Bayesian method restricted TensorFlow scikit-learn fact even limited machine learning use Bayesian approach tune pretty much blackbox model demonstrate Ill tune traffic 3 min read 3 min read Aaron Frederick Jan 25 2019 Member-only Creating AI GameBoy Part 2 Collecting Data Screen Welcome part 2 Creating AI GameBoy missed Part 1 Coding Controller click catch edition going intelligently get information game various image processing classification technique important 5 min read 5 min read Semi Koen Jan 25 2019 Member-only Statistics Grammar Data Science Part 2/5 Statistics refresher kick start Data Science journey 2nd article Statistics Grammar Data Science series covering various type probability distribution plot Revision Bookmarks rest article easy access Article Series Part 1 Data Types Measures Central Tendency Measures 6 min read 6 min read Thomas Nield Jan 25 2019 Member-only Data Science Become Vague Lets Specialize Break would opposed downplaying term data science breaking specialized discipline misunderstand think global data science movement wa necessary positive impact curmudgeon corporate world campaign ha everybody bought 10 min read 10 min read Felix Kuestahler Exploring emerging world decentralization content opinion expressed context legal entity http //atnode.ch/ Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month ibrahim zahir 7 killer web apps start using 2023 Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Jan 25, 2019 Member-only Listen Save Python Statistic Tutorial Series Python Tutorial: Twitter Account Retrieval of Swiss Government Members Start your journey into social media analysis of politicians Click on the link above the Title for a list of all tutorial articles. This is the first article. The tutorial will show you how to extract a list of tweeting Swiss Government Members via the Twitter API. The extracted data will be put into a Panda Dataframe and then visualized via the powerful Plotly package. The result will look as follows: Where to get the data from? The twitter account SoMePolis maintains a list of Swiss government members, who have a Twitter account. The goal of the program is to fetch all list members (government members who are tweeting) and extract some key figures (followers and friends) Create a Twitter Developer Account and App After the confirmation, you then create your first App in your developer account. Having done that, Twitter will generate API keys and tokens which you will use in your program to communicate with the Twitter API. Its time to write the program. We will create a class TwitterClient which offers two methods get_government_members and create_plotly_table The program will use the following python packages Thats is by using these four libraries we are ready to go. __init__ Within the Class initialization method, we have to establish a connection with the Twitter API. For that, we use our keys and tokens which are loaded from a yaml file, which is located outside of the current directory in the secret directory. This is a common approach, to locate sensitive data in a directory which only lives on the local computer and never will be checked in a remote source system as Github. Yaml is a human-readable data serialization language. It is commonly used for configuration files but could be used in many applications where data is being stored. It has a straightforward syntax, for our case four <key>:<value> pairs for the security tokens, which are loaded: As a side information, when you enhance your program and require additional non-sensitive configuration data, you would introduce a second yaml file with public information, which you can check-in with the normal source code. In the next step, we use OAuth to authenticate with Twitter and gain access to their API. OAuth is an open standard for access delegation, commonly used as a way for Internet users to grant websites or applications access to their information on other websites but without giving them the passwords. I.e., you will provide them with your secret access token and keys. We create a private class variable _api which holds the reference to the Twitter API. Via this reference object you can now interact with the Twitter API, all methods available are described here. We use a private variable for the API because we don t want to offer the API directly to consumers of our classes but rather provide higher level methods as getgovernment_members . Defining the scope of your class variables public, protected, private is an important design decision, when you are designing classes which will be used within a program or module. get_government_members In this method, we will fetch all twitter politician accounts out of the above described list BundesparlamentarierInnen of the twitter account SoMePolis. In a first step, we get all Twitters lists of the SomePolis account and search for the required one. Having found it, we will use its list.id to retrieve all members (twitter accounts) of the list. To retrieve all members of a list we have to use a so-called Cursor ( position 7 ) Its an important concept in API interfaces to manage data retrieval of arbitrary large lists (i.e., a list may have millions of entries) If you use in the program the following code sequence, you would just return the first 25 members of the list. With this strategy, the Twitter API will ensure that it will not have to deliver huge data amount in a single request to a client program. In case you want all members you have to call multiple times the list_members. In each request, a packet of 25 list member accounts will be returned. This concept is called pagination and is used throughout public APIs of various providers (you can find more information here: Tweeply Cursor Tutorial ) The list has about 110 members, that means the list_members method will be called five times. In the end, the result variable will have the 110 Twitter Accounts accounts. create_plotly_table Finally, we extract out of the list of twitter accounts certain data for further processing. We will prepare a Panda Dataframe, which is a two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). Of our list of Twitter accounts we will extract We have to create columns for the data frame which is achieved via the following command sequence We iterate through our lists and extract for each record the screen_name . Finally, we create a plotly Table: There are the two main modules that we will need to generate our Plotly graphs. You will have to create an account at https://plot.ly (when you first run the application), to which your generated table will be uploaded. As a result, you will get a table similar like this one in your account. Account Creation at Plotly The first time you run the program, you will be asked to sign up and create an account at plotly. Having created an account you will have to retrieve the access token (by creating a new one if necessary) And store it in the file .credentials in the home directory in the folder .plotly Thats it for today as an exercise try to add the following code sequence. Exercise Try to identify the party of each government person, by anaylzing its screen_name or description. As a guiding help, check out the following guide of parties in Switzerland . Main parties are CVP, FDP, SP, SVP as well as Grne. There is a high chance that the abbreviation is mentioned by a person (see screenshot below). So build up a new Panda Column and add it to the Data Frame for visualizing. The source code ( lesson1.py ) can be found here: https://github.com/talfco/clb-sentiment The solution to the exercise here Originally published at  dev.cloudburo.net  on January 26, 2019. 4 4 4 Enjoy the read? Reward the writer. Beta Your tip will go to Felix Kuestahler through a third-party platform of their choice, letting them know you appreciate their story. More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Partha Deka Jan 25, 2019 Empowering a citizen data scientist for hardware design & manufacturing Improving productivity of a hardware design and manufacturing professional with an advanced AI tool Authors: Partha Deka and Rohit Mittal What is a citizen data scientist? Expert data scientists rely on custom coding to make sense out of data. The use case could be data cleansing, data imputation, creating segments, finding patterns in the data, building 4 min read 4 min read Share your ideas with millions of readers. Lak Lakshmanan Jan 25, 2019 How to do Bayesian hyper-parameter tuning on a blackbox model Optimization of arbitrary functions on Cloud ML Engine Google Cloud ML Engine offers a hyper-parameter tuning service that uses Bayesian methods. It is not restricted to TensorFlow or scikit-learn. In fact, it is not even limited to machine learning. You can use the Bayesian approach to tune pretty much any blackbox model. To demonstrate, Ill tune a traffic 3 min read 3 min read Aaron Frederick Jan 25, 2019 Member-only Creating AI for GameBoy Part 2: Collecting Data From the Screen Welcome to part 2 of Creating an AI for GameBoy! If you missed Part 1: Coding a Controller, click here to catch up. In this edition, I will be going over how to intelligently get information out of the game through various image processing and classification techniques. This is important 5 min read 5 min read Semi Koen Jan 25, 2019 Member-only Statistics is the Grammar of Data Science Part 2/5 Statistics refresher to kick start your Data Science journey This is the 2nd article of the Statistics is the Grammar of Data Science series, covering the various types of probability distributions and how we plot them. Revision Bookmarks to the rest of the articles for easy access: Article Series Part 1: Data Types | Measures of Central Tendency | Measures of 6 min read 6 min read Thomas Nield Jan 25, 2019 Member-only Data Science Has Become Too Vague Lets Specialize and Break it Up! I would not be opposed to downplaying the term data science and breaking it up into specialized disciplines. Do not misunderstand, I think the global data science movement was necessary and had a positive impact on the curmudgeon corporate world. But the campaign has been won and everybody is bought 10 min read 10 min read Felix Kuestahler Exploring the emerging world of decentralization. All content and opinion expressed in the context of my legal entity https://atnode.ch/ More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month ibrahim zahir 7 killer web apps you should start using in 2023 Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 369,\n",
       "  'url': 'https://towardsdatascience.com/what-library-can-load-image-in-python-and-what-are-their-difference-d1628c6623ad',\n",
       "  'title': 'What libraries can load image in Python and what are their difference?',\n",
       "  'subtitle': 'Summarization & Comparison of…',\n",
       "  'claps': 124,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 3,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 6.125020560804907e-05,\n",
       "  'text': 'Towards Data Science Mar 30 2019 Member-only Listen Save library load image Python difference Summarization Comparison imread face computer vision project first need load image preprocessing various library perform imread want consolidate popular library loading image difference article go Library loading image four library usually used loading image Colour channel loading image usually plt.imshow img used plot image Lets plot doge may spot OpenCV image look odd matplotlib PIL skimage represent image RGB Red Green Blue order OpenCV reverse order BGR Blue Green Red Easy Fix convert image BGR RGB using cv2.cvtColor img cv2.COLOR_BGR2RGB plotting using plt.imshow Efficiency may ask one efficient library loading image function defined track time result follow Pillow Image.Open seems efficient based result study may go back source code find difference Cheatsheet combined information Jupyter Notebook Feel free download cheatsheet happy coding Source http //blog.csdn.net/renelian1572/article/details/78761278 http //github.com/ZhangXinNan/LearnPractice/blob/master/cv/opencv/test_cvlib.py 157 1 157 157 1 Towards Data Science home data science Medium publication sharing concept idea code Abhishek Mukherjee Mar 30 2019 Trust interpretability machine learning machine learning model always need interpretable Given choice interpretable model inaccurate non-interpretable model accurate wouldnt rather choose non-interpretable accurate model word reason sacrificing accuracy altar interpretability 9 min read 9 min read Share idea million reader Christine Calo Mar 30 2019 Microsoft Introduction AI Part 1 Machine Learning bit like wanted learn Artificial Intelligence although felt little intimidated math involved Maybe thought concept difficult understand would depth recently completed Microsoft Introduction AI course 17 min read 17 min read Okoh Anita Mar 30 2019 Repetition Songs Python Tutorial One Ed Sheeran song case study Everyone ha heard song know song sound like carelessly say everyone define song word benefit doubt song according Wikipedia single work music typically intended sung 8 min read 8 min read Alex Blyakhman Mar 30 2019 Member-only Getting Started Google BigQuerys Machine Learning Titanic Dataset still Beta BigQuery ML ha available since mid last year however didnt get around working Google cloud-based Machine Learning offering recently non-data scientist first impression whats like ability run ML model 12 min read 12 min read Jeremie Harris Mar 30 2019 problem data science job posting Every notice something realize probably noticed long time ago start see everywhere wonder people arent talking every wa yesterday wa scrolling 5 min read 5 min read Kevin Luk journey Data Science Medium Black_Raven James Ng Geek Culture Face Recognition 46 line code Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Mar 30, 2019 Member-only Listen Save What libraries can load image in Python and what are their difference? Summarization & Comparison of . imread() When we face computer vision project, first of all we need to load the images before any preprocessing. There are various libraries out there to perform imread() . Here I want to consolidate the popular libraries for loading image and their difference. This article will go through: Library for loading image There are four libraries that are usually used for loading images. Colour channel After loading the image, usually plt.imshow(img) will be used to plot the images. Lets plot some doge ! You may spot that the OpenCV image above looks odd. It is because matplotlib, PIL and skimage represent image in RGB (Red, Green, Blue) order, while OpenCV is in reverse order ! ( BGR Blue, Green, Red) Easy Fix Just convert the image from BGR to RGB using cv2.cvtColor(img, cv2.COLOR_BGR2RGB) before plotting using plt.imshow() . Efficiency So, you may ask which one is the most efficient library in loading the image. Here a function is defined to track the time: The result is as follow: Pillow Image.Open() seems to be the most efficient based on the result. For further study, we may go back to the source code to find out more about the difference! Cheatsheet I have combined the above information into a Jupyter Notebook. Feel free to download the cheatsheet and happy coding! Source: https://blog.csdn.net/renelian1572/article/details/78761278 https://github.com/ZhangXinNan/LearnPractice/blob/master/cv/opencv/test_cvlib.py 157 1 157 157 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Abhishek Mukherjee Mar 30, 2019 Trust and interpretability in machine learning Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldnt you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability? 9 min read 9 min read Share your ideas with millions of readers. Christine Calo Mar 30, 2019 Microsoft Introduction to AI Part 1 Machine Learning Are you a bit like me and have wanted to learn about Artificial Intelligence although felt a little intimidated by the maths involved? Maybe you thought the concepts were too difficult to understand and you would be out of your depth. I recently completed the Microsoft Introduction to AI course 17 min read 17 min read Okoh Anita Mar 30, 2019 Repetition in Songs: A Python Tutorial One of Ed Sheeran songs as a case study Everyone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung 8 min read 8 min read Alex Blyakhman Mar 30, 2019 Member-only Getting Started with Google BigQuerys Machine Learning Titanic Dataset While still in Beta, BigQuery ML has been available since mid last year; however, I didnt get around to working with this Google cloud-based Machine Learning offering until recently. As a non-data scientist, my first impression whats not to like? After all, the ability to run ML models from 12 min read 12 min read Jeremie Harris Mar 30, 2019 The problem with data science job postings Every once in a while, you notice something that you realize you probably should have noticed a long time ago. You start to see it everywhere. You wonder why more people arent talking about it. For me, every once in a while was yesterday when I was scrolling through the 5 min read 5 min read Kevin Luk My journey into Data Science More from Medium Black_Raven (James Ng) in Geek Culture Face Recognition in 46 lines of code Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 1753,\n",
       "  'url': 'https://towardsdatascience.com/boost-your-efficiency-and-process-excel-files-with-python-cae650c85d6c',\n",
       "  'title': 'Boost your efficiency and process Excel-files with\\xa0Python',\n",
       "  'subtitle': 'Load, transform, modify and save Excel-files…',\n",
       "  'claps': 261,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-11-01',\n",
       "  'clap_prop': 0.00012892180373952263,\n",
       "  'text': \"Towards Data Science Nov 1 2019 Member-only Listen Save Boost efficiency process Excel-files Python Load transform modify save Excel-files Python improve reporting process work data get touch excel Even dont use client colleague use Excel great made table calculation smaller data set always hated kind excel sheet one million row hundred column workbook slow tend crash calculation started use python handling large excel file offer another big advantage create code reproducible provide documentation well Lets jump Reading Excel-files Python file want process contains nearly 1 million row 16 column Python provides read_excel read Excel-files DataFrame see data look clean far column header seems wrong lot excel map contain headline information guide reader skip part define header row argument header= 1 specifies want use second row excel sheet header previous row skipped Use Pandas calculation typical question marketing department could much sale different country year finished calculation 86 ms. One big advantage processing Excel-files Python kind calculation much faster done Excel complex operation greater speed advantage Another requirement could sale department need data country grouped year category Since want supply data national market save calculation different worksheet Saving result Excel next step want save file Excel supply sale marketing department create pd.ExcelWriter object create different worksheet Easy isnt Lets look new created workbook see DataFrames saved correctly specified worksheet sent great result department receive mail next day ask formatting visualization Since transform kind data every month decide perform task Python well Formatting visualization add formatting visualization create writer object see first part code first example create writer object xlsxwriter give u access Excel-features chart formatting gain access feature need get workbook object workbook writer.book worksheet object worksheet writer.sheet 'Sales_Sums example perform modification first sheet add chart specify range data =Sales_Sums B 2 B 7 add worksheet cell A9 way add formatting sale data add 3 color scale range B2 B7 visually highlight low high value also adjust width first second column worksheet.set_column 0,1,30 also format column header sale data rename 2019 Sales Data last step save file result much better provides big advantage compared Excel reproduce exactly file next month one click Conclusion Python great processing Excel-files handle large file much easier create reproducible code provide documentation colleague also saw easily access advanced feature Python could automate whole reporting process reading Creating chart Using Pandas XlsxWriter create Excel chart introduction creation Excel file chart using Pandas XlsxWriter pandas-xlsxwriter-charts.readthedocs.io Excel report Pandas pivot Generating Excel Reports Pandas Pivot Table previous pivot table article described use panda pivot_table function combine present data pbpython.com Formatting Excel-files Python Tutorial 2 Adding formatting XLSX File previous section created simple spreadsheet using Python XlsxWriter module converted xlsxwriter.readthedocs.io enjoy Medium Towards Data Science didnt sign yet feel free use referral link join community 386 2 386 386 2 Towards Data Science home data science Medium publication sharing concept idea code Benedikt Droste Data Analyst management consultancy Interested data science web scraping storytelling http //www.linkedin.com/in/benedikt-droste-893b1b189/ Medium Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Anmol Tomar CodeX Say Goodbye Loops Python Welcome Vectorization Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Towards Data Science Nov 1, 2019 Member-only Listen Save Boost your efficiency and process Excel-files with Python Load, transform, modify and save Excel-files with Python to improve your reporting processes If you work with data, you will get in touch with excel. Even if you dont use it by yourself, your clients or colleagues will use it. Excel is great for what it is made: table calculation for smaller data sets. But I always hated the kind of excel sheets with one million rows and hundreds of columns. This workbooks are slow and tend to crash after some calculation. So I started to use python for handling large excel files which offer another big advantage: You create code which is reproducible and provide documentation as well. Lets jump in! Reading Excel-files with Python The file we want to process contains nearly 1 million rows and 16 columns: Python provides read_excel() to read Excel-files as DataFrame: As you can see the data looks clean so far but our column header seems to be wrong. A lot of excel maps contain headlines or other information to guide the reader. We can skip this parts and define a header row: The argument header=[1] specifies that we want to use the second row in the excel sheet as header. All previous rows are skipped. Use Pandas to do some calculations A typical question of the marketing department could be how much sales we had for the different countries in each year: We finished this calculation in 86 ms. One big advantage of processing Excel-files with Python is that any kind of calculation is much faster done as in Excel itself. The more complex the operations, the greater the speed advantages. Another requirement could be that the sales department needs the data for each country grouped by years and categories. Since they want to supply the data to the national markets, we have to save the calculations in different worksheets: Saving the results as Excel In a next step, we want to save our files as Excel again to supply it to the sales and marketing department. We will create a pd.ExcelWriter object and create the different worksheets: Easy, isnt it? Lets have a look at the new created workbook: As you can see our DataFrames were saved correctly to the specified worksheets. After we sent our great result to both departments, we receive a mail on the next day: They ask for some formatting and visualization. Since we have to transform this kind of data every month, we decide to perform the tasks in Python as well. Formatting and visualization To add formatting and visualization, we have to create a writer object again: As you can see the first part of the code is the same as in the first example. We create a writer object. xlsxwriter gives us access to Excel-features such as charts and formatting. To gain access to this features, we need to get the workbook object workbook = writer.book and the worksheet object worksheet = writer.sheet['Sales_Sums'] . In this example, we will perform the modifications on our first sheet. We add a chart, specify the range for the data ( =Sales_Sums!$B$2:$B$7' ) and add it to our worksheet in cell A9 . In the same way we add formatting for our sales data. We add a 3 color scale on the range B2:B7 to visually highlight low or high values. We also adjust the width of the first and second column worksheet.set_column(0,1,30) . We also format the column header for our sales data and rename it to 2019 Sales Data . In a last step, we save out the file: This result is much better and provides a big advantage compared to Excel. We can reproduce exactly the same file next month with one click. Conclusion Python is great for processing Excel-files. You can handle large files much easier, you create reproducible code and you provide a documentation for your colleagues. We also saw the we have easily access to advanced features of Python. You could automate your whole reporting process. Further reading: Creating charts: Using Pandas and XlsxWriter to create Excel charts An introduction to the creation of Excel files with charts using Pandas and XlsxWriter. pandas-xlsxwriter-charts.readthedocs.io Excel reports with Pandas pivots: Generating Excel Reports from a Pandas Pivot Table The previous pivot table article described how to use the pandas pivot_table function to combine and present data in an pbpython.com Formatting Excel-files with Python: Tutorial 2: Adding formatting to the XLSX File In the previous section we created a simple spreadsheet using Python and the XlsxWriter module. This converted the xlsxwriter.readthedocs.io If you enjoy Medium and Towards Data Science and didnt sign up yet, feel free to use my referral link to join the community. 386 2 386 386 2 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Benedikt Droste Data Analyst in a management consultancy | Interested in data science, web scraping & storytelling | https://www.linkedin.com/in/benedikt-droste-893b1b189/ More from Medium Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Anmol Tomar in CodeX Say Goodbye to Loops in Python, and Welcome Vectorization! Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 876,\n",
       "  'url': 'https://towardsdatascience.com/making-3-easy-maps-with-python-fb7dfb1036',\n",
       "  'title': 'Making 3 Easy Maps With\\xa0Python',\n",
       "  'subtitle': 'Mapping Starbucks locations in Los Angeles\\xa0County',\n",
       "  'claps': 267,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-22',\n",
       "  'clap_prop': 0.00013188552336571856,\n",
       "  'text': 'Towards Data Science Apr 22 2019 Member-only Listen Save Making 3 Easy Maps Python working geospatial data Ive often needed visualize data natural way possible map Wouldnt nice could use Python quickly easily create interactive map data Well using data set Starbucks location Los Angeles County tutorial end introductory post able create Lets need get familiar data snapshot first row need worry latitude longitude zip field analysis needed Python import loading Starbucks data loading LA County GeoJSON Basic Point Map Creating basic point map Starbucks LA County latitude/longitude pair dataframe pretty straightforward Opening laPointMap.html see following map clearly see Starbucks LA County little red dot within LA County region course customize color shape dot Choropleth Map actually didnt know choropleth map wa playing map Python turn useful visualizing aggregated geospatial data choropleth map answer question zip code LA County Starbucks choropleth map essentially color zip code based value variable number Starbucks store case Lets first go basic code needed create one Since Ive personally found difficult understand get component place choropleth let take look separate visual see work choropleth need know color fill zip code 90001 example check panda dataframe referenced data field search key_on column zip code find column listed column numStores know need fill color corresponding 3 store zip code 90001 look GeoJSON referenced geo_path field find zip code 90001 associated shape info tell shape draw zip code map link ha necessary information Lets look resulting choropleth laChoropleth.html see come nice color bar top reference Heatmap choropleth map see area south LA County seem Starbucks store general get bit specific maybe figure lot Starbucks store small vicinity Basically let create heatmap highlight Starbucks hotspot LA County main parameter heatmap need trial error radius control big circle around Starbucks store blur control much circle blend together higher radius mean given Starbucks influence wider area higher blur mean two Starbucks away still contribute hotspot parameter Lets see picture heatmap laHeatmap.html Hmm cool kind seems like everything red Heatmaps might valuable zoom Lets zoom bit see identify specific hotspot Nice pretty clear map hotspot not-hotspots notspots map One stand Downtown Los Angeles understandably thats regret havent yet found way embed actual interactive version map Medium post wa able show screenshots strongly encourage run small bit code post play interactive map totally different experience hope post helped bit Ill see next one full notebook containing code used analysis found GitHub 333 1 333 333 1 Towards Data Science home data science Medium publication sharing concept idea code Anas Al-Masri Apr 22 2019 Member-only Creating Voice Recognition Calculator Android App Automatic Speech Recognition one famous topic Machine Learning nowadays lot newcomer every day investing time expertise post build simple end-to-end voice-activated calculator app take speech input return speech output 7 min read 7 min read Share idea million reader Nimish Mishra Apr 22 2019 Member-only Data analytics MODIS data MODIS Moderate Resolution Imaging Spectroradiometer imaging sensor board NASAs satellite Terra Aqua orbiting earth capturing imagery understand study various phenomenon earth surface extract whole bunch product image ranging geolocation cloud mask atmospheric product 10 min read 10 min read Marco Peixeiro Apr 22 2019 Member-only Introduction Natural Language Processing NLP Bias AI practical guide working natural data removing bias AI Natural language processing NLP field revolutionized deep learning voice assistant Gmails Smart Compose deep learning ha made possible machine understand u intuitive way course working natural data different working tabular 7 min read 7 min read Sambasivarao K Apr 22 2019 Member-only Region Interest Pooling technique made object detection faster viable major hurdle going image classification object detection fixed size input requirement network existing fully connected layer object detection proposal different shape need converting proposal fixed shape 4 min read 4 min read Avishalom Shalit Apr 22 2019 Innocent Interpretations Suspicious Statistics General Election Data Exploration part 1 Looking 2019 election Israel result appear weird sure evidence actual malfeasance simpler explanation favourite analogy statistic made Cassie Kozyrkov analogy English/American legal system Null Hypothesis presumption innocence leading acquittal rejection due presentation evidence guilt beyond reasonable doubt P value 5 min read 5 min read Ritvik Kharkar Software Engineer Mathemagician Home Chef Medium Nik Piepenbreier Better Programming Make Awesome Maps Python Geopandas Alex Mitrani Towards Data Science Creating Choropleth Maps Pythons Folium Library Vinod Dhole JovianData Science Machine Learning Interesting Heatmaps Using Python Folium MecSimCalc Level Coding Python tutorial use Folium publish interactive map Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 22, 2019 Member-only Listen Save Making 3 Easy Maps With Python In working with geospatial data, Ive often needed to visualize this data in the most natural way possible: a map. Wouldnt it be nice if we could use Python to quickly and easily create interactive maps of your data? Well be using a data set on all Starbucks locations in Los Angeles County for this tutorial. By the end of this introductory post you will be able to create: Lets do it! You will need To get familiar with the data, heres a snapshot of the first few rows: We only need to worry about the latitude, longitude, and zip fields for this analysis. Here are the needed Python imports, loading the Starbucks data, and loading the LA County GeoJSON: Basic Point Map Creating a basic point map of all Starbucks in LA County from the latitude/longitude pairs in our dataframe is pretty straightforward. Opening up laPointMap.html , we see the following map: We can clearly see all the Starbucks in LA County as little red dots within the LA County region. Of course, you can customize any of the colors and shapes of the dots. Choropleth Map I actually didnt know what a choropleth map was before playing with maps in Python but it turns out they are very useful in visualizing aggregated geospatial data. Our choropleth map will answer the question: Which zip codes in LA County have the most Starbucks? . The choropleth map essentially colors in each zip code based on the value of some other variable, the number of Starbucks stores in our case. Lets first go over the basic code needed to create one: Since Ive personally found it more difficult to understand how to get all the components in place for a choropleth, lets take a look at a separate visual to see how it works. The choropleth needs to know what color to fill in for zip code 90001, for example. It checks the pandas dataframe referenced by the data field, searches the key_on column for the zip code and finds the other column listed in columns which is numStores . It then knows that it needs to fill in the color corresponding to 3 stores in zip code 90001 . It then looks in the GeoJSON referenced by the geo_path field, and finds zip code 90001 and its associated shape info , which tells it which shape to draw for that zip code on the map. Through these links, it has all the necessary information. Lets look at the resulting choropleth in laChoropleth.html ! We see that it comes with a nice color bar at the top for reference. Heatmap In the choropleth map above, we see that areas in south LA County seem to have more Starbucks stores in general, but can we get a bit more specific? Can we maybe figure out where there are a lot of Starbucks stores in a small vicinity? Basically, lets create a heatmap to highlight Starbucks hotspots in LA County. The main parameters in the heatmap that need some trial and error are radius which controls how big the circles are around each Starbucks store and blur which controls how much the circles blend together. A higher radius means any given Starbucks influences a wider area and a higher blur means that two Starbucks which are further away from each other can still contribute to a hotspot. The parameters are up to you! Lets see a picture of our heatmap in laHeatmap.html. Hmm cool but it kind of seems like everything is red. Heatmaps might be more valuable if you zoom in. Lets zoom in a bit and see if we can identify more specific hotspots. Nice! Its pretty clear from the above map that we have some hotspots and some not-hotspots (notspots?) in the map. One that stands out is in Downtown Los Angeles (understandably). And thats about it! My only regret is that I havent yet found a way to embed the actual interactive versions of these maps in a Medium post so I was only able to show you screenshots. I strongly encourage you to run the small bits of code through this post to play with the interactive maps for yourself. Its a totally different experience. I hope this post helped a bit and Ill see you in the next one! The full notebook containing all code used in this analysis can be found at my GitHub here . 333 1 333 333 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Anas Al-Masri Apr 22, 2019 Member-only Creating A Voice Recognition Calculator Android App Automatic Speech Recognition is one of the most famous topics in Machine Learning nowadays, with a lot of newcomers every day investing their time and expertise into it. In this post, we will build a simple end-to-end voice-activated calculator app that takes speech as input and returns speech as output 7 min read 7 min read Share your ideas with millions of readers. Nimish Mishra Apr 22, 2019 Member-only Data analytics with MODIS data MODIS (or Moderate Resolution Imaging Spectroradiometer) is an imaging sensor on board the NASAs satellites Terra and Aqua, orbiting the earth capturing imagery to understand and study various phenomena on earths surface. You can extract a whole bunch of products from these images, ranging from geolocation, cloud mask, atmospheric products 10 min read 10 min read Marco Peixeiro Apr 22, 2019 Member-only Introduction to Natural Language Processing (NLP) and Bias in AI A practical guide to working with natural data and removing bias in AI Natural language processing (NLP) is a field that is being revolutionized by deep learning. From voice assistants to Gmails Smart Compose, deep learning has made it possible for machines to understand us in a more intuitive way. Of course, working with natural data is very different than working with tabular 7 min read 7 min read Sambasivarao. K Apr 22, 2019 Member-only Region of Interest Pooling The technique which made object detection faster and viable. The major hurdle for going from image classification to object detection is fixed size input requirement to the network because of existing fully connected layers. In object detection, each proposal will be of a different shape. So there is a need for converting all the proposals to fixed shape as 4 min read 4 min read Avishalom Shalit Apr 22, 2019 Innocent Interpretations for Some Suspicious Statistics; General Election Data Exploration. (part 1) Looking at the 2019 elections in Israel. Some results appear weird, sure, but is there evidence of actual malfeasance, or is there a simpler explanation? My favourite analogy in statistics(made by Cassie Kozyrkov) is the analogy to the English/American legal system. The Null Hypothesis being the presumption of innocence (leading to acquittal) and the rejection of that can only be due to the presentation of evidence of guilt beyond a reasonable doubt. the P value we 5 min read 5 min read Ritvik Kharkar Software Engineer Mathemagician Home Chef More from Medium Nik Piepenbreier in Better Programming Make Awesome Maps in Python and Geopandas Alex Mitrani in Towards Data Science Creating Choropleth Maps with Pythons Folium Library Vinod Dhole in JovianData Science and Machine Learning Interesting Heatmaps Using Python Folium MecSimCalc in Level Up Coding Python tutorial on how to use Folium to publish an interactive map Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4097,\n",
       "  'url': 'https://towardsdatascience.com/the-evolution-of-the-us-electric-grid-f18bce6473d5',\n",
       "  'title': 'The Evolution Of The US Electric\\xa0Grid',\n",
       "  'subtitle': '-',\n",
       "  'claps': 1,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 4,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-01-26',\n",
       "  'clap_prop': 4.939532710326538e-07,\n",
       "  'text': \"Towards Data Science Jan 25 2019 Member-only Listen Save Evolution US Electric Grid past 15 year US electric grid ha undergone massive shift mostly better Renewable energy natural gas coal GHG emission see US grid ha evolved recent year Lets also put table form ease access time-span every energy source declined except natural gas non-hydro renewable energy good news environment emission intensity electric sector ha dropped new low chart show emission intensity US power sector mean 1 MWh electricity result 35 le emission Q2 2018 recent data point available 2003 gain offset increased electricity production total US power sector emission still substantially lower 2003 Total electric-sector emission still saw 26 reduction time-frame Due electricity sector may finally lose mantle largest emitting sector US transportation sector Natural Gas Coal Since 2003 profound shift occurred coal natural gas natural gas ha steady rise coal ha seen continual decline isnt likely stop soon renewable energy natural gas still expanding rapid pace future look increasingly grim coal taking look coal capacity change peak future coal electric grid Preliminary estimate showing 2018 continued trend large coal retirement possibly even breaking record coal addition coming complete halt decline look terminal Renewable Energy renewable energy break see energy source producing gain rise wind solar energy evident 2003 hydroelectricity made almost 78 renewable energy production 2018 share dropped 39.7 36 renewable energy wind 13.6 solar Wind energy likely overcome hydro near future According EIA 2017 wind generated 6.3 electricity solar generated 1.3 figure likely higher 2018 full year data available natural gas ha outpaced renewable growth date EIA predicts lead charge Meanwhile coal share generation expected continue decline next year Causes large shift past 15 year result shifting economics Natural gas solar wind energy experienced significant decline cost recent year 2 2 2 Get email whenever Brayden Gerrard publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code zgr Gen Jan 25 2019 Member-only Notes Artificial Intelligence Machine Learning Deep Learning curious people AI ha intriguing topic 2018 according McKinsey mentioned key enabler 1 3 spot Gartner Top 10 Strategic Technology Trends 2019 AI became catch-all term refers computer program automatically doe something 22 min read 22 min read Share idea million reader Felix Kuestahler Jan 25 2019 Member-only Python Tutorial Twitter Account Retrieval Swiss Government Members Start journey social medium analysis politician Click link Title list tutorial article first article tutorial show extract list tweeting Swiss Government Members via Twitter API extracted data put Panda Dataframe 7 min read 7 min read Tobias Hill Jan 25 2019 got 1 better accuracy data augmentation hard let go thing put love yes really thought wa done 5 th article series accuracy/error metric MNIST example started haunting wa quite sure could improve 4 min read 4 min read Partha Deka Jan 25 2019 Empowering citizen data scientist hardware design manufacturing Improving productivity hardware design manufacturing professional advanced AI tool Authors Partha Deka Rohit Mittal citizen data scientist Expert data scientist rely custom coding make sense data use case could data cleansing data imputation creating segment finding pattern data building 4 min read 4 min read Lak Lakshmanan Jan 25 2019 Bayesian hyper-parameter tuning blackbox model Optimization arbitrary function Cloud ML Engine Google Cloud ML Engine offer hyper-parameter tuning service us Bayesian method restricted TensorFlow scikit-learn fact even limited machine learning use Bayesian approach tune pretty much blackbox model demonstrate Ill tune traffic 3 min read 3 min read Brayden Gerrard Electric Vehicles Green Energy Data Science Contact gerrard.brayden gmail dot com Medium Barry Gander Ancient Rome Fall Real Story Even Scarier America Connects Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Laurel B. Miller Sweary Mommy Naughty Snaps Observing Wannabe Model Attempts Hunt Clients Wild Luay Rahil ILLUMINATION Elon Musk Eliminated Remote Work Working Home `` n't Work '' Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Jan 25, 2019 Member-only Listen Save The Evolution Of The US Electric Grid Over the past 15 years, the US electric grid has undergone some massive shifts (mostly for the better). Renewable energy and natural gas are up, while coal and GHG emissions are down. Here we can see how the US grid has evolved in recent years: Lets also put it in table form for ease of access: So during this time-span, every energy source declined except for natural gas and non-hydro renewable energy. This is good news for the environment, as emissions intensity in our electric sector has dropped to new lows: This chart shows the emissions intensity of the US power sector. That means that 1 MWh of electricity results in over 35% less emissions in Q2 2018 (the most recent data point available) than it did in 2003. Some of these gains have been offset from increased electricity production, but total US power sector emissions are still substantially lower than in 2003: Total electric-sector emissions still saw a 26% reduction during that time-frame. Due to this, the electricity sector may finally lose its mantle as the largest emitting sector in the US to the transportation sector. Natural Gas And Coal Since 2003, the most profound shift occurred between coal and natural gas. While natural gas has been on a steady rise, coal has seen a continual decline that isnt likely to stop soon: With both renewable energy and natural gas still expanding at rapid pace, the future looks increasingly grim for coal. By taking a look at coal capacity changes, we can peak into the future of coal in the electric grid: Preliminary estimates are showing that 2018 continued the trend of large coal retirements possibly even breaking the record. With coal additions coming to a complete halt, the decline looks to be terminal from here. Renewable Energy From renewable energy, we can break it down further to see what energy sources are producing the gains. The rise of wind and solar energy is very evident. In 2003, hydroelectricity made up almost 78% of renewable energy production. By 2018, that share had dropped to 39.7%. 36% of renewable energy is now wind, and 13.6% is now solar. Wind energy is likely to overcome hydro in the near future. According to the EIA , in 2017 wind generated 6.3% of electricity while solar generated 1.3%. Both of these figures are likely to be higher in 2018 once the full year of data is available. While natural gas has outpaced renewable growth to date, the EIA predicts that they will lead the charge from here on out. Meanwhile, coals share of generation is expected to continue to decline in the next few years. Causes The large shifts over the past 15 years have been a result of shifting economics. Natural gas, solar, and wind energy all experienced significant declines in cost in recent years. 2 2 2 Get an email whenever Brayden Gerrard publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. zgr Gen Jan 25, 2019 Member-only Notes on Artificial Intelligence, Machine Learning and Deep Learning for curious people AI has been the most intriguing topic of 2018 according to McKinsey. It is mentioned as the key enabler now at the #1 and #3 spot of Gartner Top 10 Strategic Technology Trends for 2019. AI became a catch-all term that refers to any computer program that automatically does something 22 min read 22 min read Share your ideas with millions of readers. Felix Kuestahler Jan 25, 2019 Member-only Python Tutorial: Twitter Account Retrieval of Swiss Government Members Start your journey into social media analysis of politicians Click on the link above the Title for a list of all tutorial articles. This is the first article. The tutorial will show you how to extract a list of tweeting Swiss Government Members via the Twitter API. The extracted data will be put into a Panda Dataframe and then 7 min read 7 min read Tobias Hill Jan 25, 2019 How I got 1% better accuracy by data augmentation It is hard to let go of things you put love into. And yes, I really thought I was done after the 5:th article in this series but then the accuracy/error metrics from the MNIST example started haunting me and I was quite sure that I could improve on 4 min read 4 min read Partha Deka Jan 25, 2019 Empowering a citizen data scientist for hardware design & manufacturing Improving productivity of a hardware design and manufacturing professional with an advanced AI tool Authors: Partha Deka and Rohit Mittal What is a citizen data scientist? Expert data scientists rely on custom coding to make sense out of data. The use case could be data cleansing, data imputation, creating segments, finding patterns in the data, building 4 min read 4 min read Lak Lakshmanan Jan 25, 2019 How to do Bayesian hyper-parameter tuning on a blackbox model Optimization of arbitrary functions on Cloud ML Engine Google Cloud ML Engine offers a hyper-parameter tuning service that uses Bayesian methods. It is not restricted to TensorFlow or scikit-learn. In fact, it is not even limited to machine learning. You can use the Bayesian approach to tune pretty much any blackbox model. To demonstrate, Ill tune a traffic 3 min read 3 min read Brayden Gerrard Electric Vehicles | Green Energy | Data Science | Contact: gerrard.brayden@gmail dot com More from Medium Barry Gander Ancient Rome Did Not Fall: Why Real Story is Even Scarier for America and How It Connects to Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Laurel B. Miller in Sweary Mommy Naughty Snaps: Observing the Wannabe Model as She Attempts to Hunt Clients in the Wild Luay Rahil in ILLUMINATION Elon Musk Eliminated Remote Work Because Working From Home \"Doesn\\'t Work\" Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 370,\n",
       "  'url': 'https://towardsdatascience.com/repetition-in-songs-a-python-tutorial-3dbd1c279f19',\n",
       "  'title': 'Repetition in Songs: A Python\\xa0Tutorial',\n",
       "  'subtitle': 'One of Ed Sheeran songs as a case\\xa0study',\n",
       "  'claps': 144,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 8,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 7.112927102870214e-05,\n",
       "  'text': \"Towards Data Science Mar 30 2019 Listen Save Repetition Songs Python Tutorial One Ed Sheeran song case study Everyone ha heard song know song sound like carelessly say everyone define song word benefit doubt song according Wikipedia single work music typically intended sung human voice distinct fixed pitch pattern using sound silence variety form often include repetition section journal article called complexity Songs computer scientist Donald Knuth capitalized tendency popular song devolve long content-rich ballad highly repetitive text may waste time agreeing notion doe raise question like repetitiveness really help song become hit music really becoming repetitive time attempt teach basic python code form case study going test hypothesis popular song really repetitive one favorite song One way test hypothesis figure unique word calculate fraction word total number word song tutorial well cover Prerequisite Knowledge get tutorial follow along running code purpose case study streamline hypothesis asking two major question Let 's get started analyzing already Basic 1 String list character character anything type keyboard one keystroke like letter number backslash However Python recognizes string anything delimited quotation mark either double quote single quote beginning end character text example Hello world case study string lyric seen 2 Variables typically descriptive name word symbol used assign store value word storage placeholder datatype quite handy order refer value time variable always assigned equal sign followed value variable way view code output use print function may already know Jupyter notebook output viewed without print function store lyric assign variable named perfect_lyrics 3 Lists created simply putting different comma-separated value square bracket number item may different type integer float string etc. even another list item example gotten sense list look like Let go back data Since one aim figure number unique word used mean need bit counting i.e count word order achieve put string list separate word using .split method Therefore dataset look like Input Output output notice word ha separated independent string whole lyric make list called split_lyrics also variable also need separate unique word rest word count would need use python function function block organized reusable code used perform single related action Python ha many built-in function like print -to show output list -to create list len count number character list string etc Python also allows create function creating function case study using function case study start set function separate unique word whole lyric need set print function i.e Input Output count number word whole lyric would need len function Input Output unique word extracted analysis ha helped answer first question many unique word used compared whole lyric song Simply put 129 unique word used 290 word total next goal figure second part question repetitive word used many time used order answer question need learn data structure Intermediate 4 Dictionaries Python unordered collection item data structure value element dictionary ha key value pair key-value pair map key associated value Dictionaries optimized retrieve value key known define dictionary enclosing comma-separated list key-value pair curly brace colon separate key associated value example simple German-English dictionary 5 Loops great trying run block code python two type loop analysis focus loop loop used iterate element sequence often used piece code want repeat n number time work like element list dictionary example Going back dataset know repeated word need know number time word appeared lyric need use dictionary store word corresponding count loop iterate counting process word appears First need store unique word dictionary Input Output need use loop count number time unique word appears whole lyric Input Output found number time word appears let 's sort view highest lowest using sorted function seem many word occurred aim find popular word word narrow list 10 word slicing learn slicing Input Output changing back dictionary additional question 10 popular word song called perfect Ed Sheeran easily extract information using key method dictionary data structure creating list word Input Output output code confidently say popular word used song called Perfect Ed Sheeran appeared 24 time Lets improve analysis little Advanced 6 Visualizing data python Various technique developed presenting data visually analysis focus data visualization strictly library Python namely Matplotlib Lets quickly visualize analysis top 10 popular word case study Summary Insights key insight draw case study spot three Conclusion case study doe help see greater percentage song made repetitive word However conclude one song Donald Knuth theory true need analyze lot song conclude hit song result repetitiveness word song Although good point Perfect Ed Sheeran wa actually Hit song still world Reading want improve Python skill article may help P.S Like anyone learn data analyst want notified next project update learning feel free sign newsletter Thanks TDS Editors 144 2 144 144 2 Towards Data Science home data science Medium publication sharing concept idea code Alex Blyakhman Mar 30 2019 Member-only Getting Started Google BigQuerys Machine Learning Titanic Dataset still Beta BigQuery ML ha available since mid last year however didnt get around working Google cloud-based Machine Learning offering recently non-data scientist first impression whats like ability run ML model 12 min read 12 min read Share idea million reader Jeremie Harris Mar 30 2019 problem data science job posting Every notice something realize probably noticed long time ago start see everywhere wonder people arent talking every wa yesterday wa scrolling 5 min read 5 min read Marco Cerliani Mar 30 2019 Member-only Predictive Maintenance detect Faults Sensors CNN interesting approach python code graphic representation Machine Learning topic Predictive Maintenance becoming popular passage time challenge easy heterogenous useful good knowledge domain touch people know underlying system work 6 min read 6 min read Tim Darling Mar 30 2019 Member-only Executives Guide Implementing AI Machine Learning lesson Ive learned applying AI/Machine Learning support business objective Chief Analytics Officer Ive bridge gap business need data scientist gap bridged experience difference well value promise artificial intelligence AI machine learning realized 7 min read 7 min read Sik-Ho Tsang Mar 30 2019 Review DeepPose Cascade CNN Human Pose Estimation Using Cascade Convolutional Neural Networks Refinement State-of-the-art Performance Four Datasets story DeepPose Google Human Pose Estimation reviewed formulated Deep Neural Network DNN -based regression problem towards body joint cascade DNN high precision pose estimate achieved 2014 CVPR paper 900 citation 6 min read 6 min read Okoh Anita Full-Stack Data Scientist Building Data Product Python Analytics Machine Learning New Technologies Obsessed| Self-care Conscious Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Towards Data Science Mar 30, 2019 Listen Save Repetition in Songs: A Python Tutorial One of Ed Sheeran songs as a case study Everyone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung by the human voice with distinct and fixed pitches and patterns using sound and silence and a variety of forms that often include the repetition of sections . In his journal article called The complexity of Songs, computer scientist Donald Knuth capitalized on the tendency of popular songs to devolve from long and content-rich ballads to highly repetitive texts. As some may waste no time agreeing with his notion, it does raise some questions like: Does repetitiveness really help songs become a hit? Is music really becoming more repetitive over time? In an attempt to teach some basic python code in the form of a case study, I am going to test this hypothesis (Are popular songs really repetitive?) with one of my favorite songs. One way to test this hypothesis is to figure out the unique words and calculate the fraction of those words to the total number of words in a song. In this tutorial, well cover: Prerequisite Knowledge To get the most out of this tutorial, you can follow along by running the codes yourself. For the purpose of this case study, we will streamline our hypothesis by asking two major questions: Let's get started analyzing already The Basic 1. A String  is a list of characters. A character is anything you can type on the keyboard in one keystroke, like a letter, a number, or a backslash. However, Python recognizes strings as anything that is delimited by quotation marks either a double quote ( ) or a single quote ( ) at the beginning and end of a character or text. For example: Hello world For this case study, a string is our lyrics as seen below 2. Variables   are typically descriptive names, words or symbols used to assign or store values. In other words, they are storage placeholders for any datatype. It is quite handy in order to refer to a value at any time. A variable is always assigned with an equal sign, followed by the value of the variable. (A way to view your code output is to use a print function. As you may already know with Jupyter notebook, an output can be viewed without a print function) To store the lyrics, we will assign it a variable named perfect_lyrics . 3. Lists   can be created  simply by putting different comma-separated values between square brackets. It can have any number of items and they may be of different types (integer, float, string etc.). It can even have another list as an item. For example: Now that we have gotten a sense of what a list looks like. Let go back to our data. Since one of our aims is to figure out the number of unique words used, it means we will need to do a bit of counting i.e to count each word. In order to achieve these, we will not only have to put our string into a list but will have to separate each word using a  .split()  method. Therefore our dataset will look like this Input Output From the above output, you will notice that each word has been separated into independent strings. And the whole lyrics make up the list called split_lyrics (which is also a variable) We will also need to separate the unique words of the rest of the words and the count. To do this, we would need to use python functions. A function is a block of organized, reusable code that is used to perform a single, related action. Python has many built-in functions like print( )-to show your output, list( )-to create a list, len( )- to count the number of characters in a list or string, etc. Python also allows you to create your own functions. But we will not be creating our own function in this case study. We will be using a few functions in the case study but we will start with the set( ) function. To separate the unique words from the whole lyrics, we need a set( ) and a print ( ) function i.e Input Output To count the number of words in the whole lyrics, we would need a len ( ) function Input & Output Doing the same for the unique words extracted Our above analysis has helped answer our first question:  How many unique words were used as compared to the whole lyrics of the song? Simply put, 129 unique words were used in over 290 words in total. Our next goal is to figure out the second part of the question What are the most repetitive words used and how many times were they used? In order to answer this question, we will need to learn more data structures. Intermediate 4. Dictionaries   in Python are unordered collections of items. While data structures have only values as an element, A dictionary has a  key:value  pair. Each key-value pair maps the key to its associated value. Dictionaries are optimized to retrieve values when the key is known. You can define a dictionary by enclosing a comma-separated list of key-value pairs in curly braces ( {} ). A colon ( : ) separates each key from its associated value. An example is a simple German-English dictionary: 5. Loops  are great when trying to run the same block of code over and over again. In python, there are two types of loops: for and while. For this analysis, we will focus more on the for loop . The for loop is used to iterate over elements of a sequence. It is often used when you have a piece of code which you want to repeat n number of times. It works like this: for all elements in a list or dictionary, do this For example Going back to our dataset To know the most repeated word(s), we need to know the number of times each word appeared in the lyrics. To do that, we will need to use both a dictionary (to store each word with their corresponding count) and a for loop (to iterate the counting process as each word appears) First, we need to store the unique words in a dictionary Input & Output Then we need to use the for loop again to count the number of times each unique word appears in the whole lyrics. Input & Output Now we have found out the number of times each word appears, let's sort them out to view them from highest to lowest using the sorted ( ) function There seem to be too many words that only occurred once. Because our aim is to find the most popular word or words, we will narrow our list to the 10 words by slicing it. You can learn more about slicing here Input & Output Then changing back to a dictionary An additional question will be: What are the 10 most popular words in the song called perfect by Ed Sheeran? We can easily extract this information by using the key ( )method under the dictionary data structure and then creating a list of those words. Input & Output From the above output code, we can confidently say that the most popular word used in the song called Perfect by Ed Sheeran is I which appeared 24 times Lets improve our analysis a little further Advanced 6. Visualizing data with python. Various techniques have been developed for presenting data visually but in this analysis, we will focus data visualization strictly on a library in Python , namely Matplotlib. Lets quickly visualize the analysis on the top 10 most popular words in our case study Summary Insights What are some key insights we can draw from this case study? I can spot out three Conclusion This case study does help to see that a greater percentage of a song is made up of repetitive words. However, we can not conclude with just one song that Donald Knuth theory is true. We will need to analyze a lot more songs to conclude that hit songs are as a result of the repetitiveness of words in a song . Although it is good to point out that Perfect by Ed Sheeran was actually a Hit song (and still is in my world). Further Reading If you want to improve your Python skills, these are some articles that may help P.S Like me, anyone can learn to be a data analyst and if you want to be notified on my next project or updates on my learning, feel free to sign up to my  newsletter Thanks to TDS Editors 144 2 144 144 2 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Alex Blyakhman Mar 30, 2019 Member-only Getting Started with Google BigQuerys Machine Learning Titanic Dataset While still in Beta, BigQuery ML has been available since mid last year; however, I didnt get around to working with this Google cloud-based Machine Learning offering until recently. As a non-data scientist, my first impression whats not to like? After all, the ability to run ML models from 12 min read 12 min read Share your ideas with millions of readers. Jeremie Harris Mar 30, 2019 The problem with data science job postings Every once in a while, you notice something that you realize you probably should have noticed a long time ago. You start to see it everywhere. You wonder why more people arent talking about it. For me, every once in a while was yesterday when I was scrolling through the 5 min read 5 min read Marco Cerliani Mar 30, 2019 Member-only Predictive Maintenance: detect Faults from Sensors with CNN An interesting approach with python code and graphic representations In Machine Learning the topic of Predictive Maintenance is becoming more popular with the passage of time. The challenges are not easy and very heterogenous: its useful to have a good knowledge of the domain or to be in touch with people who know how the underlying system works. 6 min read 6 min read Tim Darling Mar 30, 2019 Member-only An Executives Guide to Implementing AI and Machine Learning Some lessons Ive learned applying AI/Machine Learning to support business objectives As a Chief Analytics Officer, Ive had to bridge the gap between business needs and data scientists. How that gap is bridged is, in my experience, the difference between how well the value and promise of artificial intelligence (AI) and machine learning is realized. 7 min read 7 min read Sik-Ho Tsang Mar 30, 2019 Review: DeepPose Cascade of CNN (Human Pose Estimation) Using Cascade of Convolutional Neural Networks for Refinement, State-of-the-art Performance on Four Datasets In this story, DeepPose, by Google, for Human Pose Estimation, is reviewed. It is formulated as a Deep Neural Network (DNN)-based regression problem towards body joints. With a cascade of DNN, high precision pose estimates are achieved. This is a 2014 CVPR paper with more than 900 citations. 6 min read 6 min read Okoh Anita Full-Stack Data Scientist | Building Data Product with Python, Analytics, and Machine Learning | New Technologies Obsessed| Self-care Conscious More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 5451,\n",
       "  'url': 'https://towardsdatascience.com/object-detection-via-color-based-image-segmentation-using-python-e9b7c72f0e11',\n",
       "  'title': 'Object detection via color-based image segmentation using\\xa0python',\n",
       "  'subtitle': 'A tutorial on contouring using…',\n",
       "  'claps': 496,\n",
       "  'responses': 4.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 0.0002450008224321963,\n",
       "  'text': 'Towards Data Science Mar 31 2019 Member-only Listen Save Object detection via color-based image segmentation using python tutorial contouring using python OpenCV Getting started already jupyter notebook IDE run python OpenCV installed skip Execution Tools hero today Anaconda free open-source distribution help installing different package sort mess isolated environment Wikipedias telling u Anaconda Anaconda free open-source distribution Python R programming language scientific computing data science machine learning application large-scale data processing predictive analytics etc aim simplify package management deployment Package version managed package management system conda Anaconda distribution used 12 million user includes 1400 popular data-science package suitable Windows Linux MacOS detailed tutorial download Anaconda anaconda Windows anaconda Linux Creating environment Open bash cmd type Type yes prompted download package open jupyter notebook browser important term Contours Contours explained simply curve joining continuous point along boundary color intensity contour useful tool shape analysis object detection recognition Thresholds Applying thresholding grayscale image make binary image set threshold value value threshold turned black value go white Execution need start gon na start simple example show color-based segmentation work Bear till get good stuff want try get image free following code Im gon na segment image 17 gray level measure level area using contouring function simply convert range intensity gray want contour highlight iteration unifying lie within range one intensity turn every intensity range black including greater smaller intensity second step threshold image color want contour right appears white others convert black step doesnt change much must done contouring work best black white threshold image applying step thresholding image would except white ring wouldve gray gray intensity 10th gray level 25515 10 way obtain area gray level really important move want stress importance topic Im Computer Engineering student Im working project called Machine learning intelligent tumor detection identification Color-based image segmentation used project help computer learn detect tumor dealing MRI scan program ha detect cancer level said MRI scan doe segmenting scan different grayscale level darkest filled cancerous cell closest white healthier part calculates degree membership tumor grayscale level information program able identify tumor stage project based soft computing fuzzy logic machine learning learn Fuzzy logic curing cancer Object detection get image free Pexels need crop image want contour leaf Since texture image irregular uneven meaning although arent many color intensity green color image change also brightness best thing unify different shade green one shade way apply contouring deal leaf one whole object Note result apply contouring image without pre-processing wanted see uneven nature leaf make OpenCV understand one object First know HSV representation color know converting RGB HSV like following Converting image HSV easier HSV get complete range one color HSV H stand Hue Saturation V value already know green color 60 255 255 green world lie within 45 100 50 75 255 255 60 15 100 50 60+ 15 255 255 15 approximation value take range convert 75 255 200 light color 3rd value must relatively large cause thats brightness color thats value make part white threshold image Since seem irregularity background well get largest contour using method largest contour course leaf get index leaf contour contour array get area center leaf Contours many feature make use contour perimeter convex hull bounding rectangle many others learn 591 5 591 591 5 Towards Data Science home data science Medium publication sharing concept idea code Sahil Dhankhad Mar 31 2019 Member-only Forget APIs Python Scraping Using Beautiful Soup Import Data File web Part 2 APIs always every website Beautiful Soup going stay forever collect kind data website Today Data play critical role every industry data coming internet company 8 min read 8 min read Share idea million reader Andrew Donaldson Mar 31 2019 Member-only Dont Fear Robots Impressive tech important remember Boston Dynamics video trailer movie isnt yet made Every time one Boston Dynamics video hit met mix amazement prediction doom hand new robot overlord impressive also well choreographed staged Impressive tech important 4 min read 4 min read Jasmine Vasandani Mar 31 2019 Member-only 10 Steps Teaching Data Science Well resource data science instructor increase demand data scientist ha parallel growth training program within company educational institution teach people data science addition focusing meeting job demand need resource instructor facilitating data science classroom 4 min read 4 min read Karan Bhanot Mar 31 2019 Member-only income 50K/yr Machine Learning tell Machine learning breaking ground numerous field including Finance could use Machine Learning model identify income individual found right dataset called Census Income Dataset 7 min read 7 min read Ane Berasategi Mar 31 2019 Member-only Semantic search brief post semantics search semantic search READ ORIGINAL POST BLOG Semantics branch linguistics studying meaning word symbolic use also including multiple meaning One morning shot elephant pajama got pajama Ill never know Groucho Marx sentence semantically ambiguous clear author 5 min read 5 min read Salma Ghoneim SDE Microsoft Passionate frontend development fascinated artificial intelligence Interested game development Medium Fnnnr tctt2022 pwnable 02 Pranjal Saxena Level Coding Step Step Guide Labeling Object Detection Training Images Using Python Chinmay Bhalerao Towards AI Working Computer Vision project code chunk help Mikolaj Buchwald YOLO COCO object recognition basic Python Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Mar 31, 2019 Member-only Listen Save Object detection via color-based image segmentation using python A tutorial on contouring using python & OpenCV. Getting started If you already have jupyter notebook or an IDE with which you can run python & OpenCV installed, just skip to Execution. Tools Our hero today is Anaconda . a free open-source distribution that helps with installing different packages & sorts out their messes into isolated environments. What Wikipedias telling us about Anaconda Anaconda is a free and open-source distribution of the Python and R programming languages for scientific computing (data science, machine learning applications, large-scale data processing, predictive analytics, etc. ), that aims to simplify package management and deployment . Package versions are managed by the package management system conda . The Anaconda distribution is used by over 12 million users and includes more than 1400 popular data-science packages suitable for Windows, Linux, and MacOS. Here are detailed tutorials on how to download Anaconda. anaconda for Windows & anaconda for Linux. Creating the environment Open the bash (cmd) and type this Type y (for yes) when prompted to download the packages. This will open jupyter notebook in the browser for you. Some important terms Contours Contours can be explained simply as a curve joining all the continuous points (along with the boundary), having the same color or intensity. The contours are a useful tool for shape analysis and object detection and recognition. Thresholds Applying thresholding on a grayscale image makes it a binary image. You set a threshold value, in which all values below this threshold are turned to black and all values above go white. Execution Now you have all you need to start. Were gonna start with a simple example just to show you how color-based segmentation works. Bear with me till we get to the good stuff. If you want to try this with me, you can get this image for free from here . In the following code, Im gonna segment this image into 17 gray levels. Then measure each levels area using contouring. In this function, I simply convert the range (of intensities) of gray that I want to contour (highlight) in this iteration by unifying all of those which lie within this range to one intensity. I turn every other intensity but this range to black (including greater & smaller intensities). The second step I threshold the image so that only the color that I want to contour right now appears white and all others convert to black. This step doesnt change much here but it must be done because contouring works best with black and white (thresholds) images. Before applying this step (thresholding) the image below would be the same except the white ring wouldve been gray (the gray intensity of the 10th gray level (25515*10 ) ) This way we obtain the area of each gray level. Is this really important? Before we move on, I want to stress the importance of this topic. Im a Computer Engineering student and Im working on a project called Machine learning for intelligent tumor detection and identification . Color-based image segmentation is used in this project to help the computer learn how to detect the tumor. When dealing with an MRI scan, the program has to detect the cancer level of said MRI scan. It does that by segmenting the scan into different grayscale levels in which the darkest is the most filled with cancerous cells and the closest to white is the healthier parts. Then it calculates the degree of membership of the tumor to each grayscale level. With this information, the program is able to identify the tumor and its stage. This project is based on soft computing, fuzzy logic & machine learning, you can learn more about it on Fuzzy logic and how it is curing cancer . Object detection You can get this image for free on Pexels from here . You just need to crop it. In this image, we want to contour the leaf only. Since the texture of this image is very irregular and uneven, meaning that although there arent many colors. The intensity of the green color in this image changes, also, its brightness. So, the best thing to do here is to unify all these different shades of green into one shade. This way when we apply contouring, it will deal with the leaf as one whole object. Note: This is the result if you apply contouring on the image without any pre-processing. I just wanted you to see how the uneven nature of the leaf makes OpenCV not understand that this is just one object. First, you have to know the HSV representation of your color , you can know it by converting its RGB to HSV just like the following. Converting the image to HSV : Its easier with HSV to get the complete range of one color. HSV, H stands for Hue, S for Saturation, V for value. We already know that the green color is [60, 255, 255]. All the greens in the world lie within [45, 100, 50] to [75, 255, 255] that is [60 15 , 100, 50] to [60+ 15 , 255, 255]. 15 is just an approximation value. We take this range and convert it to [75, 255, 200 ] or any other light color ( 3rd value must be relatively large) cause thats the brightness of the color, thats the value that will make this part be white when we threshold the image. Since there seem to be irregularities in the background as well, We can get the largest contour using this method, The largest contour is, of course, the leaf. We can get the index of the leaf contour in the contours array, from that we get the area and center of the leaf. Contours have many other features that you can make use of such as the contour perimeter, convex hull, bounding rectangle, and many others. You can learn more about it from here . 591 5 591 591 5 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Sahil Dhankhad Mar 31, 2019 Member-only Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2 APIs are not always there for you for every website, but Beautiful Soup is going to stay with you forever to collect any kind of data from any website. Today, Data play a critical role in every industry. And most of this data is coming from the internet. Most company 8 min read 8 min read Share your ideas with millions of readers. Andrew Donaldson Mar 31, 2019 Member-only Dont Fear the Robots Impressive as the tech here is, it is important to remember that the Boston Dynamics videos are trailers for a movie that isnt yet made. Every time one of these Boston Dynamics videos hit, they are met with a mix of amazement and predictions of doom at the hands of our new robot overlords. It is impressive. It is also very well choreographed and staged. Impressive as the tech here is, it is important 4 min read 4 min read Jasmine Vasandani Mar 31, 2019 Member-only 10 Steps to Teaching Data Science Well A resource for data science instructors. With the increase in demand for data scientists, there has been a parallel growth in training programs within companies and educational institutions that teach people data science. In addition to focusing on meeting the job demand, there need to be more resources for instructors facilitating data science classrooms. 4 min read 4 min read Karan Bhanot Mar 31, 2019 Member-only Will your income be more than $50K/yr? Machine Learning can tell Machine learning is breaking grounds in numerous fields including Finance. What if we could use Machine Learning models to identify incomes of individuals? I found just the right dataset for this, called Census Income Dataset. 7 min read 7 min read Ane Berasategi Mar 31, 2019 Member-only Semantic search A brief post on semantics, search, and semantic search READ THE ORIGINAL POST IN MY BLOG Semantics is a branch of linguistics studying the meanings of words, their symbolic use, also including their multiple meanings. One morning I shot an elephant in my pajamas. How he got into my pajamas Ill never know. Groucho Marx This sentence is semantically ambiguous: its not clear if the author 5 min read 5 min read Salma Ghoneim SDE at Microsoft, Passionate about frontend development, fascinated by artificial intelligence, Interested in game development. More from Medium Fnnnr [tctt2022] pwnable 02 Pranjal Saxena in Level Up Coding Step by Step Guide for Labeling Object Detection Training Images Using Python Chinmay Bhalerao in Towards AI Working on a Computer Vision project? These code chunks will help you!!! Mikolaj Buchwald YOLO and COCO object recognition basics in Python Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5458,\n",
       "  'url': 'https://towardsdatascience.com/you-dont-need-a-masters-to-be-a-data-scientist-9ab690c7bddd',\n",
       "  'title': 'You Don’t Need a Masters To Be a Data Scientist',\n",
       "  'subtitle': 'How I did\\xa0it',\n",
       "  'claps': 82,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 4.050416822467761e-05,\n",
       "  'text': 'Towards Data Science Apr 1 2019 Member-only Listen Save Dont Need Masters Data Scientist Heres weird story became data scientist without pursuing master degree paid resource list simply sharing done hands-on video tutorial machine learning deep learning artificial intelligence checkout YouTube channel University enrolled chemical engineering McGill University one top university Canada ha quite good reputation worldwide chose chemical engineering wa still passionate clean energy technology wa motivated contributing cleaner sustainable way producing energy Unfortunately program fell short meeting expectation class addressed topic career perspective mostly big oil company least thats saw time Simply put wa enjoying degree Feeling wa going anywhere pursuing chemical engineering tried finding new field friend mine wa web developer even started web development company seemed successful passion inspired check Learning web development university set learn web development completing bachelor chemical engineering first started reading MDNs web development guide immediately fell love wa dry learned lot reading documentation trying thing However wanted push skill forward took Colt Steeles Web Developer Bootcamp read many great review course thought wa worth paying dollar try turned amazing course absolutely enjoyed learned great deal used course build simple portfolio small project presented recruiter university career fair landed first job web developer wa ecstatic managed set foot tech world interviewer asked transitioned chemical engineering programming prove required skill whereas computer science student software engineering student simply show CV December 2017 completed bachelor assured job start July 2018 end university new job January 2018 July 2018 wa free school work 6 month relaxing waiting start new job web developer However simply cant nothing Thats learned craze data scientist Everywhere wa reading wa sexiest job jazz figured check started learning data science Dataquest approach fitted learning style since lot free time knew could complete curriculum fast fact crammed entire data scientist data engineer path 2 month experience wa amazing opinion learned lot get complete many project build solid data science portfolio However main drawback wa understand wa applying worked lacked theory behind algorithm Therefore took Andrew Ngs Machine Learning course heard course think probably one highest rated course Coursera absolutely loved learned much math behind machine learning However like course wa taught Matlab/Octave wa hard translate learned Python learned great deal fundamental machine learning still felt wa lacking knowledge Learning data science working July 2018 started new job wa excited wa juggling learning web development data science started data science spare time work decided read book Introduction Statistical Learning friend recommended book saying wa best introduction data science gave shot wa pleasantly surprised Reading book wa actually enjoyable wa serious study wa taking note forced apply every algorithm Python building small project really helped master traditional machine learning algorithm importantly gained confidence skill something weird happened wa browsing Facebook work yes work admit saw ad data scientist position one largest bank Canada ad simply said looking data scientist Take quiz thought nothing lose took quiz got 11/13 Even wa impressed score week later get phone interview one hour telling interviewer transitioned chemical engineering programming recruiter decided make go final step final step consisted completing data science project presenting senior data scientist decided rework project completed studying Dataquest presented week later got job offer accepted January 2019 started new job data scientist Since January 2019 Since learning work opportunity multiplied get collaborate incredibly smart motivated people company encourages u learn explore experiment innovate feel lucky type organization proud managed learn data science acquire relevant skill land job deep learning natural next step already working right right attitude mindset yes firmly believe anyone accomplish far exception way fitted personality learning style could However many drawback well hard passionate disciplined subject accomplish Yet found way wa definitely rewarding need master data scientist think company looking diploma anymore looking skilled people instead end really matter set skill Whether decide pursue master realize different way reaching result case make sure gain following skill aspiring data scientist wish best luck absolutely love field think class took online really helped love understand field end key passionate subject willing share passion many people possible opportunity come wish lot success 102 1 102 102 1 Get email whenever publish Get freebie course announcement VIP invitation event straight inbox Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Shaw Lu Apr 1 2019 Member-only Visualizing Beta Distribution Bayesian Updating Seeing believing build intuition simulating visualizing inspecting every step Beta distribution one esoteric distribution compared Bernoulli Binomial Geometric distribution also rare practice doe readily available real-world analogy help intuition make matter worse online tutorial tend intimidate reader complex formula beta 7 min read 7 min read Share idea million reader Rudradeb Mitra Apr 1 2019 Member-only Successful Adoption AI Need Female Leaders woman posse right quality lead AI project successfully enable environment collaboration inclusion build real-world AI product exciting breakthrough twenty-first century occur technology expanding concept mean human John Naisbitt Part Differences Men Women story woman men work differently 8 min read 8 min read Manu Siddharth Jha Mar 31 2019 Member-only Artificial Intelligence next big thing Hollywood Movies captured imagination people ever since came limelight Right first motion picture late 1880s upcoming latest sci-fi blockbuster cinema ha become medium love joy passion movie lover Almost every country across globe 5 min read 5 min read Guy Tsror Mar 31 2019 Member-only romantic side data science Analyzing relationship year worth text message wa supposed private project one thought would upload private Google Colab notebook share whomever relevant one person referred OJ lightbulb clean share 11 min read 11 min read Salma Ghoneim Mar 31 2019 Member-only Object detection via color-based image segmentation using python tutorial contouring using python OpenCV Getting started already jupyter notebook IDE run python OpenCV installed skip Execution Tools hero today Anaconda free open-source distribution help installing different package sort mess isolated environment Wikipedias telling u 7 min read 7 min read Marco Peixeiro Senior data scientist Author Instructor write hands-on article focus practical skill Medium Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Akshad Singi Better Humans 4 Unsexy One-Minute Habits Save 30+ Hours Every Week Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 1, 2019 Member-only Listen Save You Dont Need a Masters To Be a Data Scientist How I did it Heres my (weird) story on how I became a data scientist without pursuing a masters degree. I am not paid for any of the resource I will list below. I am simply sharing what I have done. For hands-on video tutorials on machine learning, deep learning, and artificial intelligence, checkout my YouTube channel . University I enrolled in chemical engineering at McGill University. It is one of the top universities in Canada, and it has quite a good reputation worldwide. I chose chemical engineering because I was (and still am) very passionate about clean energy technologies. I was motivated in contributing to a cleaner and more sustainable way of producing energy. Unfortunately, the program fell short of meeting my expectations. Very few classes addressed this topic, and the career perspectives were mostly with big oil companies (or at least, thats what I saw at that time). Simply put, I was not enjoying my degree at all. Feeling I was not going anywhere pursuing chemical engineering, I tried finding a new field. A friend of mine was a web developer and he even had started his own web development company. He seemed successful and his passion for it inspired me to check it out. Learning web development while university So I set myself to learn web development while completing my bachelors in chemical engineering. I first started reading MDNs web development guide . I immediately fell in love with it. It was very dry, but I learned a lot while reading the documentation and trying things on my own. However, I wanted to push my skills forward, so I took Colt Steeles  The Web Developer Bootcamp  . I had read many great reviews about this course, and I thought it was worth paying a few dollars to try it out. It turned out to be an amazing course! I absolutely enjoyed it and learned a great deal. I used the course to build a simple portfolio of small projects and presented them to recruiters at my universitys career fair. This is how I landed my first job as a web developer. I was ecstatic! I managed to set my foot in the tech world on my own. All interviewers asked me why I transitioned from chemical engineering to programming, and I had to prove them that I had the required skills, whereas computer science students or software engineering students simply had to show a CV. In December 2017, I completed my bachelors and I had an assured job to start in July 2018. Between the end of university and my new job From January 2018 to July 2018, I was free! No school, no work, just 6 months of relaxing and waiting to start my new job as a web developer. However, I simply cant do nothing. Thats when I learned about the craze for data scientists. Everywhere, I was reading it was the sexiest job , and all that jazz. I figured I had to check it out. I started learning data science on Dataquest . Their approach fitted more my learning style and since I had a lot of free time, I knew I could complete the curriculum fast. In fact, I crammed the entire data scientist and data engineer paths in 2 months. The experience was amazing in my opinion. I learned a lot, and you get to complete many projects, which will build a very solid data science portfolio. However, the main drawback for me was that I did not understand what I was applying. It worked, but I lacked the theory behind the algorithms. Therefore, I took Andrew Ngs Machine Learning course. Again, I had heard about this course and I think it is probably one of the highest rated course on Coursera. I absolutely loved it and learned much more about the math behind machine learning. However, I did not like that the course was taught in Matlab/Octave, so it was hard to translate what I learned in Python. I learned a great deal about the fundamentals of machine learning, but I still felt I was lacking some knowledge. Learning data science while working In July 2018, I started my new job. I was very excited, but I was juggling between learning more about web development and data science. So I started doing data science on my spare time (after work). I decided to read the book  An Introduction to Statistical Learning  . A friend recommended this book, saying it was the best introduction to data science. I gave it a shot, and I was pleasantly surprised! Reading this book was actually enjoyable. I was very serious in my study. I was taking notes and I forced myself to apply every algorithm in Python by building very small projects. This really helped master most of the traditional machine learning algorithms and most importantly, I gained confidence in my skills. Then, something weird happened I was browsing Facebook at work (yes, at work, I admit it), and I saw an ad for a data scientist position at one of the largest banks in Canada. The ad simply said: Were looking for data scientists! Take the quiz! I thought I had nothing to lose. I took the quiz and got 11/13. Even I was impressed with my score! A week later, I get a phone interview. After one hour (and after telling the interviewer how I transitioned from chemical engineering to programming), the recruiter decided to make go through the final step. The final step consisted in completing a data science project and presenting it to senior data scientists. I decided to rework a project I completed while studying with Dataquest, and I presented it. A week later, I got a job offer and I accepted it. In January 2019, I started my new job as a data scientist. Since January 2019 Since then, the learning and work opportunities have just multiplied. I get to collaborate with incredibly smart and motivated people, and the company encourages us to learn, explore, experiment and innovate. I feel very lucky to be in that type of organization, and I am very proud that I managed to learn data science and acquire the relevant skills by myself to land a job. Now, deep learning is a natural next step, which I am already working on right now. Can you do the same? With the right attitude and mindset, yes. I firmly believe that anyone can accomplish the same. I am far from being an exception. I did it this way because it fitted my personality and my learning style. I could: However, there were many drawbacks as well: What I did is hard. You have to be passionate and disciplined about a subject to accomplish what I did. Yet, I found this way was definitely the most rewarding. So, do I need a masters to be a data scientist? No. I think that companies are not looking at diplomas anymore, and are looking for skilled people instead. In the end, what really matters, is your set of skills. Whether you decide to pursue a masters or not, realize that it is only a different way of reaching the same result. In any case, make sure you gain the following skills: If you are an aspiring data scientist, I wish you best of luck! I absolutely love the field, and I think that the classes I took online really helped love and understand this field. In the end, the key is being passionate about a subject and to be willing to share your passion with as many people as possible. Then, the opportunities will come. I wish you a lot of success! 102 1 102 102 1 Get an email whenever I publish! Get freebies, course announcement, VIP invitations to events and more straight into your inbox! Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Shaw Lu Apr 1, 2019 Member-only Visualizing Beta Distribution and Bayesian Updating Seeing is believing: build intuition by simulating, visualizing, and inspecting every step Beta distribution is one of the more esoteric distributions compared to Bernoulli, Binomial and Geometric distributions. It is also rare in practice because it does not have a readily available real-world analogy that helps intuition. To make matters worse, online tutorials tend to intimidate readers with complex formula (beta & 7 min read 7 min read Share your ideas with millions of readers. Rudradeb Mitra Apr 1, 2019 Member-only For the Successful Adoption of AI, We Need More Female Leaders Why women possess the right qualities to lead AI projects more successfully and enable an environment for collaboration and inclusion to build real-world AI products. The most exciting breakthroughs of the twenty-first century will not occur because of technology, but because of an expanding concept of what it means to be human John Naisbitt Part I: Differences between Men and Women A story of how women and men can work differently 8 min read 8 min read Manu Siddharth Jha Mar 31, 2019 Member-only Is Artificial Intelligence the next big thing in Hollywood? Movies have captured the imagination of people ever since they came into the limelight. Right from the first motion picture in the late 1880s to the upcoming latest sci-fi blockbuster, cinema has become a medium of love, joy, and passion for all movie lovers. Almost every country across the globe 5 min read 5 min read Guy Tsror Mar 31, 2019 Member-only The romantic side of data science: Analyzing a relationship through a year worth of text messages This was supposed to be a private project, one I thought I would upload to a private Google Colab notebook, and share with just whomever it is relevant for (which is, one other person, referred to here as OJ). But then, a lightbulb why not clean it up and share 11 min read 11 min read Salma Ghoneim Mar 31, 2019 Member-only Object detection via color-based image segmentation using python A tutorial on contouring using python & OpenCV. Getting started If you already have jupyter notebook or an IDE with which you can run python & OpenCV installed, just skip to Execution. Tools Our hero today is Anaconda. a free open-source distribution that helps with installing different packages & sorts out their messes into isolated environments. What Wikipedias telling us about 7 min read 7 min read Marco Peixeiro Senior data scientist | Author | Instructor. I write hands-on articles with a focus on practical skills. More from Medium Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Akshad Singi in Better Humans 4 Unsexy One-Minute Habits That Save Me 30+ Hours Every Week Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 2674,\n",
       "  'url': 'https://towardsdatascience.com/deploy-ml-models-at-scale-151204549f41',\n",
       "  'title': 'Deploy ML models at\\xa0scale',\n",
       "  'subtitle': 'Part 1: API service for ML\\xa0models',\n",
       "  'claps': 107,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 4,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-05-27',\n",
       "  'clap_prop': 5.285300000049395e-05,\n",
       "  'text': 'Towards Data Science May 27 2019 Listen Save Deploy ML model scale Part 1 API service ML model Lets assume built ML model happy performance next step deploy model production blog series cover deploy model large scale consumption scalable Infrastructure using AWS using docker container service blog start first step building API framework ML model running local machine purpose blog let consider Sentiment classification model built order deploy model follow step Convert model .hdf5 file .pkl file case model built sklearn would best save .pkl file Alternatively deep learning model recommended save model HDF file main difference .pkl .hdf pickle requires large amount memory save data structure disk HDF designed efficiently store large data set Save model Pickle .pkl Save model HDF .hdf5 trained deep learning model kera tensorflow save model architecture weight using .hdf5 file system Implement Flask API Step 1 Load saved model per previous section one following method depending type file i.e hdf5 pkl HDF5 PKL Step 2 Import flask create flask application object shown Step 3 next step build test API function return API working string used ensure health API deployed production use app.route python decorator decorator function take another function extends behaviour latter function without explicitly modifying Step 4 next step build POST request api processing request sentiment model using path name /sentiment function read json input convert panda dataframe extract relevant field json call get_sentiment_DL function processing get_sentiment_DL function contains trained model ha loaded via hdf5 file finally return back result model form json result Step 5 detailed model processing step performed get_sentiment_DL function case deep learning sentiment model passing 2. text_data Input text sentiment classification 3. word_idx Word index GloVe file detail model Step 6 Add section run app Host set 0.0.0.0 hosting local server However configure network setting Debug set True time building API functionality Port set 5005 however configured per requirement Run API run API open command line window go directory code stored Run python script running command sentiment.py name file API implementation running command able see result command line window API running test API going browser typing 0.0.0.0:5005/apitest get result browser pas data API using python shown address case http //0.0.0.0:5005/sentiment result model returned stored response field Conclusion conclusion covered step deploy model api service local computer next step deploy cloud server micro service following blog cover use container service docker deploy AWS 121 2 121 121 2 Towards Data Science home data science Medium publication sharing concept idea code Joseph Magiya May 26 2019 Member-only Pearson Coefficient Correlation Explained Ive come realize lot confusion different type co-relation perform data set Let clear smoke starting Pearson Coefficient Correlation correlation Correlation bi-variate analysis measure strength association 4 min read 4 min read Share idea million reader Mai Nguyen May 26 2019 Member-only Exploratory Data Analysis Python B2B Marketing deep dive B2B Marketing using Data Visualization project focus conducting Exploratory Data Analysis EDA B2B Marketing using Python use data Olist e-commerce platform connects small medium business top Marketplaces Brazil example Besides providing method code also want discus 10 min read 10 min read Vincent Tatan May 26 2019 Member-only 12 minute Stocks Analysis Pandas Scikit-Learn Analyse Visualize Predict stock price quickly Python One day friend mine told key financial freedom investing stock greatly true market boom still remains attractive option today trade stock part time Given easy access online trading platform many 12 min read 12 min read Gilbert Tanner May 26 2019 Member-only Google Coral USB Accelerator Introduction Speeding machine learning model small form factor Last year Google Next conference Google announced building two new hardware product around Edge TPUs purpose allow edge device like Raspberry Pi microcontrollers exploit power artificial intelligence application image classification object detection 8 min read 8 min read Alexander Osipenko May 26 2019 GaleShapley algorithm simply explained article learn stable pairing stable marriage problem learn solve problem using Game Theory Gale-Shapley algorithm particular use Python create solution using theorem original paper 1962 stable marriage pairing problem real 4 min read 4 min read Prajwal Shreyas Data Scientist/ ML Engineer Experienced building deploying large scale ML model enhance business value Medium Dharmaraj Deploying Deep Learning Model using Flask API Nate DiRenzo Towards Data Science Deploying ML Models using Streamlit Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Maria Gusarova Build Beautiful Machine Learning Web App Streamlit Python Tutorial Python code included Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science May 27, 2019 Listen Save Deploy ML models at scale Part 1: API service for ML models Lets assume that you have built a ML model and that you are happy with its performance. Then the next step is to deploy the model into production. In this blog series I will cover how you can deploy your model for large scale consumption with in a scalable Infrastructure using AWS using docker container service. In this blog I will start with the first step of building an API framework for the ML model and running it in you local machine. For the purpose of this blog, lets consider the Sentiment classification model built here . In order to deploy this model we will follow the below steps: Convert the model into  .hdf5  file or .pkl file In case the model is a built on sklearn, it would be best to save it as a .pkl file. Alternatively if it is a deep learning model then it is recommended to save the model as a HDF file. The main difference between .pkl and .hdf is, pickle requires a large amount of memory to save a data structure to disk, where as HDF is designed to efficiently store large data sets. Save model in Pickle(.pkl): Save model in HDF(.hdf5): Once you have trained your deep learning model in keras or tensorflow you can save the model architecture and its weights using a .hdf5 file system. Implement a Flask API Step 1: Load the saved model (as per previous section) by one of the following methods depending on the type of file i.e. hdf5 or pkl. HDF5: PKL: Step 2: Import flask and create a flask application object as shown below: Step 3: The next step is to build a test API function which returns the API working string. This can be used to ensure the health of the API when it is deployed in production. Here we use @app.route, which is a python decorator ( a decorator is a function that takes another function and extends the behaviour of the latter function without explicitly modifying it) Step 4: The next step is to build a POST request api for processing requests to our sentiment model. We are using the path name /sentiment. The function reads the json input and converts it into pandas dataframe. It extracts the relevant fields from the json and calls the get_sentiment_DL function for processing. get_sentiment_DL function contains the trained model which has been loaded via hdf5 file. It finally will return back the results of the model in the form of json result. Step 5 : The detailed model processing steps will be performed by get_sentiment_DL function. In the case of our deep learning sentiment model we are passing: 2. text_data: Input text for sentiment classification 3. word_idx: Word index from the GloVe file (details of the model here ). Step 6: Add the below section to run the app. Here Host is set as 0.0.0.0 as we are hosting in our local server. However you can configure it to your network settings. Debug can be set to True at the time of building the API functionality. The Port is set to 5005, however this can be configured as per your requirement. Run the API To run the API, open a command line window and go to the directory where the code is stored. Run the python script by running the below command (sentiment.py is the name of the file with the above API implementation). On running the above command you will be able to see the below result in your command line window: Once the API is running you can test the API by going to your browser and typing 0.0.0.0:5005/apitest. You will get the below result in you browser. You can now pass any data to the API using python as shown below. The address in our case is http://0.0.0.0:5005/sentiment . The results of the model will be returned and stored in response field. Conclusion In conclusion, we have covered steps to deploy the model into an api service in your local computer. The next step is to deploy this in a cloud server as a micro service. In following blogs I will cover the use of container service such as docker and deploy it in AWS. 121 2 121 121 2 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Joseph Magiya May 26, 2019 Member-only Pearson Coefficient of Correlation Explained. Ive come to realize there is a lot of confusion about the different types of co-relation that you can perform on a data set. Let me clear up the smoke on this starting with the Pearson Coefficient of Correlation. What is correlation? Correlation is a bi-variate analysis that measures the strength of association 4 min read 4 min read Share your ideas with millions of readers. Mai Nguyen May 26, 2019 Member-only Exploratory Data Analysis with Python in B2B Marketing A deep dive into B2B Marketing using Data Visualization This project focuses on conducting Exploratory Data Analysis (EDA) for B2B Marketing using Python. We will use data from Olist, an e-commerce platform that connects small and medium business with top Marketplaces in Brazil, as an example. Besides providing the method and the code, I also want to discuss the 10 min read 10 min read Vincent Tatan May 26, 2019 Member-only In 12 minutes: Stocks Analysis with Pandas and Scikit-Learn Analyse, Visualize and Predict stocks prices quickly with Python One day, a friend of mine told me that the key to financial freedom is investing in stocks. While it is greatly true during the market boom, it still remains an attractive options today to trade stocks part time. Given the easy access to online trading platform, there are many 12 min read 12 min read Gilbert Tanner May 26, 2019 Member-only Google Coral USB Accelerator Introduction Speeding up machine learning models in a small form factor Last year at the Google Next conference Google announced that they are building two new hardware products around their Edge TPUs. Their purpose is to allow edge devices like the Raspberry Pi or other microcontrollers to exploit the power of artificial intelligence applications such as image classification and object detection 8 min read 8 min read Alexander Osipenko May 26, 2019 GaleShapley algorithm simply explained From this article, you will learn about stable pairing or stable marriage problem. You will learn how to solve that problem using Game Theory and the Gale-Shapley algorithm in particular. We will use Python to create our own solution using theorem from the original paper from 1962. What is a stable marriage or pairing problem? In the real 4 min read 4 min read Prajwal Shreyas Data Scientist/ ML Engineer Experienced in building and deploying large scale ML models to enhance business value. More from Medium Dharmaraj Deploying Deep Learning Model using Flask API Nate DiRenzo in Towards Data Science Deploying ML Models using Streamlit Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Maria Gusarova Build A Beautiful Machine Learning Web App With Streamlit | Python Tutorial (Python code included) Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5454,\n",
       "  'url': 'https://towardsdatascience.com/visualizing-beta-distribution-7391c18031f1',\n",
       "  'title': 'Visualizing Beta Distribution and Bayesian\\xa0Updating',\n",
       "  'subtitle': 'Seeing is believing: build intuition by simulating…',\n",
       "  'claps': 587,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 0.0002899505700961678,\n",
       "  'text': 'Towards Data Science Apr 1 2019 Member-only Listen Save Visualizing Beta Distribution Bayesian Updating Seeing believing build intuition simulating visualizing inspecting every step Beta distribution one esoteric distribution compared Bernoulli Binomial Geometric distribution also rare practice doe readily available real-world analogy help intuition make matter worse online tutorial tend intimidate reader complex formula beta gamma function weird sounding name conjugate prior post Ill waive technical detail supplement badly needed intuition visual animation simulated experiment Introduction Beta distribution probability distribution probability doe mean Everyone started learning probability flipping coin let start coin First define p probability landing head coin fair likely coin land head half time case p 50 likely value p. wait also possible unfair coin behaves accidentally like fair coin rule possibility coin unfair even observe head half time Notice using probability describe probability p essence Beta distribution describes likely p take value 0 1 Beta distribution parametrized Beta Together describe probability p take certain value formula youll need get post Prior Prior believe conducting experiment look side coin see one side head side tail may believe coin fair one fair coin ha magnitude describes confident belief youre confident may assign high value 10 le confident guy would probably assign 3 difference summarized Sure curve centered p 50 quantify confidence Think imaginary coin flip actually flip coin 1 number head get 1 number tail imagine flipped 18 time got 9 head 9 tail 10 confident coin fair someone imago 4 trial resulted 2 head 2 tail 3 reason need subtract 1 setting 1 reduce numerator constant 1 give u uninformative prior assume zero imaginary coin flip Uninformative Prior flipping even looking coin know coin know nothing dont know anything say anything happen equally likely fair coin two-headed coin two-tailed coin mixture alloy ha one side heavier dont know anything probability landing head uniformly distributed special case Beta parametrized Beta =1 =1 Depending use case may may want uninformative prior Sometimes experiment dont want already know bias way interpret data case want leverage result earlier experiment dont learn everything scratch case however learn new evidence using Bayesian updating Bayesian Updating previously thought imaginary coin flip Although conceptual convenience good news Beta distribution doe distinguish imaginary real flip coin observe head simply update 1 vice versa process called Bayesian updating see proof Building Intuition Simulation Lets start uninformative prior suppose coin indeed fair expect happen flip coin observe roughly equal number head tail flip confident become coin fair probability density function grows sharper sharper p 50 Effect Prior consider case coin biased 20 towards head start uninformative prior instantly emerges center around p 20 surprising biased prior belief Note also simulated complement event p 80 important property Beta distribution trial end two possible outcome start prior belief coin fair well see peak mode Beta distribution converges slowly true value stronger prior belief slowly well accept truth differ case prior strong enough 100 unable converge true value 1000 iteration prior symmetric well see Beta closer prior converges faster peak higher one prior intuitive reality consistent believe gladly accept become confident believe contrast slower accept truth contradicts believe Baseball Batting Statistics concrete example real-world application let consider baseball batting average adapted post national batting average 0.27 new player join game record prior performance may compare national average see good prior formulated Beta =81 =219 give 0.27 expectation swing bat update along way 1000 bat observe 300 hit 700 miss posterior becomes Beta =81 300 =219 700 expectation 381 381 919 0.293 Summary post introduces Beta distribution demystifies basic property using simulation visualization built intuition mean describe probability probability distribution prior interacts evidence relates real-world scenario Bayesian updating powerful concept ha wide range application business intelligence signal filtering stochastic process modeling study application near future next post close inspection Google Analytics multi-armed bandit experiment first animation post actually come 8-armed bandit experiment Extended Reading following blog cover topic relevant AB testing in-depth review key concept mentioned article 766 3 766 766 3 Towards Data Science home data science Medium publication sharing concept idea code Rudradeb Mitra Apr 1 2019 Member-only Successful Adoption AI Need Female Leaders woman posse right quality lead AI project successfully enable environment collaboration inclusion build real-world AI product exciting breakthrough twenty-first century occur technology expanding concept mean human John Naisbitt Part Differences Men Women story woman men work differently 8 min read 8 min read Share idea million reader Manu Siddharth Jha Mar 31 2019 Member-only Artificial Intelligence next big thing Hollywood Movies captured imagination people ever since came limelight Right first motion picture late 1880s upcoming latest sci-fi blockbuster cinema ha become medium love joy passion movie lover Almost every country across globe 5 min read 5 min read Guy Tsror Mar 31 2019 Member-only romantic side data science Analyzing relationship year worth text message wa supposed private project one thought would upload private Google Colab notebook share whomever relevant one person referred OJ 11 min read 11 min read Salma Ghoneim Mar 31 2019 Member-only Object detection via color-based image segmentation using python tutorial contouring using python OpenCV Getting started already jupyter notebook IDE run python OpenCV installed skip Execution Tools hero today Anaconda free open-source distribution help installing different package sort mess isolated environment Wikipedias telling u 7 min read 7 min read Sahil Dhankhad Mar 31 2019 Member-only Forget APIs Python Scraping Using Beautiful Soup Import Data File web Part 2 APIs always every website Beautiful Soup going stay forever collect kind data website Today Data play critical role every industry data coming internet company 8 min read 8 min read Shaw Lu Data Scientist Coupang Medium Anil Tilbe Towards AI Bayesian Inference Best 5 Models 10 Best Practices Machine Learning Pedro Martins de Bastos Towards Data Science use Bayesian Inference prediction Python KARRI TIRUMALA VINAY Data Science Dying Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 1, 2019 Member-only Listen Save Visualizing Beta Distribution and Bayesian Updating Seeing is believing: build intuition by simulating, visualizing, and inspecting every step Beta distribution is one of the more esoteric distributions compared to Bernoulli, Binomial and Geometric distributions. It is also rare in practice because it does not have a readily available real-world analogy that helps intuition. To make matters worse, online tutorials tend to intimidate readers with complex formula (beta & gamma functions) and weird sounding names (conjugate prior). So in this post, Ill waive those technical details and supplement some badly needed intuition through visual animations and simulated experiments. Introduction Beta distribution is a probability distribution of probabilities. What does it mean? Everyone started learning probability by flipping a coin. So lets start with a coin. First, define p as the probability of landing a head . If the coin is fair, then it is most likely that the coin will land head half of the time. In this case, p = 50% is the most likely value for p. But wait, it is also possible to have an unfair coin that behaves accidentally like a fair coin. So we cannot rule out the possibility that the coin is unfair, even if we observe heads half of the time. Notice how I am using probability to describe probability p . This is the essence of Beta distribution: it describes how likely  p  can take on each value between 0 and 1. Beta distribution is parametrized by Beta(, ). Together and describe the probability that p takes on a certain value. Here is the only formula youll need to get through this post. Prior Prior is what we believe before conducting an experiment. If you look at both sides of a coin and see that one side is head and the other side is tail, you may believe that the coin is a fair one. A fair coin has = , and the magnitude describes how confident you are about your belief. If youre confident, then you may assign high values to = = 10. A less confident guy would probably assign = = 3. The difference is summarized below. Sure, both curve are centered at p = 50%, but how to quantify confidence? Think of  and  as imaginary coin flips before you actually flip the coin: - 1 is the number of heads you get, and - 1 is the number of tails. If you imagine that you have flipped 18 times, and got 9 heads and 9 tails ( = = 10) , you are more confident that the coin is fair, than someone who imagines only 4 trials that resulted in 2 heads and 2 tails ( = = 3). The reason we need to subtract 1 is that, by setting = = 1, we reduce numerator to a constant 1. This gives us the uninformative prior we assume zero imaginary coin flips. Uninformative Prior Before flipping or even looking at the coin, what do we know about the coin? We know nothing, and when we dont know anything, we say anything can happen. It is equally likely to be a fair coin, to be a two-headed coin, to be a two-tailed coin, or any mixture of alloy that has one side heavier than the other. When we dont know anything, the probability of landing head is uniformly distributed. This is a special case of Beta, and is parametrized as Beta(=1, =1). Depending on the use case, we may or may not want an uninformative prior. Sometimes during experiments, we dont want what we already know bias the way we interpret data. In other cases, we want to leverage the results from earlier experiments, so we dont have to learn everything from scratch. In both cases, however, we learn from new evidence using Bayesian updating. Bayesian Updating We have previously thought of  and  as imaginary coin flips. Although this is a conceptual convenience, the good news is that Beta distribution does not distinguish the imaginary and the real. If we flip the coin and observe a head, we simply update  + 1 (vice versa for  ). This process is called Bayesian updating (see here for a proof). Building Intuition: A Simulation Lets start with an uninformative prior, and suppose the coin is indeed fair. What do you expect to happen? As we flip the coin, we will observe a roughly equal number of heads and tails, and the more we flip, the more confident we become that the coin is fair. So the probability density function grows sharper and sharper at p = 50%. The Effect of Prior Now consider the case where the coin is biased 20% towards head, and we start with an uninformative prior. It instantly emerges up at and centers around p = 20%. This is not surprising because we are not biased in our prior belief. Note that I also simulated the complement event ( p = 80%). This is an important property of Beta distribution: each trial can only end up with two possible outcomes. If we start with a prior belief that the coin is fair, well see that the peak (mode) of Beta distribution converges more slowly to the true values. The stronger our prior belief, the more slowly well accept the truth if they differ. In the case below, the prior is strong enough ( = = 100) that  we are unable to converge to the true values in 1000 iterations. If the prior is not symmetric, well see that the Beta closer to the prior converges faster (and its peak higher) than the one further from prior. This is intuitive: if the reality is consistent with what we believe, we gladly accept it and become more confident with what we believe. In contrast, we are slower to accept the truth if it contradicts what we believe. Baseball Batting Statistics For a concrete example of real-world application, lets consider the baseball batting average (adapted from this post ). The national batting average is 0.27. If some new player joins the game with no records on prior performance, we may compare him to the national average to see if he is any good. The prior is formulated as Beta(=81, =219) to give the 0.27 expectation . As he swings his bat, we update  and  along the way. After 1000 bats, we observe 300 hits and 700 misses. The posterior becomes Beta(=81 + 300, =219 + 700), with expectation  381 / (381 + 919) = 0.293. Summary This post introduces Beta distribution and demystifies its basic properties using simulation and visualization. You should have built up some intuition on what it means to describe probability with a probability distribution, how prior interacts with evidence, and how it relates to real-world scenarios. Bayesian updating is a very powerful concept and has a wide range of applications in business intelligence, signal filtering, and stochastic process modeling. I will study some of those applications in the near future. The next post is a close inspection on Google Analytics multi-armed bandit experiment (the first animation in this post actually comes from an 8-armed bandit experiment). Extended Reading The following blogs covers topics relevant to AB testing, and more in-depth review of key concepts mentioned in this article. 766 3 766 766 3 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Rudradeb Mitra Apr 1, 2019 Member-only For the Successful Adoption of AI, We Need More Female Leaders Why women possess the right qualities to lead AI projects more successfully and enable an environment for collaboration and inclusion to build real-world AI products. The most exciting breakthroughs of the twenty-first century will not occur because of technology, but because of an expanding concept of what it means to be human John Naisbitt Part I: Differences between Men and Women A story of how women and men can work differently 8 min read 8 min read Share your ideas with millions of readers. Manu Siddharth Jha Mar 31, 2019 Member-only Is Artificial Intelligence the next big thing in Hollywood? Movies have captured the imagination of people ever since they came into the limelight. Right from the first motion picture in the late 1880s to the upcoming latest sci-fi blockbuster, cinema has become a medium of love, joy, and passion for all movie lovers. Almost every country across the globe 5 min read 5 min read Guy Tsror Mar 31, 2019 Member-only The romantic side of data science: Analyzing a relationship through a year worth of text messages This was supposed to be a private project, one I thought I would upload to a private Google Colab notebook, and share with just whomever it is relevant for (which is, one other person, referred to here as OJ). 11 min read 11 min read Salma Ghoneim Mar 31, 2019 Member-only Object detection via color-based image segmentation using python A tutorial on contouring using python & OpenCV. Getting started If you already have jupyter notebook or an IDE with which you can run python & OpenCV installed, just skip to Execution. Tools Our hero today is Anaconda. a free open-source distribution that helps with installing different packages & sorts out their messes into isolated environments. What Wikipedias telling us about 7 min read 7 min read Sahil Dhankhad Mar 31, 2019 Member-only Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2 APIs are not always there for you for every website, but Beautiful Soup is going to stay with you forever to collect any kind of data from any website. Today, Data play a critical role in every industry. And most of this data is coming from the internet. Most company 8 min read 8 min read Shaw Lu Data Scientist @ Coupang More from Medium Anil Tilbe in Towards AI Bayesian Inference: The Best 5 Models and 10 Best Practices for Machine Learning Pedro Martins de Bastos in Towards Data Science How to use Bayesian Inference for predictions in Python KARRI TIRUMALA VINAY Data Science is Dying? Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 2290,\n",
       "  'url': 'https://medium.com/datadriveninvestor/advanced-git-commands-that-can-boost-your-productivity-707476a2a06',\n",
       "  'title': 'Advanced Git Commands that can Boost Your Productivity',\n",
       "  'subtitle': '-',\n",
       "  'claps': 71,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Data Driven Investor',\n",
       "  'date': '2019-12-27',\n",
       "  'clap_prop': 3.507068224331842e-05,\n",
       "  'text': \"Sign DataDrivenInvestor Dec 27 2019 Listen Save Advanced Git Commands Boost Productivity else learn besides git add git commit struggling git point know FEELING lost progress messed Git know bad feel always need ask coworkers Git help Today day change post cover basic advanced git command use work Four main git command talked cherry-pick reset revert reflog post also talk squash split undo change AI forecast 'Disruption productivity Data Driven Investor growing concern white-collar job disappear spread machine learning www.datadriveninvestor.com post meant someone started learning Git requires familiarity Git said let get started BACKGROUND dive command need understand one simple concept Git Git made commit tree One branch sequence commits One easy way think git tree think n-ary tree treenode ha pointer parent node path root leaf-node one branch leaf-node latest commit within branch tracing parent node pointer iteratively get sequence commits branch said start git journey GIT COMMANDS Git Cherry-pick command add selected commit current branch creates new commit copy doe delete original commit cherry-pick Examples Git Reset command move branch pointer point different commit within branch git reset Difference git reset git reset -- hard Git reset use command branch pointer point previous commit reverted commit change still example Git reset -- hard use command branch pointer point past commits reverted commit change gone Thus use hard reset commit squash split accidentally used git reset -- hard lost change use git reflog go back previous state detail talked reflog section Git Rebase command used rebase current branch onto new branch process rebase find new base cherry-pick unduplicated change onto base reassign branch pointer Visual explanation git rebase Examples Let 's say want combine d-e-f one commit git rebase HEAD~3 -i see something similar following choose squash commit-84b30b8 commit-e487eb1 commit-f1d3f6c one single commit f1d3f6c following Git Reflog command return list recent used commit sha-hash operation ha done Combine git reset one useful command need undo something use git reflog use git reflog typically something like Examples Lets say messed branch accidentally using git reset -- hard need find back change following allows u go back commit operation Useful Tips Squashing Splitting three way commit squash one way splitting Undo change two way undo change RECAP post learned Thanks Ankita De 71 71 71 DataDrivenInvestor empowerment data knowledge expertise subscribe DDIntel http //ddintel.datadriveninvestor.com Serhii Biloshkurskyi Dec 27 2019 Documentation software development come mind hear project documentation people really ideal procedure fully completed important manage Really interesting question Ill take question bringing mind documentation 4 min read 4 min read Share idea million reader Alex Mitrani Dec 26 2019 Time Series Trend Analysis Time-dependent trend unique feature time series analysis sequence event matter need analyze possible trend trend ultimately used creating model predict future value recently published article working time series data creating OHLC 4 min read 4 min read Robert Reinold Dec 26 2019 Path Maximized IoT Introduction Internet Things IoT movement embed sensor controller asset enable application create safer efficient local globalized system 3 min read 3 min read Freda Xin Dec 26 2019 List Comprehensions Python Set Builder Notation Set Theory post discus similarity connection list comprehension Python set builder notation Set Theory also briefly compare list comprehension Haskell List Comprehensions Python list comprehension Python let construct new list iterating element 5 min read 5 min read Fabiansyah Cahyo Dec 26 2019 Extreme Learning Machine Simple Classification last week friend college asked help implementing code extreme learning machine time didnt quite understand algorithm decided help anyway learning understanding 4 min read 4 min read Recommended Medium Toro Cloud microservices evolutionary revolutionary TechGenyz TechGenyz WooCommerce Magento eCommerce Project platform choose Paige McFarlain smucs Comparing Performance Community Detection Algorithms Pythons NetworkX C++s Boost Stanley Masinde Idea Ingenious Piece Setting proper server permission Piyush Mehta Getting started Ansible Cliff Weitzman Never Procrastinate Bullet Proof Way Tonya Nguyen Lab 1 Physical Computing ProjectAgoraEng Project Agora Engineering Catch glimpse Project Agoras Header Bidding Solution Help Terms Privacy Get Medium app Sign Jay Shi Software Engineer Google write Python tutorial stuff help become better software engineer Medium Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Jacob Bennett Level Coding Use Git like senior engineer Frank Andrade Geek Culture Top 5 Paid Subscriptions Ill Never Cancel Programmer Dr. Derek Austin Better Programming Prefer Regular Merge Commits Squash Commits Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Sign In DataDrivenInvestor Dec 27, 2019 Listen Save Advanced Git Commands that can Boost Your Productivity What else can we learn besides git add and git commit ? We have all been struggling with git at some point. We all know that FEELING when we lost all the progress because we messed up Git. We all know how bad we feel when we always need to ask coworkers for some Git help. Today is the day to change that ;). This post will cover up some basic advanced git commands that you can use during work. Four main git commands will be talked about cherry-pick, reset, revert, reflog. This post will also talk about squash, split and undo changes. AI forecast: 'Disruption, then productivity' | Data Driven Investor There is growing concern about all the white-collar jobs that will disappear with the spread of machine learning and www.datadriveninvestor.com This post is not meant for someone who just started learning Git. It requires that you have some familiarity with Git. With all that being said, lets get started! BACKGROUND Before we dive into the commands, we just need to understand one simple concept of Git. Git is made of commit trees. One branch is a sequence of commits. One easy way to think of the git trees is to think of a n-ary tree. Each treenode has a pointer to its parent node. Each path from root to any leaf-node is one branch. The leaf-node here is the latest commit within that branch, and by tracing its parent node pointer iteratively, we can get a sequence of all commits for that branch. With that being said, we can now start on the git journey. GIT COMMANDS Git Cherry-pick This command adds the selected commit to the current branch. It creates a new commit copy and does not delete the original commit. Why and when to cherry-pick? Examples: Git Reset This command moves the branch pointer to point to a different commit within that branch. Why and when git reset Difference between git reset and git reset --hard Git reset: When we use this command, the branch pointer points to the previous commit, but the reverted commit changes are still here. For example: Git reset --hard: When we use this command, the branch pointer points to the past commits, and the reverted commit changes are gone. Thus, we cannot use hard reset to do commit squash or split. When you accidentally used git reset --hard and you lost the changes, you can use git reflog to go back to the previous state. More details will be talked about in the reflog section. Git Rebase This command is used to rebase the current branch onto a new branch The process of rebase: find the new base cherry-pick all the unduplicated changes onto that base reassign the branch pointer. Visual explanation: Why and when git rebase: Examples: Let's say now we want to combine d-e-f into one commit we can do: git rebase HEAD~3 -i And we will see something similar to the following: Then we can choose to squash commit-84b30b8, commit-e487eb1 and commit-f1d3f6c into one single commit f1d3f6c by doing the following: Git Reflog This command returns a list most recent used commit sha-hash and what operation has been done on them. Combine with git reset , this is one of the most useful command when we need to undo something. Why and when to use git reflog When we use git reflog, we typically something like this: Examples: Lets say we have messed up the branch by accidentally using git reset --hard, and we need to find back those changes, we can do the following: This allows us to go back to the commit we do the operation before. Some Useful Tips Squashing and Splitting There are three ways to do commit squash There is one way to do splitting: Undo changes: There are two ways to undo changes RECAP In this post, we learned about: Thanks to Ankita De 71 71 71 More from DataDrivenInvestor empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com Serhii Biloshkurskyi Dec 27, 2019 Documentation of software development What comes to mind when you hear about project documentation? Are there people who are really into it? Is it ideal? Is the procedure fully completed, and how important is it? How do you manage it? Really interesting questions! Ill take you through these questions bringing to mind what documentation 4 min read 4 min read Share your ideas with millions of readers. Alex Mitrani Dec 26, 2019 Time Series and Trend Analysis Time-dependent trends are a unique feature of time series analysis. If the sequence of events matters, then you need to analyze possible trends. These trends can ultimately be used for creating models that predict future values. I recently published articles about working with time series data and creating OHLC 4 min read 4 min read Robert Reinold Dec 26, 2019 The Path to Maximized IoT Introduction The Internet of Things (IoT) is the movement to embed sensors and controllers into assets to enable applications that create safer and more efficient local and globalized systems. 3 min read 3 min read Freda Xin Dec 26, 2019 List Comprehensions in Python and Set Builder Notation in Set Theory In this post, I will discuss the similarities and connections between list comprehensions in Python and set builder notation in Set Theory. I will also briefly compare them with list comprehensions in Haskell. List Comprehensions in Python A list comprehension in Python will let you construct a new list by iterating through each element 5 min read 5 min read Fabiansyah Cahyo Dec 26, 2019 Extreme Learning Machine for Simple Classification So last week, my friend in college asked my help about implementing the code of extreme learning machine. At that time, I didnt quite understand about the algorithm but I decided to help anyway while learning and understanding it. 4 min read 4 min read Recommended from Medium Toro Cloud Are microservices evolutionary or revolutionary? TechGenyz in TechGenyz WooCommerce or Magento for an eCommerce Project: Which platform to choose? Paige McFarlain in smucs Comparing the Performance of Community Detection Algorithms in Pythons NetworkX and C++s Boost Stanley Masinde in An Idea (by Ingenious Piece) Setting proper server permissions Piyush Mehta Getting started with Ansible Cliff Weitzman Never Procrastinate: My Bullet Proof Way Tonya Nguyen Lab 1: Physical Computing ProjectAgoraEng in Project Agora Engineering Catch a glimpse of Project Agoras Header Bidding Solution About  Help  Terms  Privacy Get the Medium app Sign In Jay Shi Software Engineer @ Google. I write about Python tutorials and stuff that can help you become a better software engineer. More from Medium Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Jacob Bennett in Level Up Coding Use Git like a senior engineer Frank Andrade in Geek Culture My Top 5 Paid Subscriptions Ill Never Cancel as a Programmer Dr. Derek Austin in Better Programming Why I Prefer Regular Merge Commits Over Squash Commits Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 361,\n",
       "  'url': 'https://towardsdatascience.com/cleaning-analyzing-and-visualizing-survey-data-in-python-42747a13c713',\n",
       "  'title': 'Cleaning, Analyzing, and Visualizing Survey Data in\\xa0Python',\n",
       "  'subtitle': 'A tutorial using ',\n",
       "  'claps': 652,\n",
       "  'responses': 7.0,\n",
       "  'reading_time': 10,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 0.00032205753271329025,\n",
       "  'text': \"Towards Data Science Mar 30 2019 Listen Save Cleaning Analyzing Visualizing Survey Data Python tutorial using panda matplotlib seaborn produce digestible insight dirty data work data D2C startup good chance asked look survey data least since SurveyMonkey one popular survey platform good chance itll SurveyMonkey data way SurveyMonkey export data necessarily ready analysis right box pretty close Ill demonstrate example question might want ask survey data extract answer quickly Well even write function make life easier plotting future question Well using panda matplotlib seaborn make sense data used Mockaroo generate data specifically survey question field used `` Custom List '' entered appropriate field could achieve effect using random.choice random module found easier let Mockaroo create whole thing tweaked data Excel mirrored structure SurveyMonkey export first reaction might Ugh horrible mean column name didnt read properly ton NaNs instead numerical representation like 0/1 1/2/3/4/5 actual text answer cellAnd actually reading MultiIndex dont worry bad might think going ignore MultiIndexes post Nobody really like working anyway team need insight ASAP well come hacky solution First order business weve asked find answer question vary age group age age -- n't column age group Well luckily u pretty easily define function create one try run like well get error Thats first row value age word age instead number Since first step convert age int fail need remove row DataFrame itll useful u later rename column well save separate variable notice since removing header 've lost information looking survey data Ideally list question option asked survey provided whoever want analysis keep separate way reference info document note look working OK let apply age_group function get age_group column Great Next let subset data focus first question answer first question vary age group Great answer variable go plot data going look good misnamed column Lets write quick function make renaming column simple Remember header earlier use create new_names_list renaming already array pas right rename first readability Isnt much nicer look Dont worry almost part get insight Notice groupby aggregation function ignore NaNs automatically make life significantly easier Lets say also dont really care analyzing under-30 customer right well plot age group OK well good 60+ group ha people group hard make fair comparison plot age group separate plot compare distribution wait might think dont really want write code 4 different plot Well course ha time Lets write another function u believe wa Jenny Bryan wonderful talk Code Smells Feels first tipped following find copying pasting code changing value really ought write function ha great guide deciding isnt worth write function something rule thumb like use would copying pasting 3 time write function also benefit convenience approach improve programming skill make people need read code happier course generated data uniform distribution would thus expect see significant difference group Hopefully survey data interesting Next let address another format question one need see interested age group given benefit Happily question actually easier deal former type Lets take look look since small DataFrame age_group appended already wo n't add Cool subsetted data cant aggregate count time like could question last question NaNs would excluded give true count response one would get number response age group overall definitely want point question understand interested different age group need preserve information tell u many people age group responded question One way go would re-encode response numerically want preserve relationship even granular level encode numerically take median average age group level interest really interested specific percentage people per age group chose interest level Itd easier convey info barplot text preserved Thats going next guessed time write another function Quick note new learner people wont say explicitly let clear visualization often made Generally speaking highly iterative process Even experienced data scientist dont write plot specification top head Generally start .plot kind='bar similar depending plot want change size color map get group properly sorted using order= specify whether label rotated set x- y-axis label invisible depending think best whoever using visualization dont intimidated long block code see people making plot Theyre usually created span minute testing different specification writing perfect code scratch one go plot another 2x2 benefit broken age group wed 4 benefit ha time Instead well loop benefit age group within benefit using couple loop 're interested 'd challenge refactor function happen many question formatted like Success wanted export individual set plot would simply add line plt.savefig _interest_by_age.png'.format benefit matplotlib would automatically save beautifully sharp rendering set plot make especially easy folk team use finding simply export plot folder people browse image able drag drop right PowerPoint presentation report could use tad padding would increase allowed height figure slightly Lets one example numerically encoding benefit mentioned earlier generate heatmap correlation interest different benefit lastly well generate correlation matrix plot correlation since data randomly generated would expect little correlation indeed find funny note SQL tutorial slightly negatively correlated drag-and-drop feature actually might expect see real data Lets one last type plot one thats closely related heatmap clustermap Clustermaps make correlation especially informative analyzing survey response use hierarchical clustering case group benefit together closely related instead eyeballing heatmap individual benefit positively negatively associated get little crazy 10+ benefit plot segmented cluster little easier look also easily change linkage type used calculation youre familiar mathematical detail hierarchical clustering available option single average ward wont get detail ward generally safe bet starting Long label often require little tweaking Id recommend renaming benefit shorter name prior using clustermap quick assessment show clustering algorithm belief drag-and-drop feature ready-made formula cluster together custom dashboard template SQL tutorial form another cluster Since correlation weak see height benefit link together form cluster tall mean probably base business decision finding Hopefully example illustrative despite weak relationship hope enjoyed quick tutorial working survey data writing function quickly generate visualization finding think know even efficient way thing feel free let know comment came needed produce insight individual question quickly possible 742 7 742 742 7 Towards Data Science home data science Medium publication sharing concept idea code Matthew Stewart Mar 30 2019 Handling Discriminatory Biases Data Machine Learning data tell racist course algorithm racist Theyre made people Stephen Bush New Statesman America Ethics Machine Learning time machine learning doe touch particularly sensitive social moral ethical issue Someone give u data set asks u predict house price based 13 min read 13 min read Share idea million reader Abraham Kang Mar 30 2019 Member-only Applied AI Going Concept ML Components Opening mind different way applying machine learning real world Abraham Kang special thanks Kunal Patel Jae Duk Seo sounding board providing input article 6 min read 6 min read Michel Kana Ph.D Mar 30 2019 Wild Wide AI responsible data science shoot first new race human Data Science good thing u improves life make thing efficient effective lead better experience however miss-steps data-driven analysis ha already exhibited example data science tool intentionally unintentionally misused 2012 9 min read 9 min read Kevin Luk Mar 30 2019 Member-only library load image Python difference Summarization Comparison .imread face computer vision project first need load image preprocessing various library perform imread want consolidate popular library loading image difference article go Libraries loading 3 min read 3 min read Abhishek Mukherjee Mar 30 2019 Trust interpretability machine learning machine learning model always need interpretable Given choice interpretable model inaccurate non-interpretable model accurate wouldnt rather choose non-interpretable accurate model word reason sacrificing accuracy altar interpretability 9 min read 9 min read Charlene Chambliss Machine Learning Engineer Primer AI Im Twitter blissfulchar LinkedIn http //www.linkedin.com/in/charlenechambliss/ Medium Terence Shin Towards Data Science 10 Best Data Visualizations 2022 Luay Rahil ILLUMINATION Elon Musk Eliminated Remote Work Working Home `` n't Work '' Anmol Tomar CodeX Say Goodbye Loops Python Welcome Vectorization Robert Ritz picky choosing partner data-driven approach Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Mar 30, 2019 Listen Save Cleaning, Analyzing, and Visualizing Survey Data in Python A tutorial using pandas , matplotlib , and seaborn to produce digestible insights from dirty data If you work in data at a D2C startup, theres a good chance you will be asked to look at survey data at least once. And since SurveyMonkey is one of the most popular survey platforms out there, theres a good chance itll be SurveyMonkey data. The way SurveyMonkey exports data is not necessarily ready for analysis right out of the box, but its pretty close. Here Ill demonstrate a few examples of questions you might want to ask of your survey data, and how to extract those answers quickly. Well even write a few functions to make our lives easier when plotting future questions. Well be using pandas , matplotlib , and seaborn to make sense of our data. I used Mockaroo to generate this data; specifically, for the survey question fields, I used \"Custom List\" and entered in the appropriate fields. You could achieve the same effect by using random.choice in the random module, but I found it easier to let Mockaroo create the whole thing for me. I then tweaked the data in Excel so that it mirrored the structure of a SurveyMonkey export. Your first reaction to this might be Ugh. Its horrible. I mean, the column names didnt read in properly, there are a ton of NaNs, instead of numerical representations like 0/1 or 1/2/3/4/5 we have the actual text answers in each cellAnd should we actually be reading this in with a MultiIndex? But dont worry, its not as bad as you might think. And were going to ignore MultiIndexes in this post. (Nobody really likes working with them anyway.) The team needs those insights ASAP so well come up with some hacky solutions. First order of business: weve been asked to find how the answers to these questions vary by age group. But age is just an age--we don\\'t have a column for age groups! Well, luckily for us, we can pretty easily define a function to create one. But if we try to run it like this, well get an error! Thats because we have that first row, and its value for age is the word age instead of a number. Since the first step is to convert each age to an int , this will fail. We need to remove that row from the DataFrame, but itll be useful for us later when we rename columns, so well save it as a separate variable. You will notice that, since removing headers , we\\'ve now lost some information when looking at the survey data by itself. Ideally, you will have a list of the questions and their options that were asked in the survey, provided to you by whoever wants the analysis. If not, you should keep a separate way to reference this info in a document or note that you can look at while working. OK, now lets apply the age_group function to get our age_group column. Great. Next, lets subset the data to focus on just the first question. How do the answers to this first question vary by age group? Great. We have the answers in a variable now. But when we go to plot this data, its not going to look very good, because of the misnamed columns. Lets write up a quick function to make renaming the columns simple: Remember headers from earlier? We can use it to create our new_names_list for renaming. Its already an array, so we can just pass it right in, or we can rename it first for readability. Isnt that so much nicer to look at? Dont worry, were almost to the part where we get some insights. Notice how groupby and other aggregation functions ignore NaNs automatically. That makes our lives significantly easier. Lets say we also dont really care about analyzing under-30 customers right now, so well plot only the other age groups. OK, this is all well and good, but the 60+ group has more people in it than the other groups, and so its hard to make a fair comparison. What do we do? We can plot each age group in a separate plot, and then compare the distributions. But wait, you might think. I dont really want to write the code for 4 different plots. Well of course not! Who has time for that? Lets write another function to do it for us. I believe it was Jenny Bryan , in her wonderful talk Code Smells and Feels, who first tipped me off to the following: If you find yourself copying and pasting code and just changing a few values, you really ought to just write a function. This has been a great guide for me in deciding when it is and isnt worth it to write a function for something. A rule of thumb I like to use is that if I would be copying and pasting more than 3 times, I write a function. There are also benefits other than convenience to this approach, such as that it: (All of which improve your programming skills and make the people who need to read your code happier!) This is, of course, generated data from a uniform distribution, and we would thus not expect to see any significant differences between groups. Hopefully your own survey data will be more interesting. Next, lets address another format of question. In this one, we need to see how interested each age group is in a given benefit. Happily, these questions are actually easier to deal with than the former type. Lets take a look: And look, since this is a small DataFrame, age_group is appended already and we won\\'t have to add it. Cool. Now we have the subsetted data, but we cant just aggregate it by count this time like we could with the other question the last question had NaNs that would be excluded to give the true count for that response, but with this one, we would just get the number of responses for each age group overall: This is definitely not what we want! The point of the question is to understand how interested the different age groups are, and we need to preserve that information. All this tells us is how many people in each age group responded to the question. So what do we do? One way to go would be to re-encode these responses numerically. But what if we want to preserve the relationship on an even more granular level? If we encode numerically, we can take the median and average of each age groups level of interest. But what if what were really interested in is the specific percentage of people per age group who chose each interest level? Itd be easier to convey that info in a barplot, with the text preserved. Thats what were going to do next. And you guessed it its time to write another function. Quick note to new learners: Most people wont say this explicitly, but let me be clear on how visualizations are often made. Generally speaking, it is a highly iterative process. Even the most experienced data scientists dont just write up a plot with all of these specifications off the top of their head. Generally, you start with .plot(kind=\\'bar\\') , or similar depending on the plot you want, and then you change size, color maps, get the groups properly sorted using order= , specify whether the labels should be rotated, and set x- or y-axis labels invisible, and more, depending on what you think is best for whoever will be using the visualizations. So dont be intimidated by the long blocks of code you see when people are making plots. Theyre usually created over a span of minutes while testing out different specifications, not by writing perfect code from scratch in one go. Now we can plot another 2x2 for each benefit broken out by age group. But wed have to do that for all 4 benefits! Again: who has time for that? Instead, well loop over each benefit, and each age group within each benefit, using a couple of for loops. But if you\\'re interested, I\\'d challenge you to refactor this into a function if you happen to have many questions that are formatted like this. Success! And if we wanted to export each individual set of plots, we would simply add the line plt.savefig(\\'{}_interest_by_age.png\\'.format(benefit)) , and matplotlib would automatically save a beautifully sharp rendering of each set of plots. This makes it especially easy for folks on other teams to use your findings; you can simply export them to a plots folder, and people can browse the images and be able to drag and drop them right into a PowerPoint presentation or other report. These could use a tad more padding, so if I were to do this again, I would increase the allowed height for the figure slightly. Lets do one more example: numerically encoding the benefits, as we mentioned earlier. Then we can generate a heatmap of the correlations between interest in different benefits. And lastly, well generate the correlation matrix and plot the correlations. Again, since the data is randomly generated, we would expect there to be little to no correlation, and that is indeed what we find. (It is funny to note that SQL tutorials are slightly negatively correlated with drag-and-drop features, which is actually what we might expect to see in real data!) Lets do one last type of plot, one thats closely related to the heatmap: the clustermap . Clustermaps make correlations especially informative in analyzing survey responses, because they use hierarchical clustering to (in this case) group benefits together by how closely related they are. So instead of eyeballing the heatmap for which individual benefits are positively or negatively associated, which can get a little crazy when you have 10+ benefits, the plot will be segmented into clusters, which is a little easier to look at. You can also easily change the linkage type used in the calculation, if youre familiar with the mathematical details of hierarchical clustering. Some of the available options are single, average, and ward I wont get into the details, but ward is generally a safe bet when starting out. Long labels often require a little tweaking, so Id recommend renaming your benefits to shorter names prior to using a clustermap. A quick assessment of this shows that the clustering algorithm believes drag-and-drop features and ready-made formulas cluster together, while custom dashboard templates and SQL tutorials form another cluster. Since the correlations are so weak, you can see that the height of when the benefits link together to form a cluster is very tall. (This means you should probably not base any business decisions on this finding!) Hopefully the example is illustrative despite the weak relationships. I hope you enjoyed this quick tutorial about working with survey data and writing functions to quickly generate visualizations of your findings! If you think you know an even more efficient way of doing things, feel free to let me know in the comments this is just what I came up with when I needed to produce insights on individual questions as quickly as possible. 742 7 742 742 7 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Matthew Stewart Mar 30, 2019 Handling Discriminatory Biases in Data for Machine Learning What do you do when data tells you to be racist? Of course algorithms are racist. Theyre made by people. Stephen Bush, New Statesman America Ethics in Machine Learning Most of the time, machine learning does not touch on particularly sensitive social, moral, or ethical issues. Someone gives us a data set and asks us to predict house prices based 13 min read 13 min read Share your ideas with millions of readers. Abraham Kang Mar 30, 2019 Member-only Applied AI: Going From Concept to ML Components Opening your mind to different ways of applying machine learning to the real world. By Abraham Kang with special thanks to Kunal Patel and Jae Duk Seo for being a sounding board and providing input for this article. 6 min read 6 min read Michel Kana, Ph.D Mar 30, 2019 Wild Wide AI: responsible data science Who shoots first the new race or the human? Data Science can do good things for us: it improves life, it makes things more efficient, more effective and leads to a better experience. There are however some miss-steps that data-driven analysis has already exhibited. Here are few examples where data science tools were intentionally or unintentionally misused: In 2012 9 min read 9 min read Kevin Luk Mar 30, 2019 Member-only What libraries can load image in Python and what are their difference? Summarization & Comparison of .imread() When we face computer vision project, first of all we need to load the images before any preprocessing. There are various libraries out there to perform imread() . Here I want to consolidate the popular libraries for loading image and their difference. This article will go through: Libraries for loading 3 min read 3 min read Abhishek Mukherjee Mar 30, 2019 Trust and interpretability in machine learning Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldnt you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability? 9 min read 9 min read Charlene Chambliss Machine Learning Engineer at Primer AI. Im on Twitter @blissfulchar, and heres my LinkedIn: https://www.linkedin.com/in/charlenechambliss/ More from Medium Terence Shin in Towards Data Science The 10 Best Data Visualizations of 2022 Luay Rahil in ILLUMINATION Elon Musk Eliminated Remote Work Because Working From Home \"Doesn\\'t Work\" Anmol Tomar in CodeX Say Goodbye to Loops in Python, and Welcome Vectorization! Robert Ritz Are you being too picky choosing a partner? A data-driven approach. Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 1263,\n",
       "  'url': 'https://medium.com/datadriveninvestor/deploy-your-machine-learning-model-using-flask-made-easy-now-635d2f12c50c',\n",
       "  'title': 'Deploy your Machine Learning Model using Flask\\u200a—\\u200aMade Easy\\xa0Now',\n",
       "  'subtitle': '-',\n",
       "  'claps': 291,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Data Driven Investor',\n",
       "  'date': '2019-09-11',\n",
       "  'clap_prop': 0.00014374040187050224,\n",
       "  'text': \"DataDrivenInvestor Sep 11 2019 Listen Save Deploy Machine Learning Model using Flask Made Easy quick cheat sheet deploy ML model 10 min Youve built machine learning model using python turn super cool good prediction result would nice way share thing friend family simpler way easiest way deploying model using flask start learning machine learning initially running simple supervised learning model important easiest one understand use Regression model blog assumption person ha fair knowledge running python code familiar simple ML library like Sci-kit Learn P andas N umpy Different user different choice come type python IDE want use popular IDEs Spyder Jupyter Notebook PyCharm editor run python compiler DDI Editor 's Pick 5 Machine Learning Books Turn Novice Expert Data Driven booming growth Machine Learning industry ha brought renewed interest people Artificial www.datadriveninvestor.com want run line line check output idle go Jupyter notebook want run chunk code altogether Spyder better choice want whole project look organized ha lot file want make look structured way good go PyCharm IDE blog using Spyder framework IDE provided Anaconda Navigator use anything choice comfortable post building simple regression model using scikit-learns built-in LinearRegression function First let u look data look simple data set 4 attribute 8 row column numerical column need data pruning cleaning data convert categorical data numerical build classifier regression model data used clean ready applied classifier Normally first clean data like filling empty value data set converting categorical data numerical data normalization etc part data cleaning pruning blog purpose mainly serving purpose deploying model using flask considered simpler data already clean requires pre-processing class label last salary column predicting using model model built saving trained model using library called pickle Pickle Python pickle module used serializing de-serializing Python object structure object Python pickled saved disk pickle doe serializes object first writing file Pickling way convert python object list dict etc character stream idea character stream contains information necessary reconstruct object another python script Heres code building simple regression model saving model serializing using pickle run model trained model get saved directory project file stored local machine model ready saved time start building flask model Setting flask-app localhost Make sure flask installed `` pip install flask `` flask code explained three section 1.Loading saved model load model.pkl file initialize flask app 2 Redirecting API home page index.html initializing app tell Flask want web page load line app.route `` '' method `` GET '' '' POST '' tell Flask load home page website use app.route define function used redirect number URI respect API start flask server redirects index.html file default case 3 Redirecting API predict result salary Since POST request reading input value request.form.values input value variable int_features convert array use model predict round final prediction two decimal place click predict button index.html predicts salary value entered user 3 input pass variable `` output `` outputted model sends back index.html template prediction_text Let u look index.html file prediction_text placeholder see output prediction salary predicted model placed index.html file 4 Starting flask server call app.run run web page locally hosted computer following script start flask server localhost default port 5000 making http //127.0.0.1:5000/ paste http //127.0.0.1:5000/ http //localhost:5000 browser press enter see server working overall structure project project ha four major part 1. model.py contains code Machine Learning model predict employee salary based training data hiring.csv file 2. app.py contains Flask APIs receives employee detail GUI API call computes predicted value based model return 3. template folder contains HTML template index.html allow user enter employee detail display predicted employee salary 4. static folder contains cs folder style.css file ha styling required index.html file Running project 1 Ensure project home directory Create machine learning model running command command prompt `` python model.py `` would compile linear regression classifier build trained model creates serialized version model save file named model.pkl 2 Run app.py using command start Flask API `` python app.py `` default flask run port 5000 3 Navigate URL http //127.0.0.1:5000/ http //localhost:5000 able view homepage Enter valid numerical value 3 input box hit button Predict everything go well able see predicted salary value HTML page folk easy deploy application using flask hope liked post start working building application code project found GitHub page http //github.com/MaajidKhan/DeployMLModel-Flask Thanks reading keep eye next article deploying deep learning model using Flask 382 2 382 382 2 DataDrivenInvestor empowerment data knowledge expertise subscribe DDIntel http //ddintel.datadriveninvestor.com Heba Tarek Sep 11 2019 Data Structure Queue article talk Queue data structure know implement queue defined ordered list data Stack Programmer 's Guide Creating Eclectic Bookshelf Data Driven Investor Every developer bookshelf possible set text cabinet myriad every collectionwww.datadriveninvestor.com Also Queue defined First In-First list FIFO mean 3 min read 3 min read Share idea million reader Rick Cesari Sep 11 2019 Whats Story Key Building Brand person story tell Isak Dinesen Karen Blixen author Africa novel Heres truth human want need product service basic primal drive instinctive urge engage tool making hunting-gathering 5 min read 5 min read Benny Lim Sep 11 2019 Member-only Bad Apple Apple fan isnt excited latest release iPhone iPad Apple Watch Apple finally announced latest lineup iPhone last night iPhone 11 iPhone 11 Pro iPhone 11 Pro Max accompany released new 10.2 inch iPad replaces 9.7 4 min read 4 min read Chris Gardener Sep 11 2019 Might Need Change bought printer year ago cheap nothing fancy print-the-odd-boarding-pass-at-home printer cost 30 Business Moves Stomach Make Allowances Gut Feelings Data Driven Investor Gut feeling turn feeling science quite clear gut know youwww.datadriveninvestor.com specifically chose cheap one rarely need print thing actively try avoid fact didnt need anything special 4 min read 4 min read Alissa Knight Sep 11 2019 Member-only Undivided Fall Decoupling Network Segmentation Micro-Segmentation Software Defined Perimeter Introduction today law regulation even latest version PCI-DSS HIPAA HITECH make network segmentation micro-segmentation compulsory comply rule 5 min read 5 min read Maajid Khan Deep Learning Engineer| AI Machine Learning Data Science Enthusiast DataAnalytics Python Java GIT Hadoop C++ Blogger Sports DataStructures Medium Dharmaraj Deploying Deep Learning Model using Flask API Nate DiRenzo Towards Data Science Deploying ML Models using Streamlit Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Maria Gusarova Build Beautiful Machine Learning Web App Streamlit Python Tutorial Python code included Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'DataDrivenInvestor Sep 11, 2019 Listen Save Deploy your Machine Learning Model using Flask Made Easy Now A quick cheat sheet to deploy your ML model in 10 min. Youve built a machine learning model using python. It turns out to be super cool with good prediction results. It would be nice if theres a way you can share this thing with your friends and family. So, how can we do this? Is there a simpler way of doing this? The easiest way of doing it is by deploying the model using flask. When we start learning machine learning, initially we do it by running a simple supervised learning model. The most important and the easiest one to understand and use is a Regression model . This blog is under the assumption that the person has a fair knowledge of running python code and is familiar with simple ML libraries like Sci-kit Learn , P andas , N umpy . Different users have different choices when it comes to the type of python IDE they want to use. Some of the popular IDEs are Spyder , Jupyter Notebook, PyCharm editor or you can just run it on python compiler. DDI Editor\\'s Pick: 5 Machine Learning Books That Turn You from Novice to Expert | Data Driven The booming growth in the Machine Learning industry has brought renewed interest in people about Artificial www.datadriveninvestor.com When you want to run line by line and check the output, its idle to go with Jupyter notebooks. When you want to run a chunk of code altogether, Spyder is the better choice and if you want your whole project to look organized, has a lot of files and you want to make it look in a structured way, its good to go with PyCharm  IDE. In our blog, we will be using Spyder framework as an IDE provided by Anaconda Navigator . You can use anything of your choice and with which you are comfortable with. In this post, we will be building a simple regression model using scikit-learns built-in LinearRegression() function. First, let us have a look at how our data looks. Its a very simple data set with 4 attributes and 8 rows. All the columns are numerical columns, so we need to do any data pruning or cleaning up the data to convert categorical data into numerical before you build your classifier (regression model). This data used here is clean and is ready to be applied for a classifier. Normally, you have to first clean up your data like filling up empty values in your data set, converting your categorical data into numerical data, normalization, etc; as part of our data cleaning and pruning. But this blog purpose is mainly serving the purpose of deploying the model using flask, so we have considered simpler data which is already clean and requires no further pre-processing. Here, the class label will be the last salary column, which we will be predicting using our model. After our model is built, we will be saving our trained model using a library called pickle. Pickle Python pickle module is used for serializing and de-serializing a Python object structure. Any object in Python can be pickled so that it can be saved on disk. What pickle does is that it serializes the object first before writing it to file. Pickling is a way to convert a python object (list, dict, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another python script. Heres the code of building a simple regression model and then saving the model by serializing it using pickle. When you run this, your model is now trained and your model gets saved in the directory where your project files are stored in your local machine. Now that your model is ready and saved. Its time we start building a flask model. Setting up flask-app over the localhost: Make sure flask is installed. ``` pip install flask ``` The flask code can be explained in three sections: 1.Loading the saved model We load the model.pkl file and initialize the flask app. 2. Redirecting the API to the home page index.html After initializing the app, we have to tell Flask what we want to do when the web page loads. The line @app.route(\"/\", methods = [\"GET\",\"POST\"]) tells Flask what to do when we load the home page of our website. We use @app.route(/) to define functions which are used to redirect them into any number of URI with respect to the API. So, when you start the flask server, it redirects to index.html file by default in our case. 3. Redirecting the API to predict the result (salary) Since it is a POST request, it will be reading the input values from request.form.values(). Now that we have the input values in the variable int_features , we will convert it into an array and then use the model to predict it and round the final prediction to two decimal places. When we click on the predict button in index.html, it predicts the salary for the values entered by the user (3 inputs), then passes on the variable ``` output ``` outputted from the model and sends it back to index.html template as prediction_text . Let us have a look over index.html file: The {{ prediction_text }} placeholder you see here is where your output prediction(salary predicted) from the model will be placed in our index.html file. 4. Starting the flask server This will call app.run() and run our web page locally, hosted on your computer. The following script starts the flask server on localhost and default port (5000) making http://127.0.0.1:5000/ Just paste  http://127.0.0.1:5000/  (or)  http://localhost:5000  on browser and press enter to see the server working. The overall structure of the project: This project has four major parts: 1. model.py This contains code for our Machine Learning model to predict employee salaries based on training data in hiring.csv file. 2. app.py This contains Flask APIs that receives employee details through GUI or API calls, computes the predicted value based on our model and returns it. 3. template This folder contains the HTML template ( index.html ) to allow user to enter employee detail and displays the predicted employee salary. 4. static This folder contains the css folder with style.css file which has the styling required for out index.html file. Running the project: 1. Ensure that you are in the project home directory. Create the machine learning model by running below command from command prompt - ``` python model.py ``` This would compile our linear regression classifier, build the trained model and creates a serialized version of our model and save it as a file named model.pkl 2. Run app.py using below command to start Flask API ``` python app.py ``` By default, flask will run on port 5000. 3. Navigate to URL  http://127.0.0.1:5000/  (or)  http://localhost:5000 You should be able to view the homepage. Enter valid numerical values in all 3 input boxes and hit the button Predict . If everything goes well, you should be able to see the predicted salary value on the HTML page! That is all folks. It is that easy to deploy an application using flask. I hope you liked this post. Now, you can start working on this and building your own application. The code for this project can be found at my GitHub page: https://github.com/MaajidKhan/DeployMLModel-Flask Thanks for reading, and keep an eye out for my next article on deploying a deep learning model using Flask! 382 2 382 382 2 More from DataDrivenInvestor empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com Heba Tarek Sep 11, 2019 Data Structure Queue In this article, we will talk about the Queue as a data structure, and we will know how to implement it. A queue can be defined as an ordered list of data, the same as Stack. A Programmer\\'s Guide to Creating an Eclectic Bookshelf | Data Driven Investor Every developer should have a bookshelf. The possible set of texts in his cabinet are myriad, but not every collectionwww.datadriveninvestor.com Also, a Queue is defined as the First In-First Out list (FIFO), which means 3 min read 3 min read Share your ideas with millions of readers. Rick Cesari Sep 11, 2019 Whats Your Story? The Key to Building Your Brand To be a person is to have a story to tell. ~ Isak Dinesen (Karen Blixen), author of Out of Africa and other novels Heres the truth humans want and need products and services. Its a basic, primal drive, this instinctive urge to engage in tool making and hunting-gathering 5 min read 5 min read Benny Lim Sep 11, 2019 Member-only A Bad Apple Why this Apple fan isnt excited about the latest release of iPhone, iPad and Apple Watch Apple finally announced their latest lineup of the iPhone last night the iPhone 11, iPhone 11 Pro and iPhone 11 Pro Max. To accompany it, they released a new 10.2 inch iPad which replaces the 9.7 4 min read 4 min read Chris Gardener Sep 11, 2019 You Might Not Need to Change WHAT You Do, But WHO You Do it For I bought a printer a few years ago. Just a cheap, nothing fancy, print-the-odd-boarding-pass-at-home printer. It cost me about 30. A Business Moves On Its Stomach: How to Make Allowances for Gut Feelings | Data Driven Investor Gut feelings, as it turns out, are more than a feeling. The science is quite clear on it: Your gut knows more than youwww.datadriveninvestor.com I specifically chose a cheap one, as I rarely have the need to print things. I actively try to avoid it, in fact. So, I didnt need anything special. 4 min read 4 min read Alissa Knight Sep 11, 2019 Member-only Undivided We Fall: Decoupling Network Segmentation from Micro-Segmentation in the Software Defined Perimeter Introduction As of today, no laws or regulations, even the latest version of PCI-DSS, HIPAA, and HITECH, do not make network segmentation or micro-segmentation compulsory to comply with the rule. 5 min read 5 min read Maajid Khan Deep Learning Engineer| AI | Machine Learning | Data Science Enthusiast | DataAnalytics | Python | Java | GIT | Hadoop | C++ | Blogger | Sports | DataStructures More from Medium Dharmaraj Deploying Deep Learning Model using Flask API Nate DiRenzo in Towards Data Science Deploying ML Models using Streamlit Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Maria Gusarova Build A Beautiful Machine Learning Web App With Streamlit | Python Tutorial (Python code included) Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5731,\n",
       "  'url': 'https://towardsdatascience.com/using-fips-to-visualize-in-plotly-14fa7a6ddcf0',\n",
       "  'title': 'Using FIPS to Visualize in\\xa0Plotly',\n",
       "  'subtitle': '-',\n",
       "  'claps': 40,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-06-08',\n",
       "  'clap_prop': 1.975813084130615e-05,\n",
       "  'text': 'Towards Data Science Jun 7 2019 Member-only Listen Save Using FIPS Visualize Plotly Recently done two visualization project Plotly visualizing average rent per square foot California average data scientist salary across United States used two different approach visualize data map Scatter plot Map Chloropleth Map Plotly Plotly one powerful visualization package available Python One advantage using Plotly may use Python generate d3 graph Plotly built top d3 take long time learn d3 Plotly help eliminate annoying moment focus understanding data Plotly ha rich library map visualization easy use Map one type Plotly could ease frustration Scatter Plot Map good idea make scatter plot map data unit city chose visualize average data scientist salary cross United States approach average salary based city process making visualization almost making scatter plot Plotly except background map mean data plotting map based longitude latitude corresponds x value respectively data frame average salary city ready obtain longitude latitude city store data frame next step assign colour want differentiate level salary final step define plot layout visualization Plotly nice may go plotly.graph_objs may find Scattergeo US map visualization referenced example official Plotly documentation may find link bottom post would like look code may also find link bottom post Scatterplot map best visualizing data based city easy interrupting ha problem try visualize data US map plenty non-US map available However doe work well city data set close instance many data point Bay Area point stacked viewer may find difficult find difference area Choropleth Map alternative approach visualize data map choropleth map Choropleth map county-based map filled colour county map look like One advantage choropleth map data point stack may think hard plot use longitude latitude plot map instead use FIPS locate county FIPS county code FIPS county code stand Federal Information Processing Standard United States federal government assigns number county country nice feature Plotlys choropleth map Plotly take FIPS county code parameter FIPS county code ha 5 digit first 2 digit represent state last 3 digit represent county example FIPS county code San Francisco County 06075 06 mean California 075 represent San Francisco Since FIPS county code designated county would plot data wrong data Plotly may find list FIPS county code federal government website included link bottom post Choropleth Map Example one project graduate school professor gave data set rent across California sourced Craigslist decided find median rent per square foot across California data set contains small amount data outside California nice thing FIPS exclude observation FIPS start 06 FIPS start value California data ready may import create_choropleth plotly.figure_factory pas FIPS value colour create choropleth final visualization look like may also find code visualization downside Plotly making choropleth map found useful US map One time attempted plot choropleth map UK map find package option support current version great visualizing US outside US Thought mentioned two type map visualization powered Plotly scatter plot map choropleth map map server different purpose map depends whether want plot city county want plot data city go scatter plot map expect longitude latitude ready verse want plot data county choropleth map good way FIPS county code ready data set coming federal government likely FIPS county code ha paired data already Therefore choropleth map Plotly handy package visualize data US national data federal government Reference Plotly Scatterplot Map http //plot.ly/python/scatter-plots-on-maps/ FIPS county source http //www.census.gov/geographies/reference-files/2013/demo/popest/2013-geocodes-all.html http //www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/ cid=nrcs143_013697 Github http //github.com/jacquessham Data Salary across US Scatterplot Map http //github.com/jacquessham/ds_salary_opt Median Rent per Square foot county across California Choropleth Map http //github.com/jacquessham/california_rent 43 43 43 Towards Data Science home data science Medium publication sharing concept idea code Dipen Chawla Jun 7 2019 Member-only Machine Learning help identify Effectiveness Adverseness Drug Building system processing text review neurological drug employing ML algorithm provide overview effectiveness adverse reaction form insightful visually informative representation ever looked common cold medicine Internet horrified number side-effects listed gone review section verify got overwhelmed sheer number review listed probably couldnt read 10 min read 10 min read Share idea million reader Robert Dargavel Smith Jun 7 2019 Member-only discover new music Spotify Artificial Intelligence Find similar sounding music artist may never heard UPDATE Try Deej-A.I Apps Google Play Discover new music using Artificial Intelligence automatically generate playlist based music soundsplay.google.com http //apps.apple.com/us/app/deej-a-i/id1529860910 mt=8 DJ question hate got anything insert totally inappropriate artist kind music listen opinion soon put music genre box becomes constrained limit 5 min read 5 min read Sachin Mehta Jun 7 2019 Member-only ESPNetv2 Semantic Segmentation Nowadays number real-world application autonomous vehicle involves visual scene understanding Semantic segmentation one main task open way visual scene understanding However one computationally expensive task computer vision 4 min read 4 min read Aldo von Wangenheim Jun 7 2019 Member-only Artificial Intelligence Paleontology Use Deep Learning search Microfossils analyze micro-tomographies sedimentary rock obtained drilling rig probe semantic segmentation posting show Deep Learning-based method fully automated microfossil identification extraction bore core sample acquired via MicroCT identification developed Deep Learning approach resulted high rate correct microfossil identification 98 IoU validate use ground 12 min read 12 min read Edoardo Barp Jun 7 2019 Implementing Simple Auto-Encoder Tensorflow Generative Adversarial Networks GAN recently risen popularity display capability initially imitating famous painter art style recently DeepFake allows seamlessly replace facial expression video keeping high output quality One 7 min read 7 min read Jacques Sham data engineer BI company Czech heritage whisky-lover aviation enthusiast gamer Concern use data science answer question Medium Nik Piepenbreier Better Programming Make Awesome Maps Python Geopandas Norris Towards Data Science Plotting Heat Maps Python using Bokeh Folium hvPlot Vinod Dhole JovianData Science Machine Learning Interesting Heatmaps Using Python Folium MecSimCalc Level Coding Python tutorial use Folium publish interactive map Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Jun 7, 2019 Member-only Listen Save Using FIPS to Visualize in Plotly Recently I have done two visualization projects with Plotly, visualizing average rent per square foot in California and average data scientist salary across the United States. I used two different approaches to visualize data on the map Scatter plot on Map and Chloropleth Map. Why Plotly? Plotly is one of the powerful visualization packages available in Python. One of the advantages of using Plotly is that you may use Python to generate d3 graphs because Plotly is built on top of d3. It takes a very long time to learn d3 but Plotly can help to eliminate the annoying moment and focus more on understanding the data. Plotly has a rich library on map visualization and easy to use. Map is one of the types in Plotly that could ease your frustration. Scatter Plot on Map  It is a good idea to make scatter plot on map when the data unit is city, so I chose to visualize average data scientist salary cross the United States with this approach because the average salary is based on city. The process of making this visualization is almost the same as making scatter plot in Plotly, except the background is a map. It means the data is plotting on the map based on longitude and latitude, which corresponds to x and y values, respectively. Once you have the data frame with average salary and cities ready, then obtain the longitude and latitude of each city and store in the same data frame. The next step is to assign colour you want to differentiate the level of salary. The final step is to define the plot and the layout of the visualization. Plotly is nice because you may go to plotly.graph_objs which you may find Scattergeo for US map. This visualization is referenced from the example from the official Plotly documentation, you may find the link at the bottom of the post. If you would like to look at my code, you may also find the link at the bottom of the post too. Scatterplot on the map is best for visualizing data based on city and very easy interrupting. It has no problem if you try to visualize data no on the US map as there is plenty of non-US maps available. However, it does not work too well if the cities in the data set are very close to each other. For instance, if you have too many data points in the Bay Area, some points stacked on each other that viewers may find it difficult to find the difference in that area. Choropleth Map  The alternative approach to visualize data on maps is choropleth map. Choropleth map is a county-based map which filled colour the county on the map. It looks like this: One advantage of a choropleth map is that data points do not stack on each other. You may think it is hard to plot as you cannot use longitude and latitude to plot on the map, but instead, you use FIPS to locate the counties. FIPS county code  FIPS county code stands for Federal Information Processing Standard which the United States federal government assigns a number on each county in the country. A nice feature of Plotlys choropleth map is that Plotly takes FIPS county code as a parameter. FIPS county code has 5 digits, the first 2 digits represent the state and the last 3 digits represent the county. For example, the FIPS county code of San Francisco County is 06075. 06 means California, and 075 represent San Francisco. Since the FIPS county code is designated to each county, you would not plot the data on the wrong data in Plotly. You may find the list of FIPS county codes the federal government website and I have included the link at the bottom of this post. Choropleth Map Example  In one of my projects in graduate school, my professor gave me a data set on rent across California sourced from Craigslist and I decided to find out the median rent per square foot across California. The data set contains a small amount of data outside of California, the nice thing about FIPS is that I can exclude the observations that do not have a FIPS start with 06 because FIPS start with other values are not California. Once the data is ready, you may import create_choropleth from plotly.figure_factory and pass FIPS, values and colour to create a choropleth. My final visualization looks like this: You may also find my codes on this visualization. The downside about Plotly making choropleth map is that I only found that useful for US map. One time I attempted to plot a choropleth map on an UK map but I cannot find any package or option support that. The current version is great for visualizing in the US but not outside the US. Thought  I have mentioned two types of map visualization powered by Plotly scatter plot on map and choropleth map. Both maps server different purpose of map, depends on whether if you want to plot by city or county. If you want to plot data by city, you should go with scatter plot on map and expect to have longitude and latitude ready. By verse, if you want to plot data by county, choropleth map is a good way and you should have FIPS county code ready. If the data set is coming from the federal government, it is very likely that the FIPS county code has been paired with data already. Therefore, choropleth map from Plotly is a handy package to visualize data on US national data from the federal government. Reference  Plotly Scatterplot on Map:  https://plot.ly/python/scatter-plots-on-maps/ FIPS county source:  https://www.census.gov/geographies/reference-files/2013/demo/popest/2013-geocodes-all.html https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697 My Github:  https://github.com/jacquessham Data Salary across the US (Scatterplot on Map):  https://github.com/jacquessham/ds_salary_opt Median Rent per Square foot by county across California (Choropleth Map):  https://github.com/jacquessham/california_rent 43 43 43 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Dipen Chawla Jun 7, 2019 Member-only How Machine Learning can help identify Effectiveness and Adverseness of a Drug Building a system for processing text reviews of neurological drugs by employing ML algorithms to provide an overview of the effectiveness or adverse reactions in the form of an insightful and visually informative representation. Have you ever looked up your common cold medicine on the Internet only to be horrified by the number of side-effects listed under it? Or gone through the review section to verify them but got overwhelmed by the sheer number of reviews listed you probably couldnt read if you had 10 min read 10 min read Share your ideas with millions of readers. Robert Dargavel Smith Jun 7, 2019 Member-only How to discover new music on Spotify with Artificial Intelligence Find similar sounding music by artists you may never have heard of before UPDATE: Try it out for yourself here, here Deej-A.I. - Apps on Google Play Discover new music using Artificial Intelligence to automatically generate playlists based on how the music soundsplay.google.com or here: https://apps.apple.com/us/app/deej-a-i/id1529860910?mt=8 As a DJ, the question I hate most after Have you got anything by [insert totally inappropriate artist here]? is What kind of music do you listen to?. In my opinion, as soon as you put music in a genre box, it becomes constrained by its limits 5 min read 5 min read Sachin Mehta Jun 7, 2019 Member-only ESPNetv2 for Semantic Segmentation Nowadays, a number of real-world applications, such as autonomous vehicles, involves visual scene understanding. Semantic segmentation is one of the main tasks that opens the way for visual scene understanding. However, it is one of the most computationally expensive tasks in computer vision. 4 min read 4 min read Aldo von Wangenheim Jun 7, 2019 Member-only Artificial Intelligence & Paleontology: Use Deep Learning to search for Microfossils How to analyze micro-tomographies of sedimentary rocks obtained from drilling rig probes with semantic segmentation In this posting we show a Deep Learning-based method for fully automated microfossil identification and extraction in bore core samples acquired via MicroCT. For the identification we developed a Deep Learning approach which resulted in a high rate of correct microfossil identification (98% IoU). To validate it we use ground 12 min read 12 min read Edoardo Barp Jun 7, 2019 Implementing a Simple Auto-Encoder in Tensorflow Generative Adversarial Networks (GAN) have recently risen in popularity through the display of some of their capabilities, initially by imitating famous painters art styles, but more recently through DeepFake, which allows to seamlessly replace facial expression in videos, while keeping a high output quality. One of the 7 min read 7 min read Jacques Sham A data engineer in a BI company with Czech heritage, a whisky-lover, an aviation enthusiast, and a gamer. Concern how to use data science to answer questions. More from Medium Nik Piepenbreier in Better Programming Make Awesome Maps in Python and Geopandas Will Norris in Towards Data Science Plotting Heat Maps in Python using Bokeh, Folium, and hvPlot Vinod Dhole in JovianData Science and Machine Learning Interesting Heatmaps Using Python Folium MecSimCalc in Level Up Coding Python tutorial on how to use Folium to publish an interactive map Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5469,\n",
       "  'url': 'https://towardsdatascience.com/is-artificial-intelligence-the-next-big-thing-in-hollywood-535688dfe388',\n",
       "  'title': 'Is Artificial Intelligence the next big thing in Hollywood?',\n",
       "  'subtitle': '-',\n",
       "  'claps': 232,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 0.00011459715887957567,\n",
       "  'text': \"Towards Data Science Mar 31 2019 Member-only Listen Save Artificial Intelligence next big thing Hollywood Movies captured imagination people ever since came limelight Right first motion picture late 1880s upcoming latest sci-fi blockbuster cinema ha become medium love joy passion movie lover Almost every country across globe ha movie industry film industry USA ha distinction biggest famous recognizable industry world today none Hollywood Hollywood universally adored moviegoer due ability churn high-quality cinema best technology Computer-generated imagery visual effect always hallmark Hollywood industry well-known using best technician equipment create breath-taking cinema last decade Hollywood ha increasingly investing experimenting innovative technology Artificial Intelligence AI AI machine learning algorithm achieve task quickly scale capable accomplishing variety thing real-time require dedicated team people used effectively new age technology bring best edits best performance advanced visual effect possible today Artificial Intelligence Game Changer Movie production studio Hollywood nowadays looking hire engineer good training AI-based deep learning machine learning algorithm could visual effect specialist work example work making digital character look realistic smoothing effect etc done easily intelligent algorithm Advanced algorithm capability automatically render advanced visual effect AI therefore help creative artist focus effort important work waste valuable time meticulously editing effect Last year superhero blockbuster Avengers Infinity War American actor Josh Brolin play titular character super villain Thanos visual effect Thanos on-screen appearance wa done well looked real life-like visual effect team accomplished feat wa using AI algorithm track Brolins facial expression included minute detail wrinkle using another algorithm map face render body Thanos Using machine learning algorithm whole process could done real-time wa done without AI would taken least week team get result using face mapping swapping technology Disney recently came robot acrobat could perform well human acrobat robot acrobat could edited using AI made look like human counterpart Thus actor could focus part job le dangerous early 2000s Peter Jacksons Lord Rings series used AI-driven software generate huge army seen three movie 2016 20th Century Fox partnered IBM Research develop movie trailer using AI movie Morgan IBMs AI cognitive system Watson wa used create horror movie trailer kept audience edge seat Apart editing performance side filmmaking process AI may soon used decide whether film made According Variety Belgian company called Scriptbook ha come AI-based algorithm analyse screenplay predict whether movie commercially successful help production studio make calculated decision movie need made Using AI Hyper-personalized User Targeting Movie studio typically spend third budget marketing movie could spend marketing money wisely targeting right audience would make efficient utilization budget Using high-end AI tool possible deliver hyper-personalized content customer per preference American movie production company Fox formed partnership Google Cloud develop Merlin AI-based learning program main objective Merlin analyse trailer identify basic pattern audience preference different type movie Online streaming giant Netflix us AI program recommendation subscriber also generate targeted mini trailer user click doe mouse-hover show consider watching kind personalized relationship wasnt available traditional Hollywood company company product removed cut direct customer relationship Studios dont know exactly watching movie TV show situation however slowly changing Subscription-based movie ticketing service MoviePass trying emulate Netflix style data-driven model around theatrical movie business Online video streaming coupled AI enables movie production company build direct-to-consumer relationship long-term basis powerful aspect Hollywood near future Disney one big movie production company understood immense value model brings ha fact scrapped deal Netflix launching subscription-based video on-demand service next year Conclusion AI machine learning yet widely adopted movie company around world due fact dont fully understand Filmmakers understand AI already started using element machine learning deep learning specific area editing performance movie industry Hollywood part world growing fast pace year year According Box office Mojo number movie released Hollywood last year totalled whopping 873 number going keep increasing future Therefore important movie studio cater every moviegoer truly understanding need expectation Success going knock door company able achieve feat technology like AI play crucial role building robust customer relationship Originally published www.greatlearning.in April 1 2019 231 231 231 Towards Data Science home data science Medium publication sharing concept idea code Guy Tsror Mar 31 2019 Member-only romantic side data science Analyzing relationship year worth text message wa supposed private project one thought would upload private Google Colab notebook share whomever relevant one person referred OJ lightbulb clean share 11 min read 11 min read Share idea million reader Salma Ghoneim Mar 31 2019 Member-only Object detection via color-based image segmentation using python tutorial contouring using python OpenCV Getting started already jupyter notebook IDE run python OpenCV installed skip Execution Tools hero today Anaconda free open-source distribution help installing different package sort mess isolated environment Wikipedias telling u 7 min read 7 min read Sahil Dhankhad Mar 31 2019 Member-only Forget APIs Python Scraping Using Beautiful Soup Import Data File web Part 2 APIs always every website Beautiful Soup going stay forever collect kind data website Today Data play critical role every industry data coming internet company 8 min read 8 min read Andrew Donaldson Mar 31 2019 Member-only Dont Fear Robots Impressive tech important remember Boston Dynamics video trailer movie isnt yet made Every time one Boston Dynamics video hit met mix amazement prediction doom hand new robot overlord impressive also well choreographed staged Impressive tech important 4 min read 4 min read Jasmine Vasandani Mar 31 2019 Member-only 10 Steps Teaching Data Science Well resource data science instructor increase demand data scientist ha parallel growth training program within company educational institution teach people data science addition focusing meeting job demand need resource instructor facilitating data science classroom Heres 4 min read 4 min read Manu Siddharth Jha Medium Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Luay Rahil ILLUMINATION Elon Musk Eliminated Remote Work Working Home `` n't Work '' Akshad Singi Better Humans 4 Unsexy One-Minute Habits Save 30+ Hours Every Week Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Mar 31, 2019 Member-only Listen Save Is Artificial Intelligence the next big thing in Hollywood? Movies have captured the imagination of people ever since they came into the limelight. Right from the first motion picture in the late 1880s to the upcoming latest sci-fi blockbuster, cinema has become a medium of love, joy, and passion for all movie lovers. Almost every country across the globe has its own movie industry. The film industry of USA has the distinction of being the biggest, most famous, and most recognizable industry in the world today. It is none other than Hollywood. Hollywood is universally adored by moviegoers due to its ability to churn out high-quality cinema with the best technology. Computer-generated imagery and visual effects were always a hallmark of Hollywood. The industry is well-known for using the best technicians and equipment to create breath-taking cinema. Over the last decade, Hollywood has been increasingly investing and experimenting with innovative technologies such as Artificial Intelligence (AI). AI and machine learning algorithms can achieve tasks quickly and at scale. They are capable of accomplishing a variety of things in real-time that require dedicated teams of people. If used effectively, these new age technologies can bring out the best edits, the best performances, and the most advanced visual effects possible today. Artificial Intelligence The Game Changer Movie production studios in Hollywood are nowadays looking to hire engineers who are good at training AI-based deep learning and machine learning algorithms that could do a visual effects specialists work. For example, work such as making a digital character look realistic, or smoothing out an effect, etc. can be done easily by intelligent algorithms. Advanced algorithms have the capability to automatically render advanced visual effects. AI, therefore, helps creative artists focus their efforts in doing other important work than waste their valuable time in meticulously editing an effect. Last years superhero blockbuster, Avengers Infinity War, had American actor Josh Brolin play the titular character of the super villain, Thanos. The visual effects of Thanos on-screen appearance was done so well that it looked very real and life-like. How the visual effects team accomplished this feat was by using an AI algorithm to track Brolins facial expressions which included minute details such as his wrinkles, and then using another algorithm to map his face renders on to the body of Thanos. Using machine learning algorithms, the whole process could be done in real-time. If this was done without AI, it would have taken at least a few weeks for the team to get the same results using face mapping and swapping technology. Disney recently came up with robot acrobats that could perform as well as human acrobats. The robot acrobats could be edited using AI and made to look like their human counterparts. Thus, the actors could focus on parts of the job which is less dangerous. In the early 2000s, Peter Jacksons Lord of the Rings series used AI-driven software to generate the huge armies seen in all three movies. In 2016, 20th Century Fox partnered with IBM Research to develop a movie trailer using AI for the movie Morgan. IBMs AI cognitive system, Watson, was used to create a horror movie trailer that kept audiences on the edge of their seats. Apart from the editing and performance sides of the filmmaking process, AI may soon be used to decide whether a film is to be made or not. According to Variety, a Belgian company called Scriptbook has come up with AI-based algorithms that can analyse a screenplay and predict whether or not a movie will be commercially successful. This will help production studios to make calculated decisions on which movies need to be made. Using AI for Hyper-personalized User Targeting Movie studios typically spend over a third of their budget on marketing their movies. If they could spend their marketing money wisely by targeting the right audience, it would make a more efficient utilization of their budget. Using high-end AI tools, its possible to deliver hyper-personalized content to customers as per their preferences. American movie production company, Fox, formed a partnership with Google Cloud to develop Merlin, an AI-based learning program. The main objective of Merlin is to analyse trailers and identify the basic patterns in audiences preferences for different types of movies. Online streaming giant, Netflix, uses AI for program recommendations for its subscribers and also to generate targeted mini trailers when a user clicks or does a mouse-hover on a show to consider watching it. This kind of a personalized relationship wasnt available with traditional Hollywood companies. Most of these companies products were removed or have been cut off from direct customer relationships. Studios dont know exactly who is watching their movies and TV shows. This situation is however slowly changing. Subscription-based movie ticketing service, MoviePass, is trying to emulate a Netflix style data-driven model around the theatrical movie business. Online video streaming coupled with AI, enables movie production companies to build direct-to-consumer relationships on a long-term basis. This will be a powerful aspect of Hollywood in the near future. Disney is one of the big movie production companies that have understood the immense value that this model brings. It has, in fact, scrapped its deal with Netflix and launching its own subscription-based video on-demand service next year. Conclusion AI and machine learning are yet to be widely adopted by movie companies around the world. This is due to the fact that they dont fully understand them. Filmmakers who do understand AI have already started using elements of machine learning and deep learning in very specific areas such as editing and performance. The movie industry in Hollywood and other parts of the world is growing at a fast pace year after year. According to Box office Mojo, the number of movies released in Hollywood last year totalled to a whopping 873. This number is only going to keep increasing in the future. Therefore, its important for movie studios to cater to every moviegoer by truly understanding their needs and expectations. Success is going to knock at the door of those companies who are able to achieve this feat. This is where technologies like AI will play a crucial role in building robust customer relationships. Originally published at  www.greatlearning.in  on April 1, 2019. 231 231 231 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Guy Tsror Mar 31, 2019 Member-only The romantic side of data science: Analyzing a relationship through a year worth of text messages This was supposed to be a private project, one I thought I would upload to a private Google Colab notebook, and share with just whomever it is relevant for (which is, one other person, referred to here as OJ). But then, a lightbulb why not clean it up and share 11 min read 11 min read Share your ideas with millions of readers. Salma Ghoneim Mar 31, 2019 Member-only Object detection via color-based image segmentation using python A tutorial on contouring using python & OpenCV. Getting started If you already have jupyter notebook or an IDE with which you can run python & OpenCV installed, just skip to Execution. Tools Our hero today is Anaconda. a free open-source distribution that helps with installing different packages & sorts out their messes into isolated environments. What Wikipedias telling us about 7 min read 7 min read Sahil Dhankhad Mar 31, 2019 Member-only Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2 APIs are not always there for you for every website, but Beautiful Soup is going to stay with you forever to collect any kind of data from any website. Today, Data play a critical role in every industry. And most of this data is coming from the internet. Most company 8 min read 8 min read Andrew Donaldson Mar 31, 2019 Member-only Dont Fear the Robots Impressive as the tech here is, it is important to remember that the Boston Dynamics videos are trailers for a movie that isnt yet made. Every time one of these Boston Dynamics videos hit, they are met with a mix of amazement and predictions of doom at the hands of our new robot overlords. It is impressive. It is also very well choreographed and staged. Impressive as the tech here is, it is important 4 min read 4 min read Jasmine Vasandani Mar 31, 2019 Member-only 10 Steps to Teaching Data Science Well A resource for data science instructors. With the increase in demand for data scientists, there has been a parallel growth in training programs within companies and educational institutions that teach people data science. In addition to focusing on meeting the job demand, there need to be more resources for instructors facilitating data science classrooms. Heres a 4 min read 4 min read Manu Siddharth Jha More from Medium Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Luay Rahil in ILLUMINATION Elon Musk Eliminated Remote Work Because Working From Home \"Doesn\\'t Work\" Akshad Singi in Better Humans 4 Unsexy One-Minute Habits That Save Me 30+ Hours Every Week Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 3190,\n",
       "  'url': 'https://towardsdatascience.com/lets-apply-machine-learning-in-behavioral-economics-eb952d0f2300',\n",
       "  'title': 'Let’s Apply Machine Learning in Behavioral Economics',\n",
       "  'subtitle': '-',\n",
       "  'claps': 120,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-25',\n",
       "  'clap_prop': 5.9274392523918454e-05,\n",
       "  'text': 'Towards Data Science Apr 25 2019 Listen Save Lets Apply Machine Learning Behavioral Economics Currently Machine Learning ML well applied behavioral economics social science lack use come unfamiliarity different approach However time generation scientist growing machine learning move social science make impact technique made computer vision interaction machine learning behavioral economics mutually beneficial one hand ML used mine broad set data find behavioral-type variable contribute emergence different behavior hand ML algorithm embedded identify bias wrong assumption would reach higher performance article aim provide sense first-mentioned application ML behavioral economics research identifying variable important shaping people behavior specifically paper simple explanation ML technique Random Forest Gradient Boosting Machine help research related field behavioral economics social psychology Behavioral economics field study examines effect psychological cognitive emotional cultural social factor human decision decision deviate implied rational thinking word human considered sufficiently rational agent psychological variable context emphasized essential determinant human decision approach help predict human behavior better certain situation directional deviation rational thinking allows designing nudge policy improve people decision Daniel Kahneman excellent book Thinking Fast Slow state human pattern seeker order understand pattern behavioral economist consider psychological characteristic individual context analyzing people behavior However difficult apply individual context-based approach massive scale Machine learning significantly resolve challenge detecting pattern searching large set data variable influential shaping pattern Machine learning often pattern recognition help automatically detect pattern data use detected pattern tool predicting future action However focus ML ha prediction power le attention ha given interpretation power example although beneficial using ML cancer diagnosed accurately earlier essential thing ML provide insight variable weight increasing cancer risk knowledge help researcher policymakers control variable better reduce cancer risk Put differently ML give u prediction power target variable also knowledge find input variable crucial predicting target variable sense application ML behavioral economics work let u look decision tree model family machine learning algorithm go observation item conclusion item target value One form model target variable ha discrete set value called classification tree model branch tree represent conjunction input variable lead leaf represent class label target variable kind decision tree model regression tree target variable ha continuous value Two famous model use ensemble decision tree reach high accuracy prediction Random Forest Gradient Boosting Machine goal decision-tree based algorithm establish model predicts value target variable based several input variable vital aspect model limitation number input variable model word le concern curse dimensionality general step building model making interpretation using technique follow first data divided two set one set training data typically includes larger portion data model built set test data model validated creating effective model based training testing data reach model predict observed behavior data model give u knowledge identify variable contribution prediction target variable process data analysis referred feature importance example hundred input data regarding massive amount observation including individual context related variable using methodology find variable critical leading people show different behavior model also provide possibility local interpretation word understand top input variable important individual prediction example two person may show behavior entirely different reason Therefore looking individual prediction one specific behavior grouping top variable mostly explain behavior help u understand feature influential driving behavior possibility due nature decision-tree based model specific path branch leave representing target value observation figure 1 powerful ability machine learning thus resolve challenge data scale also enable u match right policy person population thousand million people word machine learning make possible targeting right nudge right people right context LinkedIn http //www.linkedin.com/in/atanehkar/ 180 2 180 180 2 Towards Data Science home data science Medium publication sharing concept idea code Ashok Chilakapati Apr 25 2019 Concept Drift Model Decay Machine Learning Concept drift drift label time essentially data lead divergence decision boundary new data model built earlier data/labels Scoring randomly sampled new data detect drift allowing u trigger expensive 7 min read 7 min read Share idea million reader Ren Bremer Apr 25 2019 use Python feature data analytics project Python tutorial Azure using OO NumPy panda SQL PySpark 1 Introduction lot company moving cloud consider tooling shall used data analytics On-premises company mostly use propriety software advanced analytics BI reporting However tooling may logical choice cloud environment 9 min read 9 min read Ryan Burge Apr 25 2019 Member-only Religious Composition Two Major Parties make lot data visualization time pretty good idea result calculation going look like write code fact result obvious dont even post anywhere 4 min read 4 min read Ashutosh Singh Apr 25 2019 Member-only Classifying Products Banned Approved using Text Mining- Part II part explain optimize existing Machine Learning model Part deployment ML model using Flask previous article series discussed business problem shown train model using fastText classification Banned Approved product based information like Product Name product description specification 5 min read 5 min read Favio Vzquez Apr 25 2019 Jungle Koalas Pandas Optimus Spark expect newest library Databricks Koalas Optimus framework Apache Spark 3.x excited data science probably know Spark+AI latest summit started yesterday April 24th 2019 great thing talk spin-off youve following co-created 8 min read 8 min read Ahmad Tanehkar Behavioral Economics Consumer Psychology Researcher Medium Samuele Mazzanti Towards Data Science Using Causal ML Instead A/B Testing Anil Tilbe Towards AI Bayesian Inference Best 5 Models 10 Best Practices Machine Learning Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 25, 2019 Listen Save Lets Apply Machine Learning in Behavioral Economics Currently, Machine Learning (ML) is not well applied in behavioral economics or social science, and this lack of use comes from unfamiliarity with this different approach. However, over time the generation of scientists who are growing up with machine learning will move into social science and make the impact that these techniques have made in computer vision. The interactions between machine learning and behavioral economics can be mutually beneficial. On the one hand, ML can be used to mine a broad set of data and find the behavioral-type variables that contribute to the emergence of different behaviors. On the other hand, ML algorithms that are embedded to identify biases and wrong assumptions would reach higher performance. This article aims to provide a sense for the first-mentioned application of ML in behavioral economics research, which is identifying variables that are important in shaping peoples behavior. More specifically this paper is a simple explanation of how ML techniques such as Random Forest and Gradient Boosting Machine can help in research related to fields such as behavioral economics or social psychology. Behavioral economics is a field of study that examines the effects of psychological, cognitive, emotional, cultural and social factors on human decisions and how these decisions deviate from those implied by rational thinking. In other words, humans will not be considered as sufficiently rational agents, and psychological variables, and contexts will be emphasized as essential determinants of humans decisions. This approach helps to predict human behavior better, and in certain situations where there are directional deviations from rational thinking, allows for designing nudge policies to improve peoples decisions. As Daniel Kahneman in his excellent book Thinking, Fast and Slow states: We (humans) are pattern seekers. In order to understand these patterns, behavioral economists consider the psychological characteristics of individuals and the context in analyzing people behavior. However, it is difficult to apply this individual and context-based approach on a massive scale. Machine learning can significantly resolve this challenge by detecting patterns and searching in a large set of data for variables that are influential in shaping the patterns. Machine learning is often about pattern recognition, and it helps to automatically detect patterns in data and then use the detected patterns as a tool for predicting future actions. However, the most focus on ML has been on its prediction power, and less attention has been given to its interpretation power. For example, although it is very beneficial that using ML cancer can be diagnosed more accurately and earlier, but the more essential thing that ML can provide is the insight about the variables having more weight in increasing cancer risk. This knowledge helps researchers and policymakers to control those variables better and reduce cancer risk. Put differently ML gives us not only the prediction power over a target variable but also the knowledge to find out which input variables are more crucial in predicting that target variable. To have a sense how this application of ML to behavioral economics works, let us look at the decision trees models, a family of machine learning algorithms that go from observations about an item to conclusions about the items target value. One form of this model where the target variable has a discrete set of values is called classification trees. In these models, branches of trees represent conjunctions of input variables that lead to the leaves which represent the class labels of a target variable. The other kind of decision tree models is regression trees, in which the target variable has continuous values. Two famous models that use an ensemble of decision trees to reach the high accuracy in prediction are Random Forest and Gradient Boosting Machine. The goal of these decision-tree based algorithms is to establish a model that predicts the value of a target variable based on several input variables. The vital aspect of these models is that there is no limitation for the number of input variables in these models. In other words, there is less concern about the curse of dimensionality for them. The general steps for building a model and making interpretation using these techniques are as follow: first, data is divided into two sets, one set is the training data which typically includes the larger portion of data on which the model is built, and the other set is test data by which the model is validated. After creating the most effective model based on the training and testing data, we will reach to a model that can predict the observed behavior in data. This model now gives us the knowledge to identify the variables that have the most contribution to the prediction of the target variable. This process in data analysis is referred to as feature importance. For example, if we have hundreds of input data regarding a massive amount of observations, including individual and context related variables, by using this methodology we can find out what variables are critical in leading people to show different behaviors. These models also provide the possibility of local interpretation. In other words, we can understand the top input variables that are most important in each individual prediction. For example, two persons may show the same behavior for entirely different reasons. Therefore, looking at each individual prediction for one specific behavior and grouping the top variables that mostly explain this behavior will help us to understand what features are more influential in driving that behavior. This possibility is due to the nature of decision-tree based models, in which there is a specific path from branches to each leave representing the target value for each observation (figure 1). These powerful abilities of machine learning, thus, not only resolve the challenges of data scale but also enable us to match the right policy for a person in a population of thousands or millions of people. In other words, machine learning makes possible targeting the right nudges to the right people, in the right context. LinkedIn:  https://www.linkedin.com/in/atanehkar/ 180 2 180 180 2 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ashok Chilakapati Apr 25, 2019 Concept Drift and Model Decay in Machine Learning Concept drift is a drift of labels with time for the essentially the same data. It leads to the divergence of decision boundary for new data from that of a model built from earlier data/labels. Scoring randomly sampled new data can detect the drift allowing us to trigger the expensive 7 min read 7 min read Share your ideas with millions of readers. Ren Bremer Apr 25, 2019 How to use Python features in your data analytics project Python tutorial in Azure using OO, NumPy, pandas, SQL, PySpark 1. Introduction A lot of companies are moving to cloud and consider what tooling shall be used for data analytics. On-premises, companies mostly use propriety software for advanced analytics, BI and reporting. However, this tooling may not be the most logical choice in a cloud environment. 9 min read 9 min read Ryan Burge Apr 25, 2019 Member-only The Religious Composition of the Two Major Parties I make a lot of data visualizations. Most of the time I have a pretty good idea of what the results of the calculations are going to look like before I write the code. In fact, some of the results are so obvious that I dont even post them anywhere 4 min read 4 min read Ashutosh Singh Apr 25, 2019 Member-only Classifying Products as Banned Or Approved using Text Mining- Part II In this part, we will explain how to optimize the existing Machine Learning model in Part I and the deployment of this ML model using Flask. In the previous article of this series, We have discussed the business problem, shown how to train the model using fastText and classification of Banned or Approved products based on information like(Product Name, product description, and specifications). 5 min read 5 min read Favio Vzquez Apr 25, 2019 The Jungle of Koalas, Pandas, Optimus and Spark What to expect from the newest library from Databricks (Koalas), the Optimus framework and Apache Spark 3.x If you are as excited about data science as me, you probably know that the Spark+AI latest summit started yesterday (April 24th 2019). And there are great things to talk about. But I will do it with a spin-off. If youve been following me you now that I co-created a 8 min read 8 min read Ahmad Tanehkar Behavioral Economics and Consumer Psychology Researcher More from Medium Samuele Mazzanti in Towards Data Science Using Causal ML Instead of A/B Testing Anil Tilbe in Towards AI Bayesian Inference: The Best 5 Models and 10 Best Practices for Machine Learning Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 3185,\n",
       "  'url': 'https://towardsdatascience.com/concept-drift-and-model-decay-in-machine-learning-a98a809ea8d4',\n",
       "  'title': 'Concept Drift and Model Decay in Machine\\xa0Learning',\n",
       "  'subtitle': '-',\n",
       "  'claps': 455,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-25',\n",
       "  'clap_prop': 0.00022474873831985746,\n",
       "  'text': 'Towards Data Science Apr 25 2019 Listen Save Concept Drift Model Decay Machine Learning Concept drift drift label time essentially data lead divergence decision boundary new data model built earlier data/labels Scoring randomly sampled new data detect drift allowing u trigger expensive re-label/re-train task needed basis Nothing last forever even carefully constructed model mound well-labeled data predictive ability decay time Broadly two way model decay Due data drift due concept drift case data drift data evolves time potentially introducing previously unseen variety data new category required thereof impact previously labeled data case concept drift interpretation data change time even general distribution data doe cause end user interpret model prediction deteriorated time same/similar data fix requires re-labeling affected old data re-training model data concept simultaneously drift well vexing matter Rather like Lakers play Patriots World Series concept/game data/players change would lot head scratching stand post detection concept drift post-production scenario model trained older data place classifying new incoming data Continuous re-labeling old data retraining thereof prohibitively expensive want detect concept drift act needed ease illustration visualization stick 2-d i.e two feature two class divvy feature space approach detection generic drift detected model need updated detail depend application general focus post concept drift Data drift subject next post series look code snippet illustration full code reproducing reported result downloaded github Update 8/21/2020 Came across nice article list reference topic neptune.ai Shibsankar Das Best Practices Dealing Concept Drift 1 Concept Drift Concept drift arises interpretation data change time even data may agreed upon belonging class past claim belong class B understanding property B changed since pure concept drift Clearly need explanation context example piece text legitimately labelled belonging one class 1960 belonging different one 2019 prediction model built 1960 going largely error data 2019 See Figure 1 extreme example perhaps one serf illustrate problem rule game change 2 Shifting Decision Boundaries Concept drift seen morphing decision boundary time Consider simulated 2-class situation Figure 2 continual morphing data decision boundary crux concept drift way detect course make effort label least new data routine basis look degradation predictive ability model feel degradation longer tolerable need rebuild model data re-labeled per current understanding data 3 Simulating Concept Drift continue similar problem shown Figure 2 quantitative light trigger rebuild signal performance model decay threshold value problem statement 3.1 Generate data class B start 1000 data point Every new batch add 125 point class data decision boundary w different batch 3.2 Build SVM classifier build SVM model labeled data obtain f1-score test/sampled data using 3.3 Detect decay Twenty five randomly chosen data point 10 batch labeled compared prediction latest model hand f1-score sample fall threshold 0.925 trigger re-label/re-train task 3.4 Re-label data decision boundary rotates deviate farther farther model decision boundary model prediction continually get worse f1-score sampled data drop 0.925 re-label data per current data decision boundary simple geometry 2-features linear data/model decision boundary misclassified new data easy figure general re-label data 3.5 Re-train complete loop model need re-trained using updated label recover predictive ability 4 Results Figure 3 confirms expecting drifting several batch newly sampled data plotted along starting/final f1-score ha deteriorated 0.925 data decision boundary plot show amount new data would misclassified previously built model Re-labeling per final data decision boundary rebuilding model point recovers perfect f1-score data plotted rebuild show updated model decision boundary perfectly separate two class Figure 4 show model decay concept drift followed recovery upon re-label rebuild task total angle rotation proxy concept drift 5 Conclusions Concept drift driven model decay expensive fix given need manual re-labeling old data Drift detected via global measure worsening f1-scores sampled new data could complete re-label data labeling manual rather formula driven likely possible real situation however Even rough identification old data need re-labeled could help lowering manual work Something requires work move data drift associated issue next post Originally published http //xplordat.com April 25 2019 536 2 536 536 2 Towards Data Science home data science Medium publication sharing concept idea code Ren Bremer Apr 25 2019 use Python feature data analytics project Python tutorial Azure using OO NumPy panda SQL PySpark 1 Introduction lot company moving cloud consider tooling shall used data analytics On-premises company mostly use propriety software advanced analytics BI reporting However tooling may logical choice cloud environment 9 min read 9 min read Share idea million reader Ryan Burge Apr 25 2019 Member-only Religious Composition Two Major Parties make lot data visualization time pretty good idea result calculation going look like write code fact result obvious dont even post anywhere 4 min read 4 min read Ashutosh Singh Apr 25 2019 Member-only Classifying Products Banned Approved using Text Mining- Part II part explain optimize existing Machine Learning model Part deployment ML model using Flask previous article series discussed business problem shown train model using fastText classification Banned Approved product based information like Product Name product description specification 5 min read 5 min read Favio Vzquez Apr 25 2019 Jungle Koalas Pandas Optimus Spark expect newest library Databricks Koalas Optimus framework Apache Spark 3.x excited data science probably know Spark+AI latest summit started yesterday April 24th 2019 great thing talk spin-off youve following co-created 8 min read 8 min read Rinu Gour Apr 25 2019 Member-only Complete Guide Learn R R Programming Technology open source programming language Also R programming language latest cutting-edge tool R Basics hottest trend Moreover R command line interface C.L.I consists prompt usually character History R John Chambers colleague developed R Bell Laboratories Basically 8 min read 8 min read Ashok Chilakapati Medium Samuele Mazzanti Towards Data Science Using Causal ML Instead A/B Testing Anil Tilbe Towards AI Bayesian Inference Best 5 Models 10 Best Practices Machine Learning Freedom Preetham Statistical Sauce Confidence Tolerance Prediction Intervals Statistical Forecasts Moez Ali Top AutoML Python library 2022 Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 25, 2019 Listen Save Concept Drift and Model Decay in Machine Learning Concept drift is a drift of labels with time for the essentially the same data. It leads to the divergence of decision boundary for new data from that of a model built from earlier data/labels. Scoring randomly sampled new data can detect the drift allowing us to trigger the expensive re-label/re-train tasks on an as needed basis Nothing lasts forever. Not even a carefully constructed model on mounds of well-labeled data. Its predictive ability decays over time. Broadly, there are two ways a model can decay. Due to data drift or due to concept drift. In case of data drift, data evolves with time potentially introducing previously unseen variety of data and new categories required thereof. But there is no impact to previously labeled data. In case of concept drift our interpretation of the data changes with time even while the general distribution of the data does not. This causes the end user to interpret the model predictions as having deteriorated over time for the same/similar data. The fix here requires a re-labeling of the affected old data and re-training the model. Both data and concept can simultaneously drift as well, further vexing the matters Rather like having the Lakers play the Patriots in the World Series, when both the concept/game and the data/players change there would be a lot of head scratching in the stands This post is about the detection of concept drift in a post-production scenario where a model trained on older data is in place for classifying new incoming data. Continuous re-labeling of old data and retraining thereof is prohibitively expensive so we want to detect concept drift and act on it as needed. For ease of illustration and visualization we stick to 2-d (i.e. two features) and two classes that divvy up the feature space. But the approach to detection is generic. Once the drift is detected, the model needs to be updated. The details will depend on the application but in general: The focus of this post is concept drift. Data drift is the subject of the next post in this series. We look at some code snippets for illustration but the full code for reproducing the reported results here can be downloaded from github . Update (8/21/2020) : Came across a nice article (and with a list of references) on this topic at neptune.ai by Shibsankar Das. Best Practices for Dealing with Concept Drift . 1. Concept Drift Concept drift arises when our interpretation of the data changes over time even while the data may not have. What we agreed upon as belonging to class A in the past, we claim now that it should belong to class B, as our understanding of the properties of A and B have changed since. This is pure concept drift. Clearly needs further explanation and context. For example a piece of text can legitimately be labelled as belonging to one class in 1960 but belonging to a different one in 2019. So the predictions from model built in 1960 are going to be largely in error for the same data in 2019. See Figure 1 below for an extreme example perhaps but one that serves to illustrate the problem when the rules of the game change. 2. Shifting Decision Boundaries Concept drift can be seen as the morphing of decision boundaries over time. Consider a simulated 2-class situation in Figure 2. So the continual morphing of the data decision boundary is at the crux of concept drift. The only way to detect it then of course is to make the effort to label at least some of the new data on a routine basis and look for degradation in the predictive ability of the model. When we feel that the degradation is no longer tolerable we will need to rebuild the model with the data re-labeled as per the current understanding of the data. 3. Simulating Concept Drift We continue with a similar problem as shown in Figure 2 but in a quantitative light so as to trigger a rebuild! signal when the performance of the model decays to a threshold value. Here is the problem statement. 3.1 Generate data The classes A and B each will start off with 1000 data points. Every new batch adds 125 points to each class. The data decision boundary ( w ) is different for each batch. 3.2 Build an SVM classifier We build an SVM model on labeled data and obtain the f1-score for test/sampled data using the same. 3.3 Detect decay Twenty five randomly chosen data points (10% of the batch) are labeled and compared with predictions from the latest model at hand. When the f1-score of the sample falls below a threshold (0.925 here) we trigger a re-label/re-train task. 3.4 Re-label As the data decision boundary rotates and deviates farther and farther from the models decision boundary, the model predictions continually get worse. When the f1-score for the sampled data drops to about 0.925 we re-label all the data as per the current data decision boundary. Now, in our simple geometry with 2-features and linear data/model decision boundaries the misclassified new data is easy to figure out. But in general it will not be so we re-label all the data. 3.5 Re-train to complete the loop The model needs to be re-trained using the updated labels so as to recover its predictive ability. 4. Results Figure 3 below confirms what we have been expecting. After drifting over several batches, the newly sampled data is plotted along with the starting/final (by when the f1-score has deteriorated to 0.925) data decision boundaries. These plots show the amount of new data that would be misclassified by the previously built model. Re-labeling as per the final data decision boundary and rebuilding the model at that point recovers the perfect f1-score. All the data is plotted after a rebuild to show that the updated models decision boundary perfectly separates the two classes. Figure 4 below shows the model decay as the concepts drift followed by a recovery upon re-label & rebuild tasks. The total angle of rotation is the proxy for concept drift. 5. Conclusions Concept drift driven model decay can be expensive to fix given the need for manual re-labeling of old data. Drift can be detected via global measures such as worsening f1-scores for sampled new data. We could do a complete re-label of all data here as our labeling is not manual but rather formula driven. It will likely not be possible in a real situation however. Even a rough identification of the old data that needs to be re-labeled could help with lowering the manual work. Something that requires further work. We will move on to data drift and the associated issues in the next post. Originally published at  http://xplordat.com  on April 25, 2019. 536 2 536 536 2 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ren Bremer Apr 25, 2019 How to use Python features in your data analytics project Python tutorial in Azure using OO, NumPy, pandas, SQL, PySpark 1. Introduction A lot of companies are moving to cloud and consider what tooling shall be used for data analytics. On-premises, companies mostly use propriety software for advanced analytics, BI and reporting. However, this tooling may not be the most logical choice in a cloud environment. 9 min read 9 min read Share your ideas with millions of readers. Ryan Burge Apr 25, 2019 Member-only The Religious Composition of the Two Major Parties I make a lot of data visualizations. Most of the time I have a pretty good idea of what the results of the calculations are going to look like before I write the code. In fact, some of the results are so obvious that I dont even post them anywhere 4 min read 4 min read Ashutosh Singh Apr 25, 2019 Member-only Classifying Products as Banned Or Approved using Text Mining- Part II In this part, we will explain how to optimize the existing Machine Learning model in Part I and the deployment of this ML model using Flask. In the previous article of this series, We have discussed the business problem, shown how to train the model using fastText and classification of Banned or Approved products based on information like(Product Name, product description, and specifications). 5 min read 5 min read Favio Vzquez Apr 25, 2019 The Jungle of Koalas, Pandas, Optimus and Spark What to expect from the newest library from Databricks (Koalas), the Optimus framework and Apache Spark 3.x If you are as excited about data science as me, you probably know that the Spark+AI latest summit started yesterday (April 24th 2019). And there are great things to talk about. But I will do it with a spin-off. If youve been following me you now that I co-created a 8 min read 8 min read Rinu Gour Apr 25, 2019 Member-only A Complete Guide to Learn R R Programming Technology is an open source programming language. Also, the R programming language is the latest cutting-edge tool. R Basics is the hottest trend. Moreover, the R command line interface (C.L.I) consists of a prompt, usually the > character. History of R John Chambers and colleagues developed R at Bell Laboratories. Basically 8 min read 8 min read Ashok Chilakapati More from Medium Samuele Mazzanti in Towards Data Science Using Causal ML Instead of A/B Testing Anil Tilbe in Towards AI Bayesian Inference: The Best 5 Models and 10 Best Practices for Machine Learning Freedom Preetham in Statistical Sauce Confidence, Tolerance, and Prediction Intervals for Statistical Forecasts. Moez Ali Top AutoML Python libraries in 2022 Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 377,\n",
       "  'url': 'https://towardsdatascience.com/applied-ai-going-from-concept-to-ml-components-7ae9c5d823d3',\n",
       "  'title': '<strong class=\"markup--strong markup--h3-strong\">Applied AI: Going From Concept to ML Components</strong>',\n",
       "  'subtitle': '-',\n",
       "  'claps': 107,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 5.285300000049395e-05,\n",
       "  'text': 'Towards Data Science Mar 30 2019 Member-only Listen Save Applied AI Going Concept ML Components Opening mind different way applying machine learning real world Abraham Kang special thanks Kunal Patel Jae Duk Seo sounding board providing input article Executive Summary Candidate Problem Many people interested automating redundant process within organization using AI Lets start concrete problem noticed lawyer typically gather fact client something bad happens fact form basis cause action negligence battery assault intentional infliction emotional distress individual sue cause action determined based legal justification fact complaint written submitted court commencement legal action complaint legal document set fact giving rise legal basis taking action another party Manually creating document time consuming similar fact result similar cause action example someone hit another person usually battery someone accidentally hurt someone else someone slip fall within store could action negligence Based problem customer would like use AI learn write complaint fact paragraph describing happened Understanding Problem Trying get AI/ML read fact figure way AI/ML write whole complaint might biting model chew may effort would take year solve However take time understand think underlying problem find existing technique slight modification could used solve different piece puzzle example look complaint start description party position plaintiff v defendant well counsel representing may class action section justification jurisdiction doe court power party description party justification venue proper court location listing cause action description fact look section think data going build individual section going come certain case answer look carefully see pattern correlation different section complaint allow think input neural network candidate output Getting Inputs Neural Network dont data per se may way parse fact existing complaint use input neural network Every complaint submitted court becomes public information plenty data solution require attorney write fact inserting directly complaint minor inconvenience able machine learning provide generated complaint Generating complete complaint may difficult let break problem Breaking Problem Logically would break generation document smaller piece Well need look one example http //www.heise.de/downloads/18/1/8/9/1/3/4/6/NP-v-Standard-Innovation-Complaint.pdf make interesting picked maker adult toy might peak curiosity Basically want eventually generate complaint pdf fact provided lawyer look document complaint find similar pattern structure think would best way break thing dont scroll time think .Really think .. Well said break thing section using templating would route would probably best break complaint cause action listed complaint cause action violation Federal Wiretap Act Illinois Eavesdropping Statute Intrusion upon Seclusion Unjust Enrichment Fraud Deceptive Business Practice Act etc ha supporting rule justification based fact two problem come cause action fact text generate supporting text cause action Finding Causes Action look fact case need find cause action law broken could sue direct solution finding cause action text think fundamentally existing technique think use look text infer meaning description text said multi-label text classification multi-label sentiment analysis ahead game http //paperswithcode.com/task/text-classification http //paperswithcode.com/task/sentiment-analysis Analyzing text determine associated cause action similar process classifying text finding sentiment related text associated problem like fact cause action need updated law introduced may alternate way create embedding fact tie cause action fact based triplet http //arxiv.org/pdf/1503.03832.pdf quadruplet loss http //arxiv.org/pdf/1704.01719.pdf push cause action sharing similar word together embedding space unrelated cause action apart use clustering technique find cause action close determinative word embeddings used supporting argument associated word individual cause action section complaint Generating Text Supporting Arguments Section Individual Causes Action figured get high level cause action text generate supporting argument text individual cause action section violation Federal Wiretap Act Illinois Eavesdropping Statute Intrusion upon Seclusion Unjust Enrichment Fraud Deceptive Business Practice Act etc. one straight forward Think neural network architecture generate text Dont scroll idea .Open mind.Use Force Text generation algorithm http //paperswithcode.com/task/data-to-text-generation http //paperswithcode.com/area/nlp/text-generation might option even best one create gibberish often better alternative might use architecture like neural network involved translation http //paperswithcode.com/task/machine-translation http //paperswithcode.com/task/unsupervised-machine-translation http //paperswithcode.com/paper/unsupervised-clinical-language-translation addition might good idea separate translation neural network cause action help neural network focus identifying key fact used generating supporting argument cause action Clean probably going good idea run candidate text supporting argument text cause action grammar checker/fixer http //paperswithcode.com/task/grammatical-error-correction way blatant mess ups fixed Conclusion hope learned apply machine learning solution broadly Let know get stuck would definitely interested hearing problem people trying solve machine learning 107 2 107 107 2 Towards Data Science home data science Medium publication sharing concept idea code Michel Kana Ph.D Mar 30 2019 Wild Wide AI responsible data science shoot first new race human Data Science good thing u improves life make thing efficient effective lead better experience however miss-steps data-driven analysis ha already exhibited example data science tool intentionally unintentionally misused 2012 9 min read 9 min read Share idea million reader Kevin Luk Mar 30 2019 Member-only library load image Python difference Summarization Comparison .imread face computer vision project first need load image preprocessing various library perform imread want consolidate popular library loading image difference article go Libraries loading 3 min read 3 min read Abhishek Mukherjee Mar 30 2019 Trust interpretability machine learning machine learning model always need interpretable Given choice interpretable model inaccurate non-interpretable model accurate wouldnt rather choose non-interpretable accurate model word reason sacrificing accuracy altar interpretability 9 min read 9 min read Christine Calo Mar 30 2019 Microsoft Introduction AI Part 1 Machine Learning bit like wanted learn Artificial Intelligence although felt little intimidated math involved Maybe thought concept difficult understand would depth recently completed Microsoft Introduction AI course 17 min read 17 min read Okoh Anita Mar 30 2019 Repetition Songs Python Tutorial One Ed Sheeran song case study Everyone ha heard song know song sound like carelessly say everyone define song word benefit doubt song according Wikipedia single work music typically intended sung 8 min read 8 min read Abraham Kang Abraham Kang fascinated nuanced detail security associated machine learning algorithm programming language associated APIs Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Anmol Tomar CodeX Say Goodbye Loops Python Welcome Vectorization Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Mar 30, 2019 Member-only Listen Save Applied AI: Going From Concept to ML Components Opening your mind to different ways of applying machine learning to the real world. By Abraham Kang with special thanks to Kunal Patel and Jae Duk Seo for being a sounding board and providing input for this article. Executive Summary Candidate Problem Many people are interested in automating redundant processes within the organization using AI. Lets start with a concrete problem, what I noticed is that lawyers typically gather facts from clients when something bad happens. These facts form the basis of causes of action (negligence, battery, assault, intentional infliction of emotional distress) that an individual can sue on. Once the causes of action have been determined based on legal justification and the facts, a complaint is written up and submitted to the court for commencement of the legal action. The complaint is a legal document which sets out the facts giving rise to a legal basis for taking action against another party. Manually creating this document can be time consuming and similar facts result in similar causes of action. For example, if someone hits another person there is usually a battery. If someone accidentally hurts someone else or someone slips and falls within a store there could be an action for negligence. Based in this problem we have a customer who would like to use AI to learn how to write a complaint from a fact paragraph describing what happened. Understanding the Problem Trying to get AI/ML to read facts and figure out a way for AI/ML to write a whole complaint might be biting off more than the model can chew and may be an effort that would take years to solve. However, if you take the time to understand and think about the underlying problem, you can find existing techniques (with some slight modifications) that could be used to solve different pieces of the puzzle. For example, when you look at a complaint it starts with a description of the parties and their positions (plaintiff vs defendant) as well as counsel representing them. There may be a class action section, a justification of jurisdiction (does court have power over parties), description of the parties, a justification of venue (are we in the proper court location), a listing of the causes of action, and description of the facts. When you look at the sections you have to think about where the data that is going to build the individual sections is going to come from. In certain cases you will not have an answer but if you look carefully you will see patterns and correlations between different sections of the complaint. This will allow you to think about what your inputs to the neural network will be and the candidate outputs. Getting Inputs for the Neural Network We dont have any data per se but there may be a way to parse the facts out of all existing complaints and use them as the input for our neural network. Every complaint that is submitted to the court becomes public information so there will be plenty of data. This solution will require attorneys to write their facts as if they were inserting them directly into the complaint, but this is a minor inconvenience to be able to have machine learning provide generated complaints. Generating a complete complaint may be difficult. So lets break the problem down. Breaking the Problem Down Logically how would you break the generation of a document down into smaller pieces? Well you need to look at one so here is an example: https://www.heise.de/downloads/18/1/8/9/1/3/4/6/NP-v-Standard-Innovation-Complaint.pdf . To make it interesting I picked a maker of adult toys so it might peak your curiosity. Basically, we want to eventually generate a complaint (above pdf) from the facts provided by a lawyer. So if you look at the document and at other complaints you will find similar patterns as to structure. So what do you think would be the best way to break things down dont scroll down until you have had time to think about it. .Really think about it.. Well if you said to break things down by section using templating, then this would be the route that would probably be best. When you break down a complaint there are causes of action listed in the complaint. Each cause of action (violation of Federal Wiretap Act, Illinois Eavesdropping Statute, Intrusion upon Seclusion, Unjust Enrichment, Fraud and Deceptive Business Practice Act, etc.) has supporting rules and justification based on the facts. So now there are two problems. How do you come up with the causes of action from the facts text and how do you generate the supporting text under each cause of action? Finding the Causes of Action When we look at the facts of the case we need to find all of the causes of action (laws that were broken) that we could sue on. There are no direct solutions for finding causes of action from text so we will have to think more fundamentally. What existing techniques do you think we can use to look at text and infer meaning or a description of the text. If you said multi-label text classification or multi-label sentiment analysis, then you are ahead of the game ( https://paperswithcode.com/task/text-classification , https://paperswithcode.com/task/sentiment-analysis). Analyzing text to determine its associated causes of action is a similar process to classifying text or finding the sentiment of related text. There are associated problems like the fact that causes of action will need to be updated as laws are introduced. There may be an alternate way to create an embedding for the facts and then tie the causes of action to the facts based on triplet (https://arxiv.org/pdf/1503.03832.pdf) or quadruplet loss (https://arxiv.org/pdf/1704.01719.pdf) to push causes of action sharing similar words together in the embedding space and unrelated causes of action further apart. Then use a clustering technique to find causes of action close to determinative word embeddings used in the supporting argument associated with the words in the individual cause of action sections of the complaint. Generating the Text in the Supporting Arguments Section of Individual Causes of Action Now that you have figured out how to get the high level causes of action from the text, how can you generate the supporting argument text for each of the individual cause of action sections (violation of Federal Wiretap Act, Illinois Eavesdropping Statute, Intrusion upon Seclusion, Unjust Enrichment, Fraud and Deceptive Business Practice Act, etc.)? This one is not so straight forward. Think about a what neural network architectures which generate text (Dont scroll down until you have some ideas). .Open your mind.Use the Force. Text generation algorithms ( https://paperswithcode.com/task/data-to-text-generation , https://paperswithcode.com/area/nlp/text-generation ) might be an option but even the best ones create gibberish often. The better alternative might be to use an architecture like neural networks involved in translation ( https://paperswithcode.com/task/machine-translation , https://paperswithcode.com/task/unsupervised-machine-translation , https://paperswithcode.com/paper/unsupervised-clinical-language-translation ). In addition, it might be a good idea to have a separate translation neural network for each cause of action to help each neural network focus on identifying the key facts used in generating a supporting argument for each cause of action. Clean Up It is probably going to be a good idea to run the candidate text for the supporting argument text for each cause of action through a grammar checker/fixer ( https://paperswithcode.com/task/grammatical-error-correction ). This way any blatant mess ups are fixed. Conclusion I hope you learned how to apply the machine learning solutions more broadly. Let me know if you get stuck as I would definitely be interested in hearing about problems that people are trying to solve with machine learning. 107 2 107 107 2 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Michel Kana, Ph.D Mar 30, 2019 Wild Wide AI: responsible data science Who shoots first the new race or the human? Data Science can do good things for us: it improves life, it makes things more efficient, more effective and leads to a better experience. There are however some miss-steps that data-driven analysis has already exhibited. Here are few examples where data science tools were intentionally or unintentionally misused: In 2012 9 min read 9 min read Share your ideas with millions of readers. Kevin Luk Mar 30, 2019 Member-only What libraries can load image in Python and what are their difference? Summarization & Comparison of .imread() When we face computer vision project, first of all we need to load the images before any preprocessing. There are various libraries out there to perform imread() . Here I want to consolidate the popular libraries for loading image and their difference. This article will go through: Libraries for loading 3 min read 3 min read Abhishek Mukherjee Mar 30, 2019 Trust and interpretability in machine learning Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldnt you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability? 9 min read 9 min read Christine Calo Mar 30, 2019 Microsoft Introduction to AI Part 1 Machine Learning Are you a bit like me and have wanted to learn about Artificial Intelligence although felt a little intimidated by the maths involved? Maybe you thought the concepts were too difficult to understand and you would be out of your depth. I recently completed the Microsoft Introduction to AI course 17 min read 17 min read Okoh Anita Mar 30, 2019 Repetition in Songs: A Python Tutorial One of Ed Sheeran songs as a case study Everyone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung 8 min read 8 min read Abraham Kang Abraham Kang is fascinated with the nuanced details and security associated with machine learning algorithms, programming languages and their associated APIs. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Anmol Tomar in CodeX Say Goodbye to Loops in Python, and Welcome Vectorization! Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5464,\n",
       "  'url': 'https://towardsdatascience.com/for-the-successful-future-of-ai-women-have-to-take-the-lead-180f09be4e50',\n",
       "  'title': 'For the Successful Future of AI, Women Have to Take the\\xa0Lead',\n",
       "  'subtitle': 'Why women possess the right qualities to…',\n",
       "  'claps': 339,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 8,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 0.00016745015888006964,\n",
       "  'text': \"Towards Data Science Apr 1 2019 Member-only Listen Save Successful Adoption AI Need Female Leaders woman posse right quality lead AI project successfully enable environment collaboration inclusion build real-world AI product exciting breakthrough twenty-first century occur technology expanding concept mean human John Naisbitt Part Differences Men Women story woman men work differently dive woman lead AI team want share fascinating story heard Tania Biland 3rd-year student Lucerne University Applied Sciences Arts story narrated Tania Last semester class got split three different group order develop safety technology solution Swiss German brand Group 1 woman group Group 2 men Group 3 Four woman one man 4 week work team present work Group 1 composed woman developed safety solution woman dark jury wa male decided tell story using persona music video order make feel woman experiencing daily basis also put emphasis fact everyone ha mother sister wife life probably dont want her/them suffer end solution wa rather simple technologically using light provide safety connected audience emotionally Group 2 mostly composed men presented high-tech solution using AI GPS video conference based argument fact number pointed competitive advantage Group 3 4 woman 1 man outcome didnt seem finished man group could agree led woman therefore spend much time discussing group dynamic instead working group different output also approached problem differently group group 1 decided start defining others work preference style order distribute responsibility keeping hierarchy flat possible hand two group elected leader team turned leader perceived dictator lead heavy conflict team spent hour discussing arguing group wa working productive science tell u gender difference science landscape regard gender difference effect behavior still evolving ha come clear set scientific explanation different behavior yet compiling research two main factor influence behavior story told Tania woman developed solution Collaborative Leadership Style adhocracy culture adapting leading position based task almost flat hierarchy derived argumentation involving stakeholder case mother wife user showing empathy problem saw bigger picture also built simpler solution wa actually finished story wa able connect dot AI project never end moving prototype phase real-world application Part II Making AI success AI product adopted Based experience three main reason AI Machine Learning ML solution move prototyping phase real-world Interestingly none three point relate technical challenge overcome creating right team make AI successfully adopted order solve challenge build successful AI product need focus collaborative community-driven approach take account opinion different stakeholder especially under-represented step achieve Step 1 Involve different group esp woman middle talent pyramid technology company focus hiring people top talent pyramid primarily historical reason fewer woman example Computer Science class le 10 percent woman However many talented woman hidden middle pyramid educating online course lack opportunity encouragement give example wa talking president Geek Girls Carrot organization promoting woman tech organizing AI workshop 125 woman applied 25 seat naturally leave behind 100 talented woman Imagine involve 100 woman instead top would give lot woman opportunity work new technology like AI Step 2 Build communal collaborative bottom-up team different stakeholder Next need collaboration men woman well different stakeholder launch product successfully real market achieved forming inclusive project community build AI product based common value belief often bigger vision Proving point past six month brought together group 50 male female student build ML model Within short time member started collaborating helping build model Four subgroup got formed one wa driven two woman supported two men data tagger group men 4 month group two woman two male built accurate model beginning woman much willing collaborate men However interestingly saw men group also ended behaving collaboratively woman group wa fascinating Step 3 Create right Organizational Structure collaboration could create organizational structure practice dont need empowerment design everybody powerful one powerless seen achieved connecting intrinsic extrinsic motivation related money creating incentive structure competitive case built community mentor wa top pyramid followed community manager engineer working building model finally data tagger Members team striving move ladder reach next level created extrinsic motivation However monetary compensation people level wa fostered collaboration context role leader bos foster Collaborative Leadership organizational structure decrease need control people give opportunity learn grow together 2 Part III Connecting Part Part II woman lead AI team story beginning female group followed Collaborative Leadership Style showing customer empathy willingness collaborate Considering limited experiment solar project saw approach use community build product helped well foster collaboration build trust among community member none mentioned quality generalized following graphic aim summarize reason many woman great fit Collaborative Leadership conclusion arguing think holistically best create right environment look beyond gender race cultural background focus collaborate human build better future Finally would like thank people men woman helped article creating experience content also refining text want receive update AI Challenges get expert interview practical tip boost AI skill subscribe monthly newsletter also Facebook LinkedIn Twitter 336 336 336 Towards Data Science home data science Medium publication sharing concept idea code Manu Siddharth Jha Mar 31 2019 Member-only Artificial Intelligence next big thing Hollywood Movies captured imagination people ever since came limelight Right first motion picture late 1880s upcoming latest sci-fi blockbuster cinema ha become medium love joy passion movie lover Almost every country across globe 5 min read 5 min read Share idea million reader Guy Tsror Mar 31 2019 Member-only romantic side data science Analyzing relationship year worth text message wa supposed private project one thought would upload private Google Colab notebook share whomever relevant one person referred OJ lightbulb clean share 11 min read 11 min read Salma Ghoneim Mar 31 2019 Member-only Object detection via color-based image segmentation using python tutorial contouring using python OpenCV Getting started already jupyter notebook IDE run python OpenCV installed skip Execution Tools hero today Anaconda free open-source distribution help installing different package sort mess isolated environment Wikipedias telling u 7 min read 7 min read Sahil Dhankhad Mar 31 2019 Member-only Forget APIs Python Scraping Using Beautiful Soup Import Data File web Part 2 APIs always every website Beautiful Soup going stay forever collect kind data website Today Data play critical role every industry data coming internet company 8 min read 8 min read Andrew Donaldson Mar 31 2019 Member-only Dont Fear Robots Impressive tech important remember Boston Dynamics video trailer movie isnt yet made Every time one Boston Dynamics video hit met mix amazement prediction doom hand new robot overlord impressive also well choreographed staged Impressive tech important 4 min read 4 min read Rudradeb Mitra Back writing 2years Building Omdena 7 startup Mentor Google Startups Speaker Book Author Deeply spiritual Medium Luay Rahil ILLUMINATION Elon Musk Eliminated Remote Work Working Home `` n't Work '' Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Mark Vassilevskiy 5 Unique Passive Income IdeasHow Make 4,580/Month Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Apr 1, 2019 Member-only Listen Save For the Successful Adoption of AI, We Need More Female Leaders Why women possess the right qualities to lead AI projects more successfully and enable an environment for collaboration and inclusion to build real-world AI products. The most exciting breakthroughs of the twenty-first century will not occur because of technology, but because of an expanding concept of what it means to be human John Naisbitt Part I: Differences between Men and Women A story of how women and men can work differently Before we dive into why more women should lead AI teams, I want to share a fascinating story I heard from Tania Biland, a 3rd-year student of Lucerne University of Applied Sciences and Arts. The story as narrated by Tania: Last semester, our class got split into three different groups in order to develop a safety technology solution for Swiss or German brands: Group 1:  Only women (my group) Group 2:  Only men Group 3:  Four women and one man After 4 weeks of work, each team had to present their work. Group 1  , composed of only women, developed a safety solution for women in the dark. As the jury was only male we decided to tell a story using a persona, music, and videos in order to make them feel what women are experiencing on a daily basis. We also put emphasis on the fact that everyone has a mother, sister or wife in their life and that they probably dont want her/them to suffer. In the end, our solution was rather  simple  ,  technologically  : using light to provide safety but connected to the audience  emotionally. Group 2  , mostly composed of men, presented a more  high-tech solution using AI  ,  GPS  and  video conferences  . They based their arguments on  facts and numbers  and pointed out their  competitive advantages  . In Group 3,  with 4 women and 1 man, the outcome didnt seem finished. The only man in the group could not agree to be led by women and they, therefore, spend too much time discussing group dynamics instead of working. The groups not only had different outputs but also approached the problem differently. My group (group 1) decided to start by defining each others work preferences and styles in order to distribute some responsibilities and keeping a hierarchy as flat as possible. On the other hand, the two other groups elected a leader for the team. It turned out that these leaders were more perceived as dictators, which lead to heavy conflicts where the teams spent hours discussing and arguing while our group was just working and productive. What science tells us about gender differences The science landscape with regards to gender differences and effects on behavior is still evolving and has not come up with a clear set of scientific explanations for different behaviors yet. By compiling most of the research, there are two main factors that influence behaviors: In the above story, as told by Tania, women developed the solution in a Collaborative Leadership Style (  adhocracy culture  ), adapting the leading position based on the tasks with an almost flat hierarchy. They derived their argumentation by involving all stakeholders (in this case the mothers and wives = users), showing empathy for their problems. They saw the bigger picture and also built a simpler solution that was actually finished. Through the story, I was able to connect the dots on why most AI projects never end up moving out from the prototype phase to a real-world application. Part II: Making AI a success Why AI products are not adopted? Based on my experience, there are three main reasons why most AI and Machine Learning (ML) solutions do not move from the prototyping phase to the real-world: Interestingly, none of the three points relate to the technical challenges, and all of them can be overcome by creating the right team. How to make AI more successfully adopted? In order to solve the above challenges and build more successful AI products, we need to focus on a more collaborative and community-driven approach . This takes into account opinions from different stakeholders, especially those who are under-represented. Below are steps to achieve that: Step 1. Involve different groups esp. women from the middle of the talent pyramid In technology, most companies focus on hiring people at the top of the talent pyramid, where for primarily historical reasons, are fewer women. For example, most Computer Science classes have less than 10 percent of women. However, many talented women are hidden in the middle of the pyramid, educating themselves through online courses but lack opportunities and encouragement. To give an example, I was talking with the president of Geek Girls Carrot, which is an organization promoting women in tech. They are organizing an AI workshop where over 125 women applied but they had only 25 seats , so naturally, they have to leave behind more than 100 talented women . Imagine, if we can involve most of the other 100 women instead of only at the top. This would give a lot more women the opportunity to work in new technologies like AI. Step 2. Build a communal and collaborative bottom-up team with different stakeholders Next, we need more collaboration between men and women as well as different stakeholders to launch products successfully in the real market. This can be achieved through forming inclusive project communities that build AI products based on common values, beliefs, and often a bigger vision. Proving the point, in the past six months, we brought together a group of more than 50 male and female students to build an ML model. Within a short time, members started collaborating and helping each other to build the models. Four subgroups got formed, and one of them was driven by two women and supported by two men (data taggers). The other groups were all men. In 4 months, the group with the two women and two male built the most accurate model. From the beginning, the women were much more willing to collaborate than men. However, more interestingly, I saw that men in the group also ended up behaving more collaboratively because of the other women in the group. This was fascinating!! Step 3. Create the right Organizational Structure for collaboration What if we could create organizational structures and practices that dont need empowerment because, by design, everybody is powerful and no one powerless? I have seen that this can be achieved by connecting intrinsic and extrinsic motivations (which is not related to money) and creating an incentive structure which is not competitive. In my case, I built the community where the mentor was at the top of the pyramid, followed by the community manager, then engineers working on building models and finally data taggers. Members from each team were striving to move up the ladder to reach the next level, which created an extrinsic motivation. However, the monetary compensation for people on the same level was the same. This fostered collaboration. In this context, the role of a leader is not to be a boss but to foster Collaborative Leadership . Such an organizational structure will decrease the need to control people and will give opportunities to learn and grow together[2]. Part III: Connecting Part I and Part II. Why women should lead AI teams In the story from the beginning, the female group followed a more Collaborative Leadership Style by showing more customer empathy and willingness to collaborate. Considering the limited experiment in the solar project, we saw that the approach to use the community to build products helped as well to foster collaboration and build trust among community members. While none of the mentioned qualities can be generalized, the following graphic aims to summarize some of the reasons why many women are a great fit for Collaborative Leadership . In conclusion, I am arguing: We should think more holistically and do our best to create the right environment where we look beyond gender, race, and cultural background and focus on how we can collaborate as humans to build a better future. Finally, I would like to thank all the people (both men and women) who helped me with this article creating the experiences and content but also refining the text. If you want to receive updates on our AI Challenges, get expert interviews, and practical tips to boost your AI skills, subscribe to our monthly newsletter. We are also on Facebook , LinkedIn , and Twitter . 336 336 336 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Manu Siddharth Jha Mar 31, 2019 Member-only Is Artificial Intelligence the next big thing in Hollywood? Movies have captured the imagination of people ever since they came into the limelight. Right from the first motion picture in the late 1880s to the upcoming latest sci-fi blockbuster, cinema has become a medium of love, joy, and passion for all movie lovers. Almost every country across the globe 5 min read 5 min read Share your ideas with millions of readers. Guy Tsror Mar 31, 2019 Member-only The romantic side of data science: Analyzing a relationship through a year worth of text messages This was supposed to be a private project, one I thought I would upload to a private Google Colab notebook, and share with just whomever it is relevant for (which is, one other person, referred to here as OJ). But then, a lightbulb why not clean it up and share 11 min read 11 min read Salma Ghoneim Mar 31, 2019 Member-only Object detection via color-based image segmentation using python A tutorial on contouring using python & OpenCV. Getting started If you already have jupyter notebook or an IDE with which you can run python & OpenCV installed, just skip to Execution. Tools Our hero today is Anaconda. a free open-source distribution that helps with installing different packages & sorts out their messes into isolated environments. What Wikipedias telling us about 7 min read 7 min read Sahil Dhankhad Mar 31, 2019 Member-only Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2 APIs are not always there for you for every website, but Beautiful Soup is going to stay with you forever to collect any kind of data from any website. Today, Data play a critical role in every industry. And most of this data is coming from the internet. Most company 8 min read 8 min read Andrew Donaldson Mar 31, 2019 Member-only Dont Fear the Robots Impressive as the tech here is, it is important to remember that the Boston Dynamics videos are trailers for a movie that isnt yet made. Every time one of these Boston Dynamics videos hit, they are met with a mix of amazement and predictions of doom at the hands of our new robot overlords. It is impressive. It is also very well choreographed and staged. Impressive as the tech here is, it is important 4 min read 4 min read Rudradeb Mitra Back to writing after 2years, Building Omdena, 7 startups, Mentor@Google for Startups, Speaker, Book Author, Deeply spiritual. More from Medium Luay Rahil in ILLUMINATION Elon Musk Eliminated Remote Work Because Working From Home \"Doesn\\'t Work\" Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Mark Vassilevskiy 5 Unique Passive Income IdeasHow I Make $4,580/Month Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4102,\n",
       "  'url': 'https://towardsdatascience.com/user-guide-to-my-first-data-product-medium-post-metric-displayer-e99e74e52b3a',\n",
       "  'title': 'User guide to My First Data Product: Medium Post Metric Displayer',\n",
       "  'subtitle': 'Origin',\n",
       "  'claps': 19,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 14,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-01-26',\n",
       "  'clap_prop': 9.385112149620422e-06,\n",
       "  'text': 'Towards Data Science Jan 25 2019 Listen Save User guide First Data Product Medium Post Metric Displayer Know Medium Post Better Data Origin regular writer Medium well data geek busy year 2018 Id like reflect achieved Medium blog Furthermore based performance 2018 plan make aggressive writing plan year 2019 think Medium writer know anti-data-centered interface design analytics platform Medium simple design quite discouraged really looking data make decision Thats main reason decided develop first data product following article show product code use creating product reader use data Medium analytics platform well showcase product run whole thinking execution process also detailed documented Anyone keen knowing Medium post performance feel free use code give sense data Problem Statement Pain Point Problem statement annoying know article performs well robust visualized manner Also article cant probably grouped classified Among metric metric may distinguishable one different group article Pain point Medium user might find difficult make analytics platform Stat Tab provided Medium data cant downloaded hardly tell much information next Constraint constraint product obvious still capable scraping output performance data every article Medium automated way Therefore user need document data manually excel column name mine Users need regularly put metric seasonal basis quarterly semi-annually product deal excel file thoroughly specific data file ready go Skills Tools Tool Python Jupyter Notebook Skills Data processing munging Pandas visualization Matplotlib clustering plus PCA Sklearn numerous function creation repetitive operation Product Outlook product simple Ive created layout interface instead introduce operational logic behind product Medium Post Metric Displayer consists numerous small handy function user pass excel file use documenting data function result automatically generated contains two feature product first one Dashboard Visualization Product second one Clustering Product first phase product roadmap sure keep expanding feature However think beta version Displayer well-rounded enough provide insight writer make decision Thats reason decided soft release product point Feel free fetch Github code use product lot fun yunhanfeng/medium_metric_data_product first data product providing data solution Medium writer track post performance github.com Part 0 Data import pre-check introducing product let take look data set first sure needed package python imported Pandas Numpy Matplotlib familiar friend used Translator Goolgetrans time reason behind sometimes write article Chinese reader Taiwan might also write post Chinese Translating Chinese title English fit Matplotlib constraint Matplotlib cant show Chinese later read excel file display first row checking column type well shape data extracted documented Medium basic seven feature Medium website wrote 45 article far Part 1 Dashboard Visualization Product Within product designed eight plus one feature function enable user return similar chart graph different metric simple excel file input Based chart graph writer able get sense good bad article perform make next-step strategy user engagement funnel Medium post basically follow funnel design product Function 0 function mainly deal translated issue Chinese title help translated title English thanks powerful Google Translate Function 1 function return static value total view basic number Medium post showing generic description performance passed file function return total view around 15K Function 2 Function 2 return top 5 article view result displayed table format well bar chart format may also get glimpse top 5 contributes total view Operate function result top 5 article view shown seems article related MSBA application draw view Function 3 Similar function 2 function return table bar chart top 5 article additional view Additional view quite strange metric definition Medium view RSS reader FB instant article AMP dont factor read ratio every article writer posted additional view experience article curated publication website Medium may additional view increase give insight good outbound article perform Operate function Function 4 Read even deeper metric regarding engagement Read metric user scroll way bottom indicates user read post function take argument file name return table bar graph Operate function result slightly different top 5 article view top 5 read Chinese post multiple possible reason behind One reader Taiwan Chinese relatively familiar language majority user tend finish reading article Second might indicate English writing long convoluted user read may need polish post concise Three apart language issue top 5 post read related information-related topic prepare application ask question review MSBA course Maybe kind topic trigger interest reading post Function 5 Read ratio another tricky metric Medium writer Read ratio read view click said Medium algorism reward article higher read ratio higher rank priority show front user function work returning top 5 article highest read ratio form displayed table bar graph Operate function interpretation result tricky certain way One shorter article possible achieve higher read ratio top one article among post link course get highest read ratio Second article get fewer view may also lead higher read ratio like article named Creation conclusion endogeneity affecting metric suggest user cross-validating metric function Function 6 function return average read ratio article Since read ratio key metric toward Medium algorism suggested writer use function track progress article performance occasionally Operate function also set based characteristic well category content light easy-to-digest article might higher average read ratio vice versa article average read ratio 0.528 bad try set goal increasing read ratio Medium website Writing structured readable laconic post good approach Function 7 next function showing top 5 article top fan Fans metric metric presenting deeper engagement toward article Readers clapped post showed like last step metric funnel achieved harder Operate function Function 8 last function first part product show average view day elapsed create new metric capture daily additional view post formula Total view Days today day article published calculated daily flow-in view toward specific article Operate function think metric important show article still bring traffic everyday basis article drive user writer Medium website help user identify cash cow Medium blog knowing article regularly promoting social medium making referred affliates good strategy keep fresh traffic streaming Medium Part 2 Clustering Product like mentioned function 8 identifying article cash cow important Thats intention proposed created clustering product unsupervised learning provided feature article segmented different group group ha different trait play different role regarding user acquisition retention engagement using clustering product user need pas excel file put number cluster want create result automatically generated several function related visualizing cluster introduce four plus one function following article Function 9 function helper function munging data create total_veiw view_per_day orginal data frame Function 10 11 two function used clustering used K-means clustering article took read read ratio fan total view view_per_day segmentation based five feature function started user need pas excel file cluster number function Function 10 return data frame cluster label Users may see helper function product function data frame return easily seen played around Function 11 return summary cluster grant user clear overview cluster performance quality think super helpful user finally design suitable promotion strategy towards different cluster Whenever want re-share revitalize traffic using product function really justify choice also suggest using many cluster argument enter different number see number grant interpretable result Operate function 10 Operate function 11 Function 12 function following previous function return ideal cluster overview may want visualize Therefore utilize principal component analysis PCA turn five-feature dimension two dimension article different group labeled 2D plane pas file cluster number get result also return explained ratio principal component 1 2 try n 3 n 5 Operate code Function 13 last function also related clustering function creates so-called Parallel Coordinates Graph showcase different cluster extent feature contributes Simply put reveals logic behind clustering writer able make decision based result try cluster 3 5 example Operate function n 3 may conclude cluster 1 top two performing article excel total view read fan view per day relatively poor read ratio distinction cluster 0 2 mainly based read ratio mean metric play great portion influence article may focus writing strategy boosting read ratio year strategy made using data product Conclusion many constrain Medium analytics platform Stat tab However creating data product hope benefit Medium community equip writer data-driven mindset better writing strategy based result generated product Happy analyzing 27 1 27 27 1 Towards Data Science home data science Medium publication sharing concept idea code Md Kamaruzzaman Jan 25 2019 Member-only Back metal Top 3 Programming language develop Big Data framework C++ Rust Go Java Data Intensive framework previous blog post Programming language rule Data Intensive Big Data Fast Data framework briefly discussed popular Big Data framework showed Java de-facto programming language Data Intensive framework Java significant advantage e.g 12 min read 12 min read Share idea million reader Brayden Gerrard Jan 25 2019 Member-only Evolution US Electric Grid past 15 year US electric grid ha undergone massive shift mostly better Renewable energy natural gas coal GHG emission see US grid ha evolved recent year 4 min read 4 min read zgr Gen Jan 25 2019 Member-only Notes Artificial Intelligence Machine Learning Deep Learning curious people AI ha intriguing topic 2018 according McKinsey mentioned key enabler 1 3 spot Gartner Top 10 Strategic Technology Trends 2019 AI became catch-all term refers computer program automatically doe something 22 min read 22 min read Felix Kuestahler Jan 25 2019 Member-only Python Tutorial Twitter Account Retrieval Swiss Government Members Start journey social medium analysis politician Click link Title list tutorial article first article tutorial show extract list tweeting Swiss Government Members via Twitter API 7 min read 7 min read Tobias Hill Jan 25 2019 got 1 better accuracy data augmentation hard let go thing put love yes really thought wa done 5 th article series accuracy/error metric MNIST example started haunting wa quite sure could improve 4 min read 4 min read Henry Feng Sr. BI Engineer Product Analytic DS UMN MSBA Tech Education Medium List http //pse.is/SGEXZ Podcast Medium Samuele Mazzanti Towards Data Science Using Causal ML Instead A/B Testing Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Antonio Blago ILLUMINATION Become Data Analyst 2023 Justin Brooke Made 3,483,510.77 Membership Site Weird Niche Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Jan 25, 2019 Listen Save User guide to My First Data Product: Medium Post Metric Displayer Know Your Medium Post Better with Data Origin As a regular writer on Medium as well as a data geek, after the busy year of 2018, Id like to reflect what I have achieved on my Medium blog. Furthermore, based on the performance in 2018, I plan to make more aggressive writing plan in the year 2019. I think most Medium writers know the anti-data-centered interface design of analytics platform of Medium. It is too simple for me. And the design quite discouraged me from really looking into data and make the decision. Thats the main reason I decided to develop my very first data product. In the following article, I will show the product and the codes I use for creating the product to readers. I will use my own data at Medium analytics platform as well to showcase how my product run. The whole thinking and execution process will also be detailed documented. Anyone who is keen about knowing more about his or her own Medium posts performance, feel free to use my code to give some sense of data. Problem Statement and Pain Point Problem statement : It is annoying to know which article performs well in a more robust and visualized manner. Also, the articles cant be probably grouped or classified. Among all the metrics, which metrics may be the most distinguishable ones for different groups of articles? Pain point : Medium users might find it difficult to make the most of the analytics platform (Stat Tab) provided by Medium. The data cant be downloaded and it hardly tells much information about what to do next. Constraint The constraint of this product is very obvious. For me, I am still not capable of scraping or output the performance data of every article from Medium in a more automated way. Therefore, users need to document those data manually just as I did in this excel. The column name should be the same as mine. Users just need to regularly put down those metrics on a seasonal basis (quarterly or semi-annually) My product can deals with those excel file thoroughly. Once you have that specific data file. We are ready to go. Skills and Tools Tool: Python Jupyter Notebook Skills: Data processing and munging with Pandas, visualization with Matplotlib, clustering plus PCA with Sklearn, numerous function creation for repetitive operation Product Outlook This product is very simple. Ive not created the layout or interface of it, but instead, I will introduce the operational logic behind this product. Medium Post Metric Displayer consists of numerous small and handy functions. Once the user passes the excel file (we use for documenting data) to those functions, the result will automatically be generated. It contains two features for this product. The first one is the Dashboard & Visualization Product. The second one is Clustering Product. It is the first phase of my product roadmap. (I am not sure if I will keep expanding its features). However, I think this beta version of Displayer is well-rounded enough to provide some insights to the writers to make decisions. Thats the reason I decided to soft release my product at this point. Feel free to fetch my Github code and use this product with lots of fun! yunhanfeng/medium_metric_data_product My first data product, providing data solution for Medium writer to track post performance github.com Part 0: Data import for pre-check Before introducing the product itself, lets take a look at our data set first. It is for sure that the needed package in python is imported. Pandas, Numpy, and Matplotlib is my familiar friend. I used Translator from Goolgetrans this time. The reason behind is that I sometimes write articles in Chinese and most of my readers are from Taiwan. They might also write posts in Chinese. Translating the Chinese title into English will be more fit into Matplotlib constraint, where Matplotlib cant show Chinese. I later read into the excel file and display the first few rows of it, further checking the column type as well as the shape. From the data extracted and documented from Medium, there are basic seven features. And from my Medium website, I wrote 45 articles so far. Part 1: Dashboard and Visualization Product Within this product, I designed eight plus one features functions which enable users to return similar charts and graphs on different metrics with a simple excel file input. Based on those charts and graphs, writers are able to get some sense of how good or bad their articles perform and further make the next-step strategy. Below is the user engagement funnel on Medium post. And basically, I just follow the funnel to design my product. Function 0 This function mainly deals with the translated issue of Chinese title. It helps translated those titles into English, thanks to powerful Google Translate. Function 1 This function returns the static value of total views. It is the basic number of all Medium posts, showing the generic description of your performance. I passed my file into this function, and it returns my total views is around 15K. Function 2 Function 2 returns the top 5 articles with the most views. The result will be displayed in both table format as well as bar chart format. You may also get a glimpse of how these top 5 contributes to your total views. Operate the function The result of my top 5 articles with most views is shown below. It seems my articles related to MSBA application draw most of views. Function 3 Similar to function 2, this function returns the table and bar chart of the top 5 articles with additional view. Additional view, for me, is quite a strange metrics. The definition from Medium is the views from RSS readers, FB instant articles and AMP, and they dont factor into read ratio. Not every article a writer posted will have an additional view. In my experience, only when your article is curated by other publication websites on Medium may the additional views increase. It gives some insights that how good those outbound articles perform. Operate the function. Function 4 Read is an even deeper metrics regarding engagement. Read is the metric if the user scrolls all the way to bottom, which indicates a user reads through the posts. The function takes the argument of the file name and returns table and bar graph. Operate the function. The result is slightly different from the top 5 articles with most views. The top 5 with most reads are all Chinese posts. There are multiple possible reasons behind. One, most of my readers are from Taiwan, Chinese is a relatively familiar language to the majority of my users. They tend to finish reading the articles more. Second, it might indicate that my English writing is too long or convoluted for users to read through. I may need to polish my post to be more concise. Three, apart from language issue, the top 5 posts with most reads are related to information-related topics, such as how to prepare for application, how to ask questions and the review of MSBA course. Maybe this kind of topics can trigger more interests in reading all through the posts. Function 5 Read ratio is another tricky metric for Medium writer. Read ratio = reads / views (clicks). It is said that Medium algorism will reward articles of higher read ratio with higher rank and priority to show in front of users. The function works the same as those above, returning top 5 articles with the highest read ratio, in the form of displayed table and bar graph. Operate the function. The interpretation of results is tricky in certain ways. One, the shorter the article, the more possible it achieve higher read ratio. The top one article among all my post is just a link. Of course, it will get the highest read ratio. Second, the article which gets fewer views may also lead to higher read ratio, just like my article named [Creation]. The conclusion is that there is some endogeneity affecting this metric. I suggest users cross-validating this metric with functions above. Function 6 The function returns the average read ratio of all the articles. Since read ratio is a key metrics toward Medium algorism. I suggested writers use this function to track the progress of article performance occasionally. Operate the function. It should also be set based on the characteristics as well as the categories of your content. Some light and easy-to-digest articles might have a higher average read ratio, vice versa. For my article, the average read ratio is 0.528, which for me, is not bad. I will try to set this as my goal of increasing read ratio for my Medium website. Writing structured, readable and laconic post is a good approach to that. Function 7 The next function is about showing the top 5 articles with top fans. Fans metric is the metric presenting deeper engagement toward articles. Readers clapped the post and showed their likes. It is the last step of metric funnel, which can be achieved harder. Operate the function. Function 8 The last function in the first part of the product is to show average view through the days elapsed. I create this new metric to capture the daily additional views for posts. The formula is: Total views / Days between today and the day that article published. For me, it calculated the daily flow-in of view toward that specific article. Operate the function I think this metric is very important. It shows that the articles still bring in traffic on an everyday basis. Those articles drive the user to the writers Medium website. It helps users to identify the cash cow of your Medium blog. After knowing these articles, regularly promoting them on social media or making them referred by affliates is a good strategy to keep fresh traffic streaming into your Medium. Part 2: Clustering Product Just like what I mentioned in function 8, identifying which articles are cash cows is important. Thats the very intention I proposed and created this clustering product. With unsupervised learning on provided features, the articles can be segmented into different groups. Each group has different traits. They can play different roles regarding user acquisition, retention, and engagement. By using this clustering product, users just need to pass the excel file and further put in number of clusters he wants to create, and the result is automatically generated. And there are several functions is related to visualizing the cluster too. I will introduce four plus one functions in the following article. Function 9 This is function is just a helper function for munging the data. I create total_veiw and view_per_day out of the orginal data frame. Function 10 & 11 These two functions are used for clustering. I used K-means for clustering articles. I took in reads, read ratio, fans, total views, view_per_day. And the segmentation is based on these five features. From the functions started here, users need to pass both excel file and cluster number to the function. Function 10 returns the data frame with cluster label. Users may see it as a helper function or a product function. The data frame it returns can be easily seen and played around. Function 11 returns the summary of each cluster. It grants users a clear overview of each cluster performance and quality. I think it is super helpful, for users can finally design suitable promotion strategy towards different cluster. Whenever you want to re-share or revitalize the traffic, using this product function can really justify your choice. And I also suggest not using too many clusters in the argument, you can just enter different numbers and see which number grant the most interpretable result. Operate the function 10 Operate function 11 Function 12 This function is following the previous function. Once you return the ideal cluster overview. You may want to visualize it. Therefore, I utilize principal component analysis (PCA) to turn five-feature dimension into two dimensions. And all the articles in different groups can be labeled on this 2D plane. Just pass the file and cluster number, you will get the result. It will also return the explained ratio of principal component 1 and 2. I will try n = 3 and n = 5 here. Operate the code Function 13 The last function is also related to clustering. This function creates the so-called Parallel Coordinates Graph. It showcases for different clusters, the extent each feature contributes. Simply put, it reveals the logic behind clustering. And writers are able to make the further decision based on the result. I will try cluster = 3 and 5 for example. Operate the function If n = 3, I may conclude that cluster 1, the top two performing articles excel at total views, reads, fans and view per day, but they have relatively poor read ratio. And the distinction between cluster 0 and 2 is mainly based on read ratio, which means this metric play a great portion of influence in most of my article. I may focus my writing strategy on boosting read ratio this year. Some strategies made by using my data product Conclusion There are many constrain in Medium analytics platform (Stat tab). However, by creating this data product, I hope I can benefit the Medium community to equip writers with some data-driven mindset and better the writing strategy based on the result generated from this product. Happy analyzing!!! 27 1 27 27 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Md Kamaruzzaman Jan 25, 2019 Member-only Back to the metal: Top 3 Programming language to develop Big Data frameworks C++, Rust, Go over Java for Data Intensive frameworks In a previous blog post: Programming language that rules the Data Intensive (Big Data, Fast Data) frameworks, I have briefly discussed some of the most popular Big Data frameworks and showed that Java is the de-facto programming language in Data Intensive frameworks. Java had significant advantages (e.g. 12 min read 12 min read Share your ideas with millions of readers. Brayden Gerrard Jan 25, 2019 Member-only The Evolution Of The US Electric Grid Over the past 15 years, the US electric grid has undergone some massive shifts (mostly for the better). Renewable energy and natural gas are up, while coal and GHG emissions are down. Here we can see how the US grid has evolved in recent years: 4 min read 4 min read zgr Gen Jan 25, 2019 Member-only Notes on Artificial Intelligence, Machine Learning and Deep Learning for curious people AI has been the most intriguing topic of 2018 according to McKinsey. It is mentioned as the key enabler now at the #1 and #3 spot of Gartner Top 10 Strategic Technology Trends for 2019. AI became a catch-all term that refers to any computer program that automatically does something 22 min read 22 min read Felix Kuestahler Jan 25, 2019 Member-only Python Tutorial: Twitter Account Retrieval of Swiss Government Members Start your journey into social media analysis of politicians Click on the link above the Title for a list of all tutorial articles. This is the first article. The tutorial will show you how to extract a list of tweeting Swiss Government Members via the Twitter API. 7 min read 7 min read Tobias Hill Jan 25, 2019 How I got 1% better accuracy by data augmentation It is hard to let go of things you put love into. And yes, I really thought I was done after the 5:th article in this series but then the accuracy/error metrics from the MNIST example started haunting me and I was quite sure that I could improve on 4 min read 4 min read Henry Feng Sr. BI Engineer | Product Analytic & DS | UMN MSBA | #Tech #Education | Medium List: https://pse.is/SGEXZ | Podcast: More from Medium Samuele Mazzanti in Towards Data Science Using Causal ML Instead of A/B Testing Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Antonio Blago in ILLUMINATION Why You Should (not) Become a Data Analyst in 2023! Justin Brooke How I Made $3,483,510.77 With a Membership Site in a Weird Niche Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 3193,\n",
       "  'url': 'https://towardsdatascience.com/cryptocurrency-price-prediction-using-lstms-tensorflow-for-hackers-part-iii-264fcdbccd3f',\n",
       "  'title': 'Cryptocurrency price prediction using LSTMs | TensorFlow for Hackers (Part\\xa0III)',\n",
       "  'subtitle': 'Predict Bitcoin\\xa0price…',\n",
       "  'claps': 242,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-25',\n",
       "  'clap_prop': 0.00011953669158990221,\n",
       "  'text': 'Towards Data Science Apr 25 2019 Listen Save Cryptocurrency price prediction using LSTMs TensorFlow Hackers Part III Predict Bitcoin price using LSTM Deep Neural Network TensorFlow 2 TL DR Build train Bidirectional LSTM Deep Neural Network Time Series prediction TensorFlow 2 Use model predict future Bitcoin price Complete source code Google Colaboratory Notebook time youll build basic Deep Neural Network model predict Bitcoin price based historical data use model however want carry risk action might asking something along line still get rich cryptocurrency course answer fairly nuanced well look might build model help along crazy journey might money problem one possible solution plan Data Overview dataset come Yahoo Finance cover available time writing data Bitcoin-USD price Lets load Pandas dataframe Note sort data Date case sample data interested total 3201 data point representing Bitcoin-USD price 3201 day ~9 year interested predicting closing price future date course Bitcoin made people really rich went really poor question remains though happen Lets look one possible model think Shall dataset somewhat different previous example data sorted time recorded equal interval 1 day sequence data called Time Series Time Series Temporal datasets quite common practice energy consumption expenditure calorie calorie weather change stock market analytics gathered user product/app even possibly love heart produce Time Series might interested plethora property regarding Time Series stationarity seasonality autocorrelation well known Autocorrelation correlation data point separated interval known lag Seasonality refers presence cyclical pattern interval doesnt every spring time series said stationarity ha constant mean variance Also covariance independent time One obvious question might ask watching Time Series data value current time step affect next one a.k.a Time Series forecasting many approach use purpose well build Deep Neural Network doe forecasting u use predict future Bitcoin price Modeling model weve built far allow operating sequence data Fortunately use special class Neural Network model known Recurrent Neural Networks RNNs purpose RNNs allow using output model new input model process repeated indefinitely One serious limitation RNNs inability capturing long-term dependency sequence e.g dependency today price 2 week ago One way handle situation using Long short-term memory LSTM variant RNN default LSTM behavior remembering information prolonged period time Lets see use LSTM Keras Data preprocessing First going squish price data range 0 1 Recall help optimization algorithm converge faster going use MinMaxScaler scikit learn scaler expects data shaped x add dummy dimension using reshape applying Lets also remove NaNs since model wont able handle well use isnan mask filter NaN value reshape data removing NaNs Making sequence LSTMs expect data 3 dimension need split data sequence preset length shape want obtain also want save data testing Lets build sequence process building sequence work creating sequence specified length position 0 shift one position right e.g 1 create another sequence process repeated possible position used save 5 data testing datasets look like model use 2945 sequence representing 99 day Bitcoin price change training going predict price 156 day future model POV Building LSTM model creating 3 layer LSTM Recurrent Neural Network use Dropout rate 20 combat overfitting training might wondering deal Bidirectional CuDNNLSTM Bidirectional RNNs allows train sequence data forward backward reversed direction practice approach work well LSTMs CuDNNLSTM Fast LSTM implementation backed cuDNN Personally think good example leaky abstraction crazy fast output layer ha single neuron predicted Bitcoin price use Linear activation function activation proportional input Training Well use Mean Squared Error loss function Adam optimizer Note want shuffle training data since using Time Series lightning-fast training thanks Google free T4 GPUs following training loss Predicting Bitcoin price Lets make model predict Bitcoin price use scaler invert transformation price longer scaled 0 1 range rather succinct model seems well test data Care try currency Conclusion Congratulations built Bidirectional LSTM Recurrent Neural Network TensorFlow 2 model preprocessing pipeline pretty generic used datasets Complete source code Google Colaboratory Notebook One interesting direction future investigation might analyzing correlation different cryptocurrencies would affect performance model Originally published http //www.curiousily.com 356 3 356 356 3 Towards Data Science home data science Medium publication sharing concept idea code Ahmad Tanehkar Apr 25 2019 Lets Apply Machine Learning Behavioral Economics Currently Machine Learning ML well applied behavioral economics social science lack use come unfamiliarity different approach 5 min read 5 min read Share idea million reader Ashok Chilakapati Apr 25 2019 Concept Drift Model Decay Machine Learning Concept drift drift label time essentially data lead divergence decision boundary new data model built earlier data/labels Scoring randomly sampled new data detect drift allowing u trigger expensive 7 min read 7 min read Ren Bremer Apr 25 2019 use Python feature data analytics project Python tutorial Azure using OO NumPy panda SQL PySpark 1 Introduction lot company moving cloud consider tooling shall used data analytics On-premises company mostly use propriety software advanced analytics BI reporting However tooling may logical choice cloud environment 9 min read 9 min read Ryan Burge Apr 25 2019 Member-only Religious Composition Two Major Parties make lot data visualization time pretty good idea result calculation going look like write code fact result obvious dont even post anywhere 4 min read 4 min read Ashutosh Singh Apr 25 2019 Member-only Classifying Products Banned Approved using Text Mining- Part II part explain optimize existing Machine Learning model Part deployment ML model using Flask previous article series discussed business problem shown train model using fastText classification Banned Approved product based information like Product Name product description specification 5 min read 5 min read Venelin Valkov Adventures Artificial Intelligence http //curiousily.com Medium Andrew datascience Towards Data Science Time series prediction LSTM Tensorflow Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Dmytro Sazonov Trading Data Analysis Trading algorithm work Ren Heinrich DataDrivenInvestor analyzed 200 DeFi Projects Found Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 25, 2019 Listen Save Cryptocurrency price prediction using LSTMs | TensorFlow for Hackers (Part III) Predict Bitcoin price using LSTM Deep Neural Network in TensorFlow 2 TL;DR Build and train an Bidirectional LSTM Deep Neural Network for Time Series prediction in TensorFlow 2. Use the model to predict the future Bitcoin price. Complete source code in Google Colaboratory Notebook This time youll build a basic Deep Neural Network model to predict Bitcoin price based on historical data. You can use the model however you want, but you carry the risk for your actions . You might be asking yourself something along the lines: Can I still get rich with cryptocurrency? Of course, the answer is fairly nuanced. Here, well have a look at how you might build a model to help you along the crazy journey. Or you might be having money problems? Here is one possible solution: Here is the plan: Data Overview Our dataset comes from Yahoo! Finance and covers all available (at the time of this writing) data on Bitcoin-USD price. Lets load it into a Pandas dataframe: Note that we sort the data by Date just in case. Here is a sample of the data were interested in: We have a total of 3201 data points representing Bitcoin-USD price for 3201 days (~9 years). Were interested in predicting the closing price for future dates. Of course, Bitcoin made some people really rich and for some went really poor. The question remains though, will it happen again? Lets have a look at what one possible model thinks about that. Shall we? Our dataset is somewhat different from our previous examples. The data is sorted by time and recorded at equal intervals (1 day). Such a sequence of data is called  Time Series  . Time Series Temporal datasets are quite common in practice. Your energy consumption and expenditure (calories in, calories out), weather changes, stock market, analytics gathered from the users for your product/app and even your (possibly in love) heart produce Time Series . You might be interested in a plethora of properties regarding your Time Series stationarity , seasonality and autocorrelation are some of the most well known. Autocorrelation is the correlation of data points separated by some interval (known as lag). Seasonality refers to the presence of some cyclical pattern at some interval (no, it doesnt have to be every spring). A time series is said to be stationarity if it has constant mean and variance. Also, the covariance is independent of the time. One obvious question you might ask yourself while watching at Time Series data is: Does the value of the current time step affects the next one? a.k.a. Time Series forecasting . There are many approaches that you can use for this purpose. But well build a Deep Neural Network that does some forecasting for us and use it to predict future Bitcoin price. Modeling All models weve built so far do not allow for operating on sequence data. Fortunately, we can use a special class of Neural Network models known as Recurrent Neural Networks (RNNs) just for this purpose. RNNs allow using the output from the model as a new input for the same model. The process can be repeated indefinitely. One serious limitation of RNNs is the inability of capturing long-term dependencies in a sequence (e.g. Is there a dependency between today`s price and that 2 weeks ago?). One way to handle the situation is by using an Long short-term memory (LSTM) variant of RNN . The default LSTM behavior is remembering information for prolonged periods of time. Lets see how you can use LSTM in Keras. Data preprocessing First, were going to squish our price data in the range [0, 1]. Recall that this will help our optimization algorithm converge faster: Were going to use the MinMaxScaler from scikit learn : The scaler expects the data to be shaped as (x, y), so we add a dummy dimension using reshape before applying it. Lets also remove NaNs since our model wont be able to handle them well: We use isnan as a mask to filter out NaN values. Again we reshape the data after removing the NaNs. Making sequences LSTMs expect the data to be in 3 dimensions. We need to split the data into sequences of some preset length. The shape we want to obtain is: We also want to save some data for testing. Lets build some sequences: The process of building sequences works by creating a sequence of a specified length at position 0. Then we shift one position to the right (e.g. 1) and create another sequence. The process is repeated until all possible positions are used. We save 5% of the data for testing. The datasets look like this: Our model will use 2945 sequences representing 99 days of Bitcoin price changes each for training. Were going to predict the price for 156 days in the future (from our model POV). Building LSTM model Were creating a 3 layer LSTM Recurrent Neural Network. We use Dropout with a rate of 20% to combat overfitting during training: You might be wondering about what the deal with Bidirectional and CuDNNLSTM is? Bidirectional RNNs allows you to train on the sequence data in forward and backward (reversed) direction. In practice, this approach works well with LSTMs. CuDNNLSTM is a Fast LSTM implementation backed by cuDNN. Personally, I think it is a good example of leaky abstraction, but it is crazy fast! Our output layer has a single neuron (predicted Bitcoin price). We use Linear activation function which activation is proportional to the input. Training Well use Mean Squared Error as a loss function and Adam optimizer. Note that we do not want to shuffle the training data since were using Time Series. After a lightning-fast training (thanks Google for the free T4 GPUs), we have the following training loss: Predicting Bitcoin price Lets make our model predict Bitcoin prices! We can use our scaler to invert the transformation we did so the prices are no longer scaled in the [0, 1] range. Our rather succinct model seems to do well on the test data. Care to try it on other currencies? Conclusion Congratulations, you just built a Bidirectional LSTM Recurrent Neural Network in TensorFlow 2. Our model (and preprocessing pipeline) is pretty generic and can be used for other datasets. Complete source code in Google Colaboratory Notebook One interesting direction of future investigation might be analyzing the correlation between different cryptocurrencies and how would that affect the performance of our model. Originally published at  https://www.curiousily.com  . 356 3 356 356 3 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ahmad Tanehkar Apr 25, 2019 Lets Apply Machine Learning in Behavioral Economics Currently, Machine Learning (ML) is not well applied in behavioral economics or social science, and this lack of use comes from unfamiliarity with this different approach. 5 min read 5 min read Share your ideas with millions of readers. Ashok Chilakapati Apr 25, 2019 Concept Drift and Model Decay in Machine Learning Concept drift is a drift of labels with time for the essentially the same data. It leads to the divergence of decision boundary for new data from that of a model built from earlier data/labels. Scoring randomly sampled new data can detect the drift allowing us to trigger the expensive 7 min read 7 min read Ren Bremer Apr 25, 2019 How to use Python features in your data analytics project Python tutorial in Azure using OO, NumPy, pandas, SQL, PySpark 1. Introduction A lot of companies are moving to cloud and consider what tooling shall be used for data analytics. On-premises, companies mostly use propriety software for advanced analytics, BI and reporting. However, this tooling may not be the most logical choice in a cloud environment. 9 min read 9 min read Ryan Burge Apr 25, 2019 Member-only The Religious Composition of the Two Major Parties I make a lot of data visualizations. Most of the time I have a pretty good idea of what the results of the calculations are going to look like before I write the code. In fact, some of the results are so obvious that I dont even post them anywhere 4 min read 4 min read Ashutosh Singh Apr 25, 2019 Member-only Classifying Products as Banned Or Approved using Text Mining- Part II In this part, we will explain how to optimize the existing Machine Learning model in Part I and the deployment of this ML model using Flask. In the previous article of this series, We have discussed the business problem, shown how to train the model using fastText and classification of Banned or Approved products based on information like(Product Name, product description, and specifications). 5 min read 5 min read Venelin Valkov Adventures in Artificial Intelligence https://curiousily.com More from Medium Andrew D #datascience in Towards Data Science Time series prediction with LSTM in Tensorflow Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Dmytro Sazonov in Trading Data Analysis Trading algorithm that !works Ren & Heinrich in DataDrivenInvestor I analyzed 200 DeFi Projects. Here Is What I Found Out. Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5461,\n",
       "  'url': 'https://towardsdatascience.com/the-romantic-side-of-data-science-analyzing-a-relationship-through-a-year-worth-of-text-messages-be7e32d81fa9',\n",
       "  'title': 'The romantic side of data science: Analyzing a relationship through a year worth of text\\xa0messages',\n",
       "  'subtitle': '-',\n",
       "  'claps': 80,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 11,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 3.95162616826123e-05,\n",
       "  'text': 'Towards Data Science Mar 31 2019 Member-only Listen Save romantic side data science Analyzing relationship year worth text message wa supposed private project one thought would upload private Google Colab notebook share whomever relevant one person referred OJ lightbulb clean share others well Basically celebrate first anniversary dating decided explore relationship communication point view look messaging behavior saying saying respond would include call log actually never tracked shame let go especially since mostly two phone call week making bit le interesting go thought process interested towards end share link tool came handy process working improving Python skill go could also find Python-related snippet handy please note story much better read desktop since infographics interactive Methodology key communication channel use WhatsApp SMS-text occasionally negligible occasional Messenger link YouTube message rare chose stick WhatsApp simplify process Story met April 2018 right-swipe Tinder spontaneous setup ever day swipe date wa first rainy walk bunch bar Montreals Plateau neighbourhood found semi-decent table get know pisco sour Fast forward one year wa trying think creative way mark occasion working data-heavy quantified-self project thought oh maybe geek one analyzing year worth text message Overall Messaging History Overall period approximately 11 month wanted ready time sent 17,408 message average 60 message per day 59 OJ ha sent 22,589 message average 80 messages85 guess know chatter timeline show peak mostly one super peak one two dead-zones little going peak usually correspond trip one u Hawaii July Toronto November NYC December Toronto March low point often traveling together Israel June Colombia February didnt need much message communication except maybe occasional bring toilet paper roll plz oreo get big shock overall OJ sends message interesting consider whether come actually word simply breaking what-would-be long message shorter one table show slightly detailed breakdown fact sending message OJs message tend shorter mine tad longer although statistically significant doe give general feel messaging behavior one fond multiple short message row prefers single longer message Interaction may ask bit interaction converse using message doe interaction look like initiating long doe usually take side respond Lets see Culture reflected habit Looking figure showcasing breakdown messaging statistic per day box plot explanation need seems like work day Monday Thursday behavior generally similar slight drop Wednesdays perhaps worth crossing data day spent together actually collect data might look future post happening Friday-Saturday interesting make lot sense know people question one u OJ observing jew meaning Shabbat Friday dark Saturday dark usually electronic device use read therefore communicate much le Friday-Saturday outlier case much closer core day Sundays back normal limitation Outliers often correlated period travel oddly keep even higher communication rate daytime creature see figure conversation happen expected daytime hour peak around 34pm towards end workday 5pm~ish Canada looking delta u 3pm also seems one gap biggest perhaps related fact peak hour productivity work thats discussion another post lack attention communication around Homework show initiative let look initiative need improve show initiative come message Well guess title paragraph might given one inspect well look first interaction day anything thats happening 7am let consider fact rarely long night therefore ruling see initiated conversation hard tell improvement seems like lag behind need step game interesting next question ask doe change time start good gradually slow merely Im big morning person Well seems like periodical ups least seems like mostly improving slowly balancing ask different maybe bit scary question first message actually look like might know negative-Nancy OJ positive-Patricia dynamic pretty easy capture first message day Im much inclined use term ugh horrible OJs vocabulary far positive anyone slow morning expect looking great moment OJ getting high mark relationship-maintenance negative morning lower initiative wonder trail behind also come responsiveness age connectivity Something keep mind diving section millennials yes despite born 80 See diagram Ive added hyper-connected phone another organ body actually hating actively trying disconnect keep phone near thats also material different post Looking reply-time distribution seems actually quite similar responsive sigh Considering bin distribution approximately 2-minute delay lag tad behind seems insignificant could interesting investigate whether responsive specific time day might valuable general distribution response time Lets look excuse lack subplots plotly sure make impossible generate using heatmaps First like saw earlier communication happens daytime around mid-afternoon expected fastest respond late night message seemingly different come delay answering daytime message pretty fast case interesting look month hopefully connected could useful measure even talking Content interesting statistic weve explored far also personal matter keep section minimal Negative Nancy Positive Patricia reading carefully might noticed traditionally relationship considered negative among u doe actually reflect say versus OJ say Seems like definite fact comparing percentage-wise actually seems bit le negative OJ shocker difference minor alternative viewpoint might reflect situation better negativity messaging rather lack positivity OJ clearly positive whole 28.9 message said positive tone stand whole 5.7 lower wa expected Looking happens throughout day excuse inserting photo wa impossible insert decent subplot seems dont demonstrate anything significantly odd perhaps except slight higher presence negative content end early morning hour sigh Nicknames anyone Finally looking word cloud visualizing content rather first message content first conclusion cant really tell much arent word stand unusual pick habit like Brits OJ frequent use word mister sir u overusing lol haha hehe Millennials cant even detect nickname side strong indicator phasing nickname useful practice understand content analyzing within specific context example doe conversation change trip saying negative context complaining bottom line wa interesting journey went thinking back couldve looked plenty aspect us word love often spot long began dating start using love long average text conversation many many question could asked seeing learn practice Happy first anniversary u OJ technical info data enthusiast wrote code project Python mentioned earlier pandas-heavy plotting wa done almost entirely plotly always best mate process stackoverflow documentation page different library used Since inquired Ive made code available GitHub Feel free fork clone play ask anything VADER great nltk tool analyzing text data sentiment Python havent got training set label available us term scoring wide lexicon word easily add see reviewing sample data export actually pretty damn accurate Word cloud generated Pythons WordCloud library decent havent worked often look better resolution customization tool feel free leave message suggestion link found useful might Hope wa insightful ask away question 98 1 98 98 1 Towards Data Science home data science Medium publication sharing concept idea code Salma Ghoneim Mar 31 2019 Member-only Object detection via color-based image segmentation using python tutorial contouring using python OpenCV Getting started already jupyter notebook IDE run python OpenCV installed skip Execution Tools hero today Anaconda free open-source distribution help installing different package sort mess isolated environment Wikipedias telling u 7 min read 7 min read Share idea million reader Sahil Dhankhad Mar 31 2019 Member-only Forget APIs Python Scraping Using Beautiful Soup Import Data File web Part 2 APIs always every website Beautiful Soup going stay forever collect kind data website Today Data play critical role every industry data coming internet company 8 min read 8 min read Andrew Donaldson Mar 31 2019 Member-only Dont Fear Robots Impressive tech important remember Boston Dynamics video trailer movie isnt yet made Every time one Boston Dynamics video hit met mix amazement prediction doom hand new robot overlord impressive also well choreographed staged Impressive tech important 4 min read 4 min read Jasmine Vasandani Mar 31 2019 Member-only 10 Steps Teaching Data Science Well resource data science instructor increase demand data scientist ha parallel growth training program within company educational institution teach people data science addition focusing meeting job demand need resource instructor facilitating data science classroom Heres 4 min read 4 min read Karan Bhanot Mar 31 2019 Member-only income 50K/yr Machine Learning tell Machine learning breaking ground numerous field including Finance could use Machine Learning model identify income individual found right dataset called Census Income Dataset used information dataset predict someone would earn income 7 min read 7 min read Guy Tsror data-nerd trying adult way life Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Mark Vassilevskiy 5 Unique Passive Income IdeasHow Make 4,580/Month Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Mar 31, 2019 Member-only Listen Save The romantic side of data science: Analyzing a relationship through a year worth of text messages This was supposed to be a private project, one I thought I would upload to a private Google Colab notebook, and share with just whomever it is relevant for (which is, one other person, referred to here as OJ). But then, a lightbulb why not clean it up and share with others as well? Basically, to celebrate our first anniversary of dating, I decided to explore our relationship from the communication point of view, and look at our messaging behaviors: what are we saying to each other, when are we saying it, how do we respond to each other, and so on. I would include call logs in it, but I actually never tracked that (shame on me!) so I had to let it go, especially since we mostly have up to two phone calls a week, making it a bit less interesting. So I will go through my thought process to those who are interested in what I did and why, and towards the end, I will share some links and tools that came in handy during the process. I am working on improving my Python skills as I go, so you could also find there some Python-related snippets that were handy. (please note: this story is much better read on desktop, since infographics are interactive) Methodology The key communication channel we use is WhatsApp. We do SMS-text occasionally, but its negligible, and the occasional Messenger link or YouTube message are very rare, so I chose to stick to WhatsApp to simplify the process. The Story We met in April of 2018, after a right-swipe on Tinder and the most spontaneous setup ever same day swipe and date! It was a first for me, and after a rainy walk between a bunch of bars in Montreals Plateau neighbourhood, we found a semi-decent table to get to know each other over pisco sour. Fast forward one year, and I was trying to think of a creative way to mark this occasion. As I am working on some other data-heavy and quantified-self projects, I thought oh, maybe I can geek this one out? and so, here we are, analyzing a year worth of text messages. Overall Messaging History Overall, in a period of approximately 11 months (wanted this to be ready in time! ), I have sent 17,408 messages (an average of 60 messages per day 59) while OJ has sent 22,589 messages (an average of 80 messages85) I guess we know whos the chatter here! The timeline shows some peaks (mostly one super peak) and one or two dead-zones with very little going on. The peaks usually correspond with trips that one of us had (Hawaii in July, Toronto in November, NYC in December and Toronto in March), and the low points are often when we both were traveling together (Israel in June, Colombia in February) and didnt need much message communications (except for maybe the occasional bring me a toilet paper roll plz or which oreos should I get?). So no big shocks here. So, overall OJ sends more messages, but it is interesting to consider whether that comes down to actually more words, or simply breaking down what-would-be long messages to shorter ones. The table below shows a slightly more detailed breakdown: in fact, while sending more messages, OJs messages tend to be shorter, while mine are a tad longer (although not statistically significant). It does give a general feel to our messaging behavior one more fond of multiple short messages in a row, while the other prefers a single longer message. Interaction: when and how? We may now ask a bit more about the interaction itself: when do we converse using messages? How does this interaction look like who is initiating, when and how long does it usually take for the other side to respond? Lets see! Culture reflected in habits Looking at the figure below, showcasing a breakdown of messaging statistics per day (box plot explanation here , if you need it! ), it seems like on most work days (Monday through Thursday) the behavior is generally similar, with a slight drop on Wednesdays (perhaps worth crossing it with data on days we spent together? I actually collect that data, and might look into it in a future post!). What is happening on Friday-Saturday is interesting, and makes a lot of sense once you know the people in question: one of us (OJ) is an observing jew (meaning, on Shabbat, which is Friday after dark until Saturday after dark, theres usually no electronic devices in use read more on this here ) therefore we communicate much less on Friday-Saturday, and the outliers in these cases are much closer to the core than on most other days. Sundays are back to normal, as there are no limitations again! Outliers are often correlated with periods of travel, in which we (oddly) keep an even higher communication rate. We are daytime creatures As we see in the figure below, most of our conversations happen in expected daytime hours with peaks around 34pm towards the end of the workday (5pm~ish) here in Canada. When looking at the delta between us, 3pm also seems to be the one where the gap is the biggest perhaps this is related to the fact that its peak hour in my productivity at work (thats a discussion for another post!) and my lack of attention to communications around then. Homework for me: show more initiative! Now lets look at initiative: who needs to improve here and show more initiative when it comes to messages? Well, I guess the title of this paragraph might have given that one out. But to inspect that, well look at the first interaction of each day, for anything thats happening after 7am (lets consider the fact that we do rarely have long nights out, therefore were ruling these out), and see who initiated the conversation! Its not hard to tell: I have some improvement to do it seems like I lag behind, and need to step my game up. An interesting next question to ask is how does this change over time? Did I start good and gradually slow down, or is it merely because Im not a big morning person? Well, seems like its periodical! We both have our ups and downs, but at least it seems like I am mostly improving, and it slowly is balancing. Now we can ask a different, maybe a bit scary question: how do these first messages actually look like? You might not know this, but I am the negative-Nancy and OJ is the positive-Patricia in our dynamics, and its pretty easy to capture just from the first messages of each day: Im much more inclined to use terms such as ugh and horrible, while OJs vocabulary is far more positive than anyone with slow mornings can expect. I am not looking so great at the moment, while OJ is getting high marks in relationship-maintenance! With my negative mornings and lower initiative, but I wonder, do I trail behind also when it comes to responsiveness? The age of connectivity Something to keep in mind before diving into this section is that we are both millennials (yes, despite me being born in the 80s See the diagram Ive added here), and as such, can be hyper-connected and have our phone as another organ of our body. I actually am hating that and actively trying to disconnect and not keep my phone near me, but thats also material for a different post! Looking at our reply-time distribution it seems that we are actually quite similar, and very responsive (sigh). Considering each bin in this distribution is approximately a 2-minute delay, I do lag a tad behind, but it seems insignificant. What could be interesting to investigate is whether we are more responsive during a specific time of day this might be more valuable than just a general distribution of our response time! Lets look below (excuse me for the lack of subplots here, but plotly sure makes it impossible to generate when using heatmaps! ): First off, like we saw earlier, most of the communication happens in daytime, around mid-afternoon. As expected, we both are not the fastest to respond to late night messages, and seemingly not too different when it comes to our delay answering any daytime messages: were pretty fast in most cases! This will be interesting to look at in a few months, hopefully I will not be as connected as before and this could be a useful measure. What am I even talking about? Content is just as interesting as the statistics weve explored so far. BUT its also a more personal matter, so I will keep this section minimal :) Negative Nancy or Positive Patricia? If you have been reading carefully, you might have noticed that traditionally in this relationship, I am considered the more negative among us. But does this actually reflect in what I say versus what OJ says? Seems like its a definite no! In fact, when comparing percentage-wise, it actually seems I am a bit less negative than OJ! This is a shocker but the differences are minor. An alternative viewpoint that might reflect the situation better is not my negativity in messaging, but rather lack of positivity. OJ is clearly more positive, with a whole 28.9% of messages said in positive tone, while I stand a whole 5.7% lower. But this was expected. Looking at what happens throughout the day (excuse me for inserting this as a photo, it was impossible to insert as a decent subplot! ), it seems we dont demonstrate anything significantly odd, perhaps except for a slight higher presence of negative content from my end in the early morning hours (sigh). Nicknames anyone? Finally, by looking at a word cloud visualizing all of the content (rather than just first messages content), the first conclusion is: we cant really tell much. There arent any words that stand out as unusual; we can pick on a few habits in there, like the Brits (OJ) frequent use of the words mister or sir, or both us overusing lol,haha and hehe. Millennials after all. We cant even detect any nickname in here, on any side, a strong indicator of our phasing in and out of nicknames. No, a more useful practice to understand content will be analyzing within specific contexts: for example, how does our conversation change during trips? What are we saying in a negative context? What are we complaining about? And so on. The bottom line This was an interesting journey for me as I went through it, and thinking back, I couldve looked at plenty other aspects: who uses the word love more often? Can we spot how long after we began dating did we start using love with each other? How long is our average text conversation? And many many more questions that could be asked. But seeing what we have, what can we learn from this practice? Happy first anniversary to us, OJ :) Some technical info for the data enthusiasts I wrote the code for this project in Python, as I mentioned earlier. Its pandas-heavy, and the plotting was done almost entirely with plotly. As always, some of my best mates in the process were stackoverflow and documentation pages of the different libraries I used. Since some of you inquired, Ive made the code available on GitHub . Feel free to fork, clone, play and ask anything about it. VADER is a great nltk tool for analyzing text data sentiment in Python, if you havent got a training set with labels available. It uses term scoring for a wide lexicon of words (that you can easily add to see below) and after reviewing a sample data from my export it actually is pretty damn accurate! Word clouds were generated with Pythons WordCloud library, and were decent. I havent worked with these very often, but am on the look out for a better resolution and customization tool, so feel free to leave a message if you have suggestions for that! Here are some links that I found useful and you might too! Hope this was insightful, ask away if you have any questions! 98 1 98 98 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Salma Ghoneim Mar 31, 2019 Member-only Object detection via color-based image segmentation using python A tutorial on contouring using python & OpenCV. Getting started If you already have jupyter notebook or an IDE with which you can run python & OpenCV installed, just skip to Execution. Tools Our hero today is Anaconda. a free open-source distribution that helps with installing different packages & sorts out their messes into isolated environments. What Wikipedias telling us about 7 min read 7 min read Share your ideas with millions of readers. Sahil Dhankhad Mar 31, 2019 Member-only Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2 APIs are not always there for you for every website, but Beautiful Soup is going to stay with you forever to collect any kind of data from any website. Today, Data play a critical role in every industry. And most of this data is coming from the internet. Most company 8 min read 8 min read Andrew Donaldson Mar 31, 2019 Member-only Dont Fear the Robots Impressive as the tech here is, it is important to remember that the Boston Dynamics videos are trailers for a movie that isnt yet made. Every time one of these Boston Dynamics videos hit, they are met with a mix of amazement and predictions of doom at the hands of our new robot overlords. It is impressive. It is also very well choreographed and staged. Impressive as the tech here is, it is important 4 min read 4 min read Jasmine Vasandani Mar 31, 2019 Member-only 10 Steps to Teaching Data Science Well A resource for data science instructors. With the increase in demand for data scientists, there has been a parallel growth in training programs within companies and educational institutions that teach people data science. In addition to focusing on meeting the job demand, there need to be more resources for instructors facilitating data science classrooms. Heres a 4 min read 4 min read Karan Bhanot Mar 31, 2019 Member-only Will your income be more than $50K/yr? Machine Learning can tell Machine learning is breaking grounds in numerous fields including Finance. What if we could use Machine Learning models to identify incomes of individuals? I found just the right dataset for this, called Census Income Dataset. I used the information in the dataset to predict if someone would earn an income 7 min read 7 min read Guy Tsror A data-nerd trying to adult my way through life. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Mark Vassilevskiy 5 Unique Passive Income IdeasHow I Make $4,580/Month Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 3181,\n",
       "  'url': 'https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e',\n",
       "  'title': 'Everything You Ever Wanted To Know About Computer Vision. Here’s A Look Why It’s So\\xa0Awesome.',\n",
       "  'subtitle': '-',\n",
       "  'claps': 604,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 11,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-25',\n",
       "  'clap_prop': 0.0002983477757037229,\n",
       "  'text': 'Towards Data Science Apr 25 2019 Listen Save Everything Ever Wanted Know Computer Vision Heres Look Awesome ne powerful compelling type AI computer vision youve almost surely experienced number way without even knowing Heres look work awesome going get better Computer vision field computer science focus replicating part complexity human vision system enabling computer identify process object image video way human recently computer vision worked limited capacity Thanks advance artificial intelligence innovation deep learning neural network field ha able take great leap recent year ha able surpass human task related detecting labeling object One driving factor behind growth computer vision amount data generate today used train make computer vision better Along tremendous amount visual data 3 billion image shared online every day computing power required analyze data accessible field computer vision ha grown new hardware algorithm ha accuracy rate object identification le decade today system reached 99 percent accuracy 50 percent making accurate human quickly reacting visual input Early experiment computer vision started 1950s wa first put use commercially distinguish typed handwritten text 1970s today application computer vision grown exponentially 2022 computer vision hardware market expected reach 48.6 billion Computer Vision Work One major open question Neuroscience Machine Learning exactly brain work approximate algorithm reality working comprehensive theory brain computation despite fact Neural Nets supposed mimic way brain work nobody quite sure thats actually true paradox hold true computer vision since decided brain eye process image difficult say well algorithm used production approximate internal mental process certain level Computer vision pattern recognition one way train computer understand visual data feed image lot image thousand million possible labeled subject various software technique algorithm allow computer hunt pattern element relate label example feed computer million image cat love subject algorithm let analyze color photo shape distance shape object border identifies profile cat mean finished computer theory able use experience fed unlabeled image find one cat Lets leave fluffy cat friend moment side let get technical simple illustration grayscale image buffer store image Abraham Lincoln pixel brightness represented single 8-bit number whose range 0 black 255 white way storing image data may run counter expectation since data certainly appears two-dimensional displayed Yet case since computer memory consists simply ever-increasing linear list address space Lets go back first picture imagine adding colored one thing start get complicated Computers usually read color series 3 value red green blue RGB 0255 scale pixel actually ha 3 value computer store addition position colorize President Lincoln would lead 12 x 16 x 3 value 576 number Thats lot memory require one image lot pixel algorithm iterate train model meaningful accuracy especially youre talking Deep Learning youd usually need ten thousand image merrier Evolution Computer Vision advent deep learning task computer vision could perform limited required lot manual coding effort developer human operator instance wanted perform facial recognition would perform following step manual work application would finally able compare measurement new image one stored database tell whether corresponded profile wa tracking fact wa little automation involved work wa done manually error margin wa still large Machine learning provided different approach solving computer vision problem machine learning developer longer needed manually code every single rule vision application Instead programmed feature smaller application could detect specific pattern image used statistical learning algorithm linear regression logistic regression decision tree support vector machine SVM detect pattern classify image detect object Machine learning helped solve many problem historically challenging classical software development tool approach instance year ago machine learning engineer able create software could predict breast cancer survival window better human expert However building feature software required effort dozen engineer breast cancer expert took lot time develop Deep learning provided fundamentally different approach machine learning Deep learning relies neural network general-purpose function solve problem representable example provide neural network many labeled example specific kind data itll able extract common pattern example transform mathematical equation help classify future piece information instance creating facial recognition application deep learning requires develop choose preconstructed algorithm train example face people must detect Given enough example lot example neural network able detect face without instruction feature measurement Deep learning effective method computer vision case creating good deep learning algorithm come gathering large amount labeled training data tuning parameter type number layer neural network training epoch Compared previous type machine learning deep learning easier faster develop deploy current computer vision application cancer detection self-driving car facial recognition make use deep learning Deep learning deep neural network moved conceptual realm practical application thanks availability advance hardware cloud computing resource Long Take Decipher Image short much Thats key computer vision thrilling Whereas past even supercomputer might take day week even month chug calculation required today ultra-fast chip related hardware along speedy reliable internet cloud network make process lightning fast crucial factor ha willingness many big company AI research share work Facebook Google IBM Microsoft notably open sourcing machine learning work allows others build work rather starting scratch result AI industry cooking along experiment long ago took week run might take 15 minute today many real-world application computer vision process happens continuously microsecond computer today able scientist call situationally aware Applications Computer Vision Computer vision one area Machine Learning core concept already integrated major product use every day CV Self-Driving Cars tech company leverage Machine Learning image application Computer vision enables self-driving car make sense surroundings Cameras capture video different angle around car feed computer vision software process image real-time find extremity road read traffic sign detect car object pedestrian self-driving car steer way street highway avoid hitting obstacle hopefully safely drive passenger destination CV Facial Recognition Computer vision also play important role facial recognition application technology enables computer match image people face identity Computer vision algorithm detect facial feature image compare database face profile Consumer device use facial recognition authenticate identity owner Social medium apps use facial recognition detect tag user Law enforcement agency also rely facial recognition technology identify criminal video feed CV Augmented Reality Mixed Reality Computer vision also play important role augmented mixed reality technology enables computing device smartphones tablet smart glass overlay embed virtual object real world imagery Using computer vision AR gear detect object real world order determine location device display place virtual object instance computer vision algorithm help AR application detect plane tabletop wall floor important part establishing depth dimension placing virtual object physical world CV Healthcare Computer vision ha also important part advance health-tech Computer vision algorithm help automate task detecting cancerous mole skin image finding symptom x-ray MRI scan Challenges Computer Vision Helping computer see turn hard Inventing machine see like deceptively difficult task hard make computer entirely sure human vision work first place Studying biological vision requires understanding perception organ like eye well interpretation perception within brain Much progress ha made charting process term discovering trick shortcut used system although like study involves brain long way go Many popular computer vision application involve trying recognize thing photograph example Outside recognition method analysis include application involves understanding pixel software safely labeled computer vision Conclusion Despite recent progress ha impressive still even close solving computer vision However already multiple healthcare institution enterprise found way apply CV system powered CNNs real-world problem trend likely stop anytime soon want get touch way know good joke connect Twitter LinkedIn Thanks reading 1K 1K 1K Towards Data Science home data science Medium publication sharing concept idea code Nishit Jain Apr 25 2019 overview Gradient Descent algorithm subtle yet powerful algorithm optimizes parameter Optimizing parameter ultimate goal every machine learning algorithm want get optimum value slope intercept get line best fit linear regression problem also want get optimum value parameter sigmoidal curve 8 min read 8 min read Share idea million reader Venelin Valkov Apr 25 2019 Cryptocurrency price prediction using LSTMs TensorFlow Hackers Part III Predict Bitcoin price using LSTM Deep Neural Network TensorFlow 2 TL DR Build train Bidirectional LSTM Deep Neural Network Time Series prediction TensorFlow 2 Use model predict future Bitcoin price Complete source code Google Colaboratory Notebook time youll build basic Deep Neural Network model predict Bitcoin price based historical 6 min read 6 min read Ahmad Tanehkar Apr 25 2019 Lets Apply Machine Learning Behavioral Economics Currently Machine Learning ML well applied behavioral economics social science lack use come unfamiliarity different approach 5 min read 5 min read Ashok Chilakapati Apr 25 2019 Concept Drift Model Decay Machine Learning Concept drift drift label time essentially data lead divergence decision boundary new data model built earlier data/labels 7 min read 7 min read Ren Bremer Apr 25 2019 use Python feature data analytics project Python tutorial Azure using OO NumPy panda SQL PySpark 1 Introduction lot company moving cloud consider tooling shall used data analytics On-premises company mostly use propriety software advanced analytics BI reporting However tooling may logical choice cloud environment 9 min read 9 min read Ilija Mihajlovic iOS developer computer science graduate passion machine learning computer vision Medium Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Shahrullohon Lutfillohonov Visual Perception Self-Driving Cars Part 5 Multi-Task Learning Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Betul Mescioglu Image Processing Tutorial Using scikit-imageImage Segmentation Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 25, 2019 Listen Save Everything You Ever Wanted To Know About Computer Vision. Heres A Look Why Its So Awesome. O ne of the most powerful and compelling types of AI is computer vision which youve almost surely experienced in any number of ways without even knowing. Heres a look at what it is, how it works, and why its so awesome (and is only going to get better). Computer vision is the field of computer science that focuses on replicating parts of the complexity of the human vision system and enabling computers to identify and process objects in images and videos in the same way that humans do. Until recently, computer vision only worked in limited capacity. Thanks to advances in artificial intelligence and innovations in deep learning and neural networks, the field has been able to take great leaps in recent years and has been able to surpass humans in some tasks related to detecting and labeling objects. One of the driving factors behind the growth of computer vision is the amount of data we generate today that is then used to train and make computer vision better. Along with a tremendous amount of visual data ( more than 3 billion images are shared online every day ), the computing power required to analyze the data is now accessible. As the field of computer vision has grown with new hardware and algorithms so has the accuracy rates for object identification. In less than a decade, todays systems have reached 99 percent accuracy from 50 percent making them more accurate than humans at quickly reacting to visual inputs. Early experiments in computer vision started in the 1950s and it was first put to use commercially to distinguish between typed and handwritten text by the 1970s, today the applications for computer vision have grown exponentially. By 2022, the computer vision and hardware market is expected to reach $48.6 billion How Does Computer Vision Work? One of the major open questions in both Neuroscience and Machine Learning is: How exactly do our brains work, and how can we approximate that with our own algorithms? The reality is that there are very few working and comprehensive theories of brain computation; so despite the fact that Neural Nets are supposed to mimic the way the brain works, nobody is quite sure if thats actually true. The same paradox holds true for computer vision since were not decided on how the brain and eyes process images, its difficult to say how well the algorithms used in production approximate our own internal mental processes. On a certain level Computer vision is all about pattern recognition. So one way to train a computer how to understand visual data is to feed it images, lots of images thousands, millions if possible that have been labeled, and then subject those to various software techniques, or algorithms, that allow the computer to hunt down patterns in all the elements that relate to those labels. So, for example, if you feed a computer a million images of cats (we all love them), it will subject them all to algorithms that let them analyze the colors in the photo, the shapes, the distances between the shapes, where objects border each other, and so on, so that it identifies a profile of what cat means. When its finished, the computer will (in theory) be able to use its experience if fed other unlabeled images to find the ones that are of cat. Lets leave our fluffy cat friends for a moment on the side and lets get more technical. Below is a simple illustration of the grayscale image buffer which stores our image of Abraham Lincoln. Each pixels brightness is represented by a single 8-bit number, whose range is from 0 (black) to 255 (white): This way of storing image data may run counter to your expectations, since the data certainly appears to be two-dimensional when it is displayed. Yet, this is the case, since computer memory consists simply of an ever-increasing linear list of address spaces. Lets go back to the first picture again and imagine adding a colored one. Now things start to get more complicated. Computers usually read color as a series of 3 values red, green, and blue (RGB) on that same 0255 scale. Now, each pixel actually has 3 values for the computer to store in addition to its position. If we were to colorize President Lincoln, that would lead to 12 x 16 x 3 values, or 576 numbers. Thats a lot of memory to require for one image, and a lot of pixels for an algorithm to iterate over. But to train a model with meaningful accuracy especially when youre talking about Deep Learning youd usually need tens of thousands of images, and the more the merrier. The Evolution Of Computer Vision Before the advent of deep learning, the tasks that computer vision could perform were very limited and required a lot of manual coding and effort by developers and human operators. For instance, if you wanted to perform facial recognition, you would have to perform the following steps: After all this manual work, the application would finally be able to compare the measurements in the new image with the ones stored in its database and tell you whether it corresponded with any of the profiles it was tracking. In fact, there was very little automation involved and most of the work was being done manually. And the error margin was still large. Machine learning provided a different approach to solving computer vision problems. With machine learning, developers no longer needed to manually code every single rule into their vision applications. Instead they programmed features, smaller applications that could detect specific patterns in images. They then used a statistical learning algorithm such as linear regression, logistic regression, decision trees or support vector machines (SVM) to detect patterns and classify images and detect objects in them. Machine learning helped solve many problems that were historically challenging for classical software development tools and approaches. For instance, years ago, machine learning engineers were able to create a software that could predict breast cancer survival windows better than human experts. However building the features of the software required the efforts of dozens of engineers and breast cancer experts and took a lot of time develop. Deep learning provided a fundamentally different approach to doing machine learning. Deep learning relies on neural networks, a general-purpose function that can solve any problem representable through examples. When you provide a neural network with many labeled examples of a specific kind of data, itll be able to extract common patterns between those examples and transform it into a mathematical equation that will help classify future pieces of information. For instance, creating a facial recognition application with deep learning only requires you to develop or choose a preconstructed algorithm and train it with examples of the faces of the people it must detect. Given enough examples (lots of examples), the neural network will be able to detect faces without further instructions on features or measurements. Deep learning is a very effective method to do computer vision. In most cases, creating a good deep learning algorithm comes down to gathering a large amount of labeled training data and tuning the parameters such as the type and number of layers of neural networks and training epochs. Compared to previous types of machine learning, deep learning is both easier and faster to develop and deploy. Most of current computer vision applications such as cancer detection, self-driving cars and facial recognition make use of deep learning. Deep learning and deep neural networks have moved from the conceptual realm into practical applications thanks to availability and advances in hardware and cloud computing resources. How Long Does It Take To Decipher An Image In short not much. Thats the key to why computer vision is so thrilling: Whereas in the past even supercomputers might take days or weeks or even months to chug through all the calculations required, todays ultra-fast chips and related hardware, along with the a speedy, reliable internet and cloud networks, make the process lightning fast. Once crucial factor has been the willingness of many of the big companies doing AI research to share their work Facebook, Google, IBM, and Microsoft, notably by open sourcing some of their machine learning work. This allows others to build on their work rather than starting from scratch. As a result, the AI industry is cooking along, and experiments that not long ago took weeks to run might take 15 minutes today. And for many real-world applications of computer vision, this process all happens continuously in microseconds, so that a computer today is able to be what scientists call situationally aware. Applications Of Computer Vision Computer vision is one of the areas in Machine Learning where core concepts are already being integrated into major products that we use every day. CV In Self-Driving Cars But its not just tech companies that are leverage Machine Learning for image applications. Computer vision enables self-driving cars to make sense of their surroundings. Cameras capture video from different angles around the car and feed it to computer vision software, which then processes the images in real-time to find the extremities of roads, read traffic signs, detect other cars, objects and pedestrians. The self-driving car can then steer its way on streets and highways, avoid hitting obstacles, and (hopefully) safely drive its passengers to their destination. CV In Facial Recognition Computer vision also plays an important role in facial recognition applications, the technology that enables computers to match images of peoples faces to their identities. Computer vision algorithms detect facial features in images and compare them with databases of face profiles. Consumer devices use facial recognition to authenticate the identities of their owners. Social media apps use facial recognition to detect and tag users. Law enforcement agencies also rely on facial recognition technology to identify criminals in video feeds. CV In Augmented Reality & Mixed Reality Computer vision also plays an important role in augmented and mixed reality, the technology that enables computing devices such as smartphones, tablets and smart glasses to overlay and embed virtual objects on real world imagery. Using computer vision, AR gear detect objects in real world in order to determine the locations on a devices display to place a virtual object. For instance, computer vision algorithms can help AR applications detect planes such as tabletops, walls and floors, a very important part of establishing depth and dimensions and placing virtual objects in physical world. CV In Healthcare Computer vision has also been an important part of advances in health-tech. Computer vision algorithms can help automate tasks such as detecting cancerous moles in skin images or finding symptoms in x-ray and MRI scans. Challenges of Computer Vision Helping computers to see turns out to be very hard. Inventing a machine that sees like we do is a deceptively difficult task, not just because its hard to make computers do it, but because were not entirely sure how human vision works in the first place. Studying biological vision requires an understanding of the perception organs like the eyes, as well as the interpretation of the perception within the brain. Much progress has been made, both in charting the process and in terms of discovering the tricks and shortcuts used by the system, although like any study that involves the brain, there is a long way to go. Many popular computer vision applications involve trying to recognize things in photographs; for example: Outside of just recognition, other methods of analysis include: Any other application that involves understanding pixels through software can safely be labeled as computer vision. Conclusion Despite the recent progress, which has been impressive, were still not even close to solving computer vision. However, there are already multiple healthcare institutions and enterprises that have found ways to apply CV systems, powered by CNNs, to real-world problems. And this trend is not likely to stop anytime soon. If you want to get in touch and by the way, you know a good joke you can connect with me on  Twitter  or  LinkedIn. Thanks for reading! 1K 1K 1K More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Nishit Jain Apr 25, 2019 An overview of the Gradient Descent algorithm The subtle yet powerful algorithm that optimizes parameters Optimizing parameters is the ultimate goal of every machine learning algorithm. You want to get the optimum value of the slope and the intercept to get the line of best fit in linear regression problems. You also want to get the optimum value for the parameters of a sigmoidal curve 8 min read 8 min read Share your ideas with millions of readers. Venelin Valkov Apr 25, 2019 Cryptocurrency price prediction using LSTMs | TensorFlow for Hackers (Part III) Predict Bitcoin price using LSTM Deep Neural Network in TensorFlow 2 TL;DR Build and train an Bidirectional LSTM Deep Neural Network for Time Series prediction in TensorFlow 2. Use the model to predict the future Bitcoin price. Complete source code in Google Colaboratory Notebook This time youll build a basic Deep Neural Network model to predict Bitcoin price based on historical 6 min read 6 min read Ahmad Tanehkar Apr 25, 2019 Lets Apply Machine Learning in Behavioral Economics Currently, Machine Learning (ML) is not well applied in behavioral economics or social science, and this lack of use comes from unfamiliarity with this different approach. 5 min read 5 min read Ashok Chilakapati Apr 25, 2019 Concept Drift and Model Decay in Machine Learning Concept drift is a drift of labels with time for the essentially the same data. It leads to the divergence of decision boundary for new data from that of a model built from earlier data/labels. 7 min read 7 min read Ren Bremer Apr 25, 2019 How to use Python features in your data analytics project Python tutorial in Azure using OO, NumPy, pandas, SQL, PySpark 1. Introduction A lot of companies are moving to cloud and consider what tooling shall be used for data analytics. On-premises, companies mostly use propriety software for advanced analytics, BI and reporting. However, this tooling may not be the most logical choice in a cloud environment. 9 min read 9 min read Ilija Mihajlovic iOS developer and computer science graduate, with a passion for machine learning and computer vision. More from Medium Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Shahrullohon Lutfillohonov Visual Perception for Self-Driving Cars! Part 5: Multi-Task Learning Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Betul Mescioglu Image Processing Tutorial Using scikit-imageImage Segmentation Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4085,\n",
       "  'url': 'https://towardsdatascience.com/back-to-the-metal-top-3-programming-language-to-develop-big-data-frameworks-in-2019-69a44a36a842',\n",
       "  'title': 'Back to the metal: Top 3 Programming language to develop Big Data frameworks',\n",
       "  'subtitle': 'C++, Rust, Go over\\xa0Java…',\n",
       "  'claps': 904,\n",
       "  'responses': 4.0,\n",
       "  'reading_time': 12,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-01-26',\n",
       "  'clap_prop': 0.000446533757013519,\n",
       "  'text': 'Towards Data Science Jan 25 2019 Member-only Listen Save Back metal Top 3 Programming language develop Big Data framework C++ Rust Go Java Data Intensive framework previous blog post Programming language rule Data Intensive Big Data Fast Data framework briefly discussed popular Big Data framework showed Java de-facto programming language Data Intensive framework Java significant advantage e.g Platform Independence Productivity JVM language timeframe 20042014 dominant Big Data framework developed last 10 year lot change happened programming language landscape classic language gone major overhaul modernization Also promising modern programming language appeared elegant feature Computer Hardware ha gone major change rise Multi-Core processor GPU TPU well Containerization Docker Kubernetes came existence became mainstream someone company want develop next disruptive Big Data framework 2019 e.g next Hadoop Kafka Spark programming language best fit Big Data domain vintage language Java language First discus limitation Java propose better alternative context Data Intensive framework development point also valid develop Cloud Native IoT Machine Learning framework Limitations Java Every programming language ha limitation Also Java dominant Programming Language Data Intensive domain ha fair share limitation discus main limitation Java context Data Intensive framework development JVM JVM play huge role Java widely adopted becoming one popular programming language like many thing life sometimes biggest strength also biggest weakness main limitation JVM listed Developer Productivity Java first appeared 1995 wa productive language time lean size simplicity time Java ha added lot feature increasing language specification size/complexity considered among productive language fact Java often criticized verbose nature needing lot boilerplate code last decade Concurrency Although Java wa released pre Multi-Core era Java offer excellent Shared Memory based Concurrency support via Thread Lock deterministic Memory Model high-level abstraction Shared Memory based Concurrency difficult program prone Data Race Java doe offer language level Message Passing based Concurrency easier program correctly Asynchronous event loop based Concurrency better choice I/O heavy task Akka high-performance library offer Message Passing Asynchronous Concurrency Java without in-built support JVM performant language native support e.g Go Erlang Node.js today world Multi-Core processor huge drawback Java Serialization Javas default serialization slow ha security vulnerability result Java serialization another thorny issue Data Intensive landscape Oracle ha labeled horrible mistake plan drop future Java version Solution Back Metal declared obsolete destined demise heydeys Java Close-to-the-Metal language gaining lot interest recent year good reason C programming language wa developed Dennis Ritchie Bell Labs time 19691973 every cycle CPU every Byte memory wa expensive reason C later C++ wa designed churn maximum performance hardware expense language complexity misconception Big Data domain one doe need care much CPU/Memory someone need performance need handle data needed add Machines Big Data Custer adding Machines/Nodes also increase Cloud provider bill Also rise Machine learning/Deep learning hardware architecture change rapidly coming year programming language give full control hardware important coming day Near Metal language another drawback used Data Intensive framework Platform dependency Currently Web Server Operating System overwhelmingly dominated Linux around 97 market share public Cloud dominated Linux well 90 market share meteoric rise Containerization Docker Kubernetes give freedom develop platform e.g Windows targeting platform e.g Linux Thus Platform dependency critical factor choose Programming Language Data Intensive framework development Dont get wrong Java still formidable language develop Data Intensive framework Javas new Virtual Machine GraalVM new Garbage Collector ZGC Java even attractive language almost domain convinced Close-to-the-Metal language dominant Java/Scala coming year develop Data Intensive framework pick three Close-to-the-Metal language potential candidate develop Data Intensive framework 2019 Java/Scala C++ Like pioneer near-Metal language C C++ also ha root Bell Lab time Bell Labs Bjarne Stroustrup ha initially implemented C++ Object Oriented C first commercial release 1985 C++ general-purpose statically typed compiled programming language support multiple programming paradigm functional imperative object-oriented Like C also near Metal language give full control hardware without Memory safety Concurrency safety Similar C C++ also belief following Moto i.e C++ give developer powerful language responsibility developer make program Memory safe Data Race free C++ also ha lot feature functionality Feature Hell probably one difficult programming language master Since 2000 C++ ha added many feature Memory Model Shared Memory based Concurrency lambda make language simpler safer Concurrency friendly change come price C++ language specification ha become bigger even complex Another issue C++ long build time remember building CORBA library taking 30 minute However modern C++ e.g C++17 using principle like Resource Acquisition Initialization RAII comparatively easier develop Memory safe Data Race free programming C++ comparison older version C++ e.g C++98 C++ still lack language-level support Message Passing Concurrency come C++20 Asynchronous event loop based Concurrency Although many C++ library support Message Passing Asynchronous event loop based Concurrency legendary Node.js Asynchronous event loop based Concurrency wa developed C++ Learning C++ difficult Mastering C++ even difficult group niche experienced C++ developer build unbeatable framework domain including Data Intensive domain example 4 node ScyllaDB written C++ outperforms 40 node Cassandra written Java Pros Cons Notable Big Data Projects Rust wa always search dream Programming Language give Performance/Control near-Metal language C C++ safety Runtime language Haskell/Python Finally Rust look like Language Promised i.e give Performance/Control like C/C++ Safety Haskell Python Inspired research programming language Cyclone safer C Graydon Hoare first developed Rust personal project wa later sponsored Mozilla active contribution David Herman Brendan Eich creator JavaScript many others Rust statically typed compiled System Programming language support Functional Imperative programming paradigm First announced 2010 first stable version released 2015 concept Ownership Borrowing offer RAII language level support enables memory thread-safe programming speed C++ without Garbage Collector Virtual Machine really set apart RUST near Metal language e.g C/C++ Go give compile time safety i.e Code compiles run thread safe memory safe discussed Fearless Concurrency Rust also offer language level concurrency support Shared Memory Concurrency Message Passing Concurrency via Channel although still lack Asynchronous event-loop based Concurrency development excellent talk Alex Crichton Mozilla explaining Rust Concurrency Rust also ha expressive type numeric type like ML languages/Haskell ha immutable data structure default result offer excellent functional Concurrency data Concurrency like ML languages/Haskell Rust Web Assembly next big thing Browser developed Mozilla high performant fast Rust code directly converted Web Assembly run Browser Another interesting feature Rust ha self-hosted Compiler i.e Compiler Rust written Rust 23 year Java yet ha self-hosted Compiler Rust also great language Data Intensive domain due memory safe data race free zero cost abstraction concurrency feature Service Mesh platform Linkered migrated Scala+Netty+Finagle Stack Rust achieved much better performance resource utilization Data Intensive runtime Weld written Rust give 30x performance gain Data Intensive framework e.g Spark Pros Cons Notable Big Data Projects Go Go second language list ha root Bell Labs Two three co-creators language Rob Pike Plan 9 UTF-8 Ken Thompson creator Unix worked Bell lab time Unix C C++ wa originated middle 2000 Google huge problem Scalability Developer Scalability 1000 developer work codebase efficiently Application Scalability Application deployed Scalable way 1000 machine Google also issue integrating fresh graduate existing multi-million line complex C++ codebase high compile time C++ codebase issue discussed detail Finding existing language C++ Java sufficient tackle issue Google ha employed two best person software industry Rob Pike Ken Thompson create new language Go wa first announced 2010 first official version released 2012 Go designer taken C basis created simple productive yet powerful statically typed compiled garbage collected System Programming language Another key feature Go compile time fast creates single executable binary file also contains Go Runtime Garbage Collector MB requires separate VM Go also offer CSP based Message Passing Concurrency Communicating Sequential Processes originated Tony Hoare paper almost like way Erlang Although instead using Actor Channel used Erlang Go us goroutine lightweight green thread channel Message Passing Another difference Erlang us point-to-point communication Actors whereas Go us flexible indirect communication goroutines result Go offer simple yet extremely scalable Concurrency Model take advantage modern Multi-Core processor excellent talk Gos Concurrency Model Rob Pike keep language simple productive Go lack lot feature like Shared Memory based Concurrency although Go offer sharing memory channel Moto communicate sharing memory instead share memory communicating many high-level abstraction e.g Generics Backed Google Go ha well accepted community/industry ha excellent toolings/libraries best Infrastructure framework Docker Kubernetes well Data Intensive framework developed using Go Pros Cons Notable Big Data Projects found helpful please share favorite forum Twitter Facebook LinkedIn Comments constructive criticism highly appreciated Thanks reading interested Programming language also read following article Top 10 In-Demand programming language learn 2020 In-depth analysis ranking top programming language job seeker new developer towardsdatascience.com Top 10 Databases Use 2021 MySQL Oracle PostgreSQL Microsoft SQL Server MongoDB Redis Elasticsearch Cassandra MariaDB IBM Db2 md-kamaruzzaman.medium.com Programming language rule Data Intensive Big Data+Fast Data framework brief overview Big Data framework towardsdatascience.com 946 5 946 946 5 Enjoy read Reward writer Beta tip go Md Kamaruzzaman third-party platform choice letting know appreciate story Get email whenever Md Kamaruzzaman publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Brayden Gerrard Jan 25 2019 Member-only Evolution US Electric Grid past 15 year US electric grid ha undergone massive shift mostly better Renewable energy natural gas coal GHG emission see US grid ha evolved recent year 4 min read 4 min read Share idea million reader zgr Gen Jan 25 2019 Member-only Notes Artificial Intelligence Machine Learning Deep Learning curious people AI ha intriguing topic 2018 according McKinsey mentioned key enabler 1 3 spot Gartner Top 10 Strategic Technology Trends 2019 AI became catch-all term refers computer program automatically doe something 22 min read 22 min read Felix Kuestahler Jan 25 2019 Member-only Python Tutorial Twitter Account Retrieval Swiss Government Members Start journey social medium analysis politician Click link Title list tutorial article first article tutorial show extract list tweeting Swiss Government Members via Twitter API extracted data put Panda Dataframe 7 min read 7 min read Tobias Hill Jan 25 2019 got 1 better accuracy data augmentation hard let go thing put love yes really thought wa done 5 th article series accuracy/error metric MNIST example started haunting wa quite sure could improve 4 min read 4 min read Partha Deka Jan 25 2019 Empowering citizen data scientist hardware design manufacturing Improving productivity hardware design manufacturing professional advanced AI tool Authors Partha Deka Rohit Mittal citizen data scientist Expert data scientist rely custom coding make sense data use case could data cleansing data imputation creating segment finding pattern data building 4 min read 4 min read Md Kamaruzzaman Enterprise Architect Certified AWS/AZURE/GCP Architect Full-stack Cloud Big Data Follow Twitter http //twitter.com/KamaruzzMd Medium Yash Prakash Code 17 Golang Packages Know Matt Welsh Using Rust startup cautionary tale Fredy Sandoval Programming language faster Golang Company Comparison Golang JavaBackend Battle 2022 Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Jan 25, 2019 Member-only Listen Save Back to the metal: Top 3 Programming language to develop Big Data frameworks C++, Rust, Go over Java for Data Intensive frameworks In a previous blog post:  Programming language that rules the Data Intensive (Big Data, Fast Data) frameworks  , I have briefly discussed some of the most popular Big Data frameworks and showed that Java is the de-facto programming language in Data Intensive frameworks. Java had significant advantages (e.g. Platform Independence, Productivity, JVM) over other languages during the timeframe 20042014 when most of the dominant Big Data frameworks were developed. In the last 10 years, lots of changes happened in the programming language landscape. Some classic languages have gone through major overhauls and modernizations. Also some very promising, modern programming languages appeared with elegant features. Computer Hardware has gone through major changes ( rise of Multi-Core processors, GPU, TPU ) as well. Containerization with Docker, Kubernetes came to existence and became mainstream. If someone or some company wants to develop the next disruptive Big Data framework in 2019 (e.g. next Hadoop, Kafka, Spark), what programming language will be the best fit? The Big Data domain vintage language Java or any other language? First I will discuss the limitations of Java and then I will propose better alternatives in the context of Data Intensive framework development . Most of the points are also valid to develop Cloud Native , IoT and Machine Learning frameworks. Limitations of Java Every programming language has its limitations. Also Java, the most dominant Programming Language in the Data Intensive domain, has its fair share of limitations. Here I will discuss the main limitations of Java in the context of Data Intensive framework development. JVM: JVM plays a huge role for Java being widely adopted and becoming one the most popular programming language. But like many things in life, sometimes the biggest strength is also the biggest weakness. The main limitations of JVM are listed below: Developer Productivity: When Java first appeared in 1995, it was a very productive language at that time with its lean size and simplicity. With time, Java has added lots of features, increasing language specification size/complexity and can no more be considered among the most productive languages. In fact, Java is often criticized for its verbose nature needing lots of boilerplate code in the last decade. Concurrency: Although Java was released in the pre Multi-Core era, Java offers excellent Shared Memory based Concurrency support via Thread, Lock, deterministic Memory Model and other high-level abstractions. Shared Memory based Concurrency is difficult to program and prone to Data Race. Java does not offer any language level Message Passing based Concurrency (easier to program correctly) or Asynchronous event loop based Concurrency (better choice for I/O heavy tasks). Akka  or other high-performance libraries can offer Message Passing or Asynchronous Concurrency in Java. But without the in-built support from JVM, they will not be as performant as languages which have native support (e.g. Go , Erlang , Node.js ). In todays world of Multi-Core processors, this is a huge drawback of Java. Serialization : Javas default serialization is very slow and has security vulnerabilities. As a result, Java serialization is another thorny issue in the Data Intensive landscape which Oracle has labeled as a  horrible mistake   and plans to drop in future Java versions. Solution: Back to the Metal Once declared obsolete and destined to demise during the heydeys of Java, the Close-to-the-Metal languages are gaining lots of interest in recent years and for good reasons. The C programming language was developed by  Dennis Ritchie  in Bell Labs during a time (19691973) when every cycle of CPU and every Byte of memory was very expensive. For this reason, C (and later C++) was designed to churn out the maximum performance from the hardware with the expense of language complexity. There is a misconception that in Big Data domain, one does not need to care too much about CPU/Memory. If someone needs more performance or need to handle more data, all is needed to add more Machines in Big Data Custer. But adding more Machines/Nodes will also increase Cloud provider bill. Also, with the rise of Machine learning/Deep learning, hardware architecture will change rapidly in the coming years. So, programming languages that give full control over hardware will only be more and more important in coming days. Near Metal languages had another drawback to be used in Data Intensive frameworks: Platform dependency. Currently, Web Server Operating System is overwhelmingly dominated by  Linux with around 97% market share   : The public Cloud is dominated by Linux as well with more than 90% market share: The meteoric rise of Containerization with Docker, Kubernetes gives freedom to develop in any platform (e.g. Windows) targeting any other platform (e.g. Linux). Thus, Platform dependency is no more a critical factor to choose Programming Language for Data Intensive framework development. Dont get me wrong, Java is still a formidable language to develop Data Intensive frameworks. With Javas new  Virtual Machine GraalVM    and new  Garbage Collector ZGC  , Java will be even more attractive language in almost any domain  . But I am convinced that Close-to-the-Metal languages will be more dominant than Java/Scala in coming years to develop Data Intensive frameworks. Here I will pick three Close-to-the-Metal languages as a potential candidate to develop Data Intensive frameworks in 2019 over Java/Scala. C++ Like the pioneer near-Metal language C, C++ also has its root in Bell Lab. During his time in Bell Labs,  Bjarne Stroustrup  has initially implemented C++ as Object Oriented C with first commercial release in 1985. C++ is a general-purpose, statically typed, compiled programming language which supports multiple programming paradigm (functional, imperative, object-oriented). Like C, it is also a near Metal language which gives full control over hardware without Memory safety or Concurrency safety. Similar to C, C++ also believes in the following Moto: i.e. C++ will give the developers a very powerful language but it the responsibility of the developers to make the program Memory safe or Data Race free. C++ also has lots of features and functionality (Feature Hell) and probably one of the most difficult programming languages to master. Since 2000, C++ has added many features (Memory Model, Shared Memory based Concurrency, lambda) to make the language simpler, safer and Concurrency friendly. But these changes have come with a price, C++ language specification has become bigger and even more complex. Another issue of C++ is its long build time (I remember building a CORBA library taking 30 minutes). However, with modern C++ (e.g. C++17  ) and using principles like  Resource Acquisition Is Initialization (RAII)  , it is comparatively easier to develop Memory safe, Data Race free programming in C++ in comparison to the older version of C++ (e.g. C++98). C++ still lacks language-level support for Message Passing Concurrency (will come in  C++20  ) and Asynchronous event loop based Concurrency. Although there are many C++ libraries which supports Message Passing and Asynchronous event loop based Concurrency (legendary Node.js Asynchronous event loop based Concurrency was developed in C++) . Learning C++ is difficult. Mastering C++ is even more difficult. But if there is a group of niche, experienced C++ developer, they can build unbeatable frameworks (in any domain including Data Intensive domain). There is the example of a  4 node ScyllaDB (written in C++) which outperforms the 40 node Cassandra (written in Java)  . Pros: Cons: Notable Big Data Projects: Rust There was always a search for a dream Programming Language which will give the Performance/Control of near-Metal languages (C, C++) and safety of Runtime languages (Haskell/Python). Finally, Rust looks like The Language that Promised i.e. it gives the Performance/Control like C/C++ with the Safety of  Haskell  /  Python  . Inspired by the research programming language  Cyclone (safer C)  ,  Graydon Hoare  first developed Rust as a personal project which was later sponsored by Mozilla with active contribution from David Herman ,  Brendan Eich    (creator of JavaScript)  and many others. Rust is a statically typed, compiled System Programming language which supports Functional and Imperative programming paradigm. First announced in 2010, its first stable version is released in 2015. With the concept of  Ownership  and  Borrowing  , it offers the  RAII  from language level support and enables memory, thread-safe programming with the speed of C++ without any Garbage Collector or Virtual Machine. What really sets apart RUST from other near Metal languages (e.g. C/C++, Go) is that it gives the compile time safety i.e. if a Code compiles, it will run thread safe and memory safe as discussed in  Fearless Concurrency in Rust  . It also offers language level concurrency support for both Shared Memory Concurrency and  Message Passing Concurrency (via Channel)  although it still lacks Asynchronous event-loop based Concurrency (in development). Here is an excellent talk by Alex Crichton from Mozilla explaining Rust Concurrency: Rust also has expressive types and numeric types like ML languages/Haskell and has immutable data structure by default. As a result, it offers excellent functional Concurrency and data Concurrency like ML languages/Haskell. As both Rust and  Web Assembly (the next big thing in Browser)  are developed by Mozilla, high performant and fast Rust code can directly be converted to Web Assembly to run on Browser. Another very interesting feature is that Rust has  self-hosted Compiler  i.e. Compiler of Rust is written in Rust (After 23 years, Java not yet has self-hosted Compiler). Rust is also a great language in the Data Intensive domain due to its memory safe, data race free, zero cost abstraction, concurrency features. The Service Mesh platform  Linkered  is migrated from Scala+Netty+Finagle Stack to Rust and achieved much better performance and resource utilization. The Data Intensive runtime  Weld  which is written in Rust can give up to 30x performance gain for Data Intensive frameworks (e.g. Spark). Pros: Cons: Notable Big Data Projects: Go Go is the second language in this list which has its roots in Bell Labs. Two of the three co-creators of the language:  Rob Pike  (  Plan 9  ,  UTF-8  ) and  Ken Thompson  (creator of Unix) worked in Bell labs during the time when Unix, C, C++ was originated there. In the middle of 2000, Google had a huge problem of Scalability: Developer Scalability (1000 of developers can not work on the same codebase efficiently) and Application Scalability (Application cannot be deployed in a Scalable way on 1000 machines). Google also had the issue of integrating fresh graduates with existing multi-million lines complex C++ codebase, high compile time of C++ codebase and some other issues discussed in detail  here  . Finding existing languages (C++, Java) not sufficient to tackle those issues, Google has employed two of the best person in the software industry: Rob Pike and Ken Thompson to create a new language. Go was first announced in 2010 with the first official version released in 2012. Go designers have taken C as their basis and created a simple, productive yet powerful statically typed, compiled, garbage collected System Programming language. Another key feature of Go is that its compile time is very fast and it creates a single executable binary file which also contains Go Runtime and Garbage Collector (few MB) and requires no separate VM. Go also offers CSP based Message Passing Concurrency (Communicating Sequential Processes, originated from Tony Hoare  paper  ) almost like the same way as Erlang. Although instead of using Actor and Channel (used by Erlang), Go uses  goroutine (lightweight green threads) and channel  for Message Passing. Another difference is Erlang uses point-to-point communication between Actors whereas Go uses flexible, indirect communication between goroutines. As a result, Go offers very simple yet extremely scalable Concurrency Model to take advantage of modern Multi-Core processors. Here is an excellent talk about Gos Concurrency Model by Rob Pike : To keep the language simple and productive, Go lacks lots of features like Shared Memory based Concurrency (although Go offers sharing memory between channel with the Moto:  Do not communicate by sharing memory; instead, share memory by communicating   ) and many high-level abstractions (e.g. Generics). Backed by Google, Go has been well accepted by the community/industry and has excellent toolings/libraries. Some of the best Infrastructure frameworks (Docker, Kubernetes), as well as Data Intensive frameworks, are developed using Go. Pros: Cons: Notable Big Data Projects: If you found this helpful, please share it on your favorite forums ( Twitter, Facebook, LinkedIn ). Comments and constructive criticisms are highly appreciated. Thanks for reading! If you are interested in Programming languages, you can also read my following articles: Top 10 In-Demand programming languages to learn in 2020 In-depth analysis and ranking of the top programming languages for job seekers and new developers towardsdatascience.com Top 10 Databases to Use in 2021 MySQL, Oracle, PostgreSQL, Microsoft SQL Server, MongoDB, Redis, Elasticsearch, Cassandra, MariaDB, IBM Db2 md-kamaruzzaman.medium.com Programming language that rules the Data Intensive (Big Data+Fast Data) frameworks. A brief overview on Big Data frameworks towardsdatascience.com 946 5 946 946 5 Enjoy the read? Reward the writer. Beta Your tip will go to Md Kamaruzzaman through a third-party platform of their choice, letting them know you appreciate their story. Get an email whenever Md Kamaruzzaman publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Brayden Gerrard Jan 25, 2019 Member-only The Evolution Of The US Electric Grid Over the past 15 years, the US electric grid has undergone some massive shifts (mostly for the better). Renewable energy and natural gas are up, while coal and GHG emissions are down. Here we can see how the US grid has evolved in recent years: 4 min read 4 min read Share your ideas with millions of readers. zgr Gen Jan 25, 2019 Member-only Notes on Artificial Intelligence, Machine Learning and Deep Learning for curious people AI has been the most intriguing topic of 2018 according to McKinsey. It is mentioned as the key enabler now at the #1 and #3 spot of Gartner Top 10 Strategic Technology Trends for 2019. AI became a catch-all term that refers to any computer program that automatically does something 22 min read 22 min read Felix Kuestahler Jan 25, 2019 Member-only Python Tutorial: Twitter Account Retrieval of Swiss Government Members Start your journey into social media analysis of politicians Click on the link above the Title for a list of all tutorial articles. This is the first article. The tutorial will show you how to extract a list of tweeting Swiss Government Members via the Twitter API. The extracted data will be put into a Panda Dataframe and then 7 min read 7 min read Tobias Hill Jan 25, 2019 How I got 1% better accuracy by data augmentation It is hard to let go of things you put love into. And yes, I really thought I was done after the 5:th article in this series but then the accuracy/error metrics from the MNIST example started haunting me and I was quite sure that I could improve on 4 min read 4 min read Partha Deka Jan 25, 2019 Empowering a citizen data scientist for hardware design & manufacturing Improving productivity of a hardware design and manufacturing professional with an advanced AI tool Authors: Partha Deka and Rohit Mittal What is a citizen data scientist? Expert data scientists rely on custom coding to make sense out of data. The use case could be data cleansing, data imputation, creating segments, finding patterns in the data, building 4 min read 4 min read Md Kamaruzzaman Enterprise Architect | Certified AWS/AZURE/GCP Architect | Full-stack | Cloud | Big Data | Follow Me On Twitter: https://twitter.com/KamaruzzMd More from Medium Yash Prakash in This Code 17 Golang Packages You Should Know Matt Welsh Using Rust at a startup: A cautionary tale Fredy Sandoval Which Programming language is faster? Golang Company Comparison between Golang and JavaBackend Battle of 2022 Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 378,\n",
       "  'url': 'https://towardsdatascience.com/wild-wide-ai-responsible-data-science-16b860e1efe9',\n",
       "  'title': 'Wild Wide AI: responsible data\\xa0science',\n",
       "  'subtitle': 'Who shoots first\\u200a—\\u200athe new race or the\\xa0human?',\n",
       "  'claps': 12,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 9,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 5.927439252391845e-06,\n",
       "  'text': \"Towards Data Science Mar 30 2019 Listen Save Wild Wide AI responsible data science shoot first new race human Data Science good thing u improves life make thing efficient effective lead better experience however miss-steps data-driven analysis ha already exhibited example data science tool intentionally unintentionally misused Data Science impartial often claimed data science algorithmic therefore biased yet saw example traditional evil discrimination exhibit data science ecosystem Bias inherited data process propelled amplified Transparency idea mindset set mechanism help prevent discrimination enable public debate establish trust make data science interact society way decision ha environment trust participant public Technology alone wont solve issue User engagement policy effort important Data responsibility Aspects responsibility data science ecosystem include fairness transparency diversity data protection area responsible data science new already edge top machine learning conference difficult interesting relevant problem Fairness Philosophers lawyer sociologist asking question many year data science context usually solve task predictive analytics predicting future performance behavior based past present observation dataset Statistical bias occurs model used solve task fit data well biased model somehow imprecise doe summarize data correctly Societal bias happens data model doe represent world correctly example occurs data representative case used data police going neighborhood use information crime particular neighborhood Societal bias also caused define world world trying impact predictive analytics world determine world like discrimination legal system two concept defining discrimination talk discrimination using term could uncomfortable racism gender sexual orientation Political correctness extreme sense ha place debate responsible data science able name concept able talk talk concept take corrective action Technical definition fairness Lets consider vendor assigning outcome member population basic case binary classification Positive outcome may offered employment accepted school offered loan offered discount Negative outcome may denied employment rejected school denied loan offered discount worry fairness outcome assigned member population Lets assume 40 got positive outcome sub-population however may treated differently process Lets assume know ahead time sub-population example red haired people Thus divide population two group people red hair people without red hair example observe 40 population got positive outcome 20 red haired received positive outcome 60 received positive outcome according definition observe disparate impact group red haired individual Another way denote situation statistical parity fails baseline definition fairness without conditioning quite sophistication using assessment real-life example court basic definition fairness written many law around globe dictate demographic individual receiving outcome demographic underlying population Assessing disparate impact vendor could say actually intend look hair color happens sensitive attribute dataset Instead vendor would say decided give positive outcome people whose hair long vendor denying accusation saying discriminating based hair color thing vendor ha adversely impacted red haired people intention care effect sub-population word blinding legal ethical excuse Removing hair color vendor process outcome assignment doe prevent discrimination occurring Disparate impact legally assessed impact intention Mitigating disparate impact detect violation statistical parity may want mitigate environment number positive outcome assign swap outcome take positive outcome somebody not-red haired group give someone else red haired group everyone agree swapping outcome individual used get positive outcome would stop getting would lead individual fairness stipulates two individual similar within particular task receive similar outcome tension group individual fairness easy resolve Individual v group fairness example individual fairness group fairness wa taken supreme court appears Ricci v. DeStefano case 2009 Firefighters took test promotion department threw test result none black firefighter scored high enough promoted fire department wa afraid could sued discrimination disparate impact admit result promote black firefighter lawsuit wa brought firefighter would eligible promotion werent promoted result wa individual fairness argument disparate treatment argument argued race wa used negatively impact case wa ruled favor white firefighter favor individual fairness Individual fairness equality everybody get box reach tree Group fairness equity view everybody get many box need able reach tree Equity cost society ha invest two intrinsically different world view logically decide one better two different point view isnt better one go back believe world world truth going somewhere middle important understand kind mitigation consistent kind belief system Formal definition fairness Friedler et al tease difference belief fairness mechanism logically follow belief paper 2016 construct space intrinsically state world made thing directly measure intelligence grit propensity commit crime risk-adverseness however want measure intelligence grit decide admit college want know propensity person recommit crime risk-adverseness justice raw property exhibited directly accessible Instead look observed space proxy greater lesser degree aligned property want measure intelligence proxy would SAT score grit would measured high-school GPA propensity commit crime family history risk-adverseness age decision space made would like decide performance college recidivism Fairness defined mapping construct space decision space via observe space Individual fairness equality belief observed space faithfully represents construct space example high-school GPA good measure grit Therefore mapping construct decision space ha low distortion Group fairness equity however say systematic distortion caused structural bias society bias going construct space observed space Furthermore distortion aligns group structure membership protected group society word society systematically discriminates continued References lecture responsible data science Harvard University Prof. Julia Stoyanovich New York University selected chapter Age Surveillance Capitalism book Shoshana Zuboff thought worry AI post Franois Chollet 10 10 10 Get email whenever Michel publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Kevin Luk Mar 30 2019 Member-only library load image Python difference Summarization Comparison .imread face computer vision project first need load image preprocessing various library perform imread want consolidate popular library loading image difference article go Libraries loading 3 min read 3 min read Share idea million reader Abhishek Mukherjee Mar 30 2019 Trust interpretability machine learning machine learning model always need interpretable Given choice interpretable model inaccurate non-interpretable model accurate wouldnt rather choose non-interpretable accurate model word reason sacrificing accuracy altar interpretability 9 min read 9 min read Christine Calo Mar 30 2019 Microsoft Introduction AI Part 1 Machine Learning bit like wanted learn Artificial Intelligence although felt little intimidated math involved Maybe thought concept difficult understand would depth recently completed Microsoft Introduction AI course 17 min read 17 min read Okoh Anita Mar 30 2019 Repetition Songs Python Tutorial One Ed Sheeran song case study Everyone ha heard song know song sound like carelessly say everyone define song word benefit doubt song according Wikipedia single work music typically intended sung 8 min read 8 min read Alex Blyakhman Mar 30 2019 Member-only Getting Started Google BigQuerys Machine Learning Titanic Dataset still Beta BigQuery ML ha available since mid last year however didnt get around working Google cloud-based Machine Learning offering recently non-data scientist first impression whats like ability run ML model 12 min read 12 min read Michel Kana Ph.D Husband Dad Founder Immersively.care Top Medium Writer 20 year AI Expert Harvard Empowering human-centered organization high-tech Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Akshad Singi Better Humans 4 Unsexy One-Minute Habits Save 30+ Hours Every Week Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Luay Rahil ILLUMINATION Elon Musk Eliminated Remote Work Working Home `` n't Work '' Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Mar 30, 2019 Listen Save Wild Wide AI: responsible data science Who shoots first the new race or the human? Data Science can do good things for us: it improves life, it makes things more efficient, more effective and leads to a better experience. There are however some miss-steps that data-driven analysis has already exhibited. Here are few examples where data science tools were intentionally or unintentionally misused: Is Data Science impartial? It is often claimed that data science is algorithmic and therefore cannot be biased. And yet, we saw examples above where all traditional evils of discrimination exhibit themselves in the data science ecosystem. Bias is inherited both in the data and in the process, is propelled and amplified. Transparency is an idea, a mindset, a set of mechanisms that can help prevent discrimination, enable public debate and establish trust. When we make data science, we interact with society. The way we do decisions has to be in an environment where we have trust from the participants, from the public. Technology alone wont solve the issue. User engagement, policy efforts are important. Data responsibility Aspects of responsibility in the data science ecosystem include: fairness, transparency, diversity and data protection. The area of responsible data science is very new but is already at the edge of all the top machine learning conferences because these are difficult but interesting and relevant problems. What is Fairness? Philosophers, lawyers, sociologists have been asking this question for many years. In the data science context we usually solve the task of predictive analytics, predicting future performance or behavior based on some past or present observation (dataset). Statistical bias occurs when models used to solve such tasks do not fit the data very well. A biased model is somehow imprecise and does not summarize the data correctly. Societal bias happens when the data or the model does not represent the world correctly. An example occurs when the data is not representative. This is the case if we only used the data for police going to the SAME neighborhood over and over, and we use this information only about crime from those particular neighborhoods. Societal bias can also be caused by how we define world. Is it the world as it is that we are trying to impact with predictive analytics or the world as it should be? Who should determine what the world should be like? What is discrimination? In most legal systems, there are two concepts defining discrimination: When we talk about discrimination, we are using terms which could be uncomfortable, such as racism, gender, sexual orientation. Political correctness in the extreme sense has no place in these debates about responsible data science. We have to be able to name concepts to be able to talk about them. Once we can talk about those concepts, we can take corrective action. Technical definition of fairness Lets consider vendors who are assigning outcomes to members of a population. This is the most basic case, a binary classification. Positive outcomes may be: offered employment, accepted to school, offered a loan, offered a discount. Negative outcomes may be: denied employment, rejected from school, denied a loan, not offered a discount. What we worry about in fairness is how outcome is assigned to members of a population. Lets assume that 40% got the positive outcome. Some sub-population however may be treated differently by this process. Lets assume that we know ahead of time what the sub-population is, for example red haired people. Thus we can divide our population into two groups: people with red hair, and people without red hair. In our example we observe that while 40% of the population got the positive outcome, only 20% of red haired received the positive outcome. 60% of the other received the positive outcome. Here, according to some definition, we observe disparate impact on the group of red haired individuals. Another way to denote this situation is that statistical parity fails. This is a baseline definition of fairness without conditioning. There is quite some sophistication about using such assessment in real-life, for example in courts. This basic definition of fairness, written into many laws around the globe, dictates that demographics of the individuals receiving any outcome are the same as demographics of the underlying population. Assessing disparate impact The vendor could say that he actually did not intend or did not look at all at hair color, which happens to be the sensitive attribute in the dataset. Instead the vendor would say that he decided to give the positive outcome to people whose hair is long. The vendor is denying the accusation and saying that he is not discriminating based on hair color. The thing is that the vendor has adversely impacted red haired people. It is not the intention that we care about, but the effect on the sub-population. In other words, blinding is not a legal or ethical excuse. Removing hair color from vendors process on outcome assignment does not prevent discrimination from occurring. Disparate impact is legally assessed on the impact, not on the intention. Mitigating disparate impact If we detect a violation of statistical parity, we may want to mitigate. In an environment in which we have a number of positive outcomes which we can assign, we have to swap some outcomes. We have to take a positive outcome from somebody in the not-red haired group and give it to someone else in the red haired group. Not everyone will agree with swapping outcomes. An individual who used to get the positive outcome would stop getting it any more. This would lead to individual fairness. It stipulates that any two individuals who are similar within a particular task should receive similar outcomes. There is a tension between group and individual fairness that is not easy to resolve. Individual vs group fairness An example in which individual fairness and group fairness was taken to the supreme court appears in the Ricci v. DeStefano case in 2009 . Firefighters took a test for promotion, and the department threw out the test results because none of the black firefighters scored high enough to be promoted. The fire department was afraid that they could be sued for discrimination and disparate impact if they were to admit results and not promote any black firefighter. But then the lawsuit was brought by the firefighters who would have been eligible for promotion but who werent promoted as a result of this. There was an individual fairness argument, a disparate treatment argument. They argued that race was used to negatively impact them. This case was ruled in favor of white firefighters, in favor of individual fairness. Individual fairness is equality, everybody gets the same box to reach the tree. Group fairness is the equity view, everybody gets as many boxes as they need to be able to reach the tree. Equity costs more because society has to invest more. These are two intrinsically different world views that we cannot logically decide which one is better. These are just two different points of view, there isnt a better one. They go back to what we believe a world as it is, is a world as it should be. The truth is going to be somewhere in the middle. It is important to understand which kinds of mitigation are consistent with which kinds of belief systems. Formal definition of fairness Friedler et. al. tease out the difference between beliefs about fairness and mechanisms that logically follow from those beliefs in their paper from 2016. The construct space is intrinsically the state of the world. It is made of things we cannot directly measure such as intelligence, grit, propensity to commit crime and risk-adverseness. We however want to measure intelligence and grit when we decide who to admit to college. We want to know the propensity of a person to recommit crime and his risk-adverseness in justice. These are raw properties which are exhibited and not directly accessible. Instead we look at the observed space where there are proxies, which are to a greater or lesser degree aligned with the properties that we want to measure. For intelligence the proxy would be SAT score, grit would be measured by high-school GPA, propensity to commit crime by family history and risk-adverseness by age. The decision space is then made of what we would like to decide: performance in college and recidivism. Fairness is defined here as a mapping from the construct space to the decision space, via the observe space. Individual fairness (equality) believes that the observed space faithfully represents the construct space. For example high-school GPA is a good measure of grit. Therefore the mapping from construct to decision space has low distortion. Group fairness (equity) however says that there is a systematic distortion caused by structural bias, society bias when going from the construct space to observed space. Furthermore this distortion aligns with groups structure, with membership in protected groups in our society. In other words the society systematically discriminates. to be continued References : lecture on responsible data science at Harvard University by Prof. Julia Stoyanovich (New York University) selected chapters from The Age of Surveillance Capitalism book by Shoshana Zuboff thoughts from What worries me about AI post by Franois Chollet. 10 10 10 Get an email whenever Michel publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Kevin Luk Mar 30, 2019 Member-only What libraries can load image in Python and what are their difference? Summarization & Comparison of .imread() When we face computer vision project, first of all we need to load the images before any preprocessing. There are various libraries out there to perform imread() . Here I want to consolidate the popular libraries for loading image and their difference. This article will go through: Libraries for loading 3 min read 3 min read Share your ideas with millions of readers. Abhishek Mukherjee Mar 30, 2019 Trust and interpretability in machine learning Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldnt you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability? 9 min read 9 min read Christine Calo Mar 30, 2019 Microsoft Introduction to AI Part 1 Machine Learning Are you a bit like me and have wanted to learn about Artificial Intelligence although felt a little intimidated by the maths involved? Maybe you thought the concepts were too difficult to understand and you would be out of your depth. I recently completed the Microsoft Introduction to AI course 17 min read 17 min read Okoh Anita Mar 30, 2019 Repetition in Songs: A Python Tutorial One of Ed Sheeran songs as a case study Everyone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung 8 min read 8 min read Alex Blyakhman Mar 30, 2019 Member-only Getting Started with Google BigQuerys Machine Learning Titanic Dataset While still in Beta, BigQuery ML has been available since mid last year; however, I didnt get around to working with this Google cloud-based Machine Learning offering until recently. As a non-data scientist, my first impression whats not to like? After all, the ability to run ML models from 12 min read 12 min read Michel Kana, Ph.D Husband & Dad. Founder @Immersively.care. Top Medium Writer. 20 years in IT. AI Expert @Harvard. Empowering human-centered organizations with high-tech. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Akshad Singi in Better Humans 4 Unsexy One-Minute Habits That Save Me 30+ Hours Every Week Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Luay Rahil in ILLUMINATION Elon Musk Eliminated Remote Work Because Working From Home \"Doesn\\'t Work\" Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4098,\n",
       "  'url': 'https://towardsdatascience.com/exploration-and-visualization-on-each-presidential-candidate-supporters-tweets-in-indonesia-a2b26c180f7e',\n",
       "  'title': 'Exploration and Visualization on each Presidential Candidate Supporter’s Tweets in Indonesia',\n",
       "  'subtitle': '-',\n",
       "  'claps': 98,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 18,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-01-26',\n",
       "  'clap_prop': 4.840742056120007e-05,\n",
       "  'text': \"Towards Data Science Jan 26 2019 Listen Save Exploration Visualization Presidential Candidate Supporters Tweets Indonesia Hand Practice using Plotly visualize finding data following Data Science Process Introduction Hello Welcome article time want write something different Ive written far article ha published implementing Artificial Intelligence especially Deep Learning article want write different topic think topic also important want develop model step done modelling Exploratory Data Analysis EDA Sorry Ive busy preparing new job Even busy still want write medium share knowledge Whenever free time write article little little article also want write telling code also implement real study case data used twitter scraped library case Analysis presidential candidate supporter Indonesia 2018 Even text Indonesian Language try make understand meaning text word data used article based tweet crawled Maybe tweet crawler ha skipped Even say data sample population representative data Ive crawled around 700.000 tweet fun reading statistic go feel free skip technical detail article Oh also include conclusion insight weve found data write analysis explanation visualization create conclusion mind want dont also EDA course write Outline EDA Taken KDNuggets step Data Science creating model Gather Data Process Data Clean Data finally EDA Actually EDA help u better making model quality improves knowing data feel learning algorithm learns data well feel think impossible instance predicted neither learner process EDA done modelling every task Data Science Machine Learning except Pure Reinforcement Learning doesnt data train example Natural Language Processing NLP Sentiment Analysis maybe make better model removing word found EDA Another example Computer Vision CV maybe found model cant learn inability model predict instance data unique noise anomaly hasnt cleaned previously data cleaning step many others benefit EDA modelling Maybe get intuition make model better step read previous article generating Deep Learning Lyric skipped part Sometimes generating lyric text dont want appear REFF example well didnt notice noticed EDA first need know step go Steps First must define Research Questions mean define want search Questions guide next step also need answered next step example Research Question List anything need search question form Typically start pin 5W 1H opinion since gain new insight later increase number question depending finding focusing main Research Question Second need collect data Make sure search dataset according research question already defined several way step search structured data set data set repository Kaggle UCL Repository also search unstructured data scrapping social medium Facebook Twitter using API possible depends research question Often find data need structured format Third Preprocess Clean Data Data Wrangling painful annoying step let head explode step step painful need know data Find pest need cleaned Example pest NLP Typo Informal Word Emoticon Elongated Word Many Hidden Bomb usually remove make visualization model better data structured format need convert structured format sake making cleaning data easier Let tell often go back step many time since often find pest next step Finally EDA Exploratory Data Analysis answer Research Question defined First Steps Usually Visualization communicate result visualize plot data see data printing result Remember result analysis seen result seen others sake UX User Experience need present data visualizing make sure reader become comfortable see result think article seen many people present result cropping screenshot printed result CLI Command Line Interface article EDA Jupyter Notebook since ha beautiful UX also used present data Technology use Step One Research Question Okay formulate question let tell 2019 presidential election Indonesia 2 candidate Joko Widodo Jokowi Maaruf Amin 01 Prabowo Subianto Sandiaga Uno Sandi 02 focus article supporter candidate social medium article focus Twitter Social Media researching googling Ive found hashtags used Jokowis Prabowos Supporter declare support simplicity Ive narrowed hashtags limit year tweet 2018 Thats Lets create research question research question done Actually many question ask sake make article become 1 hour read time cut question want ask twelve Step Two Collect Data step use Twint library article scope data follow scraped 706.208 tweet step Step Three Preprocess Data Okay since article focus EDA make step short possible First read csv panda tweet pd.read_csv tweet.csv encoding=utf-8 make simple formalize word remove stopwords formalize two way first regex substitute known slang word formal word later need dictionary slang word formal word formalized word better way cleaning formalize_rule function use nltk TweetTokenizer Well want try regex thats also implement removing stopword formalize_word apply DataFrame Note step reality come back step several EDAs Sometimes find pest EDA step done move onto last step Step Four Exploratory Data Analysis Since Research Questions want answer answered end Data Science process step Without ado let answer question Wait define function used multiple time defined DataFrame filter use often later also create function output statistic DataFrame statistic plotted Plotly 0 many instance data 1 frequency supporter tweet dataset take attribute shape supporter tweet need show plot Jupyter Lets set plot many way visualize data Since comparable use Pie chart visualize data two minimal component needed plot plotly First data data set data type chart want visualize combine multi kind type chart example visualize Bar chart Pie Chart visualization Second layout layout container visualization customize title legend axis many combine container chart putting go.Figure figure figure ready plotted Plot Analysis Prabowos supporter tweet frequency higher Jokowis supporter tweet 2 top-30 frequency supporter tweet month Since data want plot sequential plot line chart First filter loop month Reverse list Dec Jan Jan Dec plot Plot Analysis Prabowos supporter tweet usually frequency Jokowis supporter tweet tweet peak September 3 seeing largest frequency tweet month word top-30 frequency month supporter tweet First set largest frequency tweet September Sep find highest frequency word using function take tweet posted September plot limit TOP 30 highest frequency Since data sequential Bar chart right choice Word cloud also good visualize word frequency dont want know frequency word Plot Analysis jokowi ha highest frequency September ha around 35k 40k frequency followed Indonesia Orang Person Presiden President Rakyat Citizen Dukung Support Allah God Prabowo others 4 seeing largest frequency tweet month Top-30 word frequency month Jokowi supporter tweet similar RQ Research Question 3 difference need filter Jokowis supporter tweet Plot Analysis jokowi word also highest difference word big ha positive word berbagi sharing tulu sincere bergerak move also ha Allah word 5 Top-30 word frequency month Prabowo supporter tweet similar RQ 4 filter Prabowo supporter tweet Plot Analysis unexpected jokowi frequency higher prabowo highest one indonesia difference word frequency big word notice ulama Muslims Scholar Cleric rezim regime cebong tadpole bad alias jokowis supporter prabowos supporter emak group mother bangsa nation also ha Allah word 6 Top-30 frequency Token accompany prabowo word Jokowi Supporters Tweet month since often plot writing code many time create function reusable filter dataframe according need Plot Analysis Jokowi highest frequency notice word interesting uang money thecebongers tadpole prestasinya achievment survei survey asing foreign country 7 Top-30 frequency Token accompany jokowi word Prabowo Supporters Tweet month filter dataframe according need Plot Analysis Prabowo highest frequency notice word interesting gerakan movement ulama mahasiswa college student rupiah Indonesia currency rezim regime 8 Top-30 frequency Token accompany prabowo word Prabowo Supporters Tweet month filter dataframe according need Analysis sandi ha highest frequency ha big gap word word got attention ulama allah emak gerakan ijtima ulamas/muslim schoolarss decision jokowi also second highest frequency 9 top-30 frequency Token accompany jokowi word Jokowi Supporters Tweet month filter dataframe according need Plot Analysis prabowo 20 highest frequency different Anyway word got attention blokir blocked pembangunan construction kepemimpinan leadership allah hebat great bergerak move 10 top-30 frequency Hashtags Prabowo Supporters Tweet Since hashtags column string format need cast type list using eval join content list call previous function see statistic data limited September Plot Analysis Hashtags eye set 2019tetapantipki 2019 Stay Anti-communism mahasiswabergerak College student move rupiahlongsor jokowilengser Rupiah Fall Jokowi stepped jokowi2periode Jokowi two Periods last hashtags hashtags Jokowis supporter hashtags mostly talk changing president negative thing Jokowi 11 Top-30 frequency Hashtags Jokowi Supporters Tweet really similar RQ 10 Plot Analysis Hashtags eye set indonesiamaju Advanced Indonesia jokowimembangunindonesia Jokowi Build Indonesia kerjanyata Visible Work diasibukkerja Hes busy working Mostly hashtags keeping Jokowi president positive thing Jokowi 2019gantipresiden hashtags Prabowos supporter 12 mean length char Jokowi Prabowo Supporters Tweet Line chart Since data comparable visualize one figure make default line chart also need new function done let plot make list contains 2 line chart show one figure :-1 Means reverse month default start December January Plot Analysis Jokowis mean char length tend rise peak November Wheras Prabowos mean char length tend rise August keep tend fall month 13 mean length word Jokowi Prabowo Supporters Tweet last RQ need new function Thats let plot Plot Analysis expected ha almost got similar result answer RQ 12 Conclusion answered Research Question defined many interesting point answer kind word top-30 frequency word president supporter tweet supporter talk president candidate president candidate opponent wont dive deeper statistic make article longer EDA notice thing cleaned make data better example tweet contains hashtag Jokowis support Prabowos supporter one tweet tweet removed dataset move back cleaning step EDA Afterwords Thats folk article mostly EDA Actually RQs Ive answered sake shorting article select must wondering result finding need dive deeper exploring data share dataset many reader want many task done dataset Topic Modelling Sentiment Analysis Detecting Anomaly detecting buzzer many interesting task anyone want write think writing welcome feedback improve article Im process learning writing really need feedback become better make sure give feedback proper manner several next article Ill go back NLP Computer Vision maybe topic Repository TBD Source Data Science Process Springboard data student often ask u question like `` doe Data Scientist '' `` doe day www.kdnuggets.com 101 101 101 Get email whenever Haryo Akbarianto Wibowo publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Henry Feng Jan 25 2019 User guide First Data Product Medium Post Metric Displayer Know Medium Post Better Data Origin regular writer Medium well data geek busy year 2018 Id like reflect achieved Medium blog Furthermore based performance 2018 plan make aggressive writing plan year 2019 14 min read 14 min read Share idea million reader Md Kamaruzzaman Jan 25 2019 Member-only Back metal Top 3 Programming language develop Big Data framework C++ Rust Go Java Data Intensive framework previous blog post Programming language rule Data Intensive Big Data Fast Data framework briefly discussed popular Big Data framework showed Java de-facto programming language Data Intensive framework Java significant advantage e.g 12 min read 12 min read Brayden Gerrard Jan 25 2019 Member-only Evolution US Electric Grid past 15 year US electric grid ha undergone massive shift mostly better Renewable energy natural gas coal GHG emission see US grid ha evolved recent year 4 min read 4 min read zgr Gen Jan 25 2019 Member-only Notes Artificial Intelligence Machine Learning Deep Learning curious people AI ha intriguing topic 2018 according McKinsey mentioned key enabler 1 3 spot Gartner Top 10 Strategic Technology Trends 2019 AI became catch-all term refers computer program automatically doe something 22 min read 22 min read Felix Kuestahler Jan 25 2019 Member-only Python Tutorial Twitter Account Retrieval Swiss Government Members Start journey social medium analysis politician Click link Title list tutorial article first article tutorial show extract list tweeting Swiss Government Members via Twitter API extracted data put Panda Dataframe 7 min read 7 min read Haryo Akbarianto Wibowo Mad AI Enthusiast write mostly Artificial Intelligence Self Development also love read Engineering Psychology Startup Love share Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Jan 26, 2019 Listen Save Exploration and Visualization on each Presidential Candidate Supporters Tweets in Indonesia Hand on Practice by using Plotly to visualize the finding on the data by following Data Science Process. Introduction Hello! Welcome to this article. This time, I want to write about something different from what Ive written so far. My articles that has been published are about implementing an Artificial Intelligence, especially about Deep Learning. In this article, I want to write about different topic. I think this topic is also important if we want to develop a model. This step should be done before doing modelling. Which is Exploratory Data Analysis (EDA) . Sorry, Ive been busy on preparing my new job . Even I am busy, I still want to write medium and share my knowledge to all of you . Whenever I have a free time, I write this article little by little. In this article, I also want to write by not only telling how to code it, but also implement it to a real study case. The data that I used is from twitter which I scraped with library. The case is about Analysis on each presidential candidates supporter in Indonesia on 2018 . Even the text is in Indonesian Language, I will try to make you understand what is the meaning of the text and word. The data that is used in this article is based on the tweet that I crawled . Maybe there are some tweets that the crawler has skipped. Even so, you can say that the data sample of the population which should be representative. The data that Ive crawled is around 700.000 tweets. If you are here to have fun on reading the statistic, go on and feel free to skip the technical detail on this article. Oh, I also will not include any conclusion on the insight that weve found in the data. I will only write the analysis and the explanation of the visualization. You can create your conclusion on your mind and If you want more, why dont you also do the EDA ? And of course write it . Outline Why do EDA? Taken from KDNuggets, the step on doing a Data Science before creating a model are Gather Data, Process the Data, Clean Data, and finally EDA. Actually, doing EDA can help us better on making the models quality improves. By knowing the data, we can feel how our learning algorithm learns our data. And well, if you feel that you think its impossible that some instance can be predicted, neither do our learner. The process EDA should be done before modelling in every tasks of Data Science and Machine Learning (except Pure Reinforcement Learning which doesnt have any data to train). For example, in Natural Language Processing (NLP), when doing Sentiment Analysis, maybe we can make a better model by removing some words which can be found when doing EDA. Another example, in Computer Vision (CV), maybe we can found that the model cant learn because of the inability of our model to predict some instances because our data have an unique noise or some anomalies, which hasnt been cleaned previously in the data cleaning step. There are many others benefit when doing EDA before modelling. Maybe you can get an intuition that can make your model better. which is why we should do this step. If you read my previous article on generating Deep Learning Lyric , I skipped this part. Sometimes in generating the lyric, there are some text that I dont want it to appear (REFF for example). And well, I didnt notice and should have noticed it if I did the EDA first. Before we do it, we need to know what are the steps. Here we go! Steps First , we must define what are the Research Questions ? That means to define what we want to search. These Questions will be the guide on what we will do on the next step. They will also need to be answered in the next step. There are some examples for a Research Question: List anything you need to search in question form. Typically, you can start to pin down to 5W 1H (What, When, Where, Why, Who, How). In my opinion, since we can gain new insights later, we can increase the number of the questions depending on our finding while focusing on our main Research Question. Second , We need to collect the data. Make sure that we search the dataset according to the research question that we have already defined. There are several ways to do this steps. We can search any structured data set in a data set repository such as Kaggle and UCL Repository. Or we can also search any unstructured data by scrapping some social media such as Facebook and Twitter using their API if possible. It depends on your research question. Often, you will not find the data that you need in a structured format out there . Third , Preprocess, Clean the Data, and Data Wrangling ! This will be the painful and annoying steps to do . Do not let your head explode on doing this step. Why this step is painful? You need to know your data. Find if there are any pests out there need to be cleaned. Example of pests in NLP are Typo, Informal Word, Emoticon, Elongated Word, any Many Hidden Bomb There that usually if we remove it, it will make our visualization and model better ! Before doing any of that, if the data is not the structured format, we need to convert it to the structured format for the sake of making cleaning data easier. Let me tell you, we will often go back to this steps many times since we often find the pests in the next step. Finally , we do EDA (Exploratory Data Analysis) . This is where we will answer some of the Research Question defined in the First Steps. Usually we will do some Visualization here to communicate our result. Why we visualize or plot our data when we can see the data by printing the result ? Remember that the result of the analysis will not be seen only for you. The result will be seen by others. For the sake of UX (User Experience) , We need to present the data by visualizing it to make sure the readers become comfortable to see the results. I do not think my article will be seen by many people if I present the result here by cropping the screenshot of the printed result in CLI (Command Line Interface). In this article, we will do EDA in Jupyter Notebook since it has beautiful UX and also can be used to present our data. Technology We will use: Step One : Research Question Okay, before we formulate the questions, let me tell you that in 2019, there will be a presidential election in Indonesia. There are 2 candidates. They are Joko Widodo (Jokowi) Maaruf Amin (NO 01) and Prabowo Subianto Sandiaga Uno (Sandi) (NO 02). What we will focus on in this article is how are the supporter of each candidates in social media. In this article, we will focus on Twitter Social Media. By researching and googling, Ive found some hashtags used by Jokowis and Prabowos Supporter to declare their support. For the simplicity, Ive narrowed down their hashtags by: We will limit the year of the tweet only on 2018. Thats it, Lets create the research question. Here are my research questions : Were done ! Actually there are too many questions that we can ask. For the sake not to make this article become 1 hour read time, we will cut the question we want to ask to twelve. Step Two : Collect the Data For this steps, we will use Twint as our library. For this article, we will scope our data as follow: I scraped 706.208 tweets in this step Step Three : Preprocess Data Okay, since this article will focus for EDA, we will make this step as short as possible. First, we will read the csv with pandas tweet = pd.read_csv(tweet.csv, encoding=utf-8\\') To make it simple, we will only formalize word and remove the stopwords We will formalize in two ways, first is with regex and then substitute known slang words to formal words. The later will need a dictionary of slang word and its formal word. This is how I formalized the word: There should be a better way to do the cleaning on the formalize_rule function such as use nltk TweetTokenizer . Well, I want to try regex and thats it. I also implement on removing stopword on the formalize_word . We apply it into our DataFrame: Note that in this step, in reality, we will come back to this step after doing several EDAs. Sometimes, we will find the pests in EDA step. We are done, move onto last step! Step Four: Exploratory Data Analysis Since all of the Research Questions that we want to answer can be answered here, we will end the Data Science process in this step. Without further ado, lets answer all of the questions! Wait, before we do that. We should define some functions that will be used multiple times. We have defined the DataFrame filter that we will use often later on. We also create some functions which output the statistic in the DataFrame. The statistic will be plotted with Plotly. 0. How many instances in the data? 1. How is the frequency of each supporters tweets in the dataset? How to do it? We take the attribute shape of each supporters tweets. We need to do this to show the plot in Jupyter: Lets set up the plot There are many ways to visualize this data. Since each of them is comparable, we can use Pie chart to visualize the data. There are two minimal components needed to plot on plotly. First is the data. The data is a set of data with the type of chart that we want to visualize. We can combine multi kinds type of chart here. For example, you can visualize a Bar chart with a Pie Chart in the visualization. Second is the layout. The layout is the container of the visualization. This is where we can customize the title, legend, axis, and many more. Then we combine the container and the charts by putting it into go.Figure (a figure). The figure is ready to be plotted. Plot It! Analysis Prabowos supporter tweets frequency is higher than Jokowis supporter tweets 2. How is the top-30 frequency of each supporters tweets each months? How to do it? Since the data that we want to plot is sequential. We can plot in line chart. First, we filter and loop for each months Reverse the list (from Dec Jan to Jan Dec) Then plot it Plot It! Analysis The Prabowos supporter tweets usually have more frequency than Jokowis supporter tweets. Their tweets are at its peak in September. 3. By seeing the largest frequency tweet by month, How is the words top-30 frequency in that month on each supporters tweet? How to do it? First, we will set that the largest frequency tweet is in September. i = Sep Then, we find the highest frequency word by using above functions. We will take only tweets posted in September and plot it. We will limit it into TOP 30 highest frequency. Since the data is not sequential, Bar chart is the right choice here. Word cloud also good in how we visualize the word frequency if we dont want to know the frequency of each words. Plot It! Analysis jokowi has the highest frequency in September. It has around 35k 40k frequency. It is followed by Indonesia, Orang (Person), Presiden (President), Rakyat (Citizen), Dukung (Support), Allah (God), Prabowo, and the others. 4. By seeing the largest frequency tweet by month, How is the Top-30 words frequency in that month on Jokowi supporters tweet? How to do it? Its similar on how we do it on RQ (Research Question) 3. The difference is that we need to filter the Jokowis supporter tweet. Plot it! Analysis The jokowi word is also the highest here and the difference with other words is big. It has positive words such as berbagi (sharing), tulus (sincere) and bergerak (move). It also has Allah word there. 5. How is the Top-30 words frequency in that month on Prabowo supporters tweet How to do it? Again, it is similar on doing RQ 4. We will filter to Prabowo supporters tweet. Plot it! Analysis Its unexpected that the jokowi frequency is higher than prabowo. The highest one is indonesia. The difference of each words frequency is not too big. The words that we should notice are ulama (Muslims Scholar or Cleric), rezim (regime), cebong (tadpole, the bad alias for jokowis supporter by prabowos supporter) , emak (group of mothers) and bangsa (nation). It also has Allah word there. 6. How is the Top-30 frequency of Token accompany prabowo word in Jokowi Supporters Tweet on that month? Before we do it, since we often plot by writing the code many times, we should create a function that is reusable. After that, we will filter the dataframe according to what we need. Plot it! Analysis Jokowi is the highest frequency here. We will notice some words that is interesting, which is uang (money), thecebongers (the tadpole), prestasinya (the achievment), survei (survey), and asing (foreign countries) 7. How is the Top-30 frequency of Token that accompany jokowi word in Prabowo Supporters Tweet on that month We will filter the dataframe according to what we need. Plot it! Analysis Prabowo is the highest frequency here. We will notice some words that is interesting, which is gerakan (movement), ulama, mahasiswa (college student), rupiah (Indonesia currency), and rezim (regime). 8. How is the Top-30 frequency of Token accompany prabowo word in Prabowo Supporters Tweet on that month? We will filter the dataframe according to what we need. Analysis sandi has the highest frequency here. It has big gap to other words. the words that got my attention are ulama, allah, emak, gerakan, and ijtima (ulamas/muslim schoolarss decision). jokowi is also the second highest frequency here. 9. How is the top-30 frequency of Token accompany jokowi word in Jokowi Supporters Tweet on that month? How we do it? We will filter the dataframe according to what we need: Plot it! Analysis prabowo is not in the 20 highest frequency here. Its different from the above. Anyway, word that got my attention are blokir (blocked), pembangunan (construction), kepemimpinan (leadership), allah, hebat (great), and bergerak (move) 10. How is the top-30 frequency of Hashtags in Prabowo Supporters Tweet? How to do it? Since the hashtags column is in string format, we need to cast the type into list by using eval . After that, we join the content of the list by and call our previous function. We will see the statistic of the data not limited on September. Plot it! Analysis Hashtags that my eyes are set on are 2019tetapantipki (2019 Will Stay Anti-communism) , mahasiswabergerak (College student move), rupiahlongsor jokowilengser (Rupiah Fall Jokowi stepped down), and jokowi2periode (Jokowi two Periods). The last hashtags should be the hashtags for Jokowis supporter. The hashtags mostly talks about changing the president and negative things about Jokowi. 11. How is the Top-30 frequency of Hashtags in Jokowi Supporters Tweet? How to do it? Its really similar to RQ 10. Plot it! Analysis Hashtags that my eyes are set on are indonesiamaju (Advanced Indonesia), jokowimembangunindonesia (Jokowi Build Indonesia), kerjanyata (Visible Work), diasibukkerja (Hes busy working). Mostly, the hashtags are about keeping Jokowi as the president and positive things about Jokowi. And again, there is a 2019gantipresiden that should be the hashtags for Prabowos supporter. 12. How is the mean of length char in Jokowi and Prabowo Supporters Tweet? How we do it? We will do it in Line chart. Since these data are comparable, we will visualize them in one figure. We will make our default line chart And we also need a new functions: We are done, lets plot them: We will make a list that contains 2 line charts and show it in one figure. The [::-1] Means that we will reverse the month. The default will start from December to January. Plot it! Analysis  Jokowis mean of chars length is tend to rise and its at its peak at November. Wheras Prabowos mean of chars length tend to rise until August and keep tend to fall after that month 13. How is the mean of length word in Jokowi and Prabowo Supporters Tweet? How to do it? Our last RQ. Its the same as above but we need new function: Thats all lets plot it: Plot it! Analysis As expected, it has almost got the similar result with the answer of RQ 12. Conclusion We have answered all the Research Question that we have defined. There are many interesting points from the answers. Such as the kinds of word in the top-30 frequency word of each presidents supporter tweets and how each supporter talk about their president candidate or their president candidates opponent. I wont dive deeper on the statistic here as it will make this article longer. After we do EDA, we should notice that there are some thing that should be cleaned to make the data better. For example, there are some tweets that contains the hashtag of Jokowis support and Prabowos supporter in one tweet. These tweets should be removed from the dataset. We should move back to the cleaning step and do EDA again. Afterwords Thats it folks for my article mostly about EDA. Actually, I have more RQs that Ive answered. But for the sake of shorting this article, I select a few of them. You must be wondering about some of the result of our finding. For that, you need to dive deeper on exploring the data. I will share the dataset if there are many readers who want it. There are many tasks that can be done for that dataset such as Topic Modelling, Sentiment Analysis, Detecting Anomaly (Such as detecting buzzer), And many interesting tasks. If anyone want me to write about it, I will think about writing it. I welcome any feedback that can improve myself and this article. Im in the process of learning on writing. I really need a feedback to become better. Just make sure to give feedback in a proper manner . For my several next articles, Ill go back to NLP or Computer Vision (maybe) topics. Repository TBD Source The Data Science Process At Springboard, our data students often ask us questions like \"what does a Data Scientist do?\". Or \"what does a day in www.kdnuggets.com 101 101 101 Get an email whenever Haryo Akbarianto Wibowo publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Henry Feng Jan 25, 2019 User guide to My First Data Product: Medium Post Metric Displayer Know Your Medium Post Better with Data Origin As a regular writer on Medium as well as a data geek, after the busy year of 2018, Id like to reflect what I have achieved on my Medium blog. Furthermore, based on the performance in 2018, I plan to make more aggressive writing plan in the year 2019. I 14 min read 14 min read Share your ideas with millions of readers. Md Kamaruzzaman Jan 25, 2019 Member-only Back to the metal: Top 3 Programming language to develop Big Data frameworks C++, Rust, Go over Java for Data Intensive frameworks In a previous blog post: Programming language that rules the Data Intensive (Big Data, Fast Data) frameworks, I have briefly discussed some of the most popular Big Data frameworks and showed that Java is the de-facto programming language in Data Intensive frameworks. Java had significant advantages (e.g. 12 min read 12 min read Brayden Gerrard Jan 25, 2019 Member-only The Evolution Of The US Electric Grid Over the past 15 years, the US electric grid has undergone some massive shifts (mostly for the better). Renewable energy and natural gas are up, while coal and GHG emissions are down. Here we can see how the US grid has evolved in recent years: 4 min read 4 min read zgr Gen Jan 25, 2019 Member-only Notes on Artificial Intelligence, Machine Learning and Deep Learning for curious people AI has been the most intriguing topic of 2018 according to McKinsey. It is mentioned as the key enabler now at the #1 and #3 spot of Gartner Top 10 Strategic Technology Trends for 2019. AI became a catch-all term that refers to any computer program that automatically does something 22 min read 22 min read Felix Kuestahler Jan 25, 2019 Member-only Python Tutorial: Twitter Account Retrieval of Swiss Government Members Start your journey into social media analysis of politicians Click on the link above the Title for a list of all tutorial articles. This is the first article. The tutorial will show you how to extract a list of tweeting Swiss Government Members via the Twitter API. The extracted data will be put into a Panda Dataframe and then 7 min read 7 min read Haryo Akbarianto Wibowo Mad AI Enthusiast. I write mostly about Artificial Intelligence and Self Development. I also love to read Engineering, Psychology and Startup. Love to share! More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 359,\n",
       "  'url': 'https://towardsdatascience.com/identifying-the-sources-of-winter-air-pollution-in-bangkok-part-ii-72539f9b767a',\n",
       "  'title': 'Identifying the Sources of Winter Air Pollution in Bangkok Part\\xa0II',\n",
       "  'subtitle': '-',\n",
       "  'claps': 738,\n",
       "  'responses': 10.0,\n",
       "  'reading_time': 9,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 0.0003645375140220985,\n",
       "  'text': \"Towards Data Science Mar 30 2019 Listen Save Identifying Sources Winter Air Pollution Bangkok Part II previous blog looked winter air pollution Bangkok main source pollution come particle smaller 2.5 micrometer PM 2.5 particle particle smaller width human hair easily enter body even making way blood Last week March 17 2019 many province northern part Thailand worst Air Quality Index AQI world due particle pollution far long term solution ha proposed source PM 2.5 particle pollution ha clearly pinpointed notebook identify source high PM 2.5 particle Bangkok machine learning model code found GitHub page High PM2.5 Culprits three major theory regarding source air pollution Bangkok 1 temperature inversion effect cold air along pollution trapped close surface Earth theory wa proposed government beginning 2019 winter season government blamed emission old diesel engine pollution 2 Agricultural burning either locally surrounding province winter lot open agricultural burning occurs throughout country official tried tackle air pollution problem reducing open agricultural burning 3 Pollution province country NGOs blamed pollution near power plant analysis procedure follows Build machine learning model ML predict air pollution level Bangkok using environmental factor weather traffic index fire map Include date-time feature local hour weekday versus weekend model capture effect human activity Identify dominant source pollution using feature importance provided ML model source pollution local AQI depend factor weather pattern wind speed humidity average temperature local traffic hour day pollution agricultural burning AQI depend active fire time lag account geographical separation Fire activity included based distance Bangkok hand pollution correlated fire map model put weight weather pattern wind direction wind speed list feature considered data source Let first walk feature included model Agricultural Burning Major Problem Farmers Southeast Asia pick January March burning season north northeastern province Thailand burning activity large enough make province among polluted place world time Bangkok one might argue region heavily industrial rather agricultural may affected much agricultural burning case tiny size PM 2.5 particle remain suspended atmosphere prolonged period travel long distance weather data average wind speed 10 km/hour reported PM 2.5 level rolling average 24 hour rough estimate current PM 2.5 reading may source far 240 km away picture show fire map measured NASAs satellite indicative agricultural burning Jan 8 2018 Feb 8 2018 yellow circle indicates area within 240 km Bangkok number fire Jan 8 ha acceptable level pollution much lower number fire Feb 8 ha unhealthy level pollution fact fire pattern closely aligns PM 2.5 pattern Weather Patterns temperature inversion effect often occurs winter temperature cooler near ground hotter air top trap cool air flowing stagnant atmospheric condition allows PM 2.5 particle remain suspended air longer hand higher humidity rain help remove particle atmosphere one reason past air pollution wa high government ha sprayed water air Unfortunately mitigation doe appear effective since volume water minuscule compared actual rain much influence doe weather pattern air pollution Lets compare weather winter versus season Temperature wind speed humidity lower winter large amount let look relationship PM 2.5 level Higher temperature disrupts temperature inversion effect wind speed humidity negative correlation pollution level windy day pollution clearly better median distribution PM 2.5 level lower windy day compared day without wind fact pollution level also depends wind direction seen plot selected four major wind direction simplicity day wind come south pollution level lower likely Thai gulf south Bangkok clean ocean wind improves air quality Wind three direction pas overland However wind better stagnant atmospheric condition calm day shift median PM 2.5 level smaller rainy day day rain fewer rainy day winter season data somewhat noisy difference observed cumulative density function Traffic Index One source PM 2.5 particle car engine exhaust campaigning public transportation usage general good environment effectiveness toward reducing PM 2.5 pollution unclear seen PM 2.5 level related time day pollution lower around 3 pm remains high night time plotted traffic data relationship pollution level noisy doe seem strong correlation Including time day weekday versus weekend information model might make relationship clear Autoregression process current PM 2.5 value also depend previous value partial autocorrelation plot show strong correlation 1 hr time lag mean PM 2.5 level autoregression process Thus include 24 hour average value model restriction model allowed see previous value future prediction importance feature directly related long particle stay atmosphere Machine Learning Model picture show dedrogram input feature calculated Spearman correlation dendrogram help identify redundant feature removed model number fire within various distance level PM 2.5 closely related feature away ended using feature model identify major contribution pollution used random forest regression fit model simplicity ease interpretation hyper-parameter tuning 25 data wa allocated validation set model wa retrained using entire dataset model achieves 0.99 R-squared training set Since purpose study understand source air pollution past focused training set plot rank importance contributing factor importance calculated decrease R-squared value upon permuting column re-normalizing sum column expected previous pollution level important predictor followed number fire closest furthest number fire far away 720 km ha influence air quality local humidity traffic even rain hour day important predictor traffic index Among weather feature humidity important feature influence feature illustrated using tree interpreter data Jan 13 2019 8 96 PM 2.5 level start average value 26 PM 2.5 level previous hour wa 62 thus model add value 20 150 fire within 240 km radius thus model add 10 pollution level value 56 1649 fire 240-480 km 896 fire 480-720 km model add value 9 8 respectively low wind speed morning rush hour 8 add 8 model six top factor account 81 total 96 predicted PM 2.5 level remaining feature right le important thus increase predicted pollution value le good day Feb 2 2019 7 pm PM 2.5 level wa 10 pollution level hour wa low thus model subtracts value 10 still lot fire area model add value 2 wind speed wa high reducing value 2 weather traffic good combination many factor result low predicted PM 2.5 level 10 Conclusions PM 2.5 level ha complex relationship various factor number fire weather pattern traffic analysis confirms suspicion many people agricultural burning root cause PM 2.5 pollution Thailand Burning activity far 720 km away Bangkok area extends Myanmar Laos Cambodia cause air problem Bangkok Solving problem easy require collaborative international effort among Southeast Asian country leave fire map March 17 2019 one worst day ever 772 15 772 772 15 Towards Data Science home data science Medium publication sharing concept idea code Sean McClure Mar 30 2019 Step-by-Step Guide Creating R Python Libraries JupyterLab R Python bread butter today machine learning language R provides powerful statistic quick visualization Python offer intuitive syntax abundant support choice interface today major AI framework article well look step involved creating library 32 min read 32 min read Share idea million reader Charlene Chambliss Mar 30 2019 Cleaning Analyzing Visualizing Survey Data Python tutorial using panda matplotlib seaborn produce digestible insight dirty data work data D2C startup good chance asked look survey data least since SurveyMonkey one popular survey platform good chance itll SurveyMonkey data way SurveyMonkey export 10 min read 10 min read Matthew Stewart Mar 30 2019 Handling Discriminatory Biases Data Machine Learning data tell racist course algorithm racist Theyre made people Stephen Bush New Statesman America Ethics Machine Learning time machine learning doe touch particularly sensitive social moral ethical issue Someone give u data set asks u predict house price based 13 min read 13 min read Abraham Kang Mar 30 2019 Member-only Applied AI Going Concept ML Components Opening mind different way applying machine learning real world Abraham Kang special thanks Kunal Patel Jae Duk Seo sounding board providing input article 6 min read 6 min read Michel Kana Ph.D Mar 30 2019 Wild Wide AI responsible data science shoot first new race human Data Science good thing u improves life make thing efficient effective lead better experience however miss-steps data-driven analysis ha already exhibited example data science tool intentionally unintentionally misused 2012 9 min read 9 min read Worasom Kundhikanjana machine learning deep learning computer vision http //github.com/worasom/ Medium Luay Rahil ILLUMINATION Elon Musk Eliminated Remote Work Working Home `` n't Work '' Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Anangsha Alammyan Books Superpower 4 Books Powerful Rewire Brain Akshad Singi Better Humans 4 Unsexy One-Minute Habits Save 30+ Hours Every Week Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Mar 30, 2019 Listen Save Identifying the Sources of Winter Air Pollution in Bangkok Part II In the previous blog , I looked at the winter air pollution in Bangkok. The main source of pollution comes from particles smaller than 2.5 micrometer (PM 2.5 particles). These particles are smaller than the width of a human hair and can easily enter our bodies, even making their way into our blood. Last week (March 17, 2019), many provinces in the northern part of Thailand had the worst Air Quality Index (AQI) in the world due to particle pollution. So far, no long term solution has been proposed because the source of the PM 2.5 particle pollution has not been clearly pinpointed. In this notebook, I identify the sources of high PM 2.5 particles in Bangkok through a machine learning model. The code can be found in my GitHub page. High PM2.5, Who Are the Culprits ? There are three major theories regarding the source of air pollution in Bangkok: (1) The temperature inversion effect where cold air along with pollution is trapped close to the surface of the Earth. This theory was proposed by the government at the beginning of the 2019 winter season. The government blamed emission from old diesel engines for the pollution. (2) Agricultural burning, either locally or from surrounding provinces. During winter, a lot of open agricultural burning occurs throughout the country. Some officials have tried to tackle the air pollution problem by reducing open agricultural burning. (3) Pollution from other provinces or countries. Some NGOs blamed the pollution on near by power plants. My analysis procedure is as follows: Build a machine learning model(ML) to predict the air pollution level in Bangkok using environmental factor such as weather, traffic index, and fire maps. Include date-time features such as local hour, and weekday versus weekend in the model to capture other effects from human activities. Identify dominant sources of pollution using the feature of importance provided by the ML model. If the source of the pollution is local, then the AQI will depend on factors such as weather patterns (wind speed, humidity, average temperature), local traffic, and hour of day. If the pollution is from agricultural burning, the AQI will depend on active fires with some time lag to account for geographical separation. Fire activities are included based on the distance from Bangkok. On the other hand, if the pollution not correlated with the fire map, then the model should put more weight on weather patterns, such as wind direction and wind speed. Here are a list of features I considered and their data sources: Let me first walk through all the features included in the model. Agricultural Burning is a Major Problem ! Farmers in Southeast Asia pick January March as their burning season. For the north and northeastern provinces in Thailand, these burning activities are large enough to make these provinces among the most polluted places in the world during this time. For Bangkok, one might argue that because the region is heavily industrial rather than agricultural, it may not be affected as much by agricultural burning. But this is not the case. Because of the tiny size of PM 2.5 particles, they remain suspended in the atmosphere for prolonged periods and can travel over very long distances. From the weather data, the average wind speed is 10 km/hour. The reported PM 2.5 level is a rolling average over 24 hours. A rough estimate is that the current PM 2.5 reading may be from sources as far as 240 km away. The picture below shows the fire map measured by NASAs satellites, indicative of agricultural burning, on Jan 8, 2018 and on Feb 8, 2018. The yellow circle indicates the area within 240 km of Bangkok. The number of fires on Jan 8, which has an acceptable level of pollution, is much lower than the number of fires on Feb 8, which has an unhealthy level of pollution. In fact, the fire pattern closely aligns with the PM 2.5 pattern. Weather Patterns The temperature inversion effect often occurs during winter because the temperature is cooler near the ground. The hotter air on top traps the cool air from flowing. This stagnant atmospheric condition allows the PM 2.5 particles to remain suspended in the air for longer. On the other hand, higher humidity or rain will help remove particles from the atmosphere. This is one reason why in the past when the air pollution was high, the government has sprayed water in the air. Unfortunately, this mitigation does not appear to be effective, since the volume of water is minuscule compared to actual rain. How much influence does weather pattern have on air pollution? Lets compare the weather in winter versus other seasons. Temperature, wind speed and humidity are all lower in winter, but not by a large amount. Now, lets look at the relationship of each of these with the PM 2.5 level. Higher temperature (which disrupts the temperature inversion effect), wind speed and humidity have a negative correlation with the pollution level. On windy days, the pollution is clearly better. The median of the distribution for PM 2.5 levels is lower on windy days compared to on days without wind. In fact, the pollution level also depends on the wind direction, as seen in this plot. I selected only four major wind directions for simplicity. On the days where the wind comes from the south, the pollution level is lower likely because the Thai gulf is to the south of Bangkok. The clean ocean wind improves the air quality. Wind from the other three directions pass overland. However, having any wind is better than the stagnant atmospheric conditions on calm days. The shift in the median PM 2.5 level is smaller between rainy days and days with no rain. There are fewer rainy days during the winter season, so the data is somewhat noisy, but a difference can be observed in the cumulative density function. Traffic Index One of the sources of PM 2.5 particles is car engine exhaust. While campaigning for more public transportation usage is in general good for the environment, the effectiveness toward reducing PM 2.5 pollution is unclear. Here is why. We have seen that PM 2.5 levels are related to the time of day. The pollution is lower around 3 pm, but remains high during the night time. When plotted against the traffic data, the relationship with the pollution level is very noisy. There does not seem to be a strong correlation. Including the time of day and weekday versus weekend information into the model might make the relationship more clear. Autoregression process The current PM 2.5 value can also depend on the previous value. The partial autocorrelation plot below shows a strong correlation at 1 hr time lag, which means the PM 2.5 level is an autoregression process. Thus I include the 24 hour average values in the model, with the restriction that the model is only allowed to see the previous value for future predictions. The importance of this feature should be directly related to how long the particles stay in the atmosphere. Machine Learning Model The picture below show a dedrogram of all input features calculated from Spearman correlation. The dendrogram helps to identify redundant features that can be removed from the model. The number of fires within various distances and the level of PM 2.5 are closely related. Other features are further away. I ended up using all of these features in the model. To identify the major contributions to the pollution, I used a random forest regression to fit the model because of its simplicity and ease of interpretation. During hyper-parameter tuning, 25% of the data was allocated for the validation set. The model was retrained again using the entire dataset. The model achieves 0.99 R-squared on the training set. Since the purpose of this study is to understand the sources of the air pollution in the past, I focused on the training set. The plot below ranks the importance of each contributing factor. The importance is calculated from the decrease in the R-squared values upon permuting the column, and re-normalizing the sum of all columns. As expected the previous pollution level is the most important predictor. This is followed by the number of fires from the closest to the furthest. The number of fires as far away as 720 km has more influence on the air quality than the local humidity, traffic, or even rain. The hour of day is a more important predictor than the traffic index. Among the weather features, humidity is the most important feature. The influence of each feature is illustrated below using a tree interpreter for the data on Jan 13, 2019 at 8 am with 96 PM 2.5 level. We start with the average value of 26. The PM 2.5 level for the previous hour was 62, thus the model adds a value 20. There were 150 fires within a 240 km radius, thus the model adds 10 to the pollution level. The value is now 56. There are 1649 fires between 240-480 km, and 896 fires between 480-720 km, and the model adds a value of 9 and 8 respectively. The low wind speed and the morning rush hour (8 am) adds 8 to the model. These six top factors account for 81 out of the total 96 predicted for the PM 2.5 level. The remaining features to the right are less important and thus increase the predicted pollution value less. On a good day such as Feb 2, 2019 at 7 pm the PM 2.5 level was 10. The pollution level in the hour before was low, thus the model subtracts a value of 10. There were still a lot of fires in the area, and the model adds a value of 2. The wind speed was high, reducing the value by 2. The weather and traffic were good. The combination of many factors results in a low predicted PM 2.5 level of 10. Conclusions The PM 2.5 level has a complex relationship with various factors: number of fires, weather patterns, and traffic. But this analysis confirms the suspicion that many people have agricultural burning is the root cause of PM 2.5 pollution in Thailand. Burning activities as far as 720 km away from Bangkok, an area which extends into Myanmar, Laos, and Cambodia, can cause air problems in Bangkok. Solving this problem will not be easy. It will require a collaborative international effort among the Southeast Asian countries. I leave you with a fire map from March 17, 2019, one of the worst days ever! 772 15 772 772 15 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Sean McClure Mar 30, 2019 Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab) R and Python are the bread and butter of todays machine learning languages. R provides powerful statistics and quick visualizations, while Python offers an intuitive syntax, abundant support, and is the choice interface to todays major AI frameworks. In this article well look at the steps involved in creating libraries 32 min read 32 min read Share your ideas with millions of readers. Charlene Chambliss Mar 30, 2019 Cleaning, Analyzing, and Visualizing Survey Data in Python A tutorial using pandas, matplotlib, and seaborn to produce digestible insights from dirty data If you work in data at a D2C startup, theres a good chance you will be asked to look at survey data at least once. And since SurveyMonkey is one of the most popular survey platforms out there, theres a good chance itll be SurveyMonkey data. The way SurveyMonkey exports 10 min read 10 min read Matthew Stewart Mar 30, 2019 Handling Discriminatory Biases in Data for Machine Learning What do you do when data tells you to be racist? Of course algorithms are racist. Theyre made by people. Stephen Bush, New Statesman America Ethics in Machine Learning Most of the time, machine learning does not touch on particularly sensitive social, moral, or ethical issues. Someone gives us a data set and asks us to predict house prices based 13 min read 13 min read Abraham Kang Mar 30, 2019 Member-only Applied AI: Going From Concept to ML Components Opening your mind to different ways of applying machine learning to the real world. By Abraham Kang with special thanks to Kunal Patel and Jae Duk Seo for being a sounding board and providing input for this article. 6 min read 6 min read Michel Kana, Ph.D Mar 30, 2019 Wild Wide AI: responsible data science Who shoots first the new race or the human? Data Science can do good things for us: it improves life, it makes things more efficient, more effective and leads to a better experience. There are however some miss-steps that data-driven analysis has already exhibited. Here are few examples where data science tools were intentionally or unintentionally misused: In 2012 9 min read 9 min read Worasom Kundhikanjana machine learning, deep learning, and computer vision https://github.com/worasom/ More from Medium Luay Rahil in ILLUMINATION Elon Musk Eliminated Remote Work Because Working From Home \"Doesn\\'t Work\" Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Anangsha Alammyan in Books Are Our Superpower 4 Books So Powerful, They Can Rewire Your Brain Akshad Singi in Better Humans 4 Unsexy One-Minute Habits That Save Me 30+ Hours Every Week Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 1749,\n",
       "  'url': 'https://towardsdatascience.com/introduction-to-mesa-agent-based-modeling-in-python-bcb0596e1c9a',\n",
       "  'title': 'Introduction to Mesa: Agent-based Modeling in\\xa0Python',\n",
       "  'subtitle': '-',\n",
       "  'claps': 176,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 8,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-11-01',\n",
       "  'clap_prop': 8.693577570174706e-05,\n",
       "  'text': 'Towards Data Science Nov 1 2019 Member-only Listen Save Introduction Mesa Agent-based Modeling Python Python-based alternative NetLogo Repast MASON agent-based modeling Agent-based modeling relies simulating action interaction autonomous agent evaluate effect system often used predict projection obtain given complex phenomenon main purpose obtain explanatory insight agent behave given particular set rule Agent-based modeling ha extensively used numerous industry biology social science network business article cover necessary step kick-start agent-based modeling project using open-source python module called Mesa 4 section tutorial 1 Setup Setup pretty straightforward Mesa Make sure create new virtual environment name environment mesaenv Open terminal change directory mesaenv activate virtual environment using following code Virtual Environment Run following command activate virtual environment depending use case Python module tutorial requires three module Base folder Create base folder called Mesa use store python file following file base folder end section Feel free download case got lost somewhere tutorial done let proceed next section 2 Schelling Segregation Model using famous Schelling Segregation model use case tutorial Please noted introductory tutorial official mesa site based Boltzmann Wealth model Schelling Segregation model better use case explain use agent-based modeling explain racial segregation issue difficult eradicated Although actual model quite simple provides explanatory insight individual might self-segregate even though explicit desire Lets look explanation model provided Mesa official github page Schelling segregation model classic agent-based model demonstrating even mild preference similar neighbor lead much higher degree segregation would intuitively expect model consists agent square grid grid cell contain one agent Agents come two color red blue happy certain number eight possible neighbor color unhappy otherwise Unhappy agent pick random empty cell move step happy model keep running unhappy agent default number similar neighbor agent need happy set 3 mean agent would perfectly happy majority neighbor different color e.g Blue agent would happy five Red neighbor three Blue one Despite model consistently lead high degree segregation agent ending neighbor different color Model.py Create new python file called model.py Agent start single agent class code quite straightforward Model model class go part one-by-one clearer understanding agent-based modeling work First foremost create Schelling class define init function constructor Variables system consists least basic agent class model class Lets start writing model first need define 5 main variable Remember add required parameter input parameter init function Grid need set grid using space module mesa Scheduler Next need scheduler scheduler special model component control order agent activated common scheduler random activation activates agent per step random order also another type called Simultaneous activation Check API reference find Data Collection Data collection essential ensure obtained necessary data step simulation use built-in datacollection module case need know whether agent happy Agents Setup setup agent using following code last part init function set following parameter running variable enables conditional shut model condition met set false agent happy Step class requires step function represent run Run.py Create new python file called run.py type following code Server.py Create new python file called server.py Import Add following import statement file HappyElement Create class called HappyElement add two function Draw function Define function called schelling_draw part act visualization part running server 3 Visualization Lets test running following code terminal Make sure base folder Mesa web browser launched see following ouput Canvas visualization canvas grid defined earlier dot represents agent Setting modify setting test affect simulation Modify affect current simulation Remember click Reset button make change FPS define frame per second speed simulation Start Step Reset following button disposal Running Click Start button see following change canvas Chart chart bottom show number agent happy number step Result model stop certain point depends setting set noticed agent pretty much segregated 4 Conclusion Lets recap learned today started simple step setup install necessary python module learned Schelling Segregation Model model easily using Mesa created 3 python file explored in-depth basic usage component available Mesa ran server see visualization modified setting play around simulation fact official Mesa github provided u lot example explore Check following link find Hope enjoyed tutorial great day see next tutorial Reference 358 3 358 358 3 Get email whenever Ng Wai Foong publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Ng Wai Foong Senior AI Engineer Yoozoo Content Writer NLP datascience programming machinelearning Linkedin http //www.linkedin.com/in/wai-foong-ng-694619185/ Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Anmol Tomar CodeX Say Goodbye Loops Python Welcome Vectorization Soumodeep Das Guide Time Series Analysis Python Dr. Vernica Gephi Meet useful network analysis tool Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Nov 1, 2019 Member-only Listen Save Introduction to Mesa: Agent-based Modeling in Python Python-based alternative to NetLogo, Repast, or MASON for agent-based modeling Agent-based modeling relies on simulating the actions and interactions of autonomous agents to evaluate their effects on the system. It is often used to predict the projections that we will obtain given a complex phenomena. The main purpose is to obtain explanatory insight on how the agents will behave given a particular set of rules. Agent-based modeling has been extensively used in numerous industry such as biology, social sciences, network and business. This article covers the necessary steps to kick-start your agent-based modeling project using an open-source python module called Mesa. There are 4 sections in this tutorial: 1. Setup Setup is pretty straightforward for Mesa. Make sure to create a new virtual environment. I name the environment as mesaenv. Open up your terminal and change the directory to mesaenv and activate the virtual environment using the following code: Virtual Environment Run the following command to activate the virtual environment depending on your use case. Python modules This tutorial requires three modules: Base folder Create a base folder called Mesa that you will use to store all the python files. You should have the following files in the base folder at the end of this sections: Feel free to download it in case you got lost somewhere in the tutorial. Once you are done, lets proceed to the next section. 2. Schelling Segregation Model We will be using the famous Schelling Segregation model as use case for this tutorial. Please be noted that the introductory tutorial on the official mesa site is based on Boltzmann Wealth model. Schelling Segregation model is a better use case to explain how we can use agent-based modeling to explain why racial segregation issue is difficult to be eradicated. Although the actual model is quite simple, it provides explanatory insights at how individuals might self-segregate even though when they have no explicit desire to do so. Lets have a look at the explanation for this model provided by the Mesa official github page: The Schelling segregation model is a classic agent-based model, demonstrating how even a mild preference for similar neighbors can lead to a much higher degree of segregation than we would intuitively expect. The model consists of agents on a square grid, where each grid cell can contain at most one agent. Agents come in two colors: red and blue. They are happy if a certain number of their eight possible neighbors are of the same color, and unhappy otherwise. Unhappy agents will pick a random empty cell to move to each step, until they are happy. The model keeps running until there are no unhappy agents. By default, the number of similar neighbors the agents need to be happy is set to 3. That means the agents would be perfectly happy with a majority of their neighbors being of a different color (e.g. a Blue agent would be happy with five Red neighbors and three Blue ones). Despite this, the model consistently leads to a high degree of segregation, with most agents ending up with no neighbors of a different color. Model.py Create a new python file called model.py. Agent: We will start off with the single agent class. The code is quite straightforward: Model: For the model class. We will go through each part one-by-one to have a clearer understanding of how agent-based modeling works. First and foremost, create a Schelling class and define a init function as constructor. Variables: The system will consists of at least a basic agent class and a model class. Lets start by writing the model first. We will need to define 5 main variables: Remember to add the required parameters as input parameters in the init function. Grid: We will need to set the grid using the space module under mesa. Scheduler: Next up, we will need to have a scheduler. The scheduler is a special model component which controls the order in which agents are activated. The most common scheduler is the random activation which activates all the agents once per step, in random order. There is also another type called Simultaneous activation. Check out the API reference to find out more. Data Collection: Data collection is essential to ensure that we obtained the necessary data after each step of the simulation. You can use the built-in datacollection module. In this case, we only need to know whether the agent is happy or not. Agents Setup: We will now setup the agent using the following code: The last part of the init function is to set the following parameters: The running variable enables conditional shut off of the model once a condition is met. We will set it to false once all the agent are happy. Step: This class requires a step function that represent each run. Run.py Create a new python file called run.py and type in the following code. Server.py Create a new python file called server.py. Import: Add the following import statement to the file: HappyElement: Create a class called HappyElement and add two functions: Draw function: Define a function called schelling_draw. This part acts as the visualization part when running the server. 3. Visualization Lets test it out by running the following code at the terminal. Make sure that you are at the base folder ( Mesa ). A web browser will be launched and you should see the following ouput: Canvas The visualization of the canvas grid that we have defined earlier. Each dot represents an agent. Setting You can modify the setting to test out how it will affects the simulation. Modify this will not affect the current simulation. Remember to click on the Reset button to make changes. FPS You can define your own frame per second to speed up the simulation. Start, Step, Reset You will have the following buttons at your disposal: Running Click on the Start button and you should see the following changes to the canvas. Chart There is a chart at the bottom to show the number of agents that are happy against the number of step. Result The model should stop at a certain point depends on the setting that you have set. You can noticed that the agents are pretty much segregated. 4. Conclusion Lets recap on what we have learned today. We started off with some simple steps to setup and install the necessary python modules. Then, we learned about the Schelling Segregation Model and how we can model it easily using Mesa. We created 3 python files and explored further in-depth on the basic usage of components available in Mesa. After that, we ran the server to see the visualization. We modified some settings and play around with the simulation. In fact, the official Mesa github provided us with a lot more examples that we can explore. Check out the following link to find out more. Hope you enjoyed the tutorial. Have a great day and see you again in the next tutorial. Reference 358 3 358 358 3 Get an email whenever Ng Wai Foong publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ng Wai Foong Senior AI Engineer@Yoozoo | Content Writer #NLP #datascience #programming #machinelearning | Linkedin: https://www.linkedin.com/in/wai-foong-ng-694619185/ More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Anmol Tomar in CodeX Say Goodbye to Loops in Python, and Welcome Vectorization! Soumodeep Das A Guide on Time Series Analysis in Python Dr. Vernica What is Gephi? Meet this useful network analysis tool Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 372,\n",
       "  'url': 'https://towardsdatascience.com/trust-and-interpretability-in-machine-learning-b7be41f01704',\n",
       "  'title': 'Trust and interpretability in machine\\xa0learning',\n",
       "  'subtitle': '-',\n",
       "  'claps': 131,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 9,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-30',\n",
       "  'clap_prop': 6.470787850527764e-05,\n",
       "  'text': 'Towards Data Science Mar 30 2019 Listen Save Trust interpretability machine learning machine learning model always need interpretable Given choice interpretable model inaccurate non-interpretable model accurate wouldnt rather choose non-interpretable accurate model word reason sacrificing accuracy altar interpretability going try clarify make model interpretable Often interpretability equated simplicity definition obviously ambiguous simple one person may another importantly advanced machine learning useful modeling complex system system called complex reason simple Demanding useful model system simple measure make little sense Perhaps go consider think interpretability traditional modeling setting Consider mathematical model internal combustion engine car doubt anyone would consider simple hand also doubt anyone would consider model internal combustion engine non-interpretable either primarily derive model deductive manner well established physical theory thermodynamics fluid dynamic use definition interpretability model considered interpretable derived least motivated trustworthy theory definition interpretability serf dual purpose understanding trust help u understand model tend understand thing deductive manner going known unknown Also definition trust model derived trust place underlying theory Indeed situation understanding trust necessary scenario interested determining causal factor behind behavior system scenario must insist corresponding model must interpretable according definition model realm physical science belong category One argue purely inductive blackbox model suitable scenario However many situation understanding might nice mean must situation really matter ability make trustworthy prediction situation could provide alternate source trust model need bound definition interpretability given common argument merit Remember machine learning way systematically building model preferably large amount data using inductive reasoning Constraining model interpretable deductive manner seriously limit accuracy question becomes generate trust blackbox model little insight inner working credible basis trust could testing testing form basis trust regular software test model need able formalize expectation could formalize expectation completely would correspond complete specification model case would really need machine learning modeling methodology really need able formalize expectation aspect model consider important easy either many concept care fairness lend convenient mathematical treatment worth pointing significant progress ha made developing testing methodology testing machine learning model personally find idea using metamorphic relation formalizing expectation particularly promising still long way concrete methodology allow u perform comprehensive testing blackbox model inability contributes trust deficit blackbox model One could question efficacy expectation-based comprehensive testing goal machine learning find undiscovered pattern data insisting model meet expectation amount pre-defining model defeat whole purpose Following line reasoning one would argue long data representative algorithm powerful enough capture pattern little reason trust model expect model result generalize overall population extent expect generalize encapsulated model performance accuracy score Thus essence asked delegate trust trifecta data algorithm performance score first need dissuade notion single performance accuracy score form sufficient basis trusting model performance score usually point estimate model expected generalize average population given current data Trust hand nuanced multidimensional concept encapsulated single coarse grained score One imagine defining granular performance score e.g population segment would require certain level understanding population determining consider important different forming expectation Let u examine data aspect argument indeed quite easy convince oneself data representative population interested contain relevant pattern spurious one Unfortunately rarely case degree data non-representative depends quite acutely situation Nonetheless identify certain high level scenario first scenario would good understanding population complete control data collection mechanism scenario choose data representative high degree confidence expect resulting model prediction applicable overall population However note good enough understanding population able draw representative sample task hand mean already understanding feature important prediction Hence case debatable blackbox model terribly useful Opinion polling predicting election result good example scenario second scenario complete control data collection prediction affect data collected scenario assume data collection mechanism unbiased wait long enough would representative sample population course lot ifs buts go assumption Firstly one doe know long long enough Thus one need assume time scale data collected long enough produce representative sample Furthermore population might change meantime Thus additional assumption time scale population change much longer time scale representative sample generated long justify assumption estimated performance reliable model predicting stock price example scenario long making investment large enough tip market whole decision make result prediction affect stock price third scenario one data collection impacted prediction moderate high risk appetite wrong prediction example product recommender system model recommender system trained data consisting ordered list product different user bought/clicked Based data model predict user likely buy/click based model prediction system decide user get see limit buy/click-on Thus prediction bias data collection product recommender system one circumvent problem somewhat keeping exploration budget fraction case system show user random set product regardless prediction model observation resulting randomized prediction used estimate performance model One still ha address concern aforementioned second scenario order access reliability estimate fourth final scenario data collection impacted prediction little risk appetite wrong prediction example suppose build model predict whether someone default mortgage loan payment mortgage loan approved based prediction prediction person default loan approved case way knowing whether person would actually defaulted difficult imagine situation institution would randomly approve otherwise loan sake data exploration situation difficult gauge reliability estimated performance resulting model without additional information Thus great idea blindly expect data representative population scenario given constraint problem hand simply might possible get unbiased representative sample Understanding limitation one data collection mechanism able deduce implication limitation honesty report part model result go long way building trust Let u consider algorithm aspect argument widespread belief flexible algorithm better flexibility equips algorithm capture complex pattern history actual successful application machine learning anything go belief would appear utterly misplaced computer vision success came able encode symmetry picture model form convolutional neural network natural language processing able build extremely accurate cross-purpose language model could encode knowledge language including structure word context model recommender system collaborative filtering algorithm including matrix factorization method make strong assumption affinity user towards item Whether would like slap label interpretability model objective fact build better model understand domain context model need operate best model come flexible algorithm come algorithm well constrained domain knowledge right amount flexibility capture relevant pattern data seen word understanding used quite time discussion realized difficult build trust without understanding end boil one perceives machine learning Yes machine learning incredibly powerful inductive modeling technique combined big data big compute allows u model system solve problem previously reach entry machine learning imply exit everything else including common sense Machine learning one element wider modeling family includes deductive modeling well domain knowledge better understand leverage interconnection element go towards robust complex system modeling Trust contextual trust multiple source eventually flow knowledge integrity specifically trust knowledge integrity individual building model Trust well adoption model come opinion wider audience convinced modeler knowledge understand limitation model machine learning otherwise integrity report 181 181 181 Towards Data Science home data science Medium publication sharing concept idea code Christine Calo Mar 30 2019 Microsoft Introduction AI Part 1 Machine Learning bit like wanted learn Artificial Intelligence although felt little intimidated math involved Maybe thought concept difficult understand would depth recently completed Microsoft Introduction AI course 17 min read 17 min read Share idea million reader Okoh Anita Mar 30 2019 Repetition Songs Python Tutorial One Ed Sheeran song case study Everyone ha heard song know song sound like carelessly say everyone define song word benefit doubt song according Wikipedia single work music typically intended sung 8 min read 8 min read Alex Blyakhman Mar 30 2019 Member-only Getting Started Google BigQuerys Machine Learning Titanic Dataset still Beta BigQuery ML ha available since mid last year however didnt get around working Google cloud-based Machine Learning offering recently non-data scientist first impression whats like ability run ML model 12 min read 12 min read Jeremie Harris Mar 30 2019 problem data science job posting Every notice something realize probably noticed long time ago start see everywhere wonder people arent talking every wa yesterday wa scrolling 5 min read 5 min read Marco Cerliani Mar 30 2019 Member-only Predictive Maintenance detect Faults Sensors CNN interesting approach python code graphic representation Machine Learning topic Predictive Maintenance becoming popular passage time challenge easy heterogenous useful good knowledge domain touch people know underlying system work 6 min read 6 min read Abhishek Mukherjee http //www.linkedin.com/in/abhimukh/ Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Akshad Singi Better Humans 4 Unsexy One-Minute Habits Save 30+ Hours Every Week Anna Wu Google Data Scientist Interview Questions Step-by-Step Solutions Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Mar 30, 2019 Listen Save Trust and interpretability in machine learning Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldnt you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability? Before going any further we should try to clarify what makes a model interpretable. Often, interpretability is equated with simplicity. This definition is obviously ambiguous; what is simple for one person may not be so for another. More importantly, advanced machine learning is most useful for modeling complex systems. These systems are called complex for a reason they are not simple! Demanding that useful models for such systems should be simple, by any measure, makes very little sense. Perhaps, we can go further if we consider how we think about interpretability in a traditional modeling setting. Consider the mathematical model of an internal combustion engine in a car. I doubt if anyone would consider that to be simple. But, on the other hand, I also doubt if anyone would consider the model of an internal combustion engine to be non-interpretable, either. This is primarily because we can derive this model in a deductive manner from well established physical theories such as thermodynamics and fluid dynamics. Why not use this as our definition of interpretability? A model should be considered to be interpretable if it can be derived (or at least motivated) from a trustworthy theory. This definition of interpretability serves the dual purpose of understanding and trust. It helps us understand the model because we tend to understand things in a deductive manner by going from the known to the unknown. Also, with such a definition, the trust in the model is derived from the trust that we place in the underlying theory. Indeed, there are situations where both understanding and trust are necessary scenarios where we are interested in determining the causal factors behind the behavior of a system. In such scenarios, we must insist that the corresponding models must be interpretable according to the above definition. Most models in the realm of physical sciences belong to this category. One can argue that purely inductive blackbox models are not suitable for such scenarios. However, there are many other situations where understanding might be nice to have, but by no means is it a must have. In these situations what really matters is the ability to make trustworthy predictions. In these situations, if we could provide an alternate source of trust, then our models need not be bound by the definition of interpretability given above. This is a common argument, and there is merit to it. Remember, machine learning is a way of systematically building models from (preferably) large amounts of data using inductive reasoning. Constraining these models to be interpretable in a deductive manner can seriously limit their accuracy. So then the question becomes how can we generate trust in a blackbox model where we have little to no insight into its inner workings. A credible basis for trust could be testing . After all, testing forms the basis of our trust in regular software. But to test a model we need be able to formalize our expectations about it. If we could formalize our expectations completely then that would correspond to a complete specification of the model itself. In that case, we would not really need machine learning or any other modeling methodology. What we really need to be able to do is to formalize our expectations about the aspects of the model that we consider important. This is not easy either, because many of the concepts that we care about, such as fairness , do not lend themselves to a convenient mathematical treatment. It is worth pointing out that significant progress has been made in developing testing methodologies for testing machine learning models. I personally find the idea of using metamorphic relations for formalizing expectations to be particularly promising. But, we are still a long way from having concrete methodologies that will allow us to perform comprehensive testing of blackbox models, and this inability of ours contributes to a trust deficit in blackbox models. One could question the efficacy of such expectation-based comprehensive testing. After all, the goal of machine learning is to find undiscovered patterns in data. By insisting that the models meet our expectations amounts to pre-defining the model, which defeats the whole purpose. Following this line of reasoning, one would argue that as long as the data is representative and our algorithms are powerful enough to capture the patterns, there is little reason not to trust the model we should expect the model results to generalize to the overall population, and the extent to which we should expect them to generalize is encapsulated in the models performance (accuracy) scores. Thus, in essence we are asked to delegate our trust to the trifecta of data, algorithms and performance scores. We first need to dissuade ourselves from the notion that a single performance (accuracy) score can form sufficient basis for trusting the model. A performance score is usually a point estimate of how a model is expected to generalize on an average over a population given the current data. Trust, on the other hand, is a nuanced multidimensional concept that cannot be encapsulated in such a single coarse grained score. One can imagine defining more granular performance scores e.g. by population segments. But, that would require a certain level of understanding of the population and determining what we consider important this is not very different from forming expectations. Let us examine the data aspect of this argument. It is, indeed, quite easy to convince oneself that if the data is representative of the population we are interested in, then it should contain all the relevant patterns and no spurious ones. Unfortunately, that is rarely the case. The degree to which the data can be non-representative depends quite acutely on the situation. Nonetheless, we can identify certain high level scenarios. In the first scenario, we would have a good understanding of the population and complete control over the data collection mechanism. In this scenario, we can choose our data to be representative, and with a high degree of confidence we can expect our resulting models predictions to be applicable to the overall population. However, note that having a good enough understanding of the population to be able to draw a representative sample for the task at hand means that we already have some understanding of which features are important for the prediction. Hence, in this case it is debatable if blackbox models are terribly useful. Opinion polling for predicting election results is a good example of this scenario. In the second scenario, we do not have complete control over the data collection, but our predictions do not affect the data collected. In this scenario, if we assume that the data collection mechanism is unbiased then were we to wait long enough, we would have a representative sample of the population. Of course, there are a lot of ifs and buts that go with this assumption. Firstly, one does not know how long is long enough. Thus one needs to assume that the time scale over which the data is collected is long enough to produce a representative sample. Furthermore, the population itself might change in the meantime. Thus, an additional assumption is that the time scale over which population changes is much longer than the time scale over which a representative sample is generated. As long as we can justify those assumptions, then the estimated performance will be reliable. A model for predicting the stock prices is an example of such a scenario as long as we are not making investments that are large enough to tip the market as a whole, the decisions that we make as the result of the predictions should not affect the stock prices. The third scenario is one where the data collection is impacted by the predictions, but we have a moderate to high risk appetite for wrong predictions. An example of this is a product recommender system. The model for a recommender systems will be trained on data consisting of ordered lists of products that different users have bought/clicked on. Based on this data the model will predict what a user is most likely to buy/click on and based on the models predictions the system will decide what the user gets to see, which limits what (s)he can buy/click-on. Thus the prediction biases the data collection. In product recommender systems, one can circumvent this problem, somewhat, by keeping an exploration budget for a fraction of the cases the system shows the user a random set of products regardless of the prediction of the model. The observations resulting from these randomized predictions can then be used to estimate the performance of the model. One still has to address the concerns of the aforementioned second scenario in order to access the reliability of these estimates. In the fourth and final scenario, the data collection is impacted by the predictions, but we have little to no risk appetite for wrong predictions. For example, suppose we have to build a model to predict whether someone will default on their mortgage loan payments. The mortgage loan will be approved or not based on the prediction. If the prediction is that the person will default, then the loan will not be approved, and in that case there is no way of knowing whether this person would have actually defaulted or not. It is difficult to imagine a situation where an institution would randomly approve (or otherwise) a loan for the sake of data exploration. In these situations, it is very difficult to gauge the reliability of the estimated performance of the resulting model without additional information. Thus, it is not such a great idea to blindly expect the data to be representative of the population. In most scenarios, given the constraints of the problem at hand, it simply might not be possible to get an unbiased representative sample. Understanding the limitations of ones data collection mechanism, being able to deduce the implications of those limitations, and having the honesty to report those as a part of the models results goes a long way in building trust. Let us now consider the algorithm aspect of the argument. It is a widespread belief that the more flexible an algorithm is the better it is, because flexibility equips an algorithm to capture more complex patterns. But if the history of the actual successful applications of machine learning are anything to go by, then this belief would appear to be utterly misplaced. In computer vision, success came when we were able to encode the symmetries in pictures into models in the form of convolutional neural networks. In natural language processing we are now able to build extremely accurate cross-purpose language models because we could encode our knowledge about languages, including structure and word context, into these models. In recommender systems most collaborative filtering algorithms including matrix factorization methods, make strong assumptions about the affinity of a user towards an item. Whether we would like to slap the label of interpretability on these models or not, it is an objective fact that we build better models when we understand the domain and the context in which the model needs to operate. The best models do not come from the most flexible algorithms, they come from algorithms that are well constrained by domain knowledge and have just the right amount of flexibility to capture the relevant patterns in the data. We have seen the word understanding being used quite a few times in the above discussion. What we should have realized by now is that it is difficult to build trust without understanding. In the end, it boils down to how one perceives machine learning. Yes, machine learning is an incredibly powerful inductive modeling technique. When combined with big data and big compute, it allows us to model systems and solve problems that were previously out of our reach. But the entry of machine learning should not imply the exit of everything else, including common sense. Machine learning is one element in the wider modeling family that includes deductive modeling as well as domain knowledge. The better we understand and leverage the interconnections between these elements, the further we will go towards robust complex system modeling. Trust is contextual and trust can have multiple sources, but eventually it flows from knowledge and integrity; specifically in our trust in the knowledge and integrity of the individuals who are building the models. Trust as well as adoption of models will come, in my opinion, only when the wider audience is convinced that the modelers have the knowledge to understand the limitations of their models (machine learning or otherwise), and the integrity to report them. 181 181 181 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Christine Calo Mar 30, 2019 Microsoft Introduction to AI Part 1 Machine Learning Are you a bit like me and have wanted to learn about Artificial Intelligence although felt a little intimidated by the maths involved? Maybe you thought the concepts were too difficult to understand and you would be out of your depth. I recently completed the Microsoft Introduction to AI course 17 min read 17 min read Share your ideas with millions of readers. Okoh Anita Mar 30, 2019 Repetition in Songs: A Python Tutorial One of Ed Sheeran songs as a case study Everyone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung 8 min read 8 min read Alex Blyakhman Mar 30, 2019 Member-only Getting Started with Google BigQuerys Machine Learning Titanic Dataset While still in Beta, BigQuery ML has been available since mid last year; however, I didnt get around to working with this Google cloud-based Machine Learning offering until recently. As a non-data scientist, my first impression whats not to like? After all, the ability to run ML models from 12 min read 12 min read Jeremie Harris Mar 30, 2019 The problem with data science job postings Every once in a while, you notice something that you realize you probably should have noticed a long time ago. You start to see it everywhere. You wonder why more people arent talking about it. For me, every once in a while was yesterday when I was scrolling through the 5 min read 5 min read Marco Cerliani Mar 30, 2019 Member-only Predictive Maintenance: detect Faults from Sensors with CNN An interesting approach with python code and graphic representations In Machine Learning the topic of Predictive Maintenance is becoming more popular with the passage of time. The challenges are not easy and very heterogenous: its useful to have a good knowledge of the domain or to be in touch with people who know how the underlying system works. 6 min read 6 min read Abhishek Mukherjee https://www.linkedin.com/in/abhimukh/ More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Akshad Singi in Better Humans 4 Unsexy One-Minute Habits That Save Me 30+ Hours Every Week Anna Wu Google Data Scientist Interview Questions (Step-by-Step Solutions!) Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 2033,\n",
       "  'url': 'https://towardsdatascience.com/automatically-analyzing-laboratory-test-data-32c27e4e3075',\n",
       "  'title': 'Automatically Analyzing Laboratory Test\\xa0Data',\n",
       "  'subtitle': 'How to write Python programs that perform your\\xa0data…',\n",
       "  'claps': 31,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 12,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-09-17',\n",
       "  'clap_prop': 1.5312551402012268e-05,\n",
       "  'text': 'Towards Data Science Sep 17 2019 Member-only Listen Save Tutorial Automatically Analyzing Laboratory Data Create Performance Map Automatically Analyzing Laboratory Test Data write Python program perform data analysis common scientist find large data set Sometimes come form gigabyte worth data single file time hundred file containing small amount data Either way hard manage Hard make sense Hard computer process need way simplify process make data set manageable help keep track everything Thats tutorial process writing Python script automatically analyze data store data meaningful intuitive file name using example taken actual research know skill youre developing practical useful first post tutorial introduced concept tutorial term heat pump water heater coefficient performance COP performance map dont mean anything might want read second post introduced companion data set split data set multiple file user-friendly name companion data set valuable part tutorial process allows follow along write exact code Ill present run code see result compare result present way ensure youre right end second part tutorial three data file containing test result specified ambient temperature file see electricity consumption heat pump temperature water storage tank air temperature surrounding water heater next step process result need write code automatically make sense data calculates COP heat pump plot data visually understand Without ado let get started Python programming need first import package package need import several package important step Bokeh used script written portion tutorial however script basis future portion worth importing bokeh dont worry moving next portion later project well need import entirety glob panda numpy importing certain function bokeh called using following code Note panda wa imported assigned pd numpy wa assigned np mean reference package writing pd np instead panda numpy package imported next step create glob list loop sequentially iterate file prepared write code analyzing data file iterate file first step configure glob need provide glob path filetype file type optional data .csv format want specify avoid including extraneous file call glob create list file type specified folder done using following line code Path variable tell glob look specified folder relevant file Note folder specified folder saved file first part tutorial second line call glob function glob package create list specified file Notice code reference Path variable specified state folder glob search file followed code stating glob include .csv file nothing else code completed glob create list .csv file folder form 1.csv 2. csv 3.csv n.csv downloaded companion data set following tutorial full path PerformanceMap_HPWH_55.csv PerformanceMap_HPWH_70.csv PerformanceMap_HPWH_95.csv list file next step create loop iterates file need open file perform data analysis need following code code automatically iterates every entry Filenames list Note way written lead Filename holding actual filename entry list instance first time loop Filename contain full path PerformanceMap_HPWH_55.csv second line us panda read file memory save Data later analysis file located sequentially opened next step add code analyzing file next step write code automatically analyze file requires fair amount knowledge equipment studied top knowledge Python programming Since assume arent reader HPWHs Ill make sure write relevant information Filtering data contain important portion process care data heat pump HPWH active heat pump device typically draw 400600 W depending ambient water temperature Meanwhile on-board electronics consume electricity filter data segment care need remove data electricity consumption le 300 W chosen significantly higher power draw on-board electronics minimum draw heat pump following line line reset data frame data device consuming 300 W. impact index data frame need reset keep data frame clean use following code Identifying change time measurement timestamp data data set easy work Fortunately know collaborating lab testing partner measurement taken every 10 second create new column data frame state long test ha active using following code Calculating change energy stored water One key parameter impacting COP HPWHs temperature water storage tank water wont mixed well enough hold single constant temperature Typically cold water bottom tank hot water top sake exercise good enough calculate average temperature tank Since lab tester wa kind enough inform u used 8 temperature measurement evenly spaced throughout tank calculate average water temperature using really care much average temperature tank change one measurement time another way identify change energy stored tank thus energy added water heat pump using following two line code first line us .shift command panda data frame create new column data frame containing Average Tank Temperature deg F data shifted one row data frame creates empty cell first row Index 0 cause error performing calculation second line code rectifies using .loc fill cell 72.0 friendly lab tester told u test started precisely 72.0 deg F every single time calculate change energy stored water every two time stamp need know constant equation put together calculate change stored energy using following line Calculating COP next step analyzing data set calculating COP function water temperature tank goal tutorial identify COP function water temperature ambient temperature provide understanding COP function water temperature specified ambient temperature lead right direction calculate COP heat pump need perform unit conversion electricity consumption currently expressed W energy added water currently expressed Btu/timestep make unit conversion use ratio 1 W 3.412142 Btu/hr convert Btu/hr Btu/s multiply 10 second per timestamp give code COP definition amount heat added water divided amount electricity consumed Thus calculated Generating Regressions table showing COP function water temperature three specified COPs better Wouldnt nice function use calculate COP enter water temperature identify COP accordingly Numpy provides tool make easy use numpy function polyfit identify coefficient refression describing COP function water temperature flexible function allowing control shape curve specifying order function end Since COP heat pump function temperature parabolic curve need second order regression example Thus coefficient identified line numpy poly1d function used create regression using coefficient done identify COP heat pump specified water temperature using regression Remember regression generated specific air temperature estimate COP using regression correct air temperature Creating 2-d performance map ultimate goal tutorial arent yet COP specified water temperature identifying calling function using water temperature input instance want find COP water temperature 72 F enter save result data saved using technique always use described Automatically Storing Results Analyzed Data Sets need 1 Ensure folder analyzed result available 2 Create new filename clearly state file contains 3 Save data case want save data new file called Analyzed data folder used show result analysis following code first line creates path new folder add \\\\Analyzed currently existing path stating looking folder called Analyzed within current folder second line determines whether folder already exists doesnt third line creates need set filename data set coefficient done combining already subsection string use string index identify portion filename want keep instance section filename first file say PerformanceMap_HPWH_50 state quite clearly file contains Since know last four character filename .csv isolate section string using index -26 -4 word want character string running 26th last 4th last including 4th last Following customize filename bit Namely state want data filename state contains analyzed data want coefficient filename state contains coefficient write filename file using following line simply save file analyzed data stored panda .to_csv function coefficient saved numpy .tofile function follows Note line saving data set index False mean index data frame saved saving table Also note numpy .tofile function requires specify separator case using comma specified sep code know worked right enormous number thing could gone wrong point process Maybe lab tester made mistake running experiment Maybe instrument broke Maybe typo code incorrect unit conversion imperative ensure none problem others occurred process Therefore next step process checking data set error cover next phase tutorial First Ill discus check data error manually way youll get firm understanding potential error checking identify Ill discus add code script check error automatically warns something go wrong Tutorial Table Contents part series article teaching skill needed automatically analyze laboratory data develop performance map heat pump water heater article series found using following link Introduction Splitting Data Sets Checking Analyzed Laboratory Data Errors Write Scripts Check Data Quality Automatically Generate Regressions Python 38 38 38 Towards Data Science home data science Medium publication sharing concept idea code Peter Grant Scientist Lawrence Berkeley National Laboratory also teach skill need build fulfilling career Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Yang Zhou TechToFreedom 9 Fabulous Python Tricks Make Code Elegant Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Sep 17, 2019 Member-only Listen Save Tutorial: Automatically Analyzing Laboratory Data to Create a Performance Map Automatically Analyzing Laboratory Test Data How to write Python programs that perform your data analysis for you Its very common that scientists find themselves with large data sets. Sometimes it comes in the form of gigabytes worth of data in a single file. Other times its hundreds of files, each containing a small amount of data. Either way, its hard to manage. Hard to make sense of. Hard for your computer to process. You need a way to simplify the process, to make the data set more manageable, and to help you keep track of everything. Thats what this tutorial is all about. Were in the process of writing Python scripts that will automatically analyze all of the data for you and store the data with meaningful, intuitive file names. All while using an example taken from actual research, so you know that the skills youre developing are practical and useful. The first post in this tutorial introduced the concepts of the tutorial. If the terms heat pump water heater, coefficient of performance (COP), and performance map dont mean anything to you, you might want to read it . The second post introduced the companion data set , and split the data set into multiple files with user-friendly names. The companion data set is a valuable part of the tutorial process, as it allows you to follow along. You can write the exact same code that Ill present, run the code, see the results, and compare it to results I present. In that way you can ensure that youre doing it right. At the end of the second part of the tutorial we now have three data files, each containing test results at a specified ambient temperature. In each of those files we see electricity consumption of the heat pump, the temperature of water in the storage tank, and the air temperature surrounding the water heater. The next step is to process those results. We need to write some code that automatically makes sense of the data, calculates the COP of the heat pump, and plots the data so that we can visually understand it. Without further ado, lets get started. As with all Python programming, we need to first import packages. What packages do I need to import? T here are several packages which will be very important for this step. They are: Bokeh will not be used in the script written during this portion of the tutorial; however, this same script will be the basis of future portions. Its worth importing bokeh now so you dont have to worry about it when moving on to the next portion later. For this project well need to import the entirety of glob, pandas, os and numpy while only importing certain functions from bokeh. These can all be called in using the following code: Note that pandas was imported and assigned to pd, and numpy was assigned to np. This means that we can now reference these packages by writing pd and np instead of pandas and numpy. Now that all of our packages are imported, the next step is to create our glob list and for loop to sequentially iterate through all of your files. When that is prepared, you can write code analyzing each of our data files. How do I iterate through all my files? T he first step is to configure glob. To do this, you need to provide glob with a path and a filetype (The file type is optional, but all of our data is in .csv format so we want to specify that to avoid including extraneous files). Then when you call glob it will create a list of all files of that type in the specified folder. This can be done using the following lines of code: The Path variable tells glob to look in the specified folder for relevant files. Note that the folder specified above is the same folder that you saved files to in the first part of this tutorial . The second line calls the glob function from the glob package to create a list of all the specified files. Notice that the code references the Path variable you specified to state what folder glob should search for files. This is then followed up by code stating that glob should include all .csv files, and nothing else. After this code is completed glob will create a list of all .csv files in the folder. It will have the form [1.csv, 2. csv, 3.csv, , n.csv]. For those who have downloaded the companion data set and are following the tutorial, it will be the full path for PerformanceMap_HPWH_55.csv, PerformanceMap_HPWH_70.csv, and PerformanceMap_HPWH_95.csv. Now that you have a list of files, the next step is to create a for loop that iterates through each of these files. You then need to open the files so you can perform data analysis on each of them. To do so, you need the following code: This code automatically iterates through every entry in the Filenames list . Note that the way this is written leads to Filename holding the actual filename of each entry in the list. For instance, on the first time through the for loop Filename will contain the full path for PerformanceMap_HPWH_55.csv. The second line uses pandas to read the file into memory, and saves it to Data for later analysis. Now that the files are located and being sequentially opened, the next step is to add code analyzing each file. And that will be our next step. How do I write code to automatically analyze each file? T his requires a fair amount of knowledge about the equipment being studied on top of the knowledge of Python programming. Since I assume you arent a reader on HPWHs, Ill make sure to write out the relevant information. Filtering the data to only contain the important portion For this process, we only care about data when the heat pump in the HPWH is active. The heat pumps in these devices typically draw 400600 W depending on the ambient and water temperatures. Meanwhile, they have on-board electronics that consume some electricity. To filter the data to the segment we care about we need to remove all data with electricity consumption less than 300 W, chosen to be significantly higher than the power draw of the on-board electronics but under the minimum draw of the heat pump. We can do this with the following line: That line resets our data frame to have only the data where the device is consuming over 300 W. But that did impact the index of the data frame, so we need to reset that to keep our data frame clean. We can use the following code for that: Identifying the change in time between measurements Now, the timestamp data in this data set is not easy to work with. Fortunately, we know from collaborating with our lab testing partner that measurements were taken every 10 seconds. So we can create a new column in our data frame that states how long the test has been active using the following code: Calculating the change in energy stored in the water One of the key parameters impacting the COP of HPWHs is the temperature of water in the storage tank. The water wont be mixed well enough to hold a single, constant temperature. Typically there will be cold water on the bottom of the tank, and hot water at the top. For the sake of this exercise, its good enough to calculate the average temperature of the tank. Since our lab tester was kind enough to inform us that (s)he used 8 temperature measurements evenly spaced throughout the tank, we can calculate the average water temperature using: Now, what we really care about here is how much the average temperature of the tank changes from one measurement time to another. This way we can identify the change in energy stored in the tank, and thus the energy added to the water by the heat pump. We can do this using the following two lines of code: The first line uses the .shift command of a pandas data frame to create a new column in the data frame containing the Average Tank Temperature (deg F) data, but shifted down one row in the data frame. This creates an empty cell in the first row (Index 0) which causes errors when performing calculations. The second line of code rectifies this by using .loc to fill this cell with 72.0. We can do this because our friendly lab tester told us that the tests started precisely at 72.0 deg F every single time. Now we can calculate the change in energy stored in the water between every two time stamps. To do this, we need to know a few constants and equations: We can put it all together and calculate the change in stored energy using the following line: Calculating the COP The next step in analyzing each data set is calculating the COP as a function of the water temperature in the tank. The goal of this tutorial is to identify the COP as a function of both water temperature and ambient temperature, and this will provide an understanding of the COP as a function of water temperature at each specified ambient temperature. It leads in the right direction. To calculate the COP of the heat pump we need to perform some unit conversions. The electricity consumption is currently expressed in W while the energy added to the water is currently expressed in Btu/timestep. To make this unit conversion we use the ratio 1 W = 3.412142 Btu/hr, then convert Btu/hr to Btu/s and multiply by the 10 seconds per timestamp. This gives the code: The COP is by definition the amount of heat added to the water divided by amount of electricity consumed. Thus, it can be calculated with: Generating Regressions Now we have a table showing the COP as a function of the water temperature at each of the three specified COPs. But we can do better than that. Wouldnt it be nice to have a function we can use to calculate the COP? Just enter the water temperature, and identify the COP accordingly? Numpy provides the tools to make this easy. We can use the numpy function polyfit to identify the coefficients of a refression describing the COP as a function of the water temperature. Its a flexible function, allowing you to control the shape of the curve by specifying the order of the function at the end. Since the COP of a heat pump as a function of temperature is a parabolic curve, we need a second order regression for this example. Thus, the coefficients can be identified with the line: The numpy poly1d function can be used to create a regression using those coefficients. This is done with: Now you can identify the COP of the heat pump at a specified water temperature using this regression. Remember that the regression is only generated for a specific air temperature, so only estimate the COP using the regression for the correct air temperature. Creating a 2-d performance map is the ultimate goal of this tutorial, but we arent there yet. The COP at a specified water temperature can be identifying by calling the function and using the water temperature as an input. For instance, if you want to find the COP when the water temperature is 72 F, you can enter: How do I save these results? The data can be saved using the same techniques we always use, as described in Automatically Storing Results from Analyzed Data Sets . We need to 1) Ensure that a folder for analyzed results is available, 2) Create a new filename that clearly states what the file contains, and 3) Save the data. In this case we want to save the data to a new file called Analyzed. It should be in the same data folder, and used to show the results of the analysis. We can do this with the following code: The first line creates the path for the new folder. It adds \\\\Analyzed to the currently existing path, stating that its looking for a folder called Analyzed within the current folder. The second line determines whether or not that folder already exists. If it doesnt, the third line creates it. After that, we need to set the filenames for both the data set and the coefficients. This can be done by combining what we already have with a subsection of the strings. We can use string indexes to identify the portion of the filename that we want to keep. For instance, the section of the filename for the first file that says PerformanceMap_HPWH_50 states quite clearly what the file contains. Since we know that the last four characters of the filenames are .csv we can isolate that section of the string by using the indices [-26:-4]. In other words, we want the characters of the string running from 26th to last to 4th to last not including the 4th to last. Following that we can customize the filenames a bit. Namely, we can state that we want the data filename to state that it contains analyzed data, and we want the coefficients filename to state that it contains coefficients. We can write the filename for both files using the following lines: Then we simply save the files. The analyzed data can be stored with the pandas .to_csv function, and the coefficients can be saved with the numpy .tofile function as follows: Note that the line saving the data sets index = False. This means that the index of the data frame will not be saved when saving the table. Also note that the numpy .tofile function requires you to specify a separator. In this case were using a comma, as specified with the sep = ,  code. How do I know that it worked right? T here are an enormous number of things that could have gone wrong by this point in the process. Maybe the lab tester made some mistakes while running the experiments. Maybe an instrument broke. Maybe theres a typo in the code, or an incorrect unit conversion. Its imperative to ensure that none of these problems, or any others, occurred during the process. Therefore the next step in the process is checking the data set for errors. We will cover this in the next phase of the tutorial. First Ill discuss how to check the data for errors manually. That way youll get a firm understanding of the potential errors were checking for and how to identify them. Then Ill discuss how to add code to this script that checks for those errors automatically, and warns you when something goes wrong. Tutorial Table of Contents This is a part of a series of articles teaching you all of the skills needed to automatically analyze laboratory data and develop a performance map of heat pump water heaters. The other articles in the series can be found using the following links: Introduction Splitting Data Sets Checking Analyzed Laboratory Data for Errors How to Write Scripts that Check Data Quality for You How to Automatically Generate Regressions in Python 38 38 38 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Peter Grant Scientist at Lawrence Berkeley National Laboratory who also teaches skills you need to build a fulfilling career. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Yang Zhou in TechToFreedom 9 Fabulous Python Tricks That Make Your Code More Elegant Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5078,\n",
       "  'url': 'https://towardsdatascience.com/kedro-prepare-to-pimp-your-pipeline-f8f68c263466',\n",
       "  'title': 'Kedro: Prepare to Pimp your\\xa0Pipeline',\n",
       "  'subtitle': 'A new Python library for production-ready data pipelines',\n",
       "  'claps': 357,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 10,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-06-04',\n",
       "  'clap_prop': 0.00017634131775865738,\n",
       "  'text': 'Towards Data Science Jun 4 2019 Listen Save Kedro New Tool Data Science new Python library production-ready data pipeline post introduce Kedro new open source tool data scientist data engineer brief description likely become standard part every professional toolchain describe use tutorial able complete fifteen minute Strap spaceflight future Suppose data scientist working senior executive make key financial decision company asks provide ad-hoc analysis thanks delivering useful insight planning Great Three month line newly-promoted executive CEO asks re-run analysis next planning meetingand code broken youve overwritten key section file remember exact environment used time maybe code OK one big Jupyter notebook file path hard coded meaning go laboriously check change one new data input great everyday principle software engineering unfamiliar data scientist extensive background programming fact many data scientist self-taught learning program part research project necessary job maybe data scientist argue code doe need high standard arent working production system code feed business decision process considered production code Data scientist may consider primary output work code doesnt mean code write shouldnt follow standard expected software engineering team fact following minimum characteristic detail principle take look useful blog post Thomas Huijskens QuantumBlack Kedro Kedro development workflow tool allows create portable data pipeline applies software engineering best practice data science code reproducible modular well-documented use Kedro worry le write production-ready code Kedro doe heavy lifting youll standardise way team collaborates allowing work efficiently Many data scientist need perform routine task data cleaning processing compilation may favourite activity form large percentage day day task Kedro make easier build data pipeline automate heavy lifting reduce amount time spent kind task Kedro ha created QuantumBlack advanced analytics firm wa acquired consultancy giant McKinsey Company 2015 QuantumBlack used Kedro 60+ project McKinsey decided open source goal enable staff client third-party developer use Kedro build upon extending open tool need data professional able complete regular task effectively potentially share enhancement community Kedro workflow scheduler Kedro workflow scheduler like Airflow Luigi Kedro make easy prototype data pipeline Airflow Luigi complementary framework great managing deployment scheduling monitoring alerting Kedro pipeline like machine build car part Airflow Luigi tell different machine switch order work together produce car QuantumBlack built Kedro-Airflow plugin providing faster prototyping time reducing barrier entry associated moving pipeline workflow scheduler need know Kedros documentation ha designed beginner get started creating Kedro project Python 3.5+ basic knowledge Python might find learning curve challenging bear follow documentation guidance fifteen minute spaceflight Kedro easy-to-follow fifteen minute tutorial based following scenario 2160 space tourism industry booming Globally thousand space shuttle company taking tourist moon back able source three fictional datasets amenity offered space shuttle customer review company information want train linear regression model us datasets predict price shuttle hire However get train model need prepare data data engineering process preparing data model building creating master table Getting set first step pip install kedro new Python 3.5+ virtual environment recommend conda confirm correctly installed typing see Kedro graphic shown additionally give version Kedro using wrote tutorial based version 0.14.0 May 2019 since active open source project may time change codebase cause minor change example project endeavour keep example code date although blog post consult Kedro documentation release note hit upon inconsistency Kedro workflow building Kedro project typically follow standard development workflow shown diagram tutorial walk step Set project template keep thing simple QuantumBlack team provided code need simply need clone example project Github Repo need set project template tutorial code Spaceflights project make sure necessary dependency please run following Python 3.5+ virtual environment Set data Kedro us configuration file make project code reproducible across different environment may need reference datasets different location set data Kedro project typically add datasets data folder configure registry data source manages loading saving data tutorial make use three datasets spaceflight company shuttling customer moon back us two data format .csv .xlsx file stored data/01_raw/ folder project directory work datasets provided Kedro project conf/base/catalog.yml file act registry datasets use Registering dataset simple adding named entry .yml file include file location path parameter given dataset type data versioning Kedro come support type data csv look catalog.yml file Spaceflights tutorial see following entry datasets used check whether Kedro load data correctly inspect first five row data open terminal window start IPython session Kedro project directory type exit want end session continue tutorial Create run pipeline next part workflow create pipeline set node Python function perform distinct individual task typical project stage project comprises three step Data engineering pipeline reviewed raw datasets Spaceflights project time consider data engineering pipeline process data prepares model within data science pipeline pipeline preprocesses two datasets merges third master table file data_engineering.py inside node folder youll find preprocess_companies preprocess_shuttles function specified node within pipeline pipeline.py function take dataframe output pre-processed data preprocessed_companies preprocessed_shuttles respectively .. Kedro run data engineering pipeline determines whether datasets registered data catalog conf/base/catalog.yml dataset registered persisted automatically path specified without need specify code function dataset registered Kedro store memory pipeline run remove afterwards tutorial preprocessed data registered conf/base/catalog.yml CSVLocalDataSet chosen simplicity choose available dataset implementation class save data example database table cloud storage like S3 Azure Blob Store etc others data_engineering.py file also includes function create_master_table data engineering pipeline us join together company shuttle review dataframes single master table Data science pipeline data science pipeline build model us datasets comprised price prediction model us simple LinearRegression implementation scikit-learn library code found file src/kedro_tutorial/nodes/price_prediction.py test size random state parameter prediction model specified conf/base/parameters.yml Kedro feed catalog pipeline executed Combining pipeline project pipeline specified pipeline.py de_pipeline preprocess data use create_master_table combine preprocessed_shuttles preprocessed_companies review master_table dataset ds_pipeline create feature train evaluate model first node ds_pipeline output 4 object X_train X_test y_train y_test used train model final node evaluate model two pipeline merged together de_pipeline ds_pipeline order add pipeline together significant since Kedro automatically detects correct execution order node overall pipeline would result specified ds_pipeline de_pipeline pipeline executed invoke kedro run see output similar following problem getting tutorial code running check working Python 3.5 environment dependency installed project still problem head Stack Overflow guidance community behaviour youre observing appears problem Kedro please feel raise issue Kedros Github repository Kedro runner two different way run Kedro pipeline specify default Kedro us SequentialRunner invoke kedro run Switching use ParallelRunner simple providing additional flag follows Thats end tutorial run basic Kedro Theres plenty documentation various configuration option optimisation use project example Contributions welcome start working Kedro want contribute example change Kedro project would welcome chance work Please consult contribution guide Acknowledgements Spaceflights example based tutorial written Kedro team QuantumBlack Labs Yetunde Dada Ivan Danov Dmitrii Deriabin Lorena Balan Gordon Wrigley Kiyohito Kunii Nasef Khan Richard Westenra Nikolaos Tsaousis kindly given early access code documentation order produce tutorial TowardsDataScience Thanks 441 3 441 441 3 Towards Data Science home data science Medium publication sharing concept idea code Shuvashish Chatterjee Jun 4 2019 Alexa find parking AI guide vacant parking slot Detecting parking lot occupancy security cam footage using deep learning Finding vacant spot parking lot tough ask even difficult manage lot incoming traffic varies lot slot vacant instant time need slot commuter finding difficult reach particular slot 8 min read 8 min read Share idea million reader Emanuele Fabbiani Jun 4 2019 Member-only Lessons real Machine Learning project part 2 trap data exploration fall pitfall data exploration get away second story series Im going brutally shrink intro Im writing share real enterprise-level Machine Learning project taught team curious know feel free check first chapter Jupyter 5 min read 5 min read Aditya Mandal Jun 4 2019 Evolution Machine Translation 1949 Warren Weaver researcher Rockefeller Foundation presented set proposal machine based translation based information theory success code breaking Second World War year machine translation research began earnest many US university described 10 min read 10 min read Ziad SALLOUM Jun 4 2019 Member-only Eligibility Traces Reinforcement Learning Sometimes looking backward isnt bad Update best way learning practicing Reinforcement Learning going http //rl-lab.com Eligibility Traces short straight forward manner Eligibility Traces kind mathematical trick improves performance Temporal Difference method Reinforcement Learning benefit Eligibility Traces 7 min read 7 min read Adam King Jun 4 2019 Member-only Optimizing deep learning trading bot using state-of-the-art technique Lets teach deep RL agent make even money using feature engineering Bayesian optimization last article used deep reinforcement learning create Bitcoin trading bot dont lose money Although agent profitable result werent impressive time going step notch massively improve model profitability reminder 17 min read 17 min read Jo Stichbury Rnin technology writer podcast host Cat herder Dereferences NULL Medium Khuyen Tran Prefect Blog Orchestrate Data Science Project Prefect 2.0 Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Naga Sanjay Continuous Training ML model Madison Hunter Towards Data Science Write Good Code Documentation Data Scientists Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Jun 4, 2019 Listen Save Kedro: A New Tool For Data Science A new Python library for production-ready data pipelines In this post, I will introduce  Kedro  , a new open source tool for data scientists and data engineers. After a brief description of what it is and why it is likely to become a standard part of every professionals toolchain, I will describe how to use it in a tutorial that you should be able to complete in fifteen minutes. Strap in for a spaceflight to the future! Suppose you are a data scientist working for a senior executive who makes key financial decisions for your company. She asks you to provide an ad-hoc analysis, and when you do, she thanks you for delivering useful insights for her planning. Great! Three months down the line, the newly-promoted executive, now your CEO, asks you to re-run the analysis for the next planning meetingand you cannot. The code is broken because youve overwritten some of the key sections of the file, and you cannot remember the exact environment you used at the time. Or maybe the code is OK, but its in one big Jupyter notebook with all the file paths hard coded, meaning that you have to go through laboriously to check and change each one for the new data inputs. Not so great! Some everyday principles of software engineering are unfamiliar to data scientists who do not all have an extensive background in programming. In fact, many data scientists are self-taught, learning to program as part of a research project or when necessary in their jobs. And, maybe, a few data scientists will argue that their code does not need to be of a high standard because they arent working on a production system. Any code that feeds some business decision process should be considered as production code Data scientists may not consider the primary output of their work to be code, but this doesnt mean that the code they write shouldnt follow the standards expected by a software engineering team. In fact, it should have the following, minimum, characteristics: For more detail on these principles, take a look at a useful blog post by Thomas Huijskens from QuantumBlack . What is Kedro? Kedro is a development workflow tool that allows you to create portable data pipelines. It applies software engineering best practices to your data science code so it is reproducible, modular and well-documented. If you use Kedro you can worry less about how to write production-ready code (Kedro does the heavy lifting for you) and youll standardise the way your team collaborates, allowing you all to work more efficiently. Many data scientists need to perform the routine tasks of data cleaning, processing and compilation that may not be their favourite activities but form a large percentage of their day to day tasks. Kedro makes it easier to build a data pipeline to automate the heavy lifting and reduce the amount of time spent on this kind of task. Kedro has been created by QuantumBlack , an advanced analytics firm, that was acquired by the consultancy giant, McKinsey & Company, in 2015. QuantumBlack have used Kedro on more than 60+ projects with McKinsey and have now decided to open source it. Their goal is to enable staff, clients and third-party developers to use Kedro and build upon it. By extending this open tool for their own needs, data professionals are able to complete their regular tasks effectively and potentially share their enhancements to the community. Is Kedro a workflow scheduler? Kedro is not a workflow scheduler like Airflow and Luigi. Kedro makes it easy to prototype your data pipeline, while Airflow and Luigi are complementary frameworks that are great at managing deployment, scheduling, monitoring and alerting. A Kedro pipeline is like a machine that builds a car part. Airflow or Luigi tell different machines to switch on or off in order to work together and produce a car. QuantumBlack have built a Kedro-Airflow plugin, providing faster prototyping time and reducing the barriers to entry associated with moving pipelines to both workflow schedulers. What do I need to know? Kedros documentation has been designed for beginners to get started creating their own Kedro projects (in Python 3.5+). If you have just the very basic knowledge of Python then you might find the learning curve more challenging but bear with it and follow the documentation for guidance. A fifteen minute spaceflight with Kedro My easy-to-follow fifteen minute tutorial will be based on the following scenario: It is 2160 and the space tourism industry is booming. Globally, there are thousands of space shuttle companies taking tourists to the moon and back. You have been able to source three (fictional) datasets about the amenities offered in each space shuttle, customer reviews and company information. You want to train a linear regression model that uses the datasets to predict the price of shuttle hire. However, before you get to train the model, you need to prepare the data by doing some data engineering, which is the process of preparing data for model building by creating a master table. Getting set up The first step, is to pip install kedro into a new Python 3.5+ virtual environment (we recommend conda ), and confirm that it is correctly installed by typing You should see the Kedro graphic as shown below: This will additionally give you the version of Kedro that you are using. I wrote this tutorial based on version 0.14.0 in May 2019 and since this is an active and open source project, there may, over time, be changes to the codebase which cause minor changes to the example project. We will endeavour to keep the example code up to date, although not this blog post, so you should consult Kedro documentation and release notes if you hit upon any inconsistencies. Kedro workflow When building a Kedro project, you will typically follow a standard development workflow as shown in the diagram below. In this tutorial, I will walk through each of the steps. Set up the project template To keep things simple, the QuantumBlack team have provided all the code you need, so you simply need to clone the example project from its Github Repo . You do not need to set up a project template in this tutorial. Once you have the code for the Spaceflights project, to make sure you have the necessary dependencies for it, please run the following in a Python 3.5+ virtual environment: Set up the data Kedro uses configuration files to make a projects code reproducible across different environments, when it may need to reference datasets in different locations. To set up data for a Kedro project, you will typically add datasets to the data folder and configure the registry of data sources that manages the loading and saving of data This tutorial makes use of three datasets for spaceflight companies shuttling customers to the moon and back, and uses two data formats: .csv and .xlsx. The files are stored in the data/01_raw/ folder of the project directory. To work with the datasets provided, all Kedro projects have a conf/base/catalog.yml file which acts as a registry of the datasets in use. Registering a dataset is as simple as adding a named entry into the .yml file to include file location (path), parameters for the given dataset, type of data and versioning. Kedro comes with support for a few types of data, such as csv, and if you look at the catalog.yml file for the Spaceflights tutorial, you will see the following entries for the datasets used: To check whether Kedro loads the data correctly, and to inspect the first five rows of data, open a terminal window and start an IPython session in the Kedro project directory (and just type ( exit() when you want to end the session and continue with the tutorial): Create and run the pipeline The next part of the workflow is to create a pipeline from a set of nodes, which are Python functions that perform distinct, individual tasks. In a typical project, this stage of the project comprises of three steps: Data engineering pipeline We have reviewed the raw datasets for the Spaceflights project, and it is now time to consider the data engineering pipeline that processes the data and prepares it for the model within the data science pipeline. This pipeline preprocesses two datasets and merges them with a third into a master table. In the file data_engineering.py inside the nodes folder youll find the preprocess_companies and preprocess_shuttles functions (specified as nodes within the pipeline in pipeline.py ). Each function takes in a dataframe and outputs a pre-processed data, preprocessed_companies and preprocessed_shuttles respectively.. When Kedro runs the data engineering pipeline, it determines whether datasets are registered in the data catalog ( conf/base/catalog.yml ). If a dataset is registered, it is persisted automatically to the path specified without a need to specify any code in the function itself. If a dataset is not registered, Kedro stores it in memory for the pipeline run and removes it afterwards. In the tutorial, the preprocessed data is registered in conf/base/catalog.yml : CSVLocalDataSet is chosen for its simplicity, but you can choose any other available dataset implementation class to save the data, for example, to a database table, cloud storage (like S3, Azure Blob Store etc.) and others. The data_engineering.py file also includes a function, create_master_table which the data engineering pipeline uses to join together the companies, shuttles and reviews dataframes into a single master table. Data science pipeline The data science pipeline, which builds a model that uses the datasets, is comprised of a price prediction model, which uses a simple LinearRegression implementation from the scikit-learn library. The code is found in the file src/kedro_tutorial/nodes/price_prediction.py . The test size and random state parameters for the prediction model are specified in conf/base/parameters.yml and Kedro feeds them into the catalog when the pipeline is executed. Combining the pipelines The projects pipelines are specified in pipeline.py : The de_pipeline will preprocess the data, then use create_master_table to combine preprocessed_shuttles , preprocessed_companies and reviews into the master_table dataset. The ds_pipeline will then create features, train and evaluate the model. The first node of the ds_pipeline outputs 4 objects: X_train , X_test , y_train , y_test ] which are then used to train the model and, in a final node, to evaluate the model. The two pipelines are merged together in de_pipeline + ds_pipeline . The order in which you add the pipelines together is not significant since Kedro automatically detects the correct execution order for all the nodes. The same overall pipeline would result if you specified ds_pipeline + de_pipeline . Both pipelines will be executed when you invoke kedro run, and you should see output similar to the following: If you have any problems getting the tutorial code up and running, you should check that you are working in a Python 3.5 environment and have all the dependencies installed for the project. If there are still problems, you should head over to Stack Overflow for guidance from the community and, if the behaviour youre observing appears to be a problem with Kedro, please feel to raise an issue on Kedros Github repository . Kedro runners There are two different ways to run a Kedro pipeline. You can specify: By default, Kedro uses a SequentialRunner when you invoke kedro run. Switching to use ParallelRunner is as simple as providing an additional flag as follows: Thats the end of the tutorial to run through the basics of Kedro. Theres plenty more documentation on the various configuration options and optimisations you can use in your own projects, for example: Contributions welcome! If you start working with Kedro and want to contribute an example or changes to the Kedro project, we would welcome the chance to work with you. Please consult the contribution guide . Acknowledgements The Spaceflights example is based on a tutorial written by the Kedro team at QuantumBlack Labs (Yetunde Dada, Ivan Danov, Dmitrii Deriabin, Lorena Balan, Gordon Wrigley, Kiyohito Kunii, Nasef Khan, Richard Westenra and Nikolaos Tsaousis), who have kindly given me early access to their code and documentation in order to produce this tutorial for TowardsDataScience. Thanks all! 441 3 441 441 3 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Shuvashish Chatterjee Jun 4, 2019 Alexa find me a parking. Can AI guide you to a vacant parking slot? Detecting parking lot occupancy from a security cam footage using deep learning Finding a vacant spot in a parking lot is a tough ask. It is even difficult to manage these lots if incoming traffic varies a lot. Which slots are vacant at this instant? What time do we need more slots? Are commuters finding it difficult to reach a particular slot 8 min read 8 min read Share your ideas with millions of readers. Emanuele Fabbiani Jun 4, 2019 Member-only Lessons from a real Machine Learning project, part 2: the traps of data exploration How to fall into the pitfalls of data exploration and get away This is the second story of the series, so Im going to brutally shrink the intro. Im writing to share what a real, enterprise-level Machine Learning project taught me and my team. If you are curious to know more, feel free to check out the first chapter: from Jupyter to 5 min read 5 min read Some Aditya Mandal Jun 4, 2019 Evolution of Machine Translation In 1949, Warren Weaver, a researcher at Rockefeller Foundation, presented a set of proposals for machine based translations which were based on information theory and successes in code breaking during the Second World War. After few years, the machine translation research began in earnest in many US universities. As described 10 min read 10 min read Ziad SALLOUM Jun 4, 2019 Member-only Eligibility Traces in Reinforcement Learning Sometimes looking backward isnt that bad Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com What is Eligibility Traces ? In short and a straight forward manner, Eligibility Traces is a kind of mathematical trick that improves the performance of Temporal Difference methods, in Reinforcement Learning. Here are the benefits of Eligibility Traces: 7 min read 7 min read Adam King Jun 4, 2019 Member-only Optimizing deep learning trading bots using state-of-the-art techniques Lets teach our deep RL agents to make even more money using feature engineering and Bayesian optimization In the last article, we used deep reinforcement learning to create Bitcoin trading bots that dont lose money. Although the agents were profitable, the results werent all that impressive, so this time were going to step it up a notch and massively improve our models profitability. As a reminder, the 17 min read 17 min read Jo Stichbury Rnin technology writer and podcast host. Cat herder. Dereferences NULL. More from Medium Khuyen Tran in The Prefect Blog Orchestrate Your Data Science Project with Prefect 2.0 Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Naga Sanjay Continuous Training of ML models. Madison Hunter in Towards Data Science How to Write Good Code Documentation for Data Scientists Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 6362,\n",
       "  'url': 'https://towardsdatascience.com/the-easiest-github-tutorial-ever-4a3aa0396039',\n",
       "  'title': 'The easiest GitHub tutorial\\xa0ever',\n",
       "  'subtitle': '-',\n",
       "  'claps': 117,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 3,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-12-05',\n",
       "  'clap_prop': 5.779253271082049e-05,\n",
       "  'text': 'Towards Data Science Dec 5 2019 Member-only Listen Save easiest GitHub tutorial ever ease Git GitHub made super simple tutorial get started learn advantage Git GitHub tutorial five super simple step basic fundamentally important understanding GitHub practical tutorial want in-depth read GitHub check article need know getting started GitHub 1 Make repository GitHub Go GitHub.com click Create Repository Give repository name description Check box Initialize repository README readme simple text file put information current project click Create repository 2 Clone repository local pc ITowork code need get repository onto pc done cloning easiest way get done GitHub Desktop course GitHub Desktop installed see following screen Choose correct location clone repository able find computer 3 Add code file repository step ignore GitHub browse pc location cloned repository create test file Name file testfile Put text save file 4 Commit push code repository ha changed update new version GitHub Go back GitHub Desktop see ha already noticed changed something Two step undertaken upload code First type small comment like Create testfile.txt click Commit master Second click Push origin 5 Verify code ha changed GitHub Finished already verify really worked see change GitHub go GitHub page repository youll see code ha changed taken first step world GitHub course much know stay tuned Thanks reading 180 180 180 Towards Data Science home data science Medium publication sharing concept idea code Joos Korstanje Data Scientist Machine Learning R Python AWS SQL Medium Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Jason Chong Geek Culture Struggling Land Data Science Job Try Virtual Internships Free Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Dec 5, 2019 Member-only Listen Save The easiest GitHub tutorial ever For those who are not at ease with Git and GitHub, I made this super simple tutorial to get started and learn what the advantage of Git and GitHub can be. After this tutorial in five super simple steps, you will have a very basic, but fundamentally important, understanding of GitHub. This is a practical tutorial. If you want a more in-depth read about GitHub, check out my other article here:   All you need to know before getting started with GitHub  . 1. Make a repository on GitHub Go to GitHub.com and click on Create Repository. Give your repository a name and a description. Check the box at Initialize this repository with a README (the readme is a simple text file in which you can put information about the current project) Then click Create repository. 2. Clone the repository to your local pc ITowork on your code, you need to get this repository onto your pc. This is done by cloning it. The easiest way to get this done is through GitHub Desktop. When you do this (of course you should have GitHub Desktop installed) you see the following screen: Choose the correct location and clone your repository. You will now be able to find it on your computer. 3. Add a code file to your repository For this step, ignore GitHub. Just browse on your pc to the location where you have cloned the repository and create a test file: Name the file testfile: Put some text in it and save the file: 4. Commit and push the code Our repository has now changed and we should update the new version to GitHub. Go back to GitHub Desktop and see that it has already noticed that you changed something: Two steps have to be undertaken to upload your code: First, type a small comment like Create testfile.txt and click on Commit to master. Second, click on Push origin: 5. Verify that the code has changed on GitHub Finished already! To verify that it really worked, you can now see your change on GitHub. Just go to the GitHub page of your repository and youll see that the code has changed! You have just taken your first step in the world of GitHub. Of course, there is much more to know, so stay tuned. Thanks for reading! 180 180 180 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Joos Korstanje Data Scientist Machine Learning R, Python, AWS, SQL More from Medium Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Jason Chong in Geek Culture Struggling to Land a Data Science Job? Try These Virtual Internships For Free Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 2835,\n",
       "  'url': 'https://towardsdatascience.com/how-to-write-python-command-line-interfaces-like-a-pro-f782450caf0d',\n",
       "  'title': 'How to Write Python Command-Line Interfaces like a\\xa0Pro',\n",
       "  'subtitle': '-',\n",
       "  'claps': 907,\n",
       "  'responses': 6.0,\n",
       "  'reading_time': 8,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-11-08',\n",
       "  'clap_prop': 0.000448015616826617,\n",
       "  'text': 'Towards Data Science Nov 8 2019 Member-only Listen Save Write Python Command-Line Interfaces like Pro Data Scientists face many repetitive similar task includes creating weekly report executing extract transform load ETL job training model using different parameter set Often end bunch Python script change parameter code every time run hate Thats got habit transforming script reusable command-line interface CLI tool increased efficiency made productive daily life started using Argparse wa enjoyable produce lot ugly code thought cant achieve without write lot code even enjoy writing CLI tool Click friend Click webpage Click aim make process writing command-line tool quick fun also preventing frustration caused inability implement intended CLI API sound great doesnt article give hands-on guide build Python CLIs using Click build example step step show basic feature benefit Click offer tutorial able write next CLI tool joy blink eye let get hand dirty Tutorial tutorial build Python CLI using Click evolves step step start basic step introduce new concept offered Click Apart Click use Poetry manage dependency package Preparation First let install Poetry various way see article use pip Next use Poetry create project named cli-tutorial add click funcy dependency create file cli.py later fill code added funcy make use later see module good refer interested reader article ready go implement first CLI side note example code available GitHub account First Click CLI initial CLI read CSV file disk process important tutorial store result Excel file path input- output file configurable user user must specify input file path Specifying output file path optional default output.xlsx Using Click code doe read invoke CLI various way Cool created first CLI using Click Note implemented read_csv process_csv write_excel assume exist supposed One issue CLIs pas parameter generic string issue string must parsed actual type fail due badly formatted user input Look example use path try load CSV file user provide string doe represent path even string formatted correctly corresponding file might exist dont right permission access Wouldnt desirable automatically validate input parse possible fail early helpful error message Ideally without write lot code Click support u specifying type argument Type Specification example CLI want user pas valid path existing file read permission requirement fulfilled load input file Additionally user specifies output file path valid path enforce passing click.Path object type argument click.option decorator click.Path one various type offered Click box also implement custom type scope tutorial detail refer interested reader Click documentation Boolean Flags Another helpful feature offered Click boolean flag Probably famous boolean flag verbose flag set true tool print lot information terminal set false thing printed Click implement add another click.option decorate set is_flag=True get verbose output need call CLI Feature Switch Assume want store result process_csv locally also want upload server Additionally one target server development- testing- production instance access three instance via different URLs One option user select server pas full URL argument ha type error-prone also tedious job situation like use feature switch simplify user life best explained code added three click.option decorator three possible server URLs important bit three option target variable server_url Depending option choose value server_url equal corresponding value defined flag_value chose one adding -- dev -- test -- prod argument execute server_url equal http //test.server.com/api/v2/upload dont specify three flag Click take value -- prod set default=True Username Password Prompts Unfortunately rather luckily server password protected upload file need username password sure provide standard click.option argument However password end command history plain text become security issue like prompt user type password without echoing terminal without storing command history username also want simple prompt echoing Nothing easier know Click code add prompt argument set prompt=True add prompt whenever user doe specify -- user argument still specify like default value used hit enter prompt default determined using function another handy feature offered Click Prompting password without echoing terminal asking confirmation common Click offer dedicated decorator called password_option important note prevent user pas password via -- password MYSECRETPASSWORD enables thats Weve build full CLI call day like give final hint next section Poetry Scripts final tip want give related Click perfectly match CLI topic creating Poetry script Poetry script create executables invoke Python function command line Setuptools script doe look like First need add following pyproject.toml file your-wanted-name alias function process defined module cli_tutorial.cli invoke allows example add multiple CLI function file define alias dont add __name__ == __main__ block Wrap article showed use Click Poetry build CLI tool joy become productive wa small subset feature offered Click many others like callback nested command choice option mention refer interested reader Click documentation might write follow-up post covering advanced topic Stay tuned thank following along post Feel free contact question comment suggestion 1.1K 8 1.1K 1.1K 8 Get email whenever Simon Hawe publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Hari Santanam Nov 8 2019 Member-only Cloud Risk Assessment Data- log analysis AWS Using EMR Cluster HiveQL S3 analyze cloud data log Cloud audit become necessity organization business governmental educational entity increasingly migrate technology infrastructure process 8 min read 8 min read Share idea million reader Bartosz Telenczuk Nov 8 2019 Managing virtual environment pyenv Python developer data scientist already heard virtual environment However managing ten environment created different project daunting pyenv help streamline creation management activating virtual environment old day virtualenv became popular would keep 5 min read 5 min read Rahil Vijay Nov 8 2019 Member-only Alternative Batch Normalization development Batch Normalization BN normalization technique wa turning point development deep learning model enabled various network train converge Despite great success BN exhibit drawback caused distinct behavior normalizing along batch dimension One 7 min read 7 min read Lj Flores Nov 7 2019 Member-only Typhoon Ruby 2014 sentiment analysis Classifying Facebook comment sentiment using Latent Dirichlet Analysis high school short internship Dr. Gerry Bagtasa university professor Institute Environmental Science Meteorology University Philippines Diliman 6 min read 6 min read Ryan P. Dalton Nov 7 2019 Member-only City Homeless Humanitarian Crisis Streets Los Angeles City Within City City Angels anyone tell many city one Traveling Westwood West Adams Silverlake Skid Row youll find nothing le microcosm United States one hundred forty nation worth 6 min read 6 min read Simon Hawe Tech programming enthusiast working Joyn mainly focusing data sciene machine learning data engineering python coding Medium Liu Zheng Better Programming Build Python Script Command-line Tool Frank Andrade Goodbye Low-Quality Udemy Courses Hello Subscription Courses Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Ryan Michael Kay Didact Publication Clear Code Write Code Easy Read Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Nov 8, 2019 Member-only Listen Save How to Write Python Command-Line Interfaces like a Pro We as Data Scientists face doing many repetitive and similar tasks. That includes creating weekly reports, executing extract, transform, load ( ETL) jobs, or training models using different parameter sets. Often, we end up having a bunch of Python scripts, where we change parameters in code every time we run them. I hate doing this! Thats why I got into the habit of transforming my scripts into reusable command-line interface (CLI) tools. This increased my efficiency and made me more productive in my daily life. I started doing this using Argparse but this was not enjoyable as I had to produce a lot of ugly code. So I thought, cant I achieve that without having to write a lot of code over and over again? Can I even enjoy writing CLI tools? Click is your friend! So what is Click? From the webpage: It (Click) aims to make the process of writing command-line tools quick and fun while also preventing any frustration caused by the inability to implement an intended CLI API. To me, that sounds great, doesnt it? In this article, I give you a hands-on guide on how to build Python CLIs using Click. I build up an example step by step that shows you the basic features and benefits Click offers. After this tutorial, you should be able to write your next CLI tool with joy and in a blink of an eye :) So lets get our hands dirty! The Tutorial In this tutorial, we build up a Python CLI using Click that evolves step by step. I start with the basics, and with each step, I introduce a new concept offered by Click. Apart from Click, I use Poetry to manage dependencies and packages. Preparation First, lets install Poetry. There are various ways of doing that, see my article , but here we use pip Next, we use Poetry to create a project named cli-tutorial, add click and funcy as a dependency, and create a file cli.py that we later fill with code I have added funcy, as I will make use of it later. To see what that module is good for, I refer the interested reader to this article . Now, we are ready to go and implement our first CLI. As a side note, all example code is available on my GitHub account. Our First Click CLI Our initial CLI reads a CSV file from disk, processes it (how is not important for this tutorial), and stores the result in an Excel file. Both, the path to the input-, and the output file should be configurable by the user. The user must specify the input file path. Specifying the output file path is optional and defaults to output.xlsx . Using Click, the code that does that reads as What do we do here? Now, you can invoke this CLI in various ways Cool, we have created our first CLI using Click! Note that, I have not implemented read_csv , process_csv , and write_excel but assume they exist and do what they are supposed to do. One issue with CLIs is that we pass parameters as generic strings. Why is that an issue? Because these strings must be parsed to the actual types, which can fail due to badly formatted user input. Look at our example where we use paths and try to load a CSV file. A user can provide a string that does not represent a path at all. And even if the string is formatted correctly, the corresponding file might not exist or you dont have the right permission to access it. Wouldnt it be desirable to automatically validate the input, parse it if possible or fail early with helpful error messages? Ideally, all that without having to write a lot of code? Click supports us with this by specifying types for our arguments. Type Specification In our example CLI, we want the user to pass in a valid path to an existing file for which we have read permissions . If these requirements are fulfilled, we can load the input file. Additionally, if the user specifies an output file path, this should be a valid path. We can enforce all that by passing a click.Path object to the type argument of the click.option decorator click.Path is one of the various types offered by Click out of the box. You can also implement custom types, but this is out of scope for this tutorial. For more details, I refer the interested readers to the Click documentation . Boolean Flags Another helpful feature offered by Click is boolean flags. Probably, the most famous boolean flag is the verbose flag. If set to true, your tool will print out a lot of information to the terminal. If set to false, only a few things are printed. With Click, we can implement that as All you have to do is add another click.option decorate and set is_flag=True . Now, to get a verbose output, you need to call the CLI as Feature Switch Assume we do not only want to store the result of process_csv locally, but we also want to upload it to a server. Additionally, there is not only one target server but a development-, a testing-, and a production instance. You can access these three instances via different URLs. One option for a user to select the server is to pass the full URL as an argument, which she has to type in. But, this is not only error-prone but also a tedious job. In situations like that, I use feature switches to simplify the users life. What they do is best explained through code Here, I have added three click.option decorators for the three possible server URLs. The important bit is, that all three options have the same target variable server_url . Depending on which option you choose,  the value of server_url is equal to the corresponding value defined in flag_value . You chose one of those by adding -- dev , -- test , or -- prod as an argument. So when you execute server_url is equal to https://test.server.com/api/v2/upload. If we dont specify any of the three flags, Click takes the value of --prod, as I set default=True . Username & Password Prompts Unfortunately, or rather luckily :), our servers are password protected. So to upload our file, we need a username and a password. For sure, you can provide those as a standard click.option arguments. However, your password ends up in your command history in plain text. This can become a security issue. We like a prompt for the user to type his password without echoing it to the terminal and without storing it in the command history. For the username, we also want a simple prompt with echoing. Nothing easier than that when you know Click. Here is the code. To add a prompt for an argument, you have to set prompt=True . This will add a prompt whenever a user does not specify the -- user argument, but she can still specify it like that. The default value is used, when you just hit enter on the prompt. The default is determined using a function, which is another handy feature offered by Click. Prompting passwords without echoing it to the terminal and asking for a confirmation is so common, that Click offers a dedicated decorator called password_option . An important note; this will not prevent the user to pass the password via --password MYSECRETPASSWORD. It just enables her not doing that. And thats it. Weve build the full CLI. Before we call it a day, I like to give you a final hint in the next section. Poetry Scripts A final tip that I want to give you, that is not related to Click but perfectly matches the CLI topic, is creating Poetry scripts . With Poetry scripts, you can create executables to invoke your Python functions from the command line, as you can do with Setuptools scripts . So how does that look like? First, you need to add the following to your pyproject.toml file The your-wanted-name is an alias for the function process defined in the module cli_tutorial.cli . Now, you can invoke that through This allows you, for example, to add multiple CLI functions to the same file, to define aliases, and you dont have to add an if __name__ == __main__ block. Wrap Up In this article, I showed you how to use Click and Poetry to build CLI tools with joy and become more productive. This was just a small subset of the features offered by Click. There are many others like callbacks , nested commands , or choice options just to mention a few. For now, I refer the interested reader to the Click documentation but I might write a follow-up post covering these advanced topics. Stay tuned and thank you for following along this post. Feel free to contact me for questions, comments, or suggestions. 1.1K 8 1.1K 1.1K 8 Get an email whenever Simon Hawe publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Hari Santanam Nov 8, 2019 Member-only Cloud Risk Assessment through Data- log analysis in AWS Using EMR Cluster, HiveQL and S3 to analyze cloud data logs Cloud and IT audits have become a necessity in most organizations as business, governmental and educational entities increasingly migrate their technology infrastructures and processes. 8 min read 8 min read Share your ideas with millions of readers. Bartosz Telenczuk Nov 8, 2019 Managing virtual environments with pyenv Most Python developers and data scientist have already heard of virtual environments. However, managing tens of environments created for different projects can be daunting. pyenv will help you to streamline the creation, management and activating virtual environments. In the old days, before the virtualenv became popular, I would keep a 5 min read 5 min read Rahil Vijay Nov 8, 2019 Member-only An Alternative To Batch Normalization The development of Batch Normalization(BN) as a normalization technique was a turning point in the development of deep learning models, it enabled various networks to train and converge. Despite its great success, BN exhibits drawbacks that are caused by its distinct behavior of normalizing along the batch dimension. One of 7 min read 7 min read Lj Flores Nov 7, 2019 Member-only Typhoon Ruby, 2014: A sentiment analysis Classifying Facebook comments sentiments using Latent Dirichlet Analysis In high school, I did a short internship with Dr. Gerry Bagtasa a university professor at the Institute of Environmental Science and Meteorology, University of the Philippines Diliman. 6 min read 6 min read Ryan P. Dalton Nov 7, 2019 Member-only The City of the Homeless: Humanitarian Crisis on the Streets of Los Angeles A City Within a City The City of Angels, as most anyone here will tell you, is many cities in one. Traveling from Westwood to West Adams or from Silverlake to Skid Row, youll find nothing less than a microcosm of the United States: one hundred and forty nations worth 6 min read 6 min read Simon Hawe Tech and programming enthusiast working at Joyn mainly focusing on data sciene, machine learning, data engineering, and python coding. More from Medium Liu Zheng in Better Programming Build Your Python Script Into a Command-line Tool Frank Andrade Goodbye Low-Quality Udemy Courses. Hello, Subscription Courses! Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Ryan Michael Kay in Didact Publication Clear Code: How To Write Code That Is Easy To Read Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4512,\n",
       "  'url': 'https://towardsdatascience.com/master-geographic-data-science-with-real-world-projects-exercises-96ac1ad14e63',\n",
       "  'title': 'Master Geographic Data Science with Real World projects & Exercises',\n",
       "  'subtitle': 'Real World projects & Exercises',\n",
       "  'claps': 125,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-05-20',\n",
       "  'clap_prop': 6.174415887908172e-05,\n",
       "  'text': 'Towards Data Science May 20 2019 Member-only Listen Save Getting started Geographic Data Science Python Tutorials Real World project Exercises first article three-part series article Getting started Geographic Data Science Python learn reading manipulating analysing Geographic data Python article series designed sequential first article lay foundation second one get intermediate advanced level Geographic data science topic third part cover relevant real-world project wrapping cement learning tutorial also ha simple exercise help learn practice code dataset Google Colab Jupyter notebook available link end article focus Geographic Vector data series another coming series learn satellite image raster data analysis series contains Three part first part tutorial learn basic loading processing geographic data using Geopandas Geopandas workhorse Geographic data science Python built top Pandas Numpy library Like Pandas Dataframe Geopandas data structure contains GeodataFrame GeoSeries Geopandas provides capability read manipulate geographic data easily also perform many essential geospatial operation including among geometric operation projection geographic analysis also visualize plot map Geopandas- provides high-level interface Matplotlib library- using .plot method GeodataFrame/GeoSeries learning objective part Introduction Geographic Data science 1 Reading Geographic data tutorial use mainly 3 datasets Geographic Vector data come different format Shapefiles Geopackage Geojson etc Loading Geodata Formats Geopandas straightforward use .read_file Let see example reading data case read country dataset First created variable hold file path used Geopandas .read_file method read country dataset Geopandas take care geometry column enables u carry geoprocessing task example plotting map good way start data exploration look first row shape data well general statistic data possible following command first 5 row data look like One example reading data Geopandas time read city dataset come Geojson file technique used read country dataset apply carry also exploration using .head .shape .describe get feeling dataset exploration go ahead plot map Plotting map Geopandas easy available .plot function Since two datasets country city data overlay display map set subplots using Matplotlib pas axis Geopandas .plot function output map two-three line code able produce nice map time small exercise part Exercise 1.1 Read river data Exercise 1.2 Read first 5 row river dataset Exercise 1.3 Visualize river dataset 2 Coordinate system Projections Coordinate reference system represent data two dimensional planar relates actual place earth glue hold attribute respective location Geodataframes ha .crs attribute give original CRS used data easy transform project coordinate However perform projection necessary CRS order carry geographic analysis get right value analysis country city river CRS Let u check country CRS output code init epsg:4326 .EPSG stand European Petroleum Survey Group authority maintains spatial reference system code 4326 indicates Geographic Coordinate System used case WGS84 World Geodetic System 1984 Different CRS different measurement coordinate defined decimal degree others defined meter common process reproject data one format another Geographic data processing source useful visualizing comparing different Projections Mercator vs. Robinson Compare Map Projections Compare map projection Mercator Robinson map-projections.net project data Mercator Mercator projection latitude-longitude quadrangle stretched along x-axis y-axis move away equator first let u geometry column country dataset output code print latitude longitude Polygons coordinate decimal degree 0 POLYGON 117.7036079039552 4.163414542001791 1 POLYGON 117.7036079039552 4.163414542001791 2 POLYGON -69.51008875199994 -17.506588197999 3 POLYGON -69.51008875199994 -17.506588197999 4 POLYGON -69.51008875199994 -17.506588197999 Let u project data see change example project EPSG:3395 widely used Mercator projection geometry column data look like 0 POLYGON 13102705.69639943 460777.6522179524 1 POLYGON 13102705.69639943 460777.6522179524 2 POLYGON -7737827.684867887 -1967028.7849201 3 POLYGON -7737827.684867887 -1967028.7849201 4 POLYGON -7737827.684867887 -1967028.7849201 Due projection geometry longer measured decimal style point metre unit easier understand difference map Let u plot original country projected country Notice different scale x map Mercator projection distorts size object go equator pole Africa look small Greenland appears much larger size try overlay projected data unprojected data data align properly Let u see plot city top projected country Remember projected city see city overlayed properly projected country dataset fall near Africa proper place order align properly also need project projection country dataset EPSG:3395 exercise Exercise 2.1 Convert city data EPSG:3395 projection plot city top countries_proj 3 Write Geographic Data easily save new data created local disk helpful want access file another time without carrying operation Let u save projected country Remember projected Geopandas ha .to_file method save file might want download file since using collab configure Google drive erased close session Google Colab guessed also need save projected city exercise 2.1 right last exercise part Exercise 3.1 Save projected city file created exercise 2.1 file Conclusion tutorial covered basic loading writing Geographic data well Geographic coordinate system projection next tutorial learn Geoprocessing manipulation Geographic data using Geopandas code available GitHub repository shakasom/GDS Geographic data science tutorial series Contribute shakasom/GDS development creating account GitHub github.com also go directly run Google Collaboraty Jupyter Notebooks directly link Google Colaboratory Edit description colab.research.google.com 179 5 179 179 5 Enjoy read Reward writer Beta tip go Abdishakur third-party platform choice letting know appreciate story Get email whenever Abdishakur publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Nick Latocha May 20 2019 know hire data scientist CEO forward email popular management newsletter subject contains two letter A.I Straight away sink seat Everyone hailing AI Data Scientist saviour company 4 min read 4 min read Share idea million reader Jovan Medford May 20 2019 Member-only Rudimentary k-Nearest Neighbors intuitive ML model Sometimes data science find picking missile launcher order kill mere ant case though going discussing probably intuitive machine learning algorithm fooled however spite simplicity kNN 5 min read 5 min read Kunal Dhariwal May 20 2019 Member-only Create Virtual Personal Assistant know Cortana Siri Google Assistant right ever imagined make virtual personal assistant customize want Today well Well building personal assistant scratch python Oh getting let 5 min read 5 min read Bilal Maqsood May 20 2019 Schedulers YARN concept configuration FIFO capacity fair scheduler choice start delving world big data number new word acronym start showing YARN also one 5 min read 5 min read Rob Salgado May 20 2019 Member-only Hyperparameter Tuning Google Cloud Platform Scikit-Learn Google Cloud Platforms AI Platform formerly ML Engine offer hyperparameter tuning service model take extra time effort learn use instead running code already virtual machine 10 min read 10 min read Abdishakur Writing Geospatial Data Science AI ML DL Python SQL GIS Top writer 1m view Medium Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Nik Piepenbreier Better Programming Make Awesome Maps Python Geopandas Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science May 20, 2019 Member-only Listen Save Getting started with Geographic Data Science in Python Tutorials, Real World projects & Exercises This is the first article of a three-part series of articles in Getting started Geographic Data Science with Python. You will learn about reading, manipulating and analysing Geographic data in Python. The articles in this series are designed to be sequential where the first article lays the foundation and the second one gets into intermediate and advanced level Geographic data science topics. The third part covers a relevant and real-world project wrapping up to cement your learning. Each tutorial also has some simple exercises to help you learn and practice. All code, dataset and Google Colab Jupyter notebooks are available from the link at the end of this article. I will focus only on Geographic Vector data in this series. In another coming series, We will learn about satellite images and raster data analysis. The series contains Three parts: This is the first part. In this tutorial, we will learn the basics of loading and processing geographic data using Geopandas. Geopandas, the workhorse of Geographic data science in Python, is built on top of Pandas and Numpy libraries. Like Pandas Dataframe, Geopandas data structure contains GeodataFrame and GeoSeries. Geopandas provides not only the capability to read and manipulate geographic data easily but also can perform many essential geospatial operations including among other geometric operations, projections and geographic analysis. You can also visualize and plot maps with Geopandas- It provides a high-level interface to the Matplotlib library- by using the .plot() method on GeodataFrame/GeoSeries. These are the learning objectives for this part, Introduction to Geographic Data science: 1. Reading Geographic data In this tutorial we will use mainly 3 datasets: Geographic (Vector) data comes in different formats (Shapefiles, Geopackage, Geojson etc). Loading most of Geodata Formats with Geopandas is straightforward. We can use .read_file(). Let see an example of reading the data. In this case, we will read the countries dataset. First, we created a variable to hold the file path and then we have used Geopandas, .read_file() method to read the countries dataset. Geopandas takes care of the geometry column which enables us to carry out geoprocessing tasks, for example, plotting maps. A good way to start your data exploration is to look at the first few rows, the shape of the data, as well as general statistics of the data. This is possible through the following commands. This is how the first 5 rows of the data looks like. One more example of reading data in Geopandas and this time we will read the cities dataset. It comes as Geojson file but the same techniques we have used to read the countries dataset apply here. We can carry out also the same exploration using .head() , .shape() and .describe() to get a feeling of what this dataset is about. Once we do the explorations, we can go ahead and plot maps. Plotting maps in Geopandas is easy and available through .plot() function. Since we have two datasets countries and cities data, we can overlay them and display it as a map. Here we set up the subplots using Matplotlib and pass the axis to Geopandas .plot() function. This is the output map. With just two-three lines of code, we are able to produce this nice map. It is time for small exercise from your part. Exercise 1.1: Read the rivers data Exercise 1.2: Read the first 5 rows of the rivers dataset Exercise 1.3: Visualize rivers dataset. 2. Coordinate systems and Projections Coordinate reference systems represent how our data as two dimensional (planar) relates to actual places on earth. It is the glue that holds the attributes to their respective locations. Geodataframes has .crs attribute that can give you the original CRS used in the data. It is easy to transform and project these coordinates. However, to perform projections, it is necessary to have the same CRS in order to carry out geographic analysis and get the right values out the analysis. The countries, cities and rivers have the same CRS. Let us check the countries CRS. This is the output of the above code {init: epsg:4326}.EPSG stands for European Petroleum Survey Group and is an authority that maintains spatial reference systems. The code 4326 indicates which Geographic Coordinate System is used, in this case (WGS84) The World Geodetic System of 1984. Different CRS have different measurements. For some, the coordinates are defined in decimal degrees while others are defined in meters. It is a common to process and reproject data from one format to another in Geographic data processing. This source is very useful in visualizing and comparing different Projections: Mercator vs. Robinson: Compare Map Projections Compare the map projections Mercator and Robinson map-projections.net We will project our data into Mercator. The Mercator projection, latitude-longitude quadrangles are stretched along the x-axis and y-axis as you move away from the equator. But first, let us the geometry column of the countries dataset. This is the output of the above code. It just prints out the latitude and longitude of the Polygons. These coordinates are in decimal degrees now. 0 (POLYGON ((117.7036079039552 4.163414542001791 1 (POLYGON ((117.7036079039552 4.163414542001791 2 (POLYGON ((-69.51008875199994 -17.506588197999 3 (POLYGON ((-69.51008875199994 -17.506588197999 4 (POLYGON ((-69.51008875199994 -17.506588197999 Let us project this data and see the changes. In this example, we project to EPSG:3395 which is the widely used Mercator projection. Now our geometry column data looks like this: 0 (POLYGON ((13102705.69639943 460777.6522179524 1 (POLYGON ((13102705.69639943 460777.6522179524 2 (POLYGON ((-7737827.684867887 -1967028.7849201 3 (POLYGON ((-7737827.684867887 -1967028.7849201 4 (POLYGON ((-7737827.684867887 -1967028.7849201 Due to the projection, the geometry is no longer measured in decimal style points but in a metre unit. It is easier to understand the difference in maps. Let us plot both the original countries and the projected countries. Notice the different scales of x and y in both maps. The Mercator projection distorts the size of the objects as we go further from the equator to the poles. That is why Africa looks small and Greenland appears much larger than its size. If you try to overlay the projected data with unprojected data, then your data will not align properly. Let us see if we can plot cities on the top of projected countries. Remember we have not projected the cities. As you can see the cities are not overlayed properly in the projected countries dataset. They fall near Africa and that is not their proper place. In order to align them properly, we also need to project the same projection of the countries dataset, EPSG:3395 and that is an exercise for you. Exercise 2.1: Convert the cities data into EPSG:3395 projection and plot cities on top of countries_proj. 3. Write Geographic Data We can easily save any new data created to our local disk. This is helpful when you want to access that file in another time without carrying out the same operations again. Let us save our projected countries. Remember we have projected it. Geopandas has .to_file() method. And that will save your file. You might want to download this file since we are using collab and did not configure it with Google drive. This will be erased when you close your session in Google Colab. If you have guessed that we also need to save the projected cities from exercise 2.1, you are right. That is your last exercise for this part. Exercise 3.1: Save the projected cities file you created in exercise 2.1 into a file Conclusion In this tutorial, we have covered the basics of loading and writing Geographic data as well as Geographic coordinate system and projections. In the next tutorial, we will learn the Geoprocessing and manipulation of Geographic data using Geopandas. The code is available in this GitHub repository: shakasom/GDS Geographic data science tutorials series. Contribute to shakasom/GDS development by creating an account on GitHub. github.com You can also go directly and run Google Collaboraty Jupyter Notebooks directly from this link: Google Colaboratory Edit description colab.research.google.com 179 5 179 179 5 Enjoy the read? Reward the writer. Beta Your tip will go to Abdishakur through a third-party platform of their choice, letting them know you appreciate their story. Get an email whenever Abdishakur publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Nick Latocha May 20, 2019 What you should know before you hire a data scientist Your CEO forwards you an email from a popular management newsletter. The subject contains the two letters A.I. Straight away, you sink into your seat. Everyone is hailing AI, and the Data Scientist, as the saviour of your company. 4 min read 4 min read Share your ideas with millions of readers. Jovan Medford May 20, 2019 Member-only Rudimentary k-Nearest Neighbors The most intuitive ML model Sometimes in data science you find yourself picking up a missile launcher in order to kill a mere ant. In this case though, we are going to be discussing what is probably the most intuitive machine learning algorithm. Do not be fooled however, in spite of its simplicity, the kNN 5 min read 5 min read Kunal Dhariwal May 20, 2019 Member-only Create your own Virtual Personal Assistant You know about Cortana, Siri and Google Assistant, right? Have you ever imagined that you can make your own virtual personal assistant and customize it as you want? Today, well be doing it here. Well be building a personal assistant from scratch in python. Oh, Before getting into it, let 5 min read 5 min read Bilal Maqsood May 20, 2019 Schedulers in YARN: from concepts to configurations FIFO, capacity or fair scheduler, the choice is yours. When we start delving into the world of big data, a number of new words and acronyms start showing up YARN is also one of them. 5 min read 5 min read Rob Salgado May 20, 2019 Member-only Hyperparameter Tuning On Google Cloud Platform With Scikit-Learn Google Cloud Platforms AI Platform (formerly ML Engine) offers a hyperparameter tuning service for your models. Why should you take the extra time and effort to learn how to use it instead of just running the code you already have on a virtual machine? 10 min read 10 min read Abdishakur Writing about Geospatial Data Science, AI, ML, DL, Python, SQL, GIS | Top writer | 1m views. More from Medium Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Nik Piepenbreier in Better Programming Make Awesome Maps in Python and Geopandas Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4139,\n",
       "  'url': 'https://medium.com/datadriveninvestor/part-i-data-visualization-using-python-7b08799f820c',\n",
       "  'title': 'Part I\\u200a—\\u200aData visualization using\\xa0Python',\n",
       "  'subtitle': '-',\n",
       "  'claps': 17,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 4,\n",
       "  'publication': 'Data Driven Investor',\n",
       "  'date': '2019-01-26',\n",
       "  'clap_prop': 8.397205607555114e-06,\n",
       "  'text': \"DataDrivenInvestor Jan 25 2019 Listen Save Part Data visualization using Python Data visualization given data set presented graphical format help detection pattern trend correlation might go undetected text-based data first part data visualization tutorial series showing get started python graph Let 's get First need import pyplot matplotlib library python documentation found matplotlib library Python 2D plotting library allows generate plot scatter plot histogram bar chart etc line code matplotlib.pyplot collection command style function make matplotlib function like MATLAB article focusing scatter plot line graph diving code let 's look basic property using plotting Scatter plot scatter plot present data point individually used want show relationship two variable plt.scatter allows generation scatter plot X represents data used x-axis graph data used y-axis plot customized change color shape size opacity etc data point size plot etc covered next tutorial tutorial series Multiple data set also presented using single scatter plot done simply adding another plt.scatter statement Line Graph Line graph present data using single line connecting data point usually used visualize time series data detect trend data period time plt.plot used generate line graph multiple data set presented graph adding plt.plot statement required also create graph combining different type plot code shown generates graph show one data set scatter plot line graph Pie Chart pie chart used show percentage proportional data used trying compare part whole code size label data represented pie chart corresponding label color specify color section pie chart instance section 22.6 presented green Doughnut Chart chart petty much serf purpose pie chart However doughnut chart contain multiple data series unlike pie chart whcih contain single data series change pie chart doughnut chart adding circle origin shown line 6 brings u end article Hope article helped get basic understanding get started data visualization using python next time .. Adios 19 19 19 DataDrivenInvestor empowerment data knowledge expertise subscribe DDIntel http //ddintel.datadriveninvestor.com Jens Erik Gould Jan 25 2019 Mexico Planning Imminent Change Auctions article wa first published 2010 Bloomberg News 3 min read 3 min read Share idea million reader Gyst Audio Jan 25 2019 Surround Sound Next Wave Audio Ubiquity audio streaming platform proliferate appliance start talking online publisher eyeing speaker well screen Hayley Grgurich Gyst Audio 4 min read 4 min read Henrique Tom Jan 25 2019 Technical Analysis Limitations best market strategy use Academics argue fundamental everything else retail trader say technical run market fact Ive already written previous article opinion correct approach 80 fundamental 3 min read 3 min read Aleksey Aleks Weyman Jan 25 2019 Member-only 5 Ingenious Apps Making World Better 2019 Technology ha inarguably revolutionized world live sign slowing 5 min read 5 min read Brennan Baraban Jan 25 2019 Learn Machines Learn doe A.I work software capable predicting interpreting human behavior Quite simply machine learn 14 min read 14 min read Krishni Software Engineer ML enthusiast Medium Demetrio 8 easy plotting categorical variable seaborn Pandas Dataframe Okan Yenign Towards Dev plot Histogram Python Matplotlib Seaborn Plotly Avi Chawla Towards Data Science Building All-In-One Audio Analysis Toolkit Python Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"DataDrivenInvestor Jan 25, 2019 Listen Save Part I Data visualization using Python Data visualization is where a given data set is presented in a graphical format. It helps the detection of patterns, trends and correlations that might go undetected in text-based data. This is the first part of the data visualization tutorial series where we will be showing you how to get started with python graphs. Let's get to it! First of all, you need to import pyplot of the matplotlib library in python(documentation can be found here ). The matplotlib library is a Python 2D plotting library which allows you to generate plots, scatter plots, histograms, bar charts etc. with just a few lines of code. matplotlib.pyplot is a collection of command style functions which makes matplotlib function like MATLAB. In this article, we will be focusing on scatter plots and line graphs. Before diving into the code, let's look at some of the basic properties we will be using when plotting. Scatter plot A scatter plot presents each data point individually. They are used when you want to show the relationship between two variables. plt.scatter allows the generation of the scatter plot. X represents the data used for the x-axis of the graph and y the data used for the y-axis. These plots can be customized to change the color, shape, size, opacity etc. of the data points, size of the plot etc. which will be covered in the next tutorials of this tutorial series. Multiple data sets can also be presented using a single scatter plot. This can be done by simply adding another plt.scatter statement. Line Graph Line graphs present data using a single line connecting all the data points. They are usually used to visualize time series data to detect trends in data over a period of time. plt.plot is used to generate a line graph and multiple data sets can be presented in the same graph by adding plt.plot statements as required. We can also create graphs combining different types of plots. The code shown below generates a graph which shows one data set as a scatter plot and the other as a line graph. Pie Chart A pie chart is used to show percentage or proportional data. It is used when trying to compare parts of a whole. In the above code, sizes and labels are the data represented in the pie chart and its corresponding label. colors specify the color of each section of the pie chart. For instance, section  A  is  22.6%  and is presented in  green  . Doughnut Chart This chart petty much serves the same purpose as a pie chart. However, doughnut charts can contain multiple data series unlike pie charts, whcih can contain only a single data series. We can change a pie chart to a doughnut chart by adding a circle to its origin as shown in line 6 . This brings us to the end of this article. Hope this article helped you to get a basic understanding of how to get started with data visualization using python. Until next time..Adios! 19 19 19 More from DataDrivenInvestor empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com Jens Erik Gould Jan 25, 2019 Mexico Not Planning `Imminent Change in Auctions This article was first published in 2010 by Bloomberg News. 3 min read 3 min read Share your ideas with millions of readers. Gyst Audio Jan 25, 2019 Surround Sound: The Next Wave in Audio is Ubiquity As audio streaming platforms proliferate and our appliances start talking, online publishers are eyeing speakers as well as screens By Hayley Grgurich for Gyst Audio 4 min read 4 min read Henrique Tom Jan 25, 2019 Technical Analysis Limitations What are the best market strategies to use? Academics argue that it is fundamentals above everything else and most of the retail traders says that is technical that runs the market. In fact, as Ive already written on previous articles, in my opinion, the correct approach is 80% fundamental which 3 min read 3 min read Aleksey (Aleks) Weyman Jan 25, 2019 Member-only 5 Ingenious Apps Making Our World Better in 2019 Technology has inarguably revolutionized the world we live in, and theres no sign of it slowing down 5 min read 5 min read Brennan D Baraban Jan 25, 2019 Learn How Machines Learn How does A.I. work? How is software capable of predicting and interpreting human behavior? Quite simply, how do machines learn? 14 min read 14 min read Krishni Software Engineer | ML enthusiast More from Medium Demetrio 8 easy plotting categorical variables with seaborn for Pandas Dataframe Okan Yenign in Towards Dev How to plot Histogram in Python? (Matplotlib, Seaborn, Plotly) Avi Chawla in Towards Data Science Building an All-In-One Audio Analysis Toolkit in Python Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 6164,\n",
       "  'url': 'https://towardsdatascience.com/make-data-acquisition-easy-with-aws-lambda-python-in-12-steps-33fe201d1bb4',\n",
       "  'title': '<strong class=\"markup--strong markup--h3-strong\">Make Data Acquisition Easy with AWS &amp; Lambda (Python) in 12\\xa0Steps</strong>',\n",
       "  'subtitle': '<strong class=\"markup--strong markup--h4-strong\">Goodbye to complex ETL pipelines</strong>',\n",
       "  'claps': 303,\n",
       "  'responses': 6.0,\n",
       "  'reading_time': 15,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-06-27',\n",
       "  'clap_prop': 0.0001496678411228941,\n",
       "  'text': 'Towards Data Science Jun 27 2019 Member-only Listen Save Make Data Acquisition Easy AWS Lambda Python 12 Steps Goodbye complex ETL pipeline SQL database complication article serve brief introduction AWS Lambda building fully serverless data pipeline article written people least basic mean basic understanding Python brand new AWS create AWS account Lambda function pull data Craigslist store S3 bucket u automatically daily Data scientist enthusiast continuously trying learn new technology make life easier Like many data science community self taught statistic CS background past life wa mechanical engineer feel like eon ago year gotten pretty good Python AWS certification Ive always learned much others thought wa turn contribute community Feel free connect LinkedIn check Github profile find article useful Lambda Often run scenario wrote Python function need run every day could set Cron machine computer option turn cloud old way AWS wa spin EC2 instance SSH give function maybe docker image set Cron job forth isnt bad bit pain Also need something run daily really need EC2 instance running day Enter Lambda yes know could also use Batch Lambda tutorial AWS ha ton service perhaps favorite AWS Lambda Basically let focus writing code dealing annoying thing like VPCs EC2 instance MySQL database etc write Python give code Lambda execute code Cloud Even better trigger code variety way every minute day put something S3 bucket etc Lambda totally awesome cheap ha quickly become used AWS service twelve concise step set Lambda function automatically pull data Craigslist every day store data JSON S3 one example could use tutorial run Python function automatically interval specify Admittedly article bit long wanted make sure would easily reproducible especially beginner ever get stuck feel free reach try debug final note everything well within free tier wont cost penny Lambda give million free request month unless plan firing baby every second good Step One Create Free AWS Account Head http //aws.amazon.com signup free account Note need verify phone number whole process take minute end select Basic plan give solid amount compute power free 12 month Step Two Create Bucket Store Data S3 Buckets place store data object data specific basically like folder computer cloud Go Services Storage S3 brought S3 splash screen Click Create bucket give bucket name pick region closest named bucket my-super-sweet-bucket name whatever want know bucket name unique globally cant awesome bucket name giving name region stick default click Next bucket created create bucket make note called need later Step Three Create Lambda Function bucket store data Lambda time create Lambda function hover Services top Compute find Lambda Since havent made Lambda yet get splash screen introducing world Lambda Click Create Function button make Lambda Next asked name function specify programming language give role Quick Aside Role Amazon Web Services vast mysterious beast lot thing going One thing worth noting nearly everything AWS us rule Least Permissions basically say default nothing get permission access anything example let say create user named Bob AWS Network default Bob log cant access AWS service Bob login cant use S3 EC2 Lambda anything else Poor Bob Unless explicitly stated access given Anyways role way programmatically granting access Basically tell AWS Lambda function want able access S3 Without role Lambda function like Poor Bob access Dont make Lambda like Bob Anyways name Lambda function whatever want named mine creatively pullCraigslistAds Since 2019 modern men woman using Python 3.7 2.7 nonsense Note Lambda used lot othe programming language use Python Permissions set Create new role basic Lambda permission fix role later tutorial Step 4 Choose Upload .zip file code whole point Lambda execute code u youve created Lambda function notice code editor right browser Often time youcan write code right editor However tutorial going use outside built Python library like Requests need thing bit different uploading zip file dont need import library dont come Python write inline need import library need Zip file Lambda run Python code problem-o dont need library like Pandas Requests Matplotlib anything else installed Pip tutorial using python-craigslist library go Upload .zip file option Step 5 Functions Code Note rush want skip step clone repository provide final Zip file need give Lambda clone repo jump Step 8 arent lazy want follow along create new folder anywhere machine save code file called lambda_function.py Basically using Python library called craigslist scrape Craigslist u first line import library Line 7 define function giving input event context want get nerdy read check AWS Docs high level event pass function meta data trigger context something pass runtime info function function dont need mess much next part series need use event example trigger Lambda something put S3 bucket use event get file name often handy Anyways rest function fairly straightforward used Python instantiate class tell data pull store JSON Lines 28 31 send data S3 using Boto3 youve never used Boto3 Python SDK plain English interact AWS via Python Boto3 let put stuff S3 invoke Lambda create bucket etc want use AWS resource Python script Boto3 answer use one basic feature output S3 bucket line 29 need update bucket name whatever named bucket Step 2 reference sample data function data scraped Craigslist ad apartment final note sample code could anything example could call API dump data JSON store JSON NoSQL database outputting JSON S3 bucket could many thing Step 6 Pip Install Dependencies Lambda run Python cloud doe version Python Anyone us Python regularly highly dependent library open source pre-written blob code use import panda pd anyone example happens Lambda go back inline import request Note sure save function clicking test else testing previous version code without request included good rule thumb always saving Lambda function easy forget debugging past version Basically Lambda fails Python environment doesnt request library get library several way Ive found easiest already lambda_function.py file saved folder Next pip install dependency folder Looking back code dependency craigslist library json datetime boto3 library required builtin Lambda already ha recently idea could pip install current directory work quite nicely install python-craigslist dependency folder along lambda_function.py Step 7 Create Zip File Thinking back Lambda function AWS chose option upload Zip file friend time make aforementioned Zip file Select everything folder put zip file naming lambda_function.zip use 7Zip literally highlight file folder right click choose 7Zip add Zip file rename lambda_function.zip directory look something like although need Zip file Step 8 Upload Zip file Lambda First feel judging using Windows dont like Anyways click Upload button select newly created zip file youve done click Save button top right bring lambda_function.py inline editor Lambda environment needed library Also point forward need make change code edit line instead repackage zip file reason would need repackage zip file new dependency another library happens repeat Steps 68 Step 9 Create Environmental Variable Going fully environmental variable bit beyond scope already lengthy tutorial basically Lambda look code call environmental variable number_of_posts many post pull Lets nice Craigslist overload system set something low like 5 10 creates environmental variable code go get invoked Step 10 Set timeout value Incoming AWS Developers Associate exam question default Lambda allow 3 second execute example simply isnt enough time change timeout value Basic Settings max 15 minute able get away 3 minute mean set function pull lot ad say 200 would need time Note Lambda charge memory compute time Step 10 Add Permission Lambda Function Im sure point article feeling length like Jane Austen novel think back Step 3 created Lambda function let know would need give Lambda permission put stuff S3 bucket Weve arrived time Heres concisely possible Step 11 Test Function Finally Lambda ready tested test see article hasnt bunch baloney stuff actually work Luckily Lambda ha test function built right look right next save button Test button Click ask create test input going skip needed pulling API scraping Basically Lambda going respond certain type input could give test input dont need need run give name click Create test Test event click Test function run Gods good see universal sign proper execution form giant green block think back function outputting data form JSON blob S3 theory data S3 bucket go S3 bucket data Huzzah Lets move onto last step automation Step 12 Automate beauty Lambda triggered invoked variety way Lambda function happen anytime object go bucket another Lambda executes time interval going use latter set run based time interval Go top function choose CloudWatch Events set standard time interval event fire Lambda CloudWatch many thing essentially monitoring tool getting data AWS Environment want get AWS certification need learn CloudWatch come many exam Go bottom keep Create new rule rule basically governs event fire creatively name Daily description creative juice really flowing say Run day Select Schedule expression put rate 1 day value Note either use rate function take Cron scheduler Cron ugly going go rate use whatever float boat thats Click Add apply new spiffy rule Lambda function click Save top apply Lambda function Lambda function pull data daily dump S3 Summary Lambda wonderful developer data scientist looking get data EC2 database hardware anything else pain let u simply write code let AWS take care annoying stuff tutorial created Python function could run locally machine set run automatically day AWS using Lambda first time go may slow get used Lambda start deploying stuff rapidly AWS Lambda bit scary first get going stick ha ton us data scientist especially data acquisition ease mentioned found article useful feel free connect LinkedIn share use case Lambda 429 9 429 429 9 Towards Data Science home data science Medium publication sharing concept idea code Rachel Wiles Jun 27 2019 solved problem handwritingrecognition Deep learning enables recognition text 99.73 accuracy go Deep learning ha widely used recognise handwriting offline handwriting recognition text analysed written information analysed binary output character background Although shift towards digital stylus writing give information pen stroke 6 min read 6 min read Share idea million reader Pier Paolo Ippolito Jun 27 2019 Member-only Understanding Cancer using Machine Learning Use Machine Learning ML Medicine becoming important One application example Cancer Detection Analysis Note Towards Data Sciences editor allow independent author publish article accordance rule guideline endorse author contribution rely author work without seeking professional advice See Reader Terms detail 8 min read 8 min read Siddhant Bhambri Jun 27 2019 Member-only Reinforcement Learning Job Scheduling thought writing first Medium article intrigued since believed could give platform share contribute something would like read fellow community member Scientific publication research paper hard comprehend layperson would 5 min read 5 min read George Seif Jun 27 2019 Member-only Nvidias New Data Science Workstation Review Benchmark Data science hot write newsletter learner called Mighty Knowledge new issue contains link key lesson best content including quote book article podcasts video every one picked specifically learning live wiser happier fuller life Sign 7 min read 7 min read Sascha Heyer Jun 27 2019 Member-only Kubeflow Components Pipelines Kubeflow large ecosystem stack different open source tool ML tool want keep thing simple therefore cover component pipeline experiment pipeline component get basic required build ML workflow many tool integrated Kubeflow cover upcoming post Kubeflow originated Google 6 min read 6 min read Shawn Cochran Data music enthusiast Medium Prakshal Jain Clairvoyant Blog Executing Snowflake Queries Lambda Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Haq Nawaz Dev Genius Develop AWS Glue ETL pipeline Python shell Taimur Ijlal Geek Culture passed AWS Certified Solutions Architect Professional exam Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Jun 27, 2019 Member-only Listen Save Make Data Acquisition Easy with AWS & Lambda (Python) in 12 Steps Goodbye to complex ETL pipelines, SQL databases and other complications This article will serve as a brief introduction to AWS Lambda and building a fully serverless data pipeline. This article is written for people with at least basic (and I mean basic ) understanding of Python, but you can be brand new to AWS. We will create an AWS account and then have a Lambda function pull data from Craigslist and store it in a S3 bucket for us automatically, daily. About Me: Data scientist and enthusiast who is continuously trying to learn new technologies that make my life easier. Like many in the data science community I am self taught and not from a statistics or CS background. In a past life I was a mechanical engineer, but that feels like eons ago. Over the years I have gotten pretty good with Python and AWS (I have a few of the certifications), and because Ive always learned so much from others I thought it was my turn to contribute to the community. Feel free to connect with me on LinkedIn or check out my Github profile if you find this article useful. What Is Lambda Often I run into this scenario of I wrote this Python function and need it to run every day. You could set up a Cron on your machine, but what if your computer is off? The other option is to turn to the cloud. The old way to do this with AWS was to spin up an EC2 instance, SSH into it, give it your function or maybe a docker image, set up a Cron job there, and so forth. That isnt too bad, but it is a bit of a pain. Also, if I just need something to run daily do I really need a EC2 instance running all day? Enter Lambda (yes I know you could also use Batch for this, but this is a Lambda tutorial). AWS has a ton of services, but perhaps my favorite is AWS Lambda. Basically it lets you focus on writing code and not dealing with annoying things like VPCs, EC2 instances, MySQL databases, etc. Just write some Python, give that code to Lambda, and it will execute that code in the Cloud. Even better, you can trigger that code in a variety of ways: every minute, once a day, when you put something into an S3 bucket, etc. Lambda is totally awesome (and cheap) and has quickly become my most used AWS service. What We Will Be Doing In twelve, concise steps we will set up a Lambda function that will automatically pull data from Craigslist every day and store that data (in JSON) in S3. This is just one example, but you could use this tutorial to run any Python function automatically at some interval you specify. Admittedly this article is a bit long, but I wanted to make sure it would be easily reproducible, especially for beginners. If you ever get stuck feel free to reach out to me and we can try to debug it. A final note is everything here should be well within free tier, so it wont cost you a penny. Lambda gives you a million free requests a month, so unless you plan on firing this baby every second you should be good. Step One: Create A Free AWS Account Head over to https://aws.amazon.com and signup for a free account. Note you will need to verify your phone number, but the whole process should only take a few minutes. At the end select the Basic plan, which gives you a solid amount of compute power for free for 12 months. Step Two: Create A Bucket To Store Our Data S3 Buckets are a place we can store data (or object data to be more specific). Its basically like a folder on your computer, but in the cloud! Go to Services > Storage > S3 and you will be brought to the S3 splash screen. Click Create bucket and give your bucket a name and pick the region closest to you. I named by bucket my-super-sweet-bucket , and you can name yours whatever you want. Just know the bucket names have to be unique globally, so you cant have my awesome bucket name. After giving it the name and region just stick with all the defaults (click Next until your bucket is created). This will create your bucket, and make a note of what you called it as we will need it later. Step Three: Create A Lambda Function Now that we have a bucket to store our data from Lambda, its time to create our Lambda function. If you hover over Services at the top and under Compute you will find Lambda. Since you havent made a Lambda yet you will get a splash screen introducing you to the world of Lambda. Click the Create a Function button to make your Lambda. Next you will be asked to name your function, specify a programming language, and give it a role. Quick Aside What is a Role? Amazon Web Services is a vast and mysterious beast, with a lot of things going on. One thing worth noting is nearly everything in AWS uses a rule of Least Permissions, which basically says by default nothing gets permissions to access anything. For example, lets say you create a user named Bob in your AWS Network. By default, when Bob logs in, he cant access any AWS service. Bob can login but he cant use S3, EC2, Lambda or anything else. Poor Bob. Unless it is explicitly stated then no access is given. Anyways, a role is a way of programmatically granting access. Basically it will tell AWS that This is my Lambda function and I want it to be able to access S3. Without this role your Lambda function will be just like Poor Bob with no access. Dont make your Lambda like Bob. Anyways, you can name your Lambda function whatever you want. I named mine creatively pullCraigslistAds. Since its 2019, and we are modern men and women, we will be using Python 3.7 and not any of that 2.7 nonsense. Note that Lambda can be used with a lot of othe programming languages, but here we will use Python. For now Permissions can be set to Create a new role with basic Lambda permissions. We fix our role later in the tutorial. Step 4: Choose Upload a .zip file for your code The whole point of Lambda is to have it execute some code for us. Once youve created your Lambda function you will notice there is a code editor right there in the browser. Often times youcan just write code right there in the editor. However, in this tutorial we are going to use some outside (not built into Python) libraries, like Requests, we will need to do things a bit different by uploading a zip file. If you dont need to import any libraries that dont come with Python you can just write it inline. If you do need to import libraries we will need the Zip file. That is, Lambda can run your Python code no problem-o but what if you dont need libraries like Pandas, Requests, Matplotlib or anything else you installed with Pip. In this tutorial we will be using the python-craigslist library, so we go with the Upload .zip file option. Step 5: Our Functions Code Note: If you are in a rush and want to skip some steps you can clone  this repository  that will provide you the final Zip file you need to give to Lambda. If you just clone the repo you can jump to Step 8. If you arent lazy and want to follow along then create a new folder anywhere on your machine and save the code below into a file called lambda_function.py . What is this doing Basically it is using a Python library called craigslist to scrape Craigslist for us. The first few lines import our libraries. Line 7 is where we define our function, giving it the input of event and context . If you want to get nerdy and read into it, check out the AWS Docs . At a high level, event passes the function meta data about the trigger and context i s something that passes runtime info to the function. For this function we dont need to mess with them much, but in the next part of this series we will need to use event. For example, if we trigger a Lambda when something is put into an S3 bucket we can use event to get that file name, which is often handy. Anyways, the rest of the function is fairly straightforward for those who have used Python: instantiate our class, tell it what data to pull, and store that in some JSON. Lines 28 to 31 are how we send that data to S3 using Boto3. If youve never used Boto3, it is a Python SDK, or in plain English it is how you can interact with AWS via Python. Boto3 lets you put stuff in S3, invoke a Lambda, create a bucket, etc. If you want to use AWS resources from a Python script than Boto3 is your answer. Here we use one of its most basic features: output to our S3 bucket. On line 29 you will need to update the bucket name to whatever you named your bucket in Step 2. For reference, below is some sample data from our function, which is just data scraped from a Craigslist ad for apartments. A final note is that this is just our sample code, but this could be anything. For example, it could call an API, dump that data to JSON and store that JSON in a NoSQL database. Here we are outputting JSON to an S3 bucket but this could be many things. Step 6: Pip Install Our Dependencies Lambda runs Python in the cloud for you, but what does its version of Python have? Anyone who uses Python regularly is highly dependent on libraries, which are open source pre-written blobs of code we can use (import pandas as pd, anyone)? As an example, what happens in our Lambda (if I go back to inline) if I import requests? Note: Be sure to save your function before clicking test, or else it is testing the previous version of the code without requests included. A good rule of thumb is to always be saving your Lambda function, its easy to forget to do it and then you are debugging the past version. Basically our Lambda fails because this Python environment doesnt have the requests library. So how do we get it (or any library)? There are several ways, but this is how Ive found it easiest. We already have our lambda_function.py file saved in a folder. Next pip install your dependencies into that same folder. Looking back at our code, the only dependency we have is the craigslist library (json, datetime, os and boto3 are the other libraries required and all are builtin so Lambda already has them). Until recently I had no idea you could do this to pip install to your current directory, but it works quite nicely: This will install python-craigslist (and its dependencies) into your folder along with your lambda_function.py Step 7: Create A Zip File Thinking back to our Lambda function on AWS, we chose the option to upload a Zip file. Now, my friends, is the time to make the aforementioned Zip file! Select everything in your folder and put it into a zip file, naming it lambda_function.zip. I use 7Zip so I literally just highlight all the files and folders, right click, choose 7Zip and add it to a Zip file. Then I rename it to lambda_function.zip. Your directory should now look something like below, although now all you need is that Zip file. Step 8: Upload your Zip file to Lambda First, I can feel you judging me for using Windows and I dont like it. Anyways, click the Upload button and select your newly created zip file. Once youve done, that click the Save button in the top right. This will bring your lambda_function.py into the inline editor and your Lambda environment will now have all your needed libraries! Also from this point forward if you need to make changes to the code you can just edit it in line instead of having to repackage your zip file. The only reason you would need to repackage the zip file is if you have a new dependency on another library. If that happens then repeat Steps 68. Step 9: Create Your Environmental Variable Going fully into environmental variables is a bit beyond the scope of this already lengthy tutorial, but basically you can do them in Lambda. If you look at our code, we call for an environmental variable number_of_posts, which is how many posts we will pull. Lets be nice to Craigslist and not overload their system, so just set it to something low like 5 or 10. This creates the environmental variable that our code will go out and get when it is invoked. Step 10: Set your timeout value Incoming AWS Developers Associate exam question! By default a Lambda will allow 3 seconds to execute. For this example that simply isnt enough time. You can change this timeout value under the Basic Settings. The max is 15 minutes, but we should be able to get away with 3 minutes. If you were mean and set your function to pull a lot of ads, say 200, you would need more time. Note that Lambda charges you for memory and compute time. Step 10: Add Permission For Your Lambda Function Im sure at this point this article is feeling length like a Jane Austen novel, but think back to Step 3 where we created our Lambda function. I let you know that we would need to give Lambda some permissions to put stuff in an S3 bucket. Weve arrived at that time now. Heres how we do this (as concisely as possible): Step 11: Test Your Function Finally our Lambda is ready to be tested! So how do we test to see if this article hasnt been a bunch of baloney and this stuff actually works? Luckily Lambda has a test function built right in! If you look right next to the save button there is a Test button. Click that! It will ask you to create a test input, but we are going to skip that for now as its not needed when we are just pulling from an API or scraping. Basically if your Lambda is going to respond to a certain type of input you could give it a test input here. We dont need that, we just need it to run so just give it a name and click Create to test it out. Now that you have a Test event click Test again and your function should run! If the Gods are good you should see the universal sign for proper execution in the form of a giant green block. If we think back to what our function is doing, it is outputting data, in the form of a JSON blob to S3. So in theory we should now have data in our S3 bucket. If you go to your S3 bucket you should now have data in there! Huzzah! Lets move onto our last step: automation! Step 12: Automate! The beauty of Lambda is it can be triggered, or invoked, in a variety of ways. You can have Lambda functions happen anytime an object goes to a bucket, when another Lambda executes, or on a time interval. We are going to use the latter here an set it to run based on a time interval. Go to the top of your function and choose CloudWatch Events. This is how you can set a standard time interval event to fire our Lambda. CloudWatch is many things, but essentially its a monitoring tool for getting data about your AWS Environment. If you do want to get any of the AWS certifications you will need to learn about CloudWatch as it comes up in many of the exams. Go down to the bottom and keep it as Create a new rule. A rule is basically what governs when this event fires. We will creatively name it Daily and in the description, because the creative juices are now really flowing, we will say Run once a day. Select Schedule expression and put in rate(1 day) for the value. Note this can either use the rate() function or take a Cron scheduler. Cron is ugly so I am going to go with the rate, but use whatever floats your boat. And thats it! Click Add to apply your new, spiffy rule to your Lambda function and then click Save at the top to apply it to your Lambda function. Your Lambda function will now pull data daily and dump it to S3! Summary Lambda is wonderful for a developer or data scientist just looking to get data as theres no EC2, theres no databases, theres no hardware or anything else that is a pain. It lets us just simply write code and let AWS take care of all the annoying stuff. In this tutorial we created a Python function, that we could run locally on our machine, and set it up to run automatically once a day on AWS using Lambda. The first time you go through this it may be slow, but once you get used to Lambda you can start deploying stuff rapidly. AWS and Lambda can be a bit scary when you first get going but stick with it as it has tons of uses for a data scientist, especially for data acquisition with ease. As I mentioned if you found this article useful feel free to connect with me on LinkedIn and share your use cases for Lambda. 429 9 429 429 9 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Rachel Wiles Jun 27, 2019 Have we solved the problem of handwritingrecognition? Deep learning enables recognition of text to 99.73% accuracy. Where do we go from here? Deep learning has been widely used to recognise handwriting. In offline handwriting recognition, text is analysed after being written. The only information that can be analysed is the binary output of a character against a background. Although shifts towards digital stylus for writing gives more information, such as pen stroke 6 min read 6 min read Share your ideas with millions of readers. Pier Paolo Ippolito Jun 27, 2019 Member-only Understanding Cancer using Machine Learning Use of Machine Learning (ML) in Medicine is becoming more and more important. One application example can be Cancer Detection and Analysis. Note from Towards Data Sciences editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each authors contribution. You should not rely on an authors works without seeking professional advice. See our Reader Terms for details. 8 min read 8 min read Siddhant Bhambri Jun 27, 2019 Member-only Reinforcement Learning in Job Scheduling The thought of writing my first Medium article intrigued me since I believed this could give me a platform to share and contribute something I would like to read from fellow community members. Scientific publications and research papers can be so hard to comprehend by a layperson that it would 5 min read 5 min read George Seif Jun 27, 2019 Member-only Nvidias New Data Science Workstation a Review and Benchmark Data science is hot. I write a newsletter for learners called Mighty Knowledge. Each new issue contains links and key lessons from the very best content including quotes, books, articles, podcasts, and videos. Each and every one is picked out specifically for learning how to live a wiser, happier, and fuller life. Sign up 7 min read 7 min read Sascha Heyer Jun 27, 2019 Member-only Kubeflow Components and Pipelines Kubeflow is a large ecosystem, a stack of different open source tools ML tools. I want to keep things simple therefore we cover components, pipelines, and experiments. With pipelines and components, you get the basics that are required to build ML workflows. There are many more tools integrated into Kubeflow and I will cover them in the upcoming posts. Kubeflow is originated at Google. 6 min read 6 min read Shawn Cochran Data and music enthusiast More from Medium Prakshal Jain in Clairvoyant Blog Executing Snowflake Queries in Lambda Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Haq Nawaz in Dev Genius Develop AWS Glue ETL pipeline with Python shell Taimur Ijlal in Geek Culture How I passed the AWS Certified Solutions Architect Professional exam Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4351,\n",
       "  'url': 'https://towardsdatascience.com/getting-started-with-geographic-data-science-in-python-part-2-f9e2b1d8abd7',\n",
       "  'title': 'Getting started with Geographic Data Science in Python\\u200a—\\u200aPart\\xa02',\n",
       "  'subtitle': 'Tutorials, Real World projects\\xa0&…',\n",
       "  'claps': 56,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-05-22',\n",
       "  'clap_prop': 2.766138317782861e-05,\n",
       "  'text': 'Towards Data Science May 22 2019 Member-only Listen Save Getting started Geographic Data Science Python Part 2 Tutorials Real World project Exercises Second article three-part series article Getting started Geographic Data Science Python learn reading manipulating analysing Geographic data Python article series designed sequential first article lay foundation second one get intermediate advanced level Geographic data science topic third part cover relevant real-world project wrapping cement learning first article accessed Master Geographic Data Science Real World project Exercises Real World project Exercises Real World project Exercisestowardsdatascience.com Learning Objectives tutorial 1 Understand GeodataFrames Geoseries 2 Perform Table join Spatial Join 3 Carry buffer overlay analysis 1 GeodataFrame Geoseries Let u read country cite dataset load data get table geographic geometry geographic geometry allow u perform spatial operation addition typical tabular data analysis panda simple excel DataFrame vs. GeoDataFrame GeoDataFrame tabular data structure contains GeoSeries.The important property GeoDataFrame always ha one GeoSeries column hold special status GeoSeries referred GeoDataFrames geometry spatial method applied GeoDataFrame spatial attribute like Area called command always act geometry column one column either dataFrame GeodataFrame One column Geometry Column called GeoeDataFrame Otherwise DataFrame column geometry column Similarly One column mean either Series Geoseries data type column Geometry column called Geoseries Let u see example data type start Dataframe two column none Geometry column therefore type data dataframe output type function pandas.core.frame.DataFrame happen geometry column table Geodatframe Similarly Geoseries single Geometry column Series datatype one column geometry column shown yield pandas.core.series.Series geopandas.geoseries.GeoSeries respectively GeoDataFrame/GeoSeries carry geographic processing task far seen including .plot Another example getting centriods polygon Let u get country centroid plot plot look like point represents country center Exercise 1.1 Create union polygon geometry Countries Hint use .unary_union Exercise 1.2 calculate area country Hint use .area 2 Table Join vs. Spatial join Table join classical query operation two separate table example sharing one column case perform table join two table joined using shared column hand spatial join relates geographic operation example joining location city country see example join/merge two table based shared column NAME pure panda operation doe entail geographic operation However spatial join merging entail geographic operation perform example spatial join want join following two table based location example country doe contain city city within country use Geopandas function .sjoin spatial join show sample 5 row see table city matched corresponding country based location used op=within take city point within country polygon could also use intersect Also could use op=contain find country contain city point 3 Buffer Analysis Buffer analysis important geoprocessing task used widely many domain get distance around point example first get city Sweden buffer around One tricky thing need know CRS/projection using get correct output want data projected projection meter used output meter classical error world Geodata used resource find CRS Sweden ha meter SWEREF99 TM EPSG Projection -- Spatial Reference Home Upload List user-contributed reference List reference spatialreference.org use 3 different buffer distance 100 200 500 single point Stockholm city plot result show concept buffering Exercise 3.1 Create buffer city Try different projection different distance Overlay sometimes need create new feature different data type like Points Lines Polygons Set operation Overlays play important role using dataset instead reading unzipped folder use built-in dataset reading mechanism Geopandas example come Geopandas documentation subset data select Africa illustrate overlay function consider following case one wish identify core portion country defined area within 500km capital using GeoDataFrame Africa GeoDataFrame capital select portion country within 500km capital specify option intersect creates new set polygon two layer overlap Changing option allows different type overlay operation example interested portion country far capital periphery would compute difference two Conclusion tutorial covered geoprocessing task Geographic data using Geopandas First studied difference dataframe Geodataframe followed exploring spatial join also done buffer analysis well Overlay analysis next tutorial apply learned preceding part project code available GitHub repository shakasom/GDS Geographic data science tutorial series Contribute shakasom/GDS development creating account GitHub github.com also go directly run Google Collaboraty Jupyter Notebooks directly link shakasom/GDS Geographic data science tutorial series Contribute shakasom/GDS development creating account GitHub github.com 56 1 56 56 1 Enjoy read Reward writer Beta tip go Abdishakur third-party platform choice letting know appreciate story Get email whenever Abdishakur publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Marie Stephen Leo May 22 2019 Member-only New York Taxi data set analysis Predicting taxi fare using Regression model Recently opportunity play New York taxi public data set hosted Google cloud Big Query platform decided apply machine learning technique data set try build predictive model using Python post Ill attempt predict 7 min read 7 min read Share idea million reader Joshua J Luo May 21 2019 Member-only Exploration Neural Networks Playing Video Games Thank team member Benjamin Guo Christian Han Cooper Shearer Justin Qu Kylar Osborne Introduction Video game arent fun provide platform neural network learn interact dynamic environment solve complex problem like real life Video game 13 min read 13 min read George Seif May 21 2019 Member-only use Pandas RIGHT way speed code Want inspired Come join Super Quotes newsletter Pandas library ha heavenly gift Data Science community Ask Data Scientist like handle datasets Python theyll undoubtedly talk Pandas Pandas epitome 4 min read 4 min read Jiawei Wang May 21 2019 Member-only Representing Human Mobility Patterns Social Network Data Using Hidden Markov Models Jiawei Wang Seth Lee Hyunsu Chae Fei Github http //github.com/sethlee0111/MobilityHMM Introduction Understanding knowing utilize human mobility helpful various application modern day example knowing people move around city city planner developer design city efficiently However 10 min read 10 min read Marco Cerliani May 21 2019 Member-only Extreme Event Forecasting LSTM Autoencoders Improve forecasting performance developing strong Neural Network architecture Dealing extreme event prediction frequent nightmare every Data Scientist Looking around found interesting resource deal problem Personally literally fall love approach released Uber Researchers paper two version available developed 7 min read 7 min read Abdishakur Writing Geospatial Data Science AI ML DL Python SQL GIS Top writer 1m view Medium Nik Piepenbreier Better Programming Make Awesome Maps Python Geopandas Maurcio Cordeiro Towards Data Science Artificial Intelligence Geospatial Analysis Pytorchs TorchGeo Part 1 Abdishakur Spatial Data Science Explore Open-source Python GIS Earth Observation library Interactively Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science May 22, 2019 Member-only Listen Save Getting started with Geographic Data Science in Python Part 2 Tutorials, Real World projects & Exercises This is the Second article of a three-part series of articles in Getting started Geographic Data Science with Python. You will learn about reading, manipulating and analysing Geographic data in Python. The articles in this series are designed to be sequential where the first article lays the foundation and the second one gets into intermediate and advanced level Geographic data science topics. The third part covers a relevant and real-world project wrapping up to cement your learning. The first article can be accessed here. Master Geographic Data Science with Real World projects & Exercises Real World projects & Exercises Real World projects & Exercisestowardsdatascience.com Learning Objectives for this tutorial are: 1. Understand GeodataFrames and Geoseries 2. Perform Table join and Spatial Join 3. Carry out the buffer and overlay analysis 1. GeodataFrame & Geoseries Let us read the countries and cites dataset. Once you load the data, what we get is a table with geographic geometries. The geographic geometries allow us to perform spatial operations in addition to the typical tabular data analysis in pandas or simple excel. DataFrame vs. GeoDataFrame. A GeoDataFrame is a tabular data structure that contains a GeoSeries.The most important property of a GeoDataFrame is that it always has one GeoSeries column that holds a special status. This GeoSeries is referred to as the GeoDataFrames geometry. When a spatial method is applied to a GeoDataFrame (or a spatial attribute like Area is called), this commands will always act on the geometry column. If you have more than one column, you have either a dataFrame or GeodataFrame. If One of the columns is a Geometry Column, then it is called a GeoeDataFrame . Otherwise, it is a DataFrame if any of the columns is not a geometry column. Similarly, One column means you have either a Series or Geoseries data type. If the only column is the Geometry column, then it is called Geoseries . Let us see an example of each data type. We start with Dataframe. We have only two columns here and none of them is a Geometry column, therefore, the type of this data will be a dataframe and the output of the type function is pandas.core.frame.DataFrame . If we happen to have any geometry column in our table, then it will be a Geodatframe as below. Similarly, a Geoseries is when we have a single Geometry column and Series datatype will be when this one column is not a geometry column as shown below. This will yield pandas.core.series.Series and geopandas.geoseries.GeoSeries respectively. With GeoDataFrame/GeoSeries you can carry out geographic processing tasks. So far we have seen few including .plot() . Another example is getting centriods of polygons. Let us get each countrys centroid and plot it. And this is how the plot looks like, each point represents the countrys center. Exercise 1.1: Create a union of all polygon geometries (Countries). Hint use (.unary_union) Exercise 1.2: calculate the area of each country. Hint use (.area) 2. Table Join vs. Spatial join Table joins is classical query operation where we have two separate tables, for example, sharing one column. In that case, you can perform a table join where the two tables are joined using the shared column. On the other hand, spatial join relates to geographic operations, for example, joining by location each city and its country. We will see both examples below. We can join/merge the two tables based on their shared column NAME. This is pure pandas operation and does not entail any geographic operations. However, in spatial join, the merging entails a geographic operation. We will perform an example of a spatial join. We want to join the following two tables based on their locations. For example, which country does contain which city or which city is within which country. We will use Geopandas function .sjoin() to do the spatial join and show a sample of 5 rows. As you can see from the below table, each city is matched with its corresponding country based on the location. We have used op=within which takes city points that are within a countries polygon. Here we could also use intersect. Also, we could use op=contain and find out which countries contain the city points. 3. Buffer Analysis Buffer analysis is an important geoprocessing task. It is used widely in many domains to get a distance around a point. In this example, we will first get a city in Sweden and then do a buffer around it. One tricky thing here is you need to know which CRS/projection you are using to get the correct output you want. If your data is not projected into projection where meters are used, then the output will not be in meters. This is a classical error in the world of Geodata. I have used this resource to find out which CRS Sweden has in meters. SWEREF99 TM: EPSG Projection -- Spatial Reference Home | Upload Your Own | List user-contributed references | List all references spatialreference.org We use here 3 different buffer distances, 100, 200, and 500 on a single point, Stockholm city. Then we plot the result to show the concept of buffering. Exercise 3.1: Create a buffer of all cities. Try different projections and different distances. Overlay We sometimes need to create new features out of different data types like Points, Lines and Polygons. Set operations or Overlays play an important role here. We will be using the same dataset but instead of reading it from our unzipped folder we can use built-in dataset reading mechanism in Geopandas. This example comes from Geopandas documentation. We can subset data to select only Africa. To illustrate the overlay function, consider the following case in which one wishes to identify the core portion of each country defined as areas within 500km of a capital using a GeoDataFrame of Africa and a GeoDataFrame of capitals. To select only the portion of countries within 500km of a capital, we specify the how option to be intersect, which creates a new set of polygons where these two layers overlap: Changing the how option allows for different types of overlay operations. For example, if we were interested in the portions of countries far from capitals (the peripheries), we would compute the difference between the two. Conclusion This tutorial covered some geoprocessing task in Geographic data using Geopandas. First, we studied differences between dataframe and Geodataframe followed by exploring spatial join. We have also done buffer analysis as well as Overlay analysis. In the next tutorial, we will apply what we have learned in this and preceding part in a project. The code is available in this GitHub repository: shakasom/GDS Geographic data science tutorials series. Contribute to shakasom/GDS development by creating an account on GitHub. github.com You can also go directly and run Google Collaboraty Jupyter Notebooks directly from this link: shakasom/GDS Geographic data science tutorials series. Contribute to shakasom/GDS development by creating an account on GitHub. github.com 56 1 56 56 1 Enjoy the read? Reward the writer. Beta Your tip will go to Abdishakur through a third-party platform of their choice, letting them know you appreciate their story. Get an email whenever Abdishakur publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Marie Stephen Leo May 22, 2019 Member-only New York Taxi data set analysis Predicting taxi fare using Regression models Recently I had the opportunity to play with the New York taxi public data set hosted by Google clouds Big Query platform. I decided to apply machine learning techniques on the data set to try and build some predictive models using Python. For this post, Ill attempt to predict the 7 min read 7 min read Share your ideas with millions of readers. Joshua J Luo May 21, 2019 Member-only An Exploration of Neural Networks Playing Video Games Thank you to my team members: Benjamin Guo, Christian Han, Cooper Shearer, Justin Qu, and Kylar Osborne. Introduction Video games arent just fun. They provide a platform for neural networks to learn how to interact with dynamic environments and solve complex problems, just like in real life. Video games have been 13 min read 13 min read George Seif May 21, 2019 Member-only How to use Pandas the RIGHT way to speed up your code Want to be inspired? Come join my Super Quotes newsletter. The Pandas library has been a heavenly gift to the Data Science community. Ask any Data Scientist how they like to handle their datasets in Python and theyll undoubtedly talk about Pandas. Pandas is the epitome of what a 4 min read 4 min read Jiawei Wang May 21, 2019 Member-only Representing Human Mobility Patterns with Social Network Data Using Hidden Markov Models By: Jiawei Wang, Seth Lee, Hyunsu Chae, Fei He Github: https://github.com/sethlee0111/MobilityHMM Introduction Understanding and knowing how to utilize human mobility can be very helpful for various applications in modern days. For example, by knowing how people move around the city, city planners and developers can design the city more efficiently. However 10 min read 10 min read Marco Cerliani May 21, 2019 Member-only Extreme Event Forecasting with LSTM Autoencoders Improve forecasting performance developing a strong Neural Network architecture Dealing with extreme event prediction is a frequent nightmare for every Data Scientist. Looking around I found very interesting resources that deal with this problem. Personally, I literally fall in love with the approach released by Uber Researchers. In their papers (two versions are available here and here) they developed 7 min read 7 min read Abdishakur Writing about Geospatial Data Science, AI, ML, DL, Python, SQL, GIS | Top writer | 1m views. More from Medium Nik Piepenbreier in Better Programming Make Awesome Maps in Python and Geopandas Maurcio Cordeiro in Towards Data Science Artificial Intelligence for Geospatial Analysis with Pytorchs TorchGeo (Part 1) Abdishakur in Spatial Data Science Explore Open-source Python GIS and Earth Observation libraries Interactively Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5990,\n",
       "  'url': 'https://towardsdatascience.com/a-beginners-guide-to-reinforcement-learning-using-rock-paper-scissors-and-tensorflow-js-37d42b6197b5',\n",
       "  'title': 'A Beginner’s Guide to Reinforcement Learning using Rock-Paper-Scissors and Tensorflow.js',\n",
       "  'subtitle': '-',\n",
       "  'claps': 28,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-10-21',\n",
       "  'clap_prop': 1.3830691588914305e-05,\n",
       "  'text': 'Towards Data Science Oct 20 2019 Listen Save Beginners Guide Reinforcement Learning using Rock-Paper-Scissors Tensorflow.js Tensorflow.js released attended lecture use case browser experience using Tensorflow extensively Python wa curious difficult would build small reinforcement learning example browser could see agent learning time chose Rock-Paper-Scissors referred RPS brevity laziness sake simple game rule knew learning would happen example tutorial Anyone interested intuitive non-math based tutorial Reinforcement Learning RL Neural Networks basic knowledge JavaScript html understand code required understand conceptual portion goal build agent able learn rule RPS using reinforcement learning neural network mean want agent able choose Rock given user chooses Scissors Reinforcement learning intuitively described following loop something based belief get positive negative reward update belief based reward framing reinforcement learning problem keep cycle mind break RPS game manner Keeping mind tutorial broken two section start script.js start learning cycle something based Agents belief context mean choosing move given Users move line 612 evaluation phase want see well Agent ha learned Therefore move chosen one highest value i.e. move Agent ha confidence done help Neural Network Aside Neural Networks neural network function tuned take input doe something input output result context RL RPS neural network used represent belief agent input network Users move output confidence Agent ha choosing Rock Paper Scissors seen Users move Rock Agent 0.7 confident choose Scissors 0.1 confident Rock 0.2 confident Paper Evaluation phase Agent choose move ha highest value/the confidence i.e. Scissors line 812 Convert move format Neural Network understand ask network output choose move highest value line 1416 training phase instead choosing move based belief move chosen randomly allows Agent explore move instead potentially stuck always choosing move next phase reinforcement cycle get reward user handled index.html reward received Agent update belief line 57 Handles getting confidence Agent ha move choose given Users move line 1012 Update belief Agent reward done using Neural Network Aside Updating Neural Network scenario user chose Rock Agent ha chosen Scissors user ha given reward -100 since move wrong move -100 added 0.7 sent back network tell network value 0.7 wa high lowered amount process called Backpropogation act updating belief Agent Agent le confident choosing Scissors scenario confident choosing either Rock Paper line 13 network updated new belief plotted method go move User choose plot belief Agent Users move line 10 Applies function make output neural network sum 1 allows User able understand output neural network easier line 25 Plots data set div index.html using Plotly.js piece complete reinforcement cycle visualization portion Next describe index.html required thing following excerpt index.html reflect point line 13 three button user move clicked call chooseMove function pas value button agent select return move line 56 button call train function user determines whether move Agent chose wa good bad tell agent update belief positively negatively line 811 Toggle whether agent learning evaluating ha learnt line 1317 Contain divs used plot belief Agent move come end tutorial see code visit repository Feel free clone play around code cloned open index.html browser see setup shown image top button Users move select move Agent choose move randomly User click Positive Reward Negative Reward button belief updated real time Click new move repeat process satisfied learned behavior switch toggle Train Evaluate move chosen Agent highest value histogram hope tutorial ha helpful provided intuitive understanding frame reinforcement learning problem neural network used help Stay tuned tutorial basic neural network reinforcement learning Thanks reading 36 1 36 36 1 Towards Data Science home data science Medium publication sharing concept idea code Suradech Kongkiatpaiboon Oct 20 2019 Member-only use Python without administrative right workplace use portable mode Python beautiful language experience work fast easy learn adapted variety field started using Python learn Data Science found lately also expand web scraping automated BOT programming Usually 5 min read 5 min read Share idea million reader Sam Mourad Oct 20 2019 Member-only Prophet v DeepAR Forecasting Food Demand collaboration Christian Aalby Svalesen global food industry face significant sustainability challenge UN estimate approximately one-third global food production wasted lost annually 66 loss food group freshness important criterion consumption addition 8 min read 8 min read Ishtiak Mahmud Oct 20 2019 Self Driving Car Localization doe self-driving car know given time One project artificial intelligence ha always fascinated self-driving car major motivation behind learning deep learning artificial intelligence One vital thing self-driving car ha accomplish perform function localization 7 min read 7 min read Alex Albano Oct 20 2019 Member-only Autonomous Distributed Networks unfulfilled libertarian dream breaking free regulation Keywords Blockchain Distributed Ledger Technology Decentralised Governance Regulatory Compliance Autonomous Distributed Networks Faultless Responsibility Intelligent Smart Contracts Summary Decentralisation embodies libertarian dream breaking free regulatory influence government bypassing via trustless network need central authority decision making Techno-libertarian crypto-anarchists maintain 17 min read 17 min read Georgi Tancev Oct 20 2019 Member-only Detecting Lesions Multiple Sclerosis Patients Deep Learning Introduction convolutional neural network medical image analysis Introduction Applications Deep Learning Deep learning part broader family machine learning method based artificial neural network One promising application deep learning image analysis e.g image segmentation classification Whereas segmentation yield probability distribution per pixel also known mask class 10 min read 10 min read Sacha Gunaratne Im thing visualization analytics predictive modelling also enjoy delving deep reinforcement learning optimization Medium Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Frank Andrade Geek Culture Top 5 Paid Subscriptions Ill Never Cancel Programmer Dennis Bakhuis Towards Data Science Python 3.14 faster C++ Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Oct 20, 2019 Listen Save A Beginners Guide to Reinforcement Learning using Rock-Paper-Scissors and Tensorflow.js Tensorflow.js had just been released and I had just attended a lecture on its use cases in the browser. I had experience using Tensorflow extensively in Python but was curious how difficult it would be to build a small reinforcement learning example in the browser in which you could see an agent learning over time. I chose Rock-Paper-Scissors which will be referred to as RPS for brevity and laziness sake from now on because of its simple game rules and because I knew the learning would happen with very few examples. Who is this tutorial for: Anyone who is interested in an intuitive non-math based tutorial on Reinforcement Learning (RL) and Neural Networks. They should have basic knowledge of JavaScript and html to understand the code but this is not required to understand the conceptual portions. The goal: To build an agent that is able to learn the rules of RPS using reinforcement learning and neural networks. This means that we want the agent to be able to choose Rock given that the user chooses Scissors. Reinforcement learning intuitively can be described as the following: loop: do something based on beliefs get a positive or negative reward update beliefs based on reward So when framing a reinforcement learning problem you have to keep the above cycle in mind. So we can break down the RPS game in the same manner. Keeping this in mind, the tutorial is broken down into two sections: We will start with the script.js. The start of the learning cycle is do something based on the Agents beliefs. In this context that means choosing a move given the Users move. lines 612: In the evaluation phase, we want to see how well the Agent has learned. Therefore, the move that is chosen is the one with the highest value. i.e., the move that the Agent has the most confidence in. This is done with the help of a Neural Network. Aside: Neural Networks A neural network is a function that can be tuned. It takes some input, does something to the input and outputs some result. In the context of RL and RPS, a neural network is used to represent the beliefs of the agent. The input into the network is the Users move and the output is the confidence the Agent has in choosing Rock, Paper and Scissors. As can be seen above, the Users move is Rock. The Agent is 0.7 confident that it should choose Scissors, 0.1 confident about Rock, and 0.2 confident about Paper. If we are in the Evaluation phase, the Agent will choose the move which has the highest value/the most confidence in. i.e., Scissors. lines 812: Convert the move into a format that the Neural Network can understand, ask the network for its output, and then choose the move with the highest value. lines 1416: In the training phase instead of choosing the move based on a belief, the move is chosen randomly. This allows the Agent to explore all moves instead of potentially being stuck always choosing the same move. The next phase of the reinforcement cycle is to get the reward from the user which will be handled in index.html. After the reward is received the Agent can update its beliefs. lines 57: Handles getting the confidence the Agent has about the moves it should choose given the Users move lines 1012: Update the beliefs of the Agent with the reward. This is done using the Neural Network. Aside: Updating a Neural Network In this scenario, the user chose Rock and the Agent has chosen Scissors. The user has given a reward of -100 since this move is the wrong move. The -100 is then added to the 0.7 and sent back into the network. This will tell the network that the value of 0.7 was too high and that it should be lowered by some amount. This process is called Backpropogation. This is the act of updating the beliefs of the Agent. The Agent should be less confident about choosing Scissors in this scenario and more confident about choosing either Rock or Paper. line 13: Once the network is updated, then the new beliefs are plotted. This method goes through all the moves that the User can choose and plots the beliefs of the Agent for each of the Users moves. line 10: Applies a function to make the outputs of the neural network sum up to 1. This allows the User to be able to understand the output of the neural network easier. line 25: Plots the data set in each div in index.html using Plotly.js Now we have all the pieces complete for the reinforcement cycle and visualization portion. Next we will describe index.html. This is required to do a few things: The following is an excerpt from index.html which reflect points above. lines 13: There are three buttons which are the users moves which when clicked will call the chooseMove function and pass it the value of the button. This will have the agent select and return a move. lines 56: These buttons will call the train function when the user determines whether the move that the Agent chose was good or bad. This will tell the agent to update its beliefs positively or negatively. lines 811: Toggle whether the agent is learning or evaluating what it has learnt lines 1317: Contain the divs that will be used to plot the beliefs of the Agent for each move. And with that we come to the end of the tutorial. To see all the code visit this repository . Feel free to clone it and play around with the code. Once you have cloned it, open index.html in a browser. You will see the setup shown in the image below. The top buttons are the Users moves. Once you select a move, the Agent will choose a move randomly. Then the User can click on the Positive Reward or Negative Reward buttons and the beliefs will be updated in real time. Click a new move and repeat the process. Once you are satisfied with the learned behavior, switch the toggle from Train to Evaluate and then the moves chosen by the Agent will be those that are highest value in each histogram. I hope that this tutorial has been helpful and provided an intuitive understanding of how to frame a reinforcement learning problem and how a neural network can be used to help with that. Stay tuned for further tutorials on the basics of neural networks and reinforcement learning. Thanks for reading. 36 1 36 36 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Suradech Kongkiatpaiboon Oct 20, 2019 Member-only How to use Python without administrative right at your workplace or use it in portable mode Python is a beautiful language from my experience. It works so fast, easy to learn and can be adapted in a variety of fields. I started using Python to learn Data Science but found out lately that it can also expand to web scraping or automated BOT programming. Usually, most 5 min read 5 min read Share your ideas with millions of readers. Sam Mourad Oct 20, 2019 Member-only Prophet vs DeepAR: Forecasting Food Demand In collaboration with Christian Aalby Svalesen The global food industry faces significant sustainability challenges, and the UN estimates that approximately one-third of the global food production is wasted or lost annually. About 66% of the losses are in food groups where freshness is an important criterion for consumption. In addition 8 min read 8 min read Ishtiak Mahmud Oct 20, 2019 Self Driving Car Localization How does a self-driving car know where it is at any given time? One of the projects of artificial intelligence which has always fascinated me is the self-driving car. It is my major motivation behind learning more about deep learning and artificial intelligence. One of the most vital things that a self-driving car has to accomplish to perform its other functions is localization 7 min read 7 min read Alex Albano Oct 20, 2019 Member-only Autonomous Distributed Networks: The unfulfilled libertarian dream of breaking free from regulations Keywords: Blockchain, Distributed Ledger Technology, Decentralised Governance, Regulatory Compliance, Autonomous Distributed Networks, Faultless Responsibility, Intelligent Smart Contracts. Summary Decentralisation embodies the libertarian dream of breaking free from the regulatory influence of governments and of bypassing via trustless networks the need for a central authority for decision making. Techno-libertarian and crypto-anarchists maintain 17 min read 17 min read Georgi Tancev Oct 20, 2019 Member-only Detecting Lesions in Multiple Sclerosis Patients with Deep Learning Introduction to convolutional neural networks in medical image analysis. Introduction Applications of Deep Learning Deep learning is part of a broader family of machine learning methods based on artificial neural networks. One of the most promising applications of deep learning is image analysis, e.g. for image segmentation or classification. Whereas segmentation yields a probability distribution per pixel (also known as mask) for each class 10 min read 10 min read Sacha Gunaratne Im into all things visualization, analytics and predictive modelling. I also enjoy delving deep into reinforcement learning and optimization. More from Medium Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Frank Andrade in Geek Culture My Top 5 Paid Subscriptions Ill Never Cancel as a Programmer Dennis Bakhuis in Towards Data Science Python 3.14 will be faster than C++ Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5867,\n",
       "  'url': 'https://towardsdatascience.com/the-complete-tensorflow-tutorial-for-newbies-dc3acc1310f8',\n",
       "  'title': 'The Complete TensorFlow Tutorial for\\xa0Newbies',\n",
       "  'subtitle': 'From installation to building a neural network for\\xa0hand…',\n",
       "  'claps': 461,\n",
       "  'responses': 4.0,\n",
       "  'reading_time': 6,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-18',\n",
       "  'clap_prop': 0.0002277124579460534,\n",
       "  'text': \"Towards Data Science Mar 18 2019 Member-only Listen Save Complete TensorFlow Tutorial Newbies installation building neural network hand sign recognition TensorFlow robust framework machine learning deep learning make easier build model deploy production popular framework among developer come surprise framework also available web-based machine learning TensorFlow.js on-device inference TensorFlow Lite Furthermore recent announcement TensorFlow 2.0 framework soon easier use syntax simplified fewer APIs support Julia programming language great time get started TensorFlow mastering important asset data scientist tutorial get running TensorFlow Note TensorFlow 2.0 stable focus previous stable version first install framework easiest way possible write function learn syntax use APIs Finally write model recognize hand sign Lets get started hands-on video tutorial machine learning deep learning artificial intelligence checkout YouTube channel Installation Lets install TensorFlow go installation process Windows machine Feel free post comment experiencing difficulty operating system Step 1 Download Anaconda first step download install Anaconda Anaconda platform make easier perform data science give access popular tool library also act package manager Download Anaconda distribution operating system follow installation step safe accept default setting Installation might take minute lot thing bundled Anaconda installation done search anaconda prompt Windows search bar Open application see something like Great Step 2 Install TensorFlow install Tensorflow simply type Wait installation complete voil set write code using TensorFlow Getting warmed rest tutorial follow notebook also grab utility needed throughout tutorial Refer whenever feel stuck Computing sigmoid function Lets compute sigmoid function using TensorFlow full code block walk code TensorFlow program usually split two part construction phase computation phase construction phase use placeholder create variable need define type variable give name simply use built-in sigmoid function Note construction phase value computed fact code doe run need computation phase computation phase create session assign result computation another variable Notice function take z input use x inside function hence need feed_dict entire block cell called TensorFlow graph TensorFlow program structured Understanding structure key mastering TensorFlow comfortable able write advanced program Computing cost let compute cost function classification problem used compute cross-entropy function writing scratch let see TensorFlow make easy u achieve result Thats one line define loss function Notice used placeholder recognize construction computation phase code cell One-hot encoding One-hot encoding technique transform mutliclass label vector 0 1 Lets see TensorFlow TensorFlow make easy manipulate data comfortable general structure let move building actual neural network classify hand sign Hand Signs Recognition little project build hand sign recognition system Specifically neural network recognize hand expressing number 0 5 Lets load dataset take look sample image see building model first flatten image normalize feature one-hot encode label Onto building model Create placeholder First write function create placeholder feature matrix label matrix write function initialize weight matrix bias matrix Awesome must define forward propagation Note code cell comment show equivalent syntax numpy Finally define function compute cost ready combine everything model Wait backpropagation Unlike previous post wrote backpropagation scratch deep learning framework take care automatically line code Putting everything single model combine function model use mini-batch gradient descent train neural network run model following line code get Great test accuracy good used small part entire dataset train long main objective tutorial wa get used TensorFlow get overview API Good job ready use TensorFlow advanced neural network application following post explore different neural network structure use TensorFlow Keras another deep learning framework build Stay tuned keep learning 461 3 461 461 3 Get email whenever publish Get freebie course announcement VIP invitation event straight inbox Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Karl Schriek Mar 18 2019 Introducing Mercury-ML open-source messenger machine learning god ancient Roman mythology god Mercury wa known messenger god Wearing winged shoe winged hat zipped Mount Olympus kingdom men saw god wa known wasnt strongest 7 min read 7 min read Share idea million reader Supratim Haldar Mar 18 2019 stop training neural-network using callback useful hack Tensorflow Keras Introduction Often training deep neural network want stop training training accuracy reach certain desired threshold Thus achieve want optimal model weight avoid wastage resource time computation power brief tutorial let learn achieve 2 min read 2 min read Aman Kumar Mar 18 2019 5 prerequisite data science 5 point worth evaluating become data scientist Data science buzz word market n't Sexiest job time 29 increase demand year year 344 increase since 2013 easy get good salary hike etc etc etc Lucrative enough 5 min read 5 min read Nhan Tran Mar 18 2019 Machine Learning Simple Linear Regression Python post guide first step approach Machine Learning using Simple Linear Regression Linear First let say shopping Walmart Whether buy good pay 2.00 parking ticket apple price 1.5 buy x item apple populate price list easy predict 6 min read 6 min read Shadab Hussain Mar 17 2019 Optimizing Jupyter Notebook Tips Tricks nbextensions Jupyter Notebooks web-based interactive tool machine learning data science community us lot used quick testing reporting tool even highly sophisticated learning material online course blog Im going list 6 min read 6 min read Marco Peixeiro Senior data scientist Author Instructor write hands-on article focus practical skill Medium Dharmaraj Deploying Deep Learning Model using Flask API Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Towards Data Science Mar 18, 2019 Member-only Listen Save The Complete TensorFlow Tutorial for Newbies From installation to building a neural network for hand signs recognition TensorFlow is a robust framework for machine learning and deep learning. It makes it easier to build models and deploy them for production. It is the most popular framework among developers. This comes with no surprise, as the framework is also available for web-based machine learning ( TensorFlow.js ) and for on-device inference ( TensorFlow Lite ). Furthermore, with the recent announcement of TensorFlow 2.0 , the framework will soon be easier to use, as the syntax will be simplified with fewer APIs, and it will support the Julia programming language. It is a great time to get started with TensorFlow, and mastering it is an important asset data scientists. This tutorial will get you up and running with TensorFlow. Note that TensorFlow 2.0 is not stable, so we will focus on the previous stable version. We will first install the framework in the easiest way possible, then we will write a few functions to learn the syntax and to use a few APIs. Finally, we will write a model that will recognize hand signs. Lets get started! For hands-on video tutorials on machine learning, deep learning, and artificial intelligence, checkout my YouTube channel . Installation Lets install TensorFlow! We will go through the installation process on a Windows machine. Feel free to post a comment if you are experiencing difficulties on other operating systems. Step 1 Download Anaconda The first step is to download and install Anaconda . Anaconda is a platform that makes it easier to perform data science; it gives access to the most popular tools and libraries and it also acts as a package manager. Download the Anaconda distribution for your operating system and follow the installation steps. It is safe to accept the default settings. Installation might take a few minutes, because a lot of things are bundled with Anaconda. Once installation is done, search for anaconda prompt in your Windows search bar. Open the application and you should see something like this: Great! Step 2 Install TensorFlow Now, to install Tensorflow, simply type: Wait for the installation to complete and voil! You are now set to write code using TensorFlow! Getting warmed up The rest of the tutorial will follow this notebook. You can also grab any utilities needed throughout the tutorial. Refer to it whenever you feel stuck! Computing the sigmoid function Lets compute the sigmoid function using TensorFlow. Here is the full code block. We will then walk through what the code is doing. A TensorFlow program is usually split into two parts: a construction phase and a computation phase . During the construction phase, we use a placeholder to create a variable. We need to define the type of variable and give it a name. Then, we simply use the built-in sigmoid function. Note that during the construction phase, there are no values being computed. In fact, the code does not run at all. That is why we need a computation phase. During the computation phase, we create a session and assign the result of our computation to another variable. Notice that the function takes z as input, but use x inside the function, hence the need of feed_dict . And the entire block cell is called a TensorFlow graph . This is how all TensorFlow programs are structured: Understanding this structure is key to mastering TensorFlow. Once you are comfortable with this, you will be able to write more advanced programs. Computing the cost Now, lets compute the cost function for a classification problem. We are used to compute the cross-entropy function and writing it from scratch. Now, lets see how TensorFlow makes it easy for us to achieve the same result: Thats it! Only one line to define the loss function! Notice again how we used the placeholders, and recognize the construction and computation phases of the code cell above. One-hot encoding One-hot encoding is a technique to transform mutliclass labels to vectors of 0s and 1s. Lets see how we can do it in TensorFlow: Again, TensorFlow makes it very easy to manipulate our data. Now that you are more comfortable to the general structure, lets move on to building an actual neural network to classify hand signs! Hand Signs Recognition In this little project, we will build a hand signs recognition system. Specifically, our neural network will recognize if a hand is expressing a number from 0 to 5. Lets load the dataset and take a look at a sample image: And you should see: Before building our model, we will first flatten the images, normalize their features, and one-hot encode the labels: Onto building the model now! Create placeholders First, we write a function to create placeholders for the feature matrix and label matrix: Then, we write a function to initialize the weight matrix and bias matrix: Awesome! Now, we must define forward propagation: Note that in the code cell above, the comments show the equivalent syntax in numpy . Finally, we define a function to compute the cost: Now, we are ready to combine everything into a model! Wait What about backpropagation? Unlike previous posts where we wrote backpropagation from scratch, deep learning frameworks take care of it automatically with a few lines of code! Putting everything into a single model Now, we combine all our functions into a model, and we will use mini-batch gradient descent to train the neural network: Now, we run the model with the following line of code: And you should get: Great! The test accuracy is not that good, because we only used a small part of the entire dataset, and we did not train for very long. The main objective of this tutorial was to get used to TensorFlow and to get an overview to its API. Good job! You are now ready to use TensorFlow for more advanced neural networks and applications. In following posts, we will explore different neural network structures and use TensorFlow or Keras (another deep learning framework) to build them. Stay tuned and keep learning! 461 3 461 461 3 Get an email whenever I publish! Get freebies, course announcement, VIP invitations to events and more straight into your inbox! Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Karl Schriek Mar 18, 2019 Introducing Mercury-ML: an open-source messenger of the machine learning gods In the ancient Roman mythology, the god Mercury was known as the messenger of the gods. Wearing winged shoes and a winged hat, he zipped between Mount Olympus and the kingdoms of men and saw to it that the will of the gods was known. He wasnt the strongest, the 7 min read 7 min read Share your ideas with millions of readers. Supratim Haldar Mar 18, 2019 How to stop training a neural-network using callback? An useful hack with Tensorflow and Keras Introduction Often, when training a very deep neural network, we want to stop training once the training accuracy reaches a certain desired threshold. Thus, we can achieve what we want (optimal model weights) and avoid wastage of resources (time and computation power). In this brief tutorial, lets learn how to achieve 2 min read 2 min read Aman Kumar Mar 18, 2019 5 prerequisites for data science. 5 points worth evaluating to become a data scientist. Data science the buzz word in market!! isn't it? Sexiest job of the time, 29% increase in demand year over year and a 344% increase since 2013, easy to get a good salary hike. etc. etc. etc. :) Lucrative enough 5 min read 5 min read Nhan Tran Mar 18, 2019 Machine Learning: Simple Linear Regression with Python In this post we will guide you the very first step to approach Machine Learning using Simple Linear Regression. What is Linear? First, lets say that you are shopping at Walmart. Whether you buy goods or not, you have to pay $2.00 for parking ticket. Each apple price $1.5, and you have to buy an (x) item of apple. Then we can populate a price list as below: Its easy to predict 6 min read 6 min read Shadab Hussain Mar 17, 2019 Optimizing Jupyter Notebook: Tips, Tricks, and nbextensions Jupyter Notebooks are a web-based and interactive tool that the machine learning and data science community uses a lot. They are used for quick testing, as a reporting tool or even as highly sophisticated learning materials in online courses. So here in this blog, Im going to list down a 6 min read 6 min read Marco Peixeiro Senior data scientist | Author | Instructor. I write hands-on articles with a focus on practical skills. More from Medium Dharmaraj Deploying Deep Learning Model using Flask API Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 4328,\n",
       "  'url': 'https://medium.com/datadriveninvestor/web-hosting-using-python-part-2-ec081e48631e',\n",
       "  'title': 'Web Hosting Using Python Part\\xa02',\n",
       "  'subtitle': '-',\n",
       "  'claps': 67,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 3,\n",
       "  'publication': 'Data Driven Investor',\n",
       "  'date': '2019-10-23',\n",
       "  'clap_prop': 3.3094869159187803e-05,\n",
       "  'text': \"DataDrivenInvestor Oct 22 2019 Listen Save Web Hosting Using Python Part 2 previous part tutorial saw run basic web page server using Flask tutorial found take look render already present HTML page also include CSS Creating HTML File basic HTML file also contains CSS Style Tags JavaScript script tag dynamic page output change button click output code shown HTML file run smoothly machine without server run Flask server need add entire HTML code file return statement really feasible Lets see easier way Flask Data Driven Investor Microsoft 'Edge Chrome Brief History wa never fan browser well exact wa fan one Chrome ha www.datadriveninvestor.com File Structure Flask Server HTML file stored folder named template directory Flask application Hence include HTML file application create folder directory Flask application name template need add Flask file template folder HTML file included template folder Changes Flask Application Making change Flask application fairly simple need specify name HTML file want render HTML code Flask default look template folder search HTML template change made Flask application line 1 line 6 line 1 import render template module line 6 include return statement passing name HTML file process run application local server remains discussed first tutorial Lets see output application local server see Flask application run perfectly fine server Thanks Reading upcoming Publications see render CSS JavaScript present separate file included HTML document Please comment view publication comment box 67 67 67 DataDrivenInvestor empowerment data knowledge expertise subscribe DDIntel http //ddintel.datadriveninvestor.com Joseph Brown Oct 22 2019 Member-only Future Mindset Key Lifetime Success know life look like five year imagined want live ten twenty thirty year time Entrepreneurial Trek Embrace Learning Data Driven Investor building multimillion-dollar company wa n't hard enough entrepreneur take extra care theirwww.datadriveninvestor.com never Well used imagine life might like fantasize good life could stress 5 min read 5 min read Share idea million reader Michael Trigg Oct 22 2019 Member-only Many Crypto Currencies Global Economy Handle must limit number cryptocurrencies first cryptocurrency enter global economy wa BitCoin people major market know first actual Bitcoin came 3rd January 2009 Satoshi Nakamoto mined genesis block bitcoin block number 0 reaped reward 4 min read 4 min read Hamza Ergder Oct 22 2019 Artificial Intelligence develops much near future robot may get ahead human development artificial intelligence idea people passed education system emphasizes human emotion-based feature rather rote education avoid passed might way 3 min read 3 min read Michael Trigg Oct 22 2019 Member-only Certain Humans Crying Climate Change Wolf Kind like shouting fire crowded theatre climate Planet Earth changing without doubt people around world tend agree matter people live hotter drier region planet know climate warming 4 min read 4 min read Daniel G. Jennings Oct 22 2019 Member-only Trucking Survive JB Hunt Market Mad House Trucking receiving lot scrutiny criticism day Many observer believe trucking industry verge major disruption Elon Musk example belief electric-powered semi soon become norm industry Consequently Musks Tesla Motors NASDAQ TSLA manufacturing marketing 5 min read 5 min read Ayush Kalla Data Scientist interest Python JavaScript Web Development free time like read Blockchain bake go hike Medium Dennis Niggl Python Plain English Creating Awesome Web App Python Streamlit Avi Chawla Towards Data Science Building All-In-One Audio Analysis Toolkit Python Yang Zhou TechToFreedom 9 Fabulous Python Tricks Make Code Elegant Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"DataDrivenInvestor Oct 22, 2019 Listen Save Web Hosting Using Python Part 2 In the previous part of this tutorial we saw how to run a basic web page on a server using Flask. The tutorial can be found here . Now we can take a look at how to render an already present HTML page which can also include CSS in itself. Creating the HTML File Below is a basic HTML file that also contains CSS (between the Style Tags) and some JavaScript (between the script tag). This is a dynamic page as its output changes on button click. The output of the above code is as shown below This HTML file runs smoothly on the machine (without any server), but to run this on a Flask server we will need to add the entire HTML code for the file in the return statement, which is not really feasible. Lets see an easier way to do it in Flask. Data Driven Investor | Microsoft Having An 'Edge' Over Chrome A Brief History I was never a fan of browsers, well to be exact I was only a fan of one, Chrome. It has been my www.datadriveninvestor.com File Structure of a Flask Server All the HTML files are stored in a folder named templates which can be in the same directory as the Flask application. Hence, to include the HTML file in the application, we create a folder in the same directory as our Flask application and name it as templates. Now we need to add our Flask file to this templates folder. The HTML file is included in the templates folder. Changes to the Flask Application Making changes to the Flask application is fairly simple. We just need to specify the name of the HTML file from which we want to render the HTML code. Flask by default looks at the templates folder to search for HTML templates. The only changes made in this Flask application is on lines 1 and line 6. On line 1 we import the render template module and on line 6 we include it in the return statement, by passing the name of the HTML file. The process to run the application on the local server remains the same as discussed in the first tutorial. Lets see the output of the application on the local server. As we can see, our Flask application runs perfectly fine now on the server. Thanks for Reading In the upcoming Publications we will see how to render CSS and JavaScript that are present as separate files and not included in the same HTML document. Please comment your views about the publication in the comment box. 67 67 67 More from DataDrivenInvestor empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com Joseph Brown Oct 22, 2019 Member-only The Future Mindset The Key for a Lifetime of Success Do you know what your life will look like in five years? Have you imagined how you want to live in ten, twenty, thirty years time? On the Entrepreneurial Trek, Embrace the Learning | Data Driven Investor As if building a multimillion-dollar company wasn't hard enough, entrepreneurs have to take extra care of theirwww.datadriveninvestor.com I never did. Well, I used to imagine what life might be like, and fantasize about how good life could be, and stress about 5 min read 5 min read Share your ideas with millions of readers. Michael Trigg Oct 22, 2019 Member-only How Many Crypto Currencies Can The Global Economy Handle? There must be a limit to the number of cryptocurrencies. The first cryptocurrency to enter the global economy was BitCoin, as most people in the major markets know. The first actual Bitcoin came into being on the 3rd of January 2009 when Satoshi Nakamoto mined the genesis block of bitcoin - block number 0 - and reaped a reward of 4 min read 4 min read Hamza Ergder Oct 22, 2019 What do we do if Artificial Intelligence develops too much? In the near future, robots may get ahead of humans with the development of artificial intelligence. There is an idea that people should be passed through an education system that emphasizes human emotion-based features rather than rote education to avoid being passed. This might be the only way we can 3 min read 3 min read Michael Trigg Oct 22, 2019 Member-only Are Certain Humans Crying Climate Change Wolf? Kind of like shouting fire in a crowded theatre. The climate on Planet Earth is changing without a doubt. Most people around the world tend to agree on that matter. Most people who live in the hotter and drier regions of the planet know the climate is warming. 4 min read 4 min read Daniel G. Jennings Oct 22, 2019 Member-only Can Trucking Survive at JB Hunt? Market Mad House Trucking is receiving a lot of scrutiny and criticism these days. Many observers believe the trucking industry is on the verge of major disruption. Elon Musk, for example, believes electric-powered semis will soon become the norm in the industry. Consequently, Musks Tesla Motors (NASDAQ: TSLA) is manufacturing and marketing an 5 min read 5 min read Ayush Kalla Data Scientist with interests in Python, JavaScript and Web Development. In my free time I like to read about Blockchain, bake and go on hikes. More from Medium Dennis Niggl in Python in Plain English Creating an Awesome Web App With Python and Streamlit Avi Chawla in Towards Data Science Building an All-In-One Audio Analysis Toolkit in Python Yang Zhou in TechToFreedom 9 Fabulous Python Tricks That Make Your Code More Elegant Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 697,\n",
       "  'url': 'https://medium.com/datadriveninvestor/data-science-ai-journey-part-1-f81ba77d5f42',\n",
       "  'title': 'Data Science & AI Journey: Part\\xa01',\n",
       "  'subtitle': 'Quick dev environment set-up to start learning data\\xa0science',\n",
       "  'claps': 127,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Data Driven Investor',\n",
       "  'date': '2019-04-07',\n",
       "  'clap_prop': 6.273206542114702e-05,\n",
       "  'text': 'DataDrivenInvestor Apr 7 2019 Listen Save Data Science AI Journey Part 1 Quick dev environment set-up start learning data science someone new data science machine learning setting development environment tricky task option available could spend lot time searching right tutorial online fiddling different command tutorial learn quickly set stable Python development environment locally using Anaconda also discover option setting environment cloud free recently started practical data community Accra first meetup attendee various background completely new data science artificial intelligence introduction brief discussion first practice session Funsho Olaniyi taking u quick introduction Python Pandas realized lot attendee didnt Python installed machine took several minute get environment set follow session along post aim facilitate process setting Python development environment first time 8 Skills Need Become Data Scientist Data Driven Investor Numbers scare nothing satisfying beautiful excel sheet speak several language www.datadriveninvestor.com favorite recommended environment research online shouldnt take much time find Jupyter Notebook data scientist favorite local development environment Part Project Jupyter Jupyter Notebook open-source web application allows create share document contain live code equation visualization narrative text mean easily code document project see output interface save format easily shared others Installation using Anaconda distribution experienced user already version Python installed computer may wish install Jupyter using pip one following command depending version Python pip install jupyter pip3 install jupyter new user strongly recommended install Jupyter using Anaconda distribution make process easier faster includes Python Jupyter Notebook commonly used package scientific computing data science install Jupyter using Anaconda go http //jupyter.org/ scroll Jupyter Notebook section click Install Notebook first try online want following page see link download Anaconda Follow select operating system version Python want install installation completed follow instruction install complication simply go terminal type following command directory jupyter notebook second open new tab default browser done Go ahead create first notebook clicking New top right corner Familiarise intuitive interface start coding either following tutorial starting project User Interface Tour handy get quick grasp everything also click Help shortcut even libraries-specific help Feel free let know need help get started think write article Jupyter Overview Bonus Python Notebooks cloud aim story wa mainly help quickly set Python environment locally start learning data science Hopefully Jupyter Notebook server running local machine able code straight browser tab continue learning data science machine learning might find situation need faster better alternative One could want train model faster Instead relying CPU central processing unit train complex model might want switch GPUs graphic processing unit TPUs tensor processing unit speed training process Due high cost currently available GPU board good option probably train model cloud Fortunately lot option available many allow get free credit processing power detailed article topic check story Kwadwo Agyapon-Ntra list option Google Colaboratory Google Colab short Kaggle Kernels Kaggle Azure Notebooks Microsoft CoCalc Datalore Binder article Data School cover detailed comparison platform hope wa helpful quickly get started data science learning journey planning publish similar story document learning journey help learner Feel free clap share also follow Medium Twitter connect LinkedIn value learn share 127 1 127 127 1 DataDrivenInvestor empowerment data knowledge expertise subscribe DDIntel http //ddintel.datadriveninvestor.com Philippe A. Abdoulaye Apr 7 2019 Member-only CIOs Urgently Learn Paris AWS Summit 2019 Last week attended Paris 2019 AWS Summit wa third participation four year Paris already 2015 London 2017 Paris year AWS Summits exciting moment bring technologist together connect collaborate learn AWS Amazon usual thing 5 min read 5 min read Share idea million reader Manu Siddharth Jha Apr 7 2019 Artificial Intelligence Win Football Matches Football sport universally loved adored people walk life one popular sport 5 min read 5 min read Anupra Chandran Apr 6 2019 Autoencoders Like Google Genome Making Genomic Data Digestible Using Autoencoder Neural Networks lucky living time much access information mean access pretty much knowledge world right fingertip 12 min read 12 min read Holly Atkinson Apr 6 2019 Member-only initialised Drizzle create hybrid web app DApp post assumes already general understanding web development using Ruby-on-Rails JavaScript ReactJS 7 min read 7 min read Harsha Angeri Apr 6 2019 Slo-Mo AI Life recognize people bet cant none real dont exist generated using AI Artificial Intelligence machine Nvidia Shocked come across cool startup education space 5 min read 5 min read Ulrich Mabou write blog feel free check latest post http //ulrich.bearblog.dev/blog/ Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Anmol Tomar CodeX Say Goodbye Loops Python Welcome Vectorization Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'DataDrivenInvestor Apr 7, 2019 Listen Save Data Science & AI Journey: Part 1 Quick dev environment set-up to start learning data science For someone new to data science or machine learning, setting up a development environment can be a tricky task. With all the options available, you could spend a lot of time searching for the right tutorial online and fiddling with different commands. In this tutorial, you will learn how to quickly set up a stable Python development environment locally using Anaconda. You will also discover some options for setting up your environment in the cloud for free. Who is this for? We recently started a practical data community in Accra. During our first meetup, we had attendees from various backgrounds, some of them completely new to data science or artificial intelligence. After introductions and brief discussions, we had our first practice session with   Funsho Olaniyi   taking us through a quick introduction to Python and Pandas. We realized that a lot of our attendees who didnt have Python installed on their machines took several minutes to get their environment set up and follow the session along. This post aims to facilitate the process of setting up a Python development environment for those doing it the first time. 8 Skills You Need to Become a Data Scientist - Data Driven Investor Numbers do not scare you? There is nothing more satisfying than a beautiful excel sheet? You speak several languages www.datadriveninvestor.com Our favorite and recommended environment If you do your own research online, it shouldnt take you much time to find out that Jupyter Notebook is data scientists favorite local development environment. Part of Project Jupyter , The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. It means that you can easily code and document your project, see the outputs in the same interface, and save it in a format that can easily be shared with others. Installation using the Anaconda distribution If you are an experienced user and you already have a version of Python installed on your computer, you may wish to install Jupyter using pip with one of the following commands depending on which version of Python you have: pip install jupyter Or pip3 install jupyter For new users, it is strongly recommended to install Jupyter using the Anaconda distribution . It makes the process easier and faster because it includes Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science. To install Jupyter using Anaconda, go to https://jupyter.org/ , scroll down to The Jupyter Notebook section and click on Install the Notebook. You can first try it online if you want to. On the following page, you should see a link to download Anaconda . Follow it, select your operating system and version of Python you want to install. Once the installation is completed(just follow the instructions; it should install with no complication), simply go to your terminal and type the following command in any directory: jupyter notebook After a few seconds, it should open a new tab on your default browser. You are done! Go ahead and create your first notebook by clicking on New in the top right corner. Familiarise yourself with the intuitive interface and start coding, either by following a tutorial or starting your own project. The User Interface Tour is very handy to get a quick grasp of everything you can do; you can also click on Help for shortcuts and even libraries-specific help. Feel free to let me know if you need any further help to get started or if you think I should write an article on Jupyter Overview. Bonus: Python Notebooks in the cloud The aim of this story was mainly to help you quickly set up a Python environment locally to start learning data science. Hopefully, by now you should have a Jupyter Notebook server running on your local machine and you should be able to code straight from a browser tab. As you continue learning data science and machine learning, you might find yourself in situations where you need a faster or better alternative. One of such could be that you want to train a model faster. Instead of relying on your CPU (central processing unit) to train complex models, you might want to switch to GPUs (graphics processing units) or TPUs (tensor processing units) to speed up the training process. Due to the high cost of currently available GPU boards, a good option will probably be to train your models in the cloud. Fortunately, they are a lot of options available to do that, and many of them will allow you to get some free credits for processing power. For a detailed article on the topic, check out this story by   Kwadwo Agyapon-Ntra   . Below is a list of some of the options: Google Colaboratory (or Google Colab for short) Kaggle Kernels (from Kaggle) Azure Notebooks (from Microsoft) CoCalc Datalore Binder This article from Data School covers a detailed comparison of those platforms. I hope that this was helpful to quickly get you started on your data science learning journey. I am planning to publish similar stories to document my own learning journey and help other learners. Feel free to clap and share. You can also follow me here on Medium, on Twitter , or connect with me on LinkedIn . #value #learn #share 127 1 127 127 1 More from DataDrivenInvestor empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com Philippe A. Abdoulaye Apr 7, 2019 Member-only What CIOs Should Urgently Learn from Paris AWS Summit 2019 Last week, I attended the Paris 2019 AWS Summit, it was my third participation in four years; Paris already in 2015, London in 2017, and again Paris this year. AWS Summits are exciting moments, they bring technologists together to connect, collaborate, and learn about AWS. Amazon as usual did things 5 min read 5 min read Share your ideas with millions of readers. Manu Siddharth Jha Apr 7, 2019 Artificial Intelligence Can Win Football Matches Football is a sport that is universally loved and adored by people from all walks of life. It is one of the most popular sports in the 5 min read 5 min read Anupra Chandran Apr 6, 2019 Autoencoders: Like Google, But for your Genome Making Genomic Data More Digestible Using Autoencoder Neural Networks Were so lucky to be living in a time with so much access to information. I mean, we have access to pretty much all the knowledge in the world, all right at our fingertips! 12 min read 12 min read Holly Atkinson Apr 6, 2019 Member-only How I initialised Drizzle to create a hybrid web app / DApp This post assumes you already have a general understanding of web development using Ruby-on-Rails, JavaScript and ReactJS. 7 min read 7 min read Harsha Angeri Apr 6, 2019 A Slo-Mo AI Life Can you recognize any of these people? I bet you cant as none of them are real. They dont exist. They were generated using an AI (Artificial Intelligence) machine by Nvidia. Shocked??? Have you come across these below cool startups in education space? 5 min read 5 min read Ulrich Mabou I now write on my own blog; feel free to check out my latest posts: https://ulrich.bearblog.dev/blog/ More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Anmol Tomar in CodeX Say Goodbye to Loops in Python, and Welcome Vectorization! Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 1380,\n",
       "  'url': 'https://towardsdatascience.com/web-scrape-twitter-by-python-selenium-part-1-b3e2db29051d',\n",
       "  'title': 'Web Scrape Twitter by Python Selenium (Part\\xa01)',\n",
       "  'subtitle': '-',\n",
       "  'claps': 24,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 7,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-11-09',\n",
       "  'clap_prop': 1.185487850478369e-05,\n",
       "  'text': 'Nov 9 2019 Member-only Listen Save Web Scrape Twitter Python Selenium Part 1 Introduction One popular application Python web scraping Web scraping mean extracting data website software programming ton data Internet used data analytics However impractical impossible manually copy store data moment web scraping need famous Python package perform web scraping like request inbuilt package BeautifulSoup Scrapy Selenium Today demonstrate use Selenium web scrape objective web scrape tweet traffic data science twitter account link PS direct way connecting Twitter using Tweepy package use Twitter example article many different application using web scraping Moreover far know function checking tweet analytics information Selenium found via Selenium Python Selenium Python Bindings 2 documentation Note official documentation would like contribute documentation fork selenium-python.readthedocs.io Content Prerequisites Begin tutorial Ending Prerequisites 2 Download Driver include path PATH http //selenium-python.readthedocs.io/installation.html driver Driver used control browser using Python program Depending browser select suitable driver either place path driver PATH environment declare path driver every time Python script would recommend include PATH environment directly dont need set path every time 3 Basic HTML language dont need in-depth knowledge HTML dont need experience web design However much easier basic knowledge structure syntax HTML dont understand anything said want know basic first recommend go w3schools HTML Tutorial HTML standard markup language Web page HTML create Website HTML easy learn www.w3schools.com Begin tutorial PS new beginner would suggest work Jupyter Notebook first face error anytime using Jupyter Notebook run script step step know problem first step open browser navigate twitter page Depending browser select correct driver webdriver Finally visit twitter page get method moment see new browser prompt directing twitter page 2 Locate input box username password time look html either Chrome Firefox right-click page select Inspect Chrome function locate html script object webpage Click email box Chrome jump particular part script see div tag class called LoginForm-input LoginForm-username particular email box click small arrow expand part see input tag inside div tag indicates pact designed user input use information input tag locate email box see multiple element like type class name better use specific element since common one element element name Like case either class name recommended since element name represent email input use class name example actually three class name class element text-input email-input js-signin-email safest way use js-signin-email locate task find element class name equal js-signin-email multiple method Selenium locate element class name even partial text link find information 4 Locating Elements Selenium Python Bindings 2 documentation various strategy locate element page use appropriate one case Selenium selenium-python.readthedocs.io Since locate email box class name use find_element_by_class_name find email box Simple right really One common problem web scraping program search element right accessing webpage Maybe Internet connection speed element loading bit later mainframe element ha correctly located webpage yet Thus Selenium prompt NoSuchElementException error message tell find element Therefore necessary allow webpage load second element found worst case done using time.sleep function inefficient Luckily Selenium provides wait look element two type wait Selenium implicit explicit introduce explicit wait want confirm presence element use explicit wait moment Explicit wait try find element found nothing happens found instead immediately prompt error message wait certain time period time continues finding element certain time Selenium still find element finally prompt TimeoutException error many method explicit wait find information 5 Waits Selenium Python Bindings 2 documentation day web apps using AJAX technique page loaded browser element within selenium-python.readthedocs.io doc provides script use wait However since frequent use explicit wait write function reuse multiple time define variable attribute wait presence element element found exit program locate email box script simplifies Since locate email box define variable indicate email box use find_element_by_class_name pas name class next step input email address rather easy use send_keys email variable input email address successful able see email box filled email address already similar process done also password box Except pas one argument send_keys Keys.ENTER passing Keys.ENTER simulates pressing Enter filling password password box skip locating login button page till everything smooth log twitter account already Congratulations made big step web scraping Ending Web scraping interesting area help automate many task normally take u hour finish However also challenging complexity website nowadays Therefore trial error inevitable finally getting expected result end part 1 next part introduce access tweet love reading face problem feel free leave comment reply comment without Pythons help Stay tuned next article Edit Part 2 article already Web Scrape Twitter Python Selenium Part 2 Continue scraping journey Selenium towardsdatascience.com 46 46 46 WY Fok Amazonian Former data science intern Amazon Germany Bachelor Statistics Master Operation Research Love working number Python SQL SAS Love podcasts audiobooks Learn go new app WY Fok Amazonian Former data science intern Amazon Germany Bachelor Statistics Master Operation Research Love working number Python SQL SAS Medium HKN MZ Python Plain English Scrape Everything Twitter Using Python Guilherme Guidolin Dynamic Web Scraping Python Obalanatosin Web Scraping Football Matches EPL Python Lilian Ugwu Sentiment Analysis Nigerians Primary Election Tweets Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Nov 9, 2019 Member-only Listen Save Web Scrape Twitter by Python Selenium (Part 1) Introduction One popular application of Python is web scraping. Web scraping means extracting data from a website by software or programming. There are tons of data on the Internet that can be used for data analytics. However, it is impractical, if not impossible, to manually copy and store data. At this moment, web scraping is what we need. There are some famous Python packages that perform web scraping like requests (an inbuilt package), BeautifulSoup, Scrapy, and Selenium. Today I will demonstrate how to use Selenium to web scrape. And the objective is to web scrape all tweet traffics of my data science twitter account link . (PS: There is a more direct way connecting Twitter by using Tweepy package. I use Twitter as an example only in this article. There are many different applications by using web scraping. Moreover, as far as I know, there is no function of checking the tweet analytics.) More information about Selenium can be found via: Selenium with Python - Selenium Python Bindings 2 documentation Note This is not an official documentation. If you would like to contribute to this documentation, you can fork this selenium-python.readthedocs.io Content Prerequisites Begin of tutorial Ending Prerequisites 2. Download a Driver and include the path in PATH https://selenium-python.readthedocs.io/installation.html#drivers A Driver is used to control your browser by using the Python program. Depending on your browser, select a suitable driver. Then you either place the path of this driver in PATH environment or you declare the path of the driver every time in your Python script. I would recommend to include in your PATH environment directly so you dont need to set the path every time. 3. Basic HTML language You dont need in-depth knowledge of HTML nor you dont need any experience in web design. However, it is much easier if you have some basic knowledge about the structure and syntax of HTML. If you dont understand anything I said or you want to know some basic first then I recommend you go to w3schools. HTML Tutorial HTML is the standard markup language for Web pages. With HTML you can create your own Website. HTML is easy to learn www.w3schools.com Begin of tutorial PS: For a new beginner, I would suggest you work in Jupyter Notebook first because you will face more errors than anytime before. By using Jupyter Notebook you can run the script step by step so that you know where the problem is. The first step is to open a browser and navigate the twitter page. Depending on your browser and select the correct driver from webdriver. Finally, visit the twitter page by get method. At this moment you should see a new browser prompts up and directing to the twitter page. 2. Locate input boxes for username and password Now its time to look at html. In either Chrome or Firefox, right-click the page and select Inspect. In Chrome, there is a function to locate the html script of any object on the webpage. Click the email box and then Chrome will jump to that particular part of the script. So here we can see a div tag with the class called LoginForm-input LoginForm-username for this particular email box. Then click the small arrow to expand this part. We can see there is an input tag inside the div tag. This indicates that this pact is designed for a user to input. So we will use information from the input tag to locate this email box. As you can see there are multiple elements here, like type, class, name,. It is better to use a more specific element since it is common that there is more than one element with the same element name. Like in this case, either class or name is recommended since the element names represent email input. Here I use class name as an example. There are actually three class names here in the class element, text-input, email-input, and js-signin-email. The safest way is to use js-signin-email to locate. So the task here is to find an element with a class name equal to js-signin-email. There are multiple methods in Selenium to locate an element, from class name to even the partial text of the link. You can find more information by 4. Locating Elements - Selenium Python Bindings 2 documentation There are various strategies to locate elements in a page. You can use the most appropriate one for your case. Selenium selenium-python.readthedocs.io Since we locate this email box by class name, so we will use find_element_by_class_name and then find the email box. Simple right? Not really One common problem for web scraping is that the program searches for the element right after accessing the webpage. Maybe because of your Internet connection speed or that element loading a bit later than the mainframe, the element has not been correctly located on the webpage yet. Thus Selenium will prompt a NoSuchElementException error message and tell you cannot find that element. Therefore, it is necessary to allow the webpage to load for a few seconds until that element is found. In the worst case, this can be done by using time.sleep function but it is inefficient. Luckily Selenium provides waits and look for the element. There are two types of waits in Selenium, implicit and explicit. Here I will introduce explicit wait. When you want to confirm the presence of an element, you can use explicit wait at this moment. Explicit wait will try and find the element. If it is found then nothing happens. But if it is not found, instead of immediately prompts an error message, it will wait for a certain time period while at the same time it continues finding the element. After a certain time if Selenium still cannot find the element then it will finally prompt an TimeoutException error. There are many methods for explicit waits. You can find further information by 5. Waits - Selenium Python Bindings 2 documentation These days most of the web apps are using AJAX techniques. When a page is loaded by the browser, the elements within selenium-python.readthedocs.io In the doc, it provides a script of how to use wait. However, since it is frequent to use this explicit wait, I write a function so I can reuse it multiple times. Here you can define the variable and the attribute of and then wait until the presence of the element. If the element is not found then it will exit the program. So to locate the email box, the script simplifies as Since we can now locate the email box. We then define a variable to indicate this email box. We can use find_element_by_class_name and then pass the name of the class. The next step is to input the email address. This is rather easy because we can use send_keys on email variable to input the email address. If successful, you should be able to see the email box is filled with the email address already. A similar process is done also for the password box. Except we pass one more argument in send_keys, Keys.ENTER By passing Keys.ENTER, this simulates pressing an Enter after filling in the password in the password box. Then we can skip locating the login button on the page. Up till now, if everything is smooth, you should log in to your twitter account already. Congratulations, you made a big step in web scraping. Ending Web scraping is an interesting area because this can help automate many tasks that normally take us hours to finish. However, it is also challenging because of the complexity of websites nowadays. Therefore trial and error are inevitable before finally getting your expected results. Here is the end of part 1. In the next part, I will introduce how to access all your tweets. If you love reading this or if you face any problem, feel free to leave your comment and I will reply to your comments without any Pythons help. Stay tuned for my next article. Edit: Part 2 of this article is already here: Web Scrape Twitter by Python Selenium (Part 2) Continue the scraping journey with Selenium towardsdatascience.com 46 46 46 More from WY Fok Amazonian. Former data science intern in Amazon Germany. Bachelor in Statistics and Master in Operation Research. Love working with number. Python / SQL / SAS Love podcasts or audiobooks? Learn on the go with our new app. WY Fok Amazonian. Former data science intern in Amazon Germany. Bachelor in Statistics and Master in Operation Research. Love working with number. Python / SQL / SAS More from Medium HKN MZ in Python in Plain English How to Scrape Everything from Twitter Using Python Guilherme Guidolin Dynamic Web Scraping with Python Obalanatosin Web Scraping Football Matches [EPL] With Python Lilian Ugwu Sentiment Analysis: Nigerians Primary Election Tweets Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 5085,\n",
       "  'url': 'https://towardsdatascience.com/how-to-capture-weather-data-on-your-home-400716bde645',\n",
       "  'title': 'How to Capture Weather Data with your own IoT Home\\xa0Station',\n",
       "  'subtitle': 'Capturing weather data, and logging\\xa0them…',\n",
       "  'claps': 28,\n",
       "  'responses': 0.0,\n",
       "  'reading_time': 22,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-06-04',\n",
       "  'clap_prop': 1.3830691588914305e-05,\n",
       "  'text': \"Towards Data Science Jun 4 2019 Listen Save Capture Weather Data IoT Home Station Capturing weather data logging locally IoT service via MQTT protocol Introduction talking physical variable temperature pressure etc. Data Scientist usually start working dataset wa created somewhere else thought capture data tutorial learn get data several different sensor sending IoT service ThingSpeak.com mobile App Thingsview log play data also explore several different communication way connecting sensor Raspberry Pi short data captured saved locally CSV file send IoT service ThingSpeak.com via MQTT protocol see block diagram complete real Weather Station final step also learn measure wind speed direction following Mauricio Pinto tutorial Supplies 1 Development Environment Jupyter Notebook Jupyter Notebook fantastic tool better open-source web application allows create share document contain live code equation visualization narrative text Jupyter notebook largely used Data Science cleaning transforming data numerical simulation statistical modeling data visualization machine learning much tutorial use Jupyter Notebook interact Raspberry Pi GPIOs directly reading sensor sending data internet Installation may skip step already Jupyter Notebook installed RPi install Jupyter Raspberry run Python 3 open Terminal enter following command terminal run command thats Amazing simple easy Jupyter Notebook running server Note default browser automatically opened address running Home Page shown picture stop server close kernel Jupyter notebook must use Ctrl C keyboard one time start Pi want use Jupyter Notebook type command Jupyter notebook terminal keep running time important need use terminal another task run program example open new Terminal window follow tutorial step step creating Notebook download final one GitHub Rpi_Weather_Station.ipynb 2 DHT22 Temperature Humidity Sensor first sensor installed DHT22 capturing air temperature relative humidity data ADAFRUIT site provides great information sensor Bellow information retrieved Overview low-cost DHT temperature humidity sensor basic slow great hobbyist want basic data logging DHT sensor made two part capacitive humidity sensor thermistor also basic chip inside doe analog digital conversion spit digital signal temperature humidity digital signal fairly easy read using microcontroller DHT22 Main characteristic Good 0100 humidity reading 25 accuracy Good -40 125C temperature reading 0.5C accuracy 0.5 Hz sampling rate every 2 second usually use sensor distance le 20m 4K7 ohm resistor connected Data VCC pin DHT22 output data pin connected Raspberry GPIO 16 Check electrical diagram connecting sensor RPi pin forget Install 4K7 ohm resistor Vcc Data pin sensor connected must also install library RPi Installing DHT Library Raspberry starting /home go /Documents Create directory install library move browser go Adafruit GitHub adafruit/Adafruit_Python_DHT Python library read DHT series humidity temperature sensor Raspberry Pi Beaglebone Black github.com Download library clicking download zip link right unzip archive Raspberry Pi recently created folder go directory library subfolder automatically created unzipped file execute command Jupyter Notebook Import Adafrut DHT Library define digital pin connect DHT RPi run code capture temperature humidity Run Cell print result portion Jupyter Notebook showing result 3 DS18B20 Temperature Sensor Sensor Overview use tutorial waterproofed version DS18B20 sensor useful capturing temperature wet condition example humid soil sensor isolated take measurement 125oC Adafrut doe recommend use 100oC due cable PVC jacket DS18B20 digital sensor make good use even long distance 1-wire digital temperature sensor fairly precise 0.5C much range give 12 bit precision onboard digital-to-analog converter work great RPi using single digital pin even connect multiple one pin one ha unique 64-bit ID burned factory differentiate sensor work 3.0 5.0V mean powered directly 3.3V provided one Raspberry pin 1 17 sensor ha 3 wire find full data DS18B20 Datasheet Sensor Installation Follow diagram make connection Installing Python Library Next let install Python library handle sensor running script test sensor check 1-Wire interface enabled RPi see print screen Enable Interfaces forget restart RPi changeing configuration Testing sensor testing sensor simple python code used portion Jupyter Notebook showing result 4 BMP180 Temperature Pressure Sensor Sensor Overview BMP180 successor BMP085 new generation high precision digital pressure sensor consumer application ultra-low power low voltage electronics BMP180 optimized use mobile phone PDAs GPS navigation device outdoor equipment low altitude noise merely 0.25m fast conversion time BMP180 offer superior performance I2C interface allows easy system integration microcontroller BMP180 based piezo-resistive technology EMC robustness high accuracy linearity well long-term stability complete BMP datasheet found BMP180 Digital Pressure Sensor Sensor Installation Follow diagram make connection Enabling I2C Interface Go RPi Configuration confirm I2C interface enabled enable restart RPi Using BMP180 everything ha installed connected okay ready turn Pi start seeing BMP180 telling world around first thing check Pi see BMP180 Try following terminal window command worked see something similar Terminal Printscreen showing BMP180 channel 77 Installing BMP180 Library Create directory install library go browser go Adafruit GITHub adafruit/Adafruit_Python_BMP Python library accessing BMP series pressure temperature sensor like BMP085/BMP180 Raspberry Pi github.com Download library clicking download zip link right unzip archive Raspberry Pi created folder go created subfolder execute following command directory library Jupyter write following code Check variable read sensor bellow code portion Jupyter Notebook showing result Note sensor pressure presented Pa Pascals See next step better understand unit 5 Measuring Weather Altitude BMP180 Sea Level Pressure Lets take time understand little bit get BMP reading skip part tutorial return later want know Sensor reading please go great tutorial http //learn.sparkfun.com/tutorials/bmp180-barome ... BMP180 wa designed accurately measure atmospheric pressure Atmospheric pressure varies weather altitude Atmospheric Pressure definition atmospheric pressure force air around exerting everything weight gas atmosphere creates atmospheric pressure common unit pressure pound per square inch psi use international notation newton per square meter called pascal Pa took 1 cm wide column air would weigh 1 kg weight pressing footprint column creates atmospheric pressure measure sensor like BMP180 cm-wide column air weighs 1Kg follows average sea level pressure 101325 pascal better 1013.25 hPa 1 hPa also known milibar mbar drop 4 every 300 meter ascend higher get le pressure youll see column top atmosphere much shorter therefore weighs le useful know measuring pressure math determine altitude air pressure 3 810 meter half sea level BMP180 output absolute pressure pascal Pa One pascal small amount pressure approximately amount sheet paper exert resting table often see measurement hectopascals 1 hPa 100 Pa library used provides output floating-point value hPa also happens equal one millibar mbar conversion pressure unit Temperature Effects temperature affect density gas density affect mass gas mass affect pressure whew atmospheric pressure change dramatically temperature Pilots know density altitude make easier take cold day hot one air denser ha greater aerodynamic effect compensate temperature BMP180 includes rather good temperature sensor well pressure sensor perform pressure reading first take temperature reading combine raw pressure reading come final temperature-compensated pressure measurement library make easy Measuring Absolute Pressure application requires measuring absolute pressure get temperature reading perform pressure reading see example sketch detail final pressure reading hPa mbar wish convert different unit using conversion factor Note absolute pressure atmosphere vary altitude current weather pattern useful thing measure Weather Observations atmospheric pressure given location earth anywhere atmosphere isnt constant complex interaction earth spin axis tilt many factor result moving area higher lower pressure turn cause variation weather see every day watching change pressure predict short-term change weather example dropping pressure usually mean wet weather storm approaching low-pressure system moving Rising pressure usually mean clear weather approaching high-pressure system moving remember atmospheric pressure also varies altitude absolute pressure house Lo Barnechea Chile altitude 950m always lower absolute pressure San Francisco example le 2 meter almost sea level weather station reported absolute pressure would difficult directly compare pressure measurement one location another large-scale weather prediction depend measurement many station possible solve problem weather station always remove effect altitude reported pressure reading mathematically adding equivalent fixed pressure make appear reading wa taken sea level higher reading San Francisco Lo Barnechea always weather pattern altitude function library called sea level P take absolute pressure P hPa station current altitude meter remove effect altitude pressure use output function directly compare weather reading station around world Determining Altitude Since pressure varies altitude use pressure sensor measure altitude caveat average pressure atmosphere sea level 1013.25 hPa mbar drop zero climb towards vacuum space curve drop-off well understood compute altitude difference two pressure measurement p p0 using specific equation use sea level pressure 1013.25 hPa baseline pressure p0 output equation current altitude sea level Theres function library called altitude P P0 let get calculated altitude explanation wa extracted BMP 180 Sparkfun tutorial 6 Sea Level Pressure Measurement could learn previous step important hand Sea Level pressure calculated real altitude measuring absolute pressure function help u case BMP180 installed real measured altitude 957 meter following updated data sensor 7 Using ADC Analog Digital Converter next step discus get UV data simple good analog sensor problem Raspberry Pi doe analog input pin Arduino NodeMCU overcome problem using analog digital A/D converter help interfacing analog sensor Raspberry Pi A/D converter use project popular MCP3008 MCP3008 10bit 8-channel ADC Analog Digital Converter use SPI bus protocol interfacing Raspberry Pi cheap doesnt require additional component give 8 analog input us four GPIOs Raspberry Pi plus power ground pin MCP3008 output range 01,023 0 mean 0V 1,023 mean 3.3V MCP3008 Pinout pin numbering MCP3008 start top/left Pin 1 CH0 half circle top see pinout diagram MCP3008 ADC ha total 16 pin 8 pin taking analog input analog input pin CH0-CH7 Pins 18 side pin 916 different function follows project use Channel 0 Pin 1 analog input SPI Raspberry Pi equipped one SPI bus ha 2 chip selects SPI master driver disabled default Raspbian enable use raspi-config confirm SPI bus enabled procedure wa done 1-Wire start import spidev Linux driver access SPI bus open configure bus access analog channel ADC testing write function connect Channel 0 MCP3008 pin 1 3.3V run function result see 1023 8 Analog UV Sensor UV sensor generates analog output proportional Ultra-Violet radiation found light-sensing spectrum us UV photodiode based Gallium Nitride detect 240370nm range light cover UVB UVA spectrum signal level photodiode small nano-ampere level module ha embedded operational amplifier amplify signal readable volt-level 0 1V sensor op-amp powered connecting VCC 3.3VDC GND power ground analog signal gotten pin output millivolt read Analog Input CH0 ADC connected RPi Using code shown last step see raw data generated UV sensor case 43 raw sensor data convert map value better handled code function readSensorUV function read UV sensor 3 time taking average converting measured value mV example raw measurement 43 fact equivalent 128mV look table curve see 128mV related radiation index 0 1 Lets create function calculate index common measurement UV radiation consider range Vout shown table start point range 110mV example UV measurement 227mV 337mv considered Index 1 previous measurement 128mV index 0 9 Complete HW SW point sensor installed tested Lets develop function capture data Note defined sensor variable global keep local returning value function better practice 10 Logging Data Locally point tool capture lot data sensor simple answer create single loop function capture data regular base saving local file code open file named rpi_weather_station.csv root directory Every 30 second timestamp plus data sensor append file see 11 IoT Sending Data Cloud Service point learned capture data sensor saving local CSV file time see send data IoT platform tutorial use ThingSpeak.com ThingSpeak open source Internet Things IoT application store retrieve data thing using REST MQTT APIs ThingSpeak enables creation sensor logging application location tracking application social network thing status update First must account ThinkSpeak.com Next follow instruction create Channel taking note Channel ID Write API Key creating channel must also define info uploaded one 8 field shown 12 MQTT Protocol ThingSpeak Connection MQTT publish/subscribe architecture wa developed primarily connect bandwidth power-constrained device wireless network simple lightweight protocol run TCP/IP socket WebSockets MQTT WebSockets secured SSL publish/subscribe architecture enables message pushed client device without device needing continuously poll server MQTT broker central point communication charge dispatching message sender rightful receiver client device connects broker publish subscribe topic access information topic contains routing information broker client want send message publishes certain topic client want receive message subscribes certain topic broker delivers message matching topic appropriate client ThingSpeak ha MQTT broker URL mqtt.thingspeak.com port 1883 ThingSpeak broker support MQTT publish MQTT subscribe case use MQTT Publish MQTT Publish starting let install Eclipse Paho MQTT Python client library implement version 3.1 3.1.1 MQTT protocol Next let import paho library initiate Thingspeak channel MQTT protocol connection method simplest requires least system resource must define topic payload tPayload '' upload IoT service send everything OK get Echo data sent ThingSpeak channel page see data 13 Logging Sensor Data IoT Service ThingSpeak Channel uploaded data know line code possible upload data IoT service let create loop function automatically regular interval time similar done Logging Data Locally simple code continuously capture data logging channel would Looking ThingSpeak channel page observe data loaded continuously field channel automatically log data future analysis complete CSV file data could also downloaded site included function save_Log also log data locally CSV file complete Jupyter notebook wa used development found Rpi_Weather_Station.ipynb 14 ThingsView ThingSpeak App logged data viewed directly local saved CSV file ThingSpeak.com site via APP example ThingsView ThingView APP developed CINETICA enables visualize ThingSpeak channel easy way enter channel ID ready go public channel application respect window setting color timescale chart type number result current version support line column chart spline chart displayed line chart private channel data displayed using default setting way read private window setting API key ThingView APP download ANDROID IPHONE 15 Measuring Wind Speed Direction Weather Station tutorial part joint project developed friend Mauricio Pinto learned capture several important data related weather Air Temperature Humidity Pressure UV Another important data added Weather Station Wind Speed Direction Mauricio great job writing detailed tutorial explained construct Anemometer mostly recycled material find project 2 part tutorial Part 1 Construction device Anemometer Wind Vane Direction Part 2 sketch using Arduino IDE Esp8266 Nodemcu transmission ThingSpeak Mauricio explained tutorial anemometer device capable measuring wind speed direction Using Hall Effect sensor wa able count many rotation cup give period time intensity wind proportional speed rotation axis simple physic equation could determine linear velocity wind moment wind direction wa measured windshield neodymium magnet reed switch see anemometer installed house located around 400 meter far Weather Station wind speed direction also sent Thingspeak.com 16 Conclusion always hope project help others find way exciting world Electronics Data Science detail final code please visit GitHub depository RPi-Weather-Station project please visit blog MJRoBot.org Saludos south world See next article Thank Marcelo 54 54 54 Towards Data Science home data science Medium publication sharing concept idea code Jan Teichmann Jun 4 2019 Member-only make success story data science team Data science resounds throughout every industry ha reached mainstream medium longer explain living long call AI peak data science hype consequence company looking towards data science 12 min read 12 min read Share idea million reader Nikolay Dimolarov Jun 4 2019 state Deep Learning outside CUDAs walled garden Deep Learning researcher afficionando happen love using Macs privately professionally every year get latest greatest disappointing AMD upgrade GPU disappointing get latest greatest Vega GPU course doe 5 min read 5 min read Rohit Agrawal Jun 4 2019 Analyzing Text Classification Techniques Youtube Data Text Classification classic problem Natural Language Processing NLP aim solve refers analyzing content raw text deciding category belongs similar someone reading Robin Sharma book classifying garbage 9 min read 9 min read Chitta Ranjan Jun 4 2019 Step-by-step understanding LSTM Autoencoder layer break LSTM autoencoder network understand layer-by-layer go input output flow layer also compare LSTM Autoencoder regular LSTM network Download free book Understanding Deep Learning learn previous post LSTM Autoencoder Extreme Rare Event Classification 1 learned build LSTM autoencoder multivariate time-series data 7 min read 7 min read Jo Stichbury Jun 4 2019 Kedro New Tool Data Science new Python library production-ready data pipeline post introduce Kedro new open source tool data scientist data engineer brief description likely become standard part every professional toolchain describe use tutorial 10 min read 10 min read Marcelo Rovai Engineer MBA Master Data Science Passionate share knowledge Data Science Electronics focus Physical Computing IoT Robotics Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Ishan Getting started MicroPython Raspberry Pi Pico Black_Raven James Ng Geek Culture Face Recognition 46 line code Ioana Mircea ILLUMINATION Create Telegram Bot Using PythonMaking 300 Per Month Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': 'Towards Data Science Jun 4, 2019 Listen Save How to Capture Weather Data with your own IoT Home Station Capturing weather data, and logging them locally and on an IoT service, via MQTT protocol. Introduction When we are talking about physical variables, as temperature, pressure, etc., as a Data Scientist, usually you start working from a dataset that was created somewhere else. But have you thought about how to capture those data yourself? On this tutorial we will learn how to get data from several different sensors, sending them to an IoT service, ThingSpeak.com and to a mobile App (Thingsview), where we can log and play with data. We will also explore several different communication ways of connecting sensors to a Raspberry Pi, as: In short, all data will be captured, saved locally on a CSV file and send to an IoT service (ThingSpeak.com), via MQTT protocol, as you can see on below block diagram: To complete a real Weather Station, on the final step you will also learn how to measure wind speed and direction, following Mauricio Pinto s tutorial. Supplies: 1. Development Environment Jupyter Notebook Jupyter Notebook is a fantastic tool, or better, an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. Jupyter notebook is largely used in Data Science, for cleaning and transforming data, doing numerical simulation, statistical modeling, data visualization, machine learning, and much more! On this tutorial, we will use Jupyter Notebook to interact with Raspberry Pi GPIOs, directly reading sensors and sending data to the internet. Installation You may skip this step if already have Jupyter Notebook installed on your RPi To install Jupyter on your Raspberry (that will run with Python 3), open Terminal and enter with following commands: Now on your terminal, run the command: And thats it!!!! Amazing! very simple and easy. The Jupyter Notebook will be running as a server on: Note that your default browser will be automatically opened on the above address, running as a Home Page, as shown at above pictures. To stop the server and close the kernels (the Jupyter notebooks), you must use [Ctrl] + [C] from your keyboard. From now one, any time that you start your Pi and want to use Jupyter Notebook, just type the command: Jupyter notebook on your terminal and keep it running all the time. This is very important! If you need to use the terminal for another task as run a program, for example, open a new Terminal window. You can follow this tutorial step by step, creating your own Notebook, or download the final one from my GitHub: Rpi_Weather_Station.ipynb . 2. DHT22 Temperature & Humidity Sensor The first sensor to be installed will be the DHT22 for capturing air temperature and relative humidity data. The ADAFRUIT site provides great information about those sensors. Bellow, some information retrieved from there: Overview The low-cost DHT temperature & humidity sensors are very basic and slow but are great for hobbyists who want to do some basic data logging. The DHT sensors are made of two parts, a capacitive humidity sensor, and a thermistor. There is also a very basic chip inside that does some analog to digital conversion and spits out a digital signal with the temperature and humidity. The digital signal is fairly easy to be read using any microcontroller. DHT22 Main characteristics: Good for 0100% humidity readings with 25% accuracy Good for -40 to 125C temperature readings 0.5C accuracy No more than 0.5 Hz sampling rate (once every 2 seconds) Once usually you will use the sensor on distances less than 20m, a 4K7 ohm resistor should be connected between Data and VCC pins. The DHT22 output data pin will be connected to Raspberry GPIO 16. Check the above electrical diagram, connecting the sensor to RPi pins as below: Do not forget to Install the 4K7 ohm resistor between Vcc and Data pins Once the sensor is connected, we must also install its library on our RPi. Installing DHT Library: On your Raspberry, starting on /home, go to /Documents Create a directory to install the library and move to there: On your browser, go to Adafruit GitHub: adafruit/Adafruit_Python_DHT Python library to read the DHT series of humidity and temperature sensors on a Raspberry Pi or Beaglebone Black. github.com Download the library by clicking the download zip link to the right and unzip the archive on your Raspberry Pi recently created folder. Then go to the directory of the library (subfolder that is automatically created when you unzipped the file), and execute the command: On The Jupyter Notebook, Import Adafrut DHT Library, define the digital pin to connect the DHT with RPi and run the code to capture temperature and humidity: Run the Cell and print the results: Below, the portion of Jupyter Notebook showing the result: 3. DS18B20 Temperature Sensor Sensor Overview: We will use in this tutorial a waterproofed version of the DS18B20 sensor. It is very useful for for capturing temperature in wet conditions, for example on humid soil. The sensor is isolated and can take measurements until 125oC (Adafrut does not recommend to use it over 100oC due to its cable PVC jacket). The DS18B20 is a digital sensor what makes it good to use even over long distances! These 1-wire digital temperature sensors are fairly precise (0.5C over much of the range) and can give up to 12 bits of precision from the onboard digital-to-analog converter. They work great with the RPi using a single digital pin, and you can even connect multiple ones to the same pin, each one has a unique 64-bit ID burned in at the factory to differentiate them. The sensor works from 3.0 to 5.0V, which means that it can be powered directly from the 3.3V provided by one of the Raspberry pins (1 or 17). The sensor has 3 wires: Here, you can find the full data: DS18B20 Datasheet Sensor Installation: Follow the above diagram and make the connections: Installing the Python Library: Next, lets install the Python library that will handle the sensor: Before running the script to test the sensor, check if the 1-Wire interface is enabled in your RPi (see below print screen) Enable Interfaces Do not forget to restart your RPi, after changeing its configuration Testing the sensor: For testing the sensor a simple python code can be used: Below, the portion of Jupyter Notebook showing the result: 4. BMP180 Temperature & Pressure Sensor Sensor Overview: The BMP180 is the successor of the BMP085, a new generation of high precision digital pressure sensors for consumer applications. The ultra-low power, low voltage electronics of the BMP180 is optimized for use in mobile phones, PDAs, GPS navigation devices and outdoor equipment. With a low altitude noise of merely 0.25m at fast conversion time, the BMP180 offers superior performance. The I2C interface allows for easy system integration with a microcontroller. The BMP180 is based on piezo-resistive technology for EMC robustness, high accuracy, and linearity as well as long-term stability. The complete BMP datasheet can be found here: BMP180 Digital Pressure Sensor Sensor Installation: Follow the above diagram and make the connections: Enabling I2C Interface Go to RPi Configuration and confirm that I2C interface is enabled. If not, enable it and restart the RPi. Using the BMP180 If everything has been installed and connected okay, you are now ready to turn on your Pi and start seeing what the BMP180 is telling you about the world around you. The first thing to do is to check if the Pi sees your BMP180. Try the following in a terminal window: If the command worked, you should see something similar to the below Terminal Printscreen, showing that the BMP180 is on channel 77. Installing the BMP180 Library: Create a directory to install the library and go there: On your browser, go to Adafruit GITHub: adafruit/Adafruit_Python_BMP Python library for accessing the BMP series pressure and temperature sensors like the BMP085/BMP180 on a Raspberry Pi github.com Download the library by clicking the download zip link to the right and unzip the archive on your Raspberry Pi created folder. Then go to the created subfolder and execute the following command in the directory of the library: On Jupyter, write the following code: Check the variables read by the sensor with bellow code: Below, the portion of Jupyter Notebook showing the result Note that the sensor pressure is presented in Pa (Pascals). See next step to better understand about this unit. 5. Measuring Weather and Altitude With BMP180 Sea Level Pressure Lets take a time to understand a little bit more about what we will get, with the BMP readings. You can skip this part of the tutorial, or return later, and if you want to know more about Sensor readings, please go to this great tutorial: https://learn.sparkfun.com/tutorials/bmp180-barome... The BMP180 was designed to accurately measure atmospheric pressure. Atmospheric pressure varies with both weather and altitude. What is Atmospheric Pressure? The definition of atmospheric pressure is a force that the air around you is exerting on everything. The weight of the gasses in the atmosphere creates atmospheric pressure. A common unit of pressure is pounds per square inch or psi. We will use here the international notation, that is newtons per square meter, which are called pascals (Pa). If you took 1 cm wide column of air would weigh about 1 kg This weight, pressing down on the footprint of that column, creates the atmospheric pressure that we can measure with sensors like the BMP180. Because that cm-wide column of air weighs about 1Kg, it follows that the average sea level pressure is about 101325 pascals, or better, 1013.25 hPa (1 hPa is also known as milibar mbar). This will drop by about 4% for every 300 meters you ascend. The higher you get, the less pressure youll see, because the column to the top of the atmosphere is that much shorter and therefore weighs less. This is useful to know because by measuring the pressure and doing some math, you can determine your altitude. The air pressure at 3, 810 meters is only half of that at sea level. The BMP180 outputs absolute pressure in pascals (Pa). One pascal is a very small amount of pressure, approximately the amount that a sheet of paper will exert resting on a table. You will more often see measurements in hectopascals (1 hPa = 100 Pa). The library used here provides outputs floating-point values in hPa, which also happens to equal one millibar (mbar). Here are some conversions to other pressure units: Temperature Effects Because temperature affects the density of a gas, and density affects the mass of a gas, and mass affects the pressure (whew), atmospheric pressure will change dramatically with temperature. Pilots know this as density altitude, which makes it easier to take off on a cold day than a hot one because the air is denser and has a greater aerodynamic effect. To compensate for temperature, the BMP180 includes a rather good temperature sensor as well as a pressure sensor. To perform a pressure reading, you first take a temperature reading, then combine that with a raw pressure reading to come up with a final temperature-compensated pressure measurement. (The library makes all of this very easy.) Measuring Absolute Pressure If your application requires measuring absolute pressure, all you have to do is get a temperature reading, then perform a pressure reading (see the example sketch for details). The final pressure reading will be in hPa = mbar. If you wish, you can convert this to a different unit using the above conversion factors. Note that the absolute pressure of the atmosphere will vary with both your altitude and the current weather patterns, both of which are useful things to measure. Weather Observations The atmospheric pressure at any given location on earth (or anywhere with an atmosphere) isnt constant. The complex interaction between the earths spin, axis tilt, and many other factors result in moving areas of higher and lower pressure, which in turn cause the variations in weather we see every day. By watching for changes in pressure, you can predict short-term changes in the weather. For example, dropping pressure usually means wet weather or a storm is approaching (a low-pressure system is moving in). Rising pressure usually means that clear weather is approaching (a high-pressure system is moving through). But remember that atmospheric pressure also varies with altitude. The absolute pressure in my house, Lo Barnechea in Chile (altitude 950m) will always be lower than the absolute pressure in San Francisco for example (less than 2 meters, almost sea level). If weather stations just reported their absolute pressure, it would be difficult to directly compare pressure measurements from one location to another (and large-scale weather predictions depend on measurements from as many stations as possible). To solve this problem, weather stations always remove the effects of altitude from their reported pressure readings by mathematically adding the equivalent fixed pressure to make it appear as if the reading was taken at sea level. When you do this, a higher reading in San Francisco than Lo Barnechea will always be because of weather patterns, and not because of altitude. To do this, there is a function in the library called  sea level(P, A)  . This takes the absolute pressure (P) in hPa, and the stations current altitude (A) in meters, and removes the effects of the altitude from the pressure. You can use the output of this function to directly compare your weather readings to other stations around the world. Determining Altitude Since pressure varies with altitude, you can use a pressure sensor to measure altitude (with a few caveats). The average pressure of the atmosphere at sea level is 1013.25 hPa (or mbar). This drops off to zero as you climb towards the vacuum of space. Because the curve of this drop-off is well understood, you can compute the altitude difference between two pressure measurements (p and p0) by using a specific equation. If you use sea level pressure (1013.25 hPa) as the baseline pressure (p0), the output of the equation will be your current altitude above sea level. Theres a function in the library called  altitude(P, P0)  that lets you get the calculated altitude. The above explanation was extracted from BMP 180 Sparkfun tutorial. 6. Sea Level Pressure Measurement As we could learn on the previous step, it is important to have on hand the Sea Level pressure, that is calculated once we have the real altitude where we are measuring the absolute pressure. The below function will help us with that: On my case, I have the BMP180 installed on a real measured altitude of 957 meters, so we can have the following updated data from sensors: 7. Using the ADC (Analog to Digital Converter) On the next step, we will discuss how to get UV data from a very simple, but good analog sensor. The problem here is that the Raspberry Pi does not have analog input pins as an Arduino or NodeMCU, but we can overcome this problem by using an analog to digital (A/D) converter which will help in interfacing the analog sensors with the Raspberry Pi. The A/D converter that we will use on this project is the popular MCP3008. MCP3008 is a 10bit 8-channel ADC (Analog to Digital Converter) which use the SPI bus protocol for interfacing with Raspberry Pi. It is cheap and doesnt require any additional components with it. It gives you 8 analog inputs and it uses just four GPIOs of Raspberry Pi, plus power and ground pins. MCP3008 output will be a range from 01,023 where 0 means 0V and 1,023 means 3.3V. MCP3008 Pinout The pins numbering of the MCP3008 starts from the top/left (Pin 1: CH0), having the half circle on top as you can see in the above pinout diagram. MCP3008 ADC has a total of 16 pins out of which 8 pins are for taking the analog input. The analog input pins are from CH0-CH7 (Pins 18). On the other side (pins 916), we have different functions as follows: On this project, we will use Channel 0 (Pin 1) as the analog input. SPI The Raspberry Pi is equipped with one SPI bus that has 2 chip selects. The SPI master driver is disabled by default on Raspbian. To enable it, use raspi-config to confirm that SPI bus is enabled (the same procedure that was done before with 1-Wire). As a start, import spidev, a Linux driver to access the SPI bus: And open and configure the bus: From there, you can access any of the analog channels of our ADC. For testing write the below function: and, connect Channel 0 (MCP3008 pin 1) to 3.3V and run the function: As a result, you should see: 1023 8. The Analog UV Sensor This UV sensor generates an analog output proportional to Ultra-Violet radiation found on the light-sensing spectrum. It uses a UV photodiode (based on Gallium Nitride), which can detect the 240370nm range of light (which covers UVB and most of UVA spectrum). The signal level from the photodiode is very small, in the nano-ampere level, so the module has embedded an operational amplifier to amplify the signal to a more readable volt-level (0 to 1V). The sensor and op-amp can be powered, by connecting VCC to 3.3VDC and GND to power ground. The analog signal can be gotten from the OUT pin. Its output will be in millivolts and will be read by Analog Input (CH0) of ADC connected to our RPi. Using the same code shown in the last step, we can see the raw data generated by our UV sensor (in this case 43): Having the raw sensor data, we should convert (or map) it for values to be better handled by the code. We can do it with the function readSensorUV(). This function reads the UV sensor 3 times, taking the average and converting the measured value to mV: For example, a raw measurement of 43 is, in fact, equivalent to 128mV: If we look at the table and curve below: we will see that 128mV should be related to radiation between index 0 and 1. Lets create a function to calculate this index that is the most common measurement of UV radiation. What we will do is consider a range, having the Vout shown at the above table as the start point, with a range of 110mV. For example, UV measurements between 227mV and 337mv will be considered Index 1. So, for the previous measurement (128mV), the index should be 0. 9. The Complete HW & SW At this point, we have all the sensors installed and tested. Lets now develop a function to capture all data at once: Note that I have defined all sensors variables as global. You can keep them local, returning the values from the function (This is a better practice). 10. Logging Data Locally At this point, you have all the tools to capture a lot of data from sensors. But what to do with them? The most simple answer is to create a single loop function to capture the data at regular bases, saving them on a local file. The above code opens a file named rpi_weather_station.csv on your root directory. Every 30 seconds, the timestamp plus the data from all sensors will be append to this file, as you can see above. 11. IoT Sending Data to a Cloud Service At this point, we have learned how to capture data from sensors, saving them on a local CSV file. Now, it is time to see how to send those data to an IoT platform. On this tutorial, we will use ThingSpeak.com . ThingSpeak is an open source Internet of Things (IoT) application to store and retrieve data from things, using REST and MQTT APIs. ThingSpeak enables the creation of sensor logging applications, location tracking applications, and a social network of things with status updates. First, you must have an account at ThinkSpeak.com. Next, follow the instructions to create a Channel, taking note of its Channel ID and Write API Key . When creating the channel, you must also define what info will be uploaded to each one of the 8 fields, as shown above. 12. MQTT Protocol and ThingSpeak Connection MQTT is a publish/subscribe architecture that was developed primarily to connect bandwidth and power-constrained devices over wireless networks. It is a simple and lightweight protocol that runs over TCP/IP sockets or WebSockets. MQTT over WebSockets can be secured with SSL. The publish/subscribe architecture enables messages to be pushed to the client devices without the device needing to continuously poll the server. The MQTT broker is the central point of communication, and it is in charge of dispatching all messages between the senders and the rightful receivers. A client is any device that connects to the broker and can publish or subscribe to topics to access the information. A topic contains routing information for the broker. Each client that wants to send messages publishes them to a certain topic, and each client that wants to receive messages subscribes to a certain topic. The broker delivers all messages with the matching topic to the appropriate clients. ThingSpeak has an MQTT broker at the URL mqtt.thingspeak.com and port 1883 . The ThingSpeak broker supports both MQTT publish and MQTT subscribe. In our case, we will use the MQTT Publish. MQTT Publish For starting, lets install the Eclipse Paho MQTT Python client library , that implements versions 3.1 and 3.1.1 of the MQTT protocol. Next, lets import the paho library: and initiate the Thingspeak channel and MQTT protocol. This connection method is the simplest and requires the least system resources: Now, you must define the topic payload (tPayload\") to be upload to your IoT service: And send it: If everything is OK you will get an Echo of the data sent and on ThingSpeak channel page, you can see the data. 13. Logging Sensor Data on IoT Service ThingSpeak Channel uploaded data Now, that we know that with only a few lines of code it is possible to upload data to an IoT service, lets create a loop function to do it automatically at a regular interval of time (similar to what we have done with Logging Data Locally). A simple code to continuously capture data, logging them on our channel would be: Looking for your ThingSpeak channel page, you will observe that the data will be loaded continuously to each field. The channel will automatically log those data for future analysis. A complete CSV file of the data could be also be downloaded from the site. We have included a function (save_Log()) to also log data locally on a CSV file: The complete Jupyter notebook that was used for development can be found here: Rpi_Weather_Station.ipynb . 14. ThingsView the ThingSpeak App The logged data can be viewed directly on local saved CSV file, on ThingSpeak.com site or via an APP, for example,  ThingsView  ! ThingView is an APP developed by CINETICA , that enables you to visualize your ThingSpeak channels in an easy way. Just enter the channel ID and you are ready to go. For public channels, the application will respect your windows settings: color, timescale, chart type and the number of results. The current version supports line and column charts, the spline charts are displayed as line charts. For private channels, the data will be displayed using the default settings, as there is no way to read the private windows settings with the API key only. The ThingView APP can be download for ANDROID and IPHONE . 15. Measuring Wind Speed and Direction This Weather Station tutorial is part of a joint project developed with my friend Mauricio Pinto . Here, we learned how to capture several important data, related to weather, as Air Temperature and Humidity, Pressure and UV. Another very important data to be added to a Weather Station are Wind Speed and Direction. Mauricio did a great job, writing a very detailed tutorial, explained how to construct an Anemometer, mostly with recycled material. You can find his project on this 2 part tutorial: Part 1 Construction of the devices Anemometer and Wind Vane Direction. Part 2 The sketch using Arduino IDE for Esp8266 Nodemcu and transmission to ThingSpeak As Mauricio explained in his tutorial, the anemometer is a device capable of measuring the wind speed and its direction. Using a Hall Effect sensor he was able to count how many rotations the cups give on a period of time, being the intensity of the wind, proportional to the speed of rotation of the axis. With some simple physics equations, he could determine the linear velocity of the wind, at that moment. The wind direction was measured through a windshield with a neodymium magnet and reed switches. Here, you can see the anemometer installed in his house (that is located around 400 meters far from my Weather Station): The wind speed and direction are also sent to Thingspeak.com. 16. Conclusion As always, I hope this project can help others find their way into the exciting world of Electronics and Data Science! For details and final code, please visit my GitHub depository: RPi-Weather-Station For more projects, please visit my blog: MJRoBot.org Saludos from the south of the world! See you in my next article! Thank you, Marcelo 54 54 54 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Jan Teichmann Jun 4, 2019 Member-only How to make a success story of your data science team. Data science resounds throughout every industry and has reached the mainstream media. I no longer have to explain what I do for a living as long as I call it AI we are peak data science hype! As a consequence, more and more companies are looking towards data science 12 min read 12 min read Share your ideas with millions of readers. Nikolay Dimolarov Jun 4, 2019 On the state of Deep Learning outside of CUDAs walled garden If you are a Deep Learning researcher or afficionando and you happen to love using Macs privately or professionally, every year you get the latest and greatest disappointing AMD upgrade for your GPU. Why is it disappointing? Because you get the latest and greatest Vega GPU that of course does 5 min read 5 min read Rohit Agrawal Jun 4, 2019 Analyzing Text Classification Techniques on Youtube Data Text Classification is a classic problem that Natural Language Processing (NLP) aims to solve which refers to analyzing the contents of raw text and deciding which category it belongs to. It is similar to someone reading a Robin Sharma book and classifying it as garbage. 9 min read 9 min read Chitta Ranjan Jun 4, 2019 Step-by-step understanding LSTM Autoencoder layers Here we will break down an LSTM autoencoder network to understand them layer-by-layer. We will go over the input and output flow between the layers, and also, compare the LSTM Autoencoder with a regular LSTM network. <<Download the free book, Understanding Deep Learning, to learn more>> In my previous post, LSTM Autoencoder for Extreme Rare Event Classification [1], we learned how to build an LSTM autoencoder for a multivariate time-series data. 7 min read 7 min read Jo Stichbury Jun 4, 2019 Kedro: A New Tool For Data Science A new Python library for production-ready data pipelines In this post, I will introduce Kedro, a new open source tool for data scientists and data engineers. After a brief description of what it is and why it is likely to become a standard part of every professionals toolchain, I will describe how to use it in a tutorial 10 min read 10 min read Marcelo Rovai Engineer, MBA, Master in Data Science. Passionate to share knowledge about Data Science and Electronics with focus on Physical Computing, IoT and Robotics. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Ishan Getting started with MicroPython on Raspberry Pi Pico Black_Raven (James Ng) in Geek Culture Face Recognition in 46 lines of code Ioana Mircea in ILLUMINATION How to Create a Telegram Bot Using PythonMaking $300 Per Month Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 4686,\n",
       "  'url': 'https://towardsdatascience.com/serverless-ml-3184c9c45f93',\n",
       "  'title': 'Serverless ML',\n",
       "  'subtitle': 'Bring your models to life with AWS\\xa0Lambdas.',\n",
       "  'claps': 105,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 10,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-12-20',\n",
       "  'clap_prop': 5.1865093458428644e-05,\n",
       "  'text': \"Towards Data Science Dec 19 2019 Listen Save Serverless ML Bring model life AWS Lambdas Introduction Working day day wrangling data set engineering interesting feature testing training heap heap model leave feeling bit disconnected real-world system Well rate Recently Ive spent time software engineer whove helped understand ML model deployed cloud leveraging serverless architecture little cost result model ensemble model fit larger system via simple API call Using AWS Lambdas dont need keep costly EC2 instance running 24/7 Lambdas spin really fast required Google Cloud similar offering called Cloud Functions Ill focusing AWS offering background course running EC2 server Lambdas abstracted away dont need bother EC2s many good reason think go serverless dont want run advert read Lambda essentially small fast container preconfigured one number different runtimes include data scientist work principally Python day tutorial Ill walk deploying simple ML model AWS Lambda accompanying github repo includes code youll need along model data training data Model exercise training great model use case used historical weather data Cape Town train model predict likelihood rain tomorrow based Cape Towns history feed city world predict whether rain tomorrow based Cape Towns history Thats ok really point Point unfamiliar Lambdas disillusioned cost housing Docker Containers EC2 instance hopefully give idea cheap easy think larger complex system model code luck creative juice stimulated Requirements look little tedious worth figuring Much need Ill gloss thing like AWS execution role key management Additionally tutorial use AWS web console CLI youre unfamiliar CLI hopefully easy introduction Python 3.6 use Python 3.6 Lambda runtime environment runtime come preconfigured layer containing scipy/numpy package OS dependency present Lambda environment default layer critical u use low level math function SKLearn see Python 3.7 runtime also ha scipy/numpy layer Docker order build layer need install Python package locally environment match Lambdas remote environment Solution Docker install youre Ubuntu get going Github repo github repo available tutorial Directory structure important Ill assume youre running command project root Youll need free account order query weather data Sign http //darksky.net/dev signup login home page show secret key Store layers/03_rain_model/keys.csv LocationIQ Sign free account Location IQ http //locationiq.com geocoding Create Access Token store layers/03_rain_model/keys.csv AWS Setup Account havent already set free AWS account Install AWS CLI AWS CLI provides command line access AWS need install typically pip configure use AWS account TLDR Ubuntu version check instruction system troubleshooting Configure Well need use console configure AWS CLI Logged AWS console Back terminal detailed instruction Create S3 bucket project Back web console S3 section create empty bucket project Bucket name unique across bucket call something remember course also get done CLI Ill call mine severless-ml-tutorial US West Oregon match config 'll need adjust code match name think place could potentially incur cost S3 storage free although scale cheap building layer delete bucket save potential cost couple bucket MB inside havent incurred cost yet little confusing Still well done Ok let build first simple Lambda expand later Create Lambda Lambda created property screen open Add API Gateway trigger configure http gateway interact new Lambda Add test event add example query test Lambda respond API call sending Lambda name city let set test event Add code last add code extremely simple assigns incoming string variable print return formatted HTML string containing string Designer panel Scroll Function code panel got simple Lambda accessible via URL send payload process return payload case HTTP response Extending Lambda layer add code interesting thing let make Lambda architecture little powerful adding layer Layers simply additional resource youre including Lambda container size constraint deal dont go crazy section get back code extend Lambda github repo youll see file required build layer nested within layer directory Typically need collate content layer upload S3 compile content Lambda layer attach layer Lambda 01 Scipy/Numpy layer layer precompiled AWS includes python library also system library need use really difficult layer build thankfully AWS done heavy lifting u dont need build layer 02 Dependencies layer layer include required Python library betond installed default collate content layer local machine using docker container resembles Lambda environment namely lambci within lambci container use pip install library local directory outside container use directory build new layer terminal project root worked expected see python directory inside 02_dependencies layer directory Zip layer content need compress layer content preparation uploading S3 Upload layer S3 Using AWS CLI instead web Console sync content zip directory location S3 good idea keep web console open see youre desired effect use uploaded zip file update create Lambda layer used Python 3.6 Lambda Remember change name bucket next S3Bucket= 03 Rain model layer file layer already exist layers/03_rain_model/ directory Docker hijinks needed need put api key file called keys.csv.template rename keys.csv Despite name main purpose layer Ill include API key addition model object NB correct/secure way manage key cloud DONT IMPORTANT KEYS Zip layer content need compress layer content preparation uploading S3 Upload layer S3 upload layer content S3 Build/update layer 03_rain_model build layer Remember change bucket name Check layer successfully created Use AWS web console make sure layer exist S3 section Lambdas section Add new Layers Lambda add required layer Lambda done set-up web console open Lambda Designer pane click Layers Dont forget click Save Thats set-up youve gotten far well done Writing seems long thats left build code Build code following step expands code previous step follow along see functionality grows add block code bottom previous overwriting previous code return brace updating saving step reload API endpoint URL submit different city youd like Alternatively find full code severless-ml-lambda.py Add geolocation block load API key us LoationIQ key geocode city submitted result printed returned via http Add weather query code take DarkSky key LocationIQ GPS co-ordinate return current weather condition city submitted Add model prediction Add code block load model pushed layer 3 make prediction weather might rain tomorrow NB run memory error test Scroll Basic setting panel increase Lambdas memory 1024 Mb youre increase timeout 10 sec Add writing S3 last step writes result query S3 really little extra show simple interact AWS infrastructure Python Lambdas Conclusion Admittedly wa quite long tutorial Hopefully ha illustrated trained model incorporated larger codebase using serverless function Enjoy Easier read http //philmassie.github.io/post/20191220/serverless_ml/ 152 1 152 152 1 Towards Data Science home data science Medium publication sharing concept idea code Ryan Burn Dec 19 2019 Member-only Form Cross-Validation Use Optimize right proxy out-of-sample prediction error Cross-validation partition dataset train validates model complementary subset average prediction error way datapoint validated out-of-sample prediction averaging error out-of-sample prediction across whole dataset hope cross-validation error act proxy 8 min read 8 min read Share idea million reader Bhanu Yerra Dec 19 2019 Car Image Classification Using Features Extracted Pre-trained Neural Networks Corvette Introduction According 2018 Used Car Market Report Outlook published Cox Automotive 40 million used vehicle sold US last year represents 70 total vehicle sold good portion sale already use online resource along various stage purchasing searching 6 min read 6 min read Shengyu Huang Dec 19 2019 Simple Python Script Document SQLite Databases Autogenerate markdown file document SQLite database issue constantly harasses work relational database documentation Entity relationship diagram standard far useful column name self-explanatory schema simply becomes large handle SchemaSpy open-source tool autogenerate ER 2 min read 2 min read Dilyan Kovachev Dec 19 2019 Member-only EPL Fantasy GW17 Recap GW18 Algorithm Picks Moneyball approach Fantasy EPL team_id 2057677 first time land one Fantasy EPL Blogs might want check original EPL blog Medium Profile get familiar overall approach improvement weve made time partner crime 7 min read 7 min read Nathan Rosidi Dec 19 2019 Member-only Technical Interview 6 Red Flags Watch Interviews Today going focus one important part recruitment process one probably feared technical interview first interview screening call recruiter usually technical interview sometimes done 5 min read 5 min read Phil Massie Data Scientist Medium Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Maria Gusarova Call Amazon SageMaker model endpoint using Amazon API Gateway AWS Lambda Barr Moses Towards Data Science Whats Next Data Engineering 2023 7 Predictions Emily Webber trained 10TB Stable Diffusion SageMaker Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Towards Data Science Dec 19, 2019 Listen Save Serverless ML Bring your models to life with AWS Lambdas. Introduction Working day in and day out wrangling data sets, engineering interesting features and testing and training heaps and heaps of models can leave you feeling a bit disconnected from real-world systems. Well, it can for me at any rate. Recently Ive spent time with software engineers whove helped me understand how ML models can be deployed in the cloud by leveraging serverless architectures, at little or no cost. As a result, models or ensembles of models can fit into larger systems via simple API calls. Using AWS Lambdas, we dont need to keep costly EC2 instances up and running 24/7. Lambdas spin up really fast as required. (Google Cloud have a similar offering called Cloud Functions but here Ill be focusing on the AWS offering.) In the background of course they are all running on EC2 servers but Lambdas are so abstracted away from them that you dont need to bother about the EC2s. There are many good reasons to think go serverless but I dont want to run an advert. You can read more here A Lambda is essentially a small fast container preconfigured with one of a number of different runtimes. These include: As a data scientist I work principally with Python these days and in this tutorial Ill walk you through deploying a simple ML model in an AWS Lambda. There is an accompanying github repo here which includes all the code youll need along with the model, data and training data. The Model This is not an exercise in training a great model. As a use case I used historical weather data for Cape Town to train a model to predict the likelihood of rain tomorrow based on Cape Towns history. You can feed it any city in the world, and it will predict whether it will rain there tomorrow based on Cape Towns history. Thats ok, its not really the point. The Point If you are unfamiliar with Lambdas or disillusioned with the costs of housing your Docker Containers on EC2 instances, hopefully this will give you an idea of how cheap and easy it can be to think about larger more complex systems of models and code. With luck your creative juices will be stimulated. Requirements This looks a little tedious but is worth figuring out. Much of it you will only need to do once. Ill gloss over some things here like AWS execution roles and key management. Additionally the tutorial will use both the AWS web console and the CLI. If youre unfamiliar with the CLI, hopefully this will be an easy introduction to it. Python 3.6 We will use the Python 3.6 Lambda runtime environment as this runtime comes with a preconfigured layer containing scipy/numpy. Because these packages have OS dependencies not present in the Lambda environment by default, this layer is critical for us to use any of the low level math functions such as those in SKLearn. I see that the Python 3.7 runtime also has a scipy/numpy layer now! Docker In order to build layers we need to install Python packages locally in an environment that matches the Lambdas remote environment. Solution: Docker ( how to install ) If youre in Ubuntu, this should get you going: Github repo A github repo is available for this tutorial. Directory structure is important and Ill assume youre running commands from the project root. Youll need a free account here in order to query weather data. Sign up at https://darksky.net/dev . After signup and login, your home page should show you your secret key. Store this in layers/03_rain_model/keys.csv . LocationIQ Sign up for a free account at Location IQ https://locationiq.com for geocoding. Create an Access Token and store this in layers/03_rain_model/keys.csv . AWS Setup Account If you havent already, set up your free AWS account. Install AWS CLI AWS CLI provides command line access to AWS. You need to install it, typically with pip, and configure it for use with your AWS account. TLDR Ubuntu version check these instructions for other systems and troubleshooting : Configure Well need to use the console to configure AWS CLI. Logged in to the AWS console: Back to your terminal ( detailed instructions ): Create an S3 bucket for the project Back to the web console, S3 section, create an empty bucket for this project. Bucket names have to be unique across all buckets so call this something you can remember. You can of course also get this done with the CLI. Ill call mine severless-ml-tutorial in US West Oregon (to match our config). you'll need to adjust the code below to match this name. I think this is the only place you could potentially incur some costs. S3 storage is not free, although at this scale it is very cheap. After building your layers delete this bucket to save potential costs. I have a couple buckets with a few MB inside and I havent incurred a cost yet. Its a little confusing. Still there? well done! Ok lets build our first simple Lambda. We will expand on it later. Create a Lambda Your Lambda will be created and its properties screen will open. Add an API Gateway trigger Now we will configure an http gateway where we can interact with our new Lambda. Add a test event Now we add an example query so that we can test how our Lambda will respond to API calls. We will be sending our Lambda the name of a city so lets set up a test event to do that. Add some code Now at last we can add some code. This is extremely simple, It assigns the incoming string to a variable, prints it out and returns a formatted HTML string containing the string. Designer panel Scroll down to Function code panel You have now got a simple Lambda, accessible via a URL, you can send it a payload, and it can process and return the payload, in this case as an HTTP response. Extending the Lambda layers Before we add more code to do more interesting things, lets make our Lambda architecture a little more powerful. We can do this by adding layers. Layers are simply additional resources youre including in your Lambda container There are size constraints to deal with so dont go too crazy. After this section we can get back to the code and extend the Lambda. If you have the github repo youll see that I have the files required to build each layer nested within a layers directory. Typically we need to collate the contents of each layer, upload them to S3, compile the contents into Lambda layers and then attach the layers to our Lambda. 01 Scipy/Numpy layer This layer is precompiled by AWS and includes not only python libraries but also system libraries that we need to use. This is a really difficult layer to build for yourself but thankfully AWS have done the heavy lifting for us. We dont need to build this layer. 02 Dependencies layer This layer will include all of the required Python libraries betond those installed by default. We will collate the contents of this layer on our local machine using a docker container that resembles the Lambda environment, namely lambci. From within lambci container, we can use pip to install the libraries to a local directory, outside of the container. We then use this directory to build our new layer. In the terminal, from the project root: If that worked as expected you should now see a python directory inside your 02_dependencies layer directory. Zip the layer contents We need to compress the layer contents in preparation for uploading to S3. Upload the layer to S3 Using the AWS CLI instead of the web Console, sync the contents of your zips directory with a location in S3. Its a good idea to keep the web console open to see that what youre doing is having the desired effect. Now we can use the uploaded zip file to update or create a Lambda layer that can be used in any Python 3.6 Lambda. Remember to change the name of your bucket next to S3Bucket=. 03 Rain model layer The files for this layer already exist on the layers/03_rain_model/ directory, so no Docker hijinks needed here. You need to put your api keys into the file called keys.csv.template and rename it to keys.csv . Despite the name and main purpose of the layer, Ill include the API keys in addition to the model object. NB This is NOT the correct/secure way to manage keys in the cloud. DONT DO THIS WITH IMPORTANT KEYS! Zip the layer contents We need to compress the layer contents in preparation for uploading to S3. Upload the layer to S3 As before, upload the layer contents to S3 Build/update layer 03_rain_model and build the layer. Remember to change your bucket name here. Check that layers have been successfully created Use the AWS web console to make sure layers exist. S3 section Lambdas section Add the new Layers to your Lambda We will add all the required layers to our Lambda and then were done with the set-up. In the web console, open your Lambda In the Designer pane, click Layers. Dont forget to click Save! Thats it for the set-up. If youve gotten this far well done! Writing this out, it seems very long. All thats left now is to build out the code! Build out the code Each of the following steps expands on the code from the previous step. To follow along and see how the functionality grows, add each block of code to the bottom of the previous, overwriting the previous codes return braces. After updating and saving at each step, reload the API endpoint URL and submit a different city if youd like. Alternatively, find the full code in severless-ml-lambda.py. Add geolocation This block loads up the API keys and uses the LoationIQ key to geocode the city you submitted. These results are printed and returned via http. Add weather query This code takes the DarkSky key and the LocationIQ GPS co-ordinates and returns the current weather conditions for the city you submitted. Add model prediction Add this code block to load up the model you pushed in layer 3 and make a prediction about weather it might rain tomorrow. NB you will run into memory errors when you test this. Scroll down to Basic settings panel and increase your Lambdas memory to 1024 Mb. While youre there increase your timeout to 10 sec. Add writing to S3 This last step writes the results of each query out to S3. Its really just a little extra to show how simple it can be to interact with the AWS infrastructure from Python and Lambdas. Conclusion Admittedly that was quite a long tutorial. Hopefully it has illustrated how a trained model can be incorporated into a larger codebase by using serverless functions. Enjoy. Easier to read at  https://philmassie.github.io/post/20191220/serverless_ml/ 152 1 152 152 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ryan Burn Dec 19, 2019 Member-only What Form of Cross-Validation Should You Use? Optimize the right proxy for out-of-sample prediction error Cross-validation partitions a dataset, trains and validates models on complementary subsets, and averages prediction errors in such a way that each datapoint is validated once as an out-of-sample prediction. By averaging errors of out-of-sample predictions across the whole dataset, we hope that the cross-validation error acts as a proxy for 8 min read 8 min read Share your ideas with millions of readers. Bhanu Yerra Dec 19, 2019 Car Image Classification Using Features Extracted from Pre-trained Neural Networks Is that a Corvette? Introduction According to the 2018 Used Car Market Report & Outlook published by Cox Automotive, 40 million used vehicles were sold in the US last year. This represents about 70% of the total vehicles sold. A good portion of these sales already use online resources along various stages of purchasing: searching 6 min read 6 min read Shengyu Huang Dec 19, 2019 A Simple Python Script to Document SQLite Databases Autogenerate a markdown file to document the SQLite databases This issue constantly harasses me when I work with relational databases: documentation. Entity relationship diagram is the standard, but it is far from useful when the column names are not self-explanatory or the schema simply becomes too large to handle. SchemaSpy is an open-source tool that can autogenerate an ER 2 min read 2 min read Dilyan Kovachev Dec 19, 2019 Member-only EPL Fantasy GW17 Recap and GW18 Algorithm Picks Our Moneyball approach to the Fantasy EPL (team_id: 2057677) If this is the first time you land on one of my Fantasy EPL Blogs, you might want to check out some of my original EPL blogs in my Medium Profile to get familiar with our overall approach and the improvements weve made over time. My partner in crime for 7 min read 7 min read Nathan Rosidi Dec 19, 2019 Member-only What Not To Do During a Technical Interview 6 Red Flags to Watch Out For In Interviews Today were going to focus on one of the most important parts of the recruitment process and the one that is probably the most feared: the technical interview. Your first interview (other than the screening call with the recruiter) is usually the technical interview, which is sometimes done over the 5 min read 5 min read Phil Massie Data Scientist More from Medium Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Maria Gusarova Call an Amazon SageMaker model endpoint using Amazon API Gateway and AWS Lambda. Barr Moses in Towards Data Science Whats Next for Data Engineering in 2023? 7 Predictions Emily Webber How I trained 10TB for Stable Diffusion on SageMaker Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 4956,\n",
       "  'url': 'https://towardsdatascience.com/machine-learning-with-big-data-86bcb39f2f0b',\n",
       "  'title': 'Handling Big Datasets for Machine\\xa0Learning',\n",
       "  'subtitle': '-',\n",
       "  'claps': 533,\n",
       "  'responses': 4.0,\n",
       "  'reading_time': 12,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-03-11',\n",
       "  'clap_prop': 0.0002632770934604045,\n",
       "  'text': \"Towards Data Science Mar 11 2019 Listen Save Handling Big Datasets Machine Learning 2.5 quintillion byte data created day 90 data world wa generated past two year prevalence data increase need learn deal large data Big Data like teenage sex everyone talk nobody really know everyone think everyone else everyone claim Dan Ariely Imagine downloading dataset full Tweets ever written data 2.3 billion people Facebook even data every webpage exists Internet analyze dataset isolated problem hit largest tech company current age datasets already becoming larger computer handle regularly work satellite data easily Terabyte range large even fit hard drive computer let alone process reasonable amount time eye-opening statistic regarding big data Storing data one thing processing developing machine learning algorithm work article discus easily create scalable parallelized machine learning platform cloud process large-scale data used research commercial non-commercial purpose done minimal cost compared developing supercomputer develop robust high-performance parallel cluster cloud also used local machine performance enhancement delve following topic post based content following GitHub repository found command required setting machine learning platform cloud found markdown file based tutorial Institute Applied Computational Science Harvard University Environment Setup Dockers Containers read one part post let part people set machine learning environment typically install everything directly operating system Oftentimes fine try download something like PyTorch TensorFlow Keras everything explodes spend hour Stack Overflow trying get thing work implore work like sake problem typically occurs dependency co-dependencies certain package specific version package Often need half package work would make sense start clean slate install version dependency required task hand ultimately save time stress using Anaconda easy efficient separate isolated container run without causing problem container called Conda environment Conda package manager Python think environment different computer know existence create new environment start blank slate need install package great part actually download package twice pointer created point specific version package want install already downloaded computer may seem pointless unless dependency issue computer promise worth knowing Another useful feature install package like one line using YAML .yml file file tell environment package want install dependency required downloaded need write file exported one line code environment already required package pretty neat right required command shown Gist example YAML file look like conda env export environment.yml command run another good reason separating thing environment like want get reproducible result data analysis widely depend version different package also operating system working creating environment.yml file contain dependency easier someone reproduce result created Conda environment essentially isolated rest system However additional thing want work environment Python package case use Docker create container application containerize whole thing using Docker case component encapsulated Docker container admit concept behind Docker container bit confusing Building docker image trivial task Fortunately however Jupyter folk created repo2docker repo2docker take GitHub repository automatically make docker image uploads docker image repository done using one line code running code code pop terminal look like following Simply copy paste URL browser access docker image get going read using repo2docker Another really useful thing use binder Binder build repo2docker provide service provide GitHub repository give working JupyterHub publish project demo etc GitHub repository associated tutorial run binder clicking link ReadMe section read using Binder Parallelization Dask Kubernetes ha taken u quite get parallelization part tutorial previous step necessary get Lets dive using Dask Kubernetes Dask ha two part associated 1 Dynamic task scheduling optimized computation like Airflow 2 Big Data collection like parallel Numpy array Pandas dataframes list Dask ha around couple year gradually growing momentum due popularity Python machine learning application Dask allows scaling 1000 core cluster Python application processed much faster regular laptop would refer anyone interested working Dask GitHub repository Tom Augspurger one main creator Dask found talked Dask doe Kubernetes come run Dask laptop allows u distribute code multiple core doe help u run code multiple system time run locally Ideally want run cloud provisioned cluster wed like cluster self-repairing wed like code respond failure expand onto machine need need cluster manager Kubernetes cluster manager think like operating system cluster provides service discovery scaling load-balancing self-healing Kubernetes think application stateless movable one machine another enable better resource utilization controlling master node cluster operating system run worker node perform bulk work node computer associated cluster loses connection break master node assign work someone new like bos would stopped working master worker node consist several piece software allow perform task get pretty complicated quickly give high-level overview Master Node Worker Node great isnt particularly helpful unless 100 computer disposal make use power Kubernetes Dask afford u Enter cloud Dask Cloud Deployment want run Dask speed machine learning code Python Kubernetes recommended cluster manager done local machine using Minikube 3 major cloud provider Microsoft Azure Google Compute Cloud Amazon Web Services probably familiar cloud computing since pretty much everywhere day common company computing infrastructure cloud since reduces capital expenditure computing equipment move operational expenditure requires le maintenance also significantly reduces running cost Unless working classified information strict regulatory requirement probably get away running thing cloud Using cloud allows leverage collective performance several machine perform task example performing hyperparameter optimization neural network need rerun model 10,000 time get best parameter selection fairly common problem would nonsensical run one computer take 2 week run model 100 computer likely finish task hour hope made good case make use cloud aware get quite expensive use powerful machine especially turn using set environment cloud must following First run following See http //docs.dask.org/en/latest/setup/kubernetes-helm.html detail Deep Learning Cloud several useful tool available building deep learning algorithm Kubernetes Dask example TensorFlow put cloud using tf.distributed kubeflow parallelism trivially used grid optimization since different model run worker node Examples found GitHub repository use research environmental scientist consulting work machine learning consultant regularly use either JupyterHub Kubernetes cluster Dask Harvards supercomputer Odyssey run infrastructure AWS real prejudice Azure Google Cloud wa taught use AWS first Example Cloud Deployment AWS section run setup Kubernetes Cluster running Dask AWS first thing need set account AWS able run following line code unless already account First download AWS command line interface configure private key supplied AWS install Amazons Elastic Container Service EKS Kubernetes using brew command Creating Kubernetes cluster ludicrously simple need run one command specify cluster name number node region case Boston choose us-east-1 run command must configure cluster following command set Helm Dask cluster Wait two minute complete install Dask Kubernetes command detail shell need command like exact pod name different cluster clone GitHub repository watch Dask go Kaggle Rossman Competition recommend got Dask cloud deployment running try running rossman_kaggle.ipynb example code Kaggle Rossman competition allowed user use data wanted try predict pharmacy sale Europe competition wa run 2015 step notebook run set coding environment multilayer perceptron order apply parallel cluster perform hyperparameter optimization step code split function run sklearn pipeline recommended way run large machine learning program several example repository run parallel cluster play Also feel free clone repository tinker much like learn learn Dask check following link dask/dask-tutorial Dask tutorial Contribute dask/dask-tutorial development creating account GitHub github.com Dask Dask 1.1.4 documentation Internally Dask encodes algorithm simple format involving Python dicts tuples function graph format docs.dask.org learn Dask Kubernetes dask/dask-kubernetes Native Kubernetes integration dask Contribute dask/dask-kubernetes development creating account github.com Dask Kubernetes Dask Kubernetes 0.7.0 documentation Currently designed run pod Kubernetes cluster ha permission launch pod kubernetes.dask.org learn Helm Kubernetes Helm Dask 1.1.4 documentation case might consider setting Kubernetes cluster one common cloud provider docs.dask.org struggling work step multiple walkthroughs go specific detail Adding Dask Jupyter Kubernetes Cluster post 're going set Dask Jupyter Kubernetes cluster running AWS n't ramhiser.com Setup private dask cluster Kubernetes alongside JupyterHub Jetstream Andrea Zonca 's blog post leverage software made available Pangeo community allow user Jupyterhub zonca.github.io setting cluster Google Cloud sadly could find one Microsoft Azure check link ogrisel/docker-distributed Experimental docker-compose setup bootstrap distributed docker-swarm cluster ogrisel/docker-distributed github.com hammerlab/dask-distributed-on-kubernetes Deploy dask-distributed google container engine using kubernetes hammerlab/dask-distributed-on-kubernetes github.com working parallel cluster perform machine learning big data big compute task Thanks reading Newsletter update new blog post extra content sign newsletter Newsletter Subscription Enrich academic journey joining community scientist researcher industry professional obtain mailchi.mp 585 3 585 585 3 Get email whenever Matthew Stewart publishes Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Aidan Morrison Mar 11 2019 data-scientists need direction hate direction theyre given Everyone know beneath hype data-scientists struggle job Hang around beer kind data-science meetup youll hear end rant Data-scientists frequently hired pet plaything aspirational executive little need service 11 min read 11 min read Share idea million reader Mohamed Chrif Haidara Mar 11 2019 Dimensionality Reduction toolbox python article derived work friend Herv Trinh recent year volume data ha exploded 80 ha led emergence many model Machine Learning since easier train model important dataset However 5 min read 5 min read David Comfort Mar 11 2019 Member-only Build Reporting Dashboard using Dash Plotly blog post provide step-by-step tutorial build reporting dashboard using Dash Python framework building analytical web application Rather go basic building Dash app provide detailed guide building multi-page dashboard data 24 min read 24 min read Daniel Shenfeld Mar 11 2019 Member-only Build AI Moat Forging link better model better product Jerry Chen introduced concept system intelligence explain product powered AI data help company build deep moat protect profit market share competitor Successful company build virtuous cycle data data generate 7 min read 7 min read Yitong Ren Mar 11 2019 Member-only Get Started Using CNN+LSTM Forecasting method consider data low granularity recurring local pattern Predicting trend ha ancient discipline yet never fallen popularity Whether stock price financial market power energy consumption sale projection corporate planning series time-based data point representation world thinking 4 min read 4 min read Matthew Stewart Environmental Data Science PhD Harvard ML consultant Critical Future Blogger TDS Content Creator EdX http //mpstewart.net Medium Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Naga Sanjay Continuous Training ML model Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"Towards Data Science Mar 11, 2019 Listen Save Handling Big Datasets for Machine Learning More than 2.5 quintillion bytes of data are created each day. 90% of the data in the world was generated in the past two years. The prevalence of data will only increase, so we need to learn how to deal with such large data. Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it. Dan Ariely Imagine downloading a dataset full of all the Tweets ever written, or the data of all the 2.3 billion people on Facebook, or even, the data for every webpage that exists on the Internet. How do you analyze such a dataset? This is not an isolated problem that only hits the largest tech companies. In the current age, datasets are already becoming larger than most computers can handle. I regularly work with satellite data and this can easily be in the Terabyte range too large to even fit on the hard drive of my computer, let alone to process it in a reasonable amount of time. Here are some eye-opening statistics regarding big data: Storing this data is one thing, but what about processing it and developing machine learning algorithms to work with it? In this article, we will discuss how to easily create a scalable and parallelized machine learning platform on the cloud to process large-scale data. This can be used for research, commercial, or non-commercial purposes and can be done with minimal cost compared to developing your own supercomputer. To develop a very robust and high-performance parallel cluster on the cloud (this can also be used on a local machine for performance enhancement) we will delve into the following topics: This post will be based on the contents of the following GitHub repository, that can be found here . All of the commands required for setting up the machine learning platform on the cloud can be found in the markdown file here . This is based on a tutorial by the Institute for Applied Computational Science at Harvard University. Environment Setup Dockers and Containers If you only read one part of this post, let it be this part. When people set up their machine learning environment, typically they install everything on the directly on their operating system. Oftentimes, this is fine, and then you try to download something like PyTorch, TensorFlow, or Keras and everything explodes and you spend hours on Stack Overflow trying to get things to work. I implore you not to work like this, for your own sake. This problem typically occurs from dependencies or co-dependencies of certain packages on specific versions of other packages. Often, you do not need half of these packages for your work. It would make more sense to start from a clean slate and only install the versions and dependencies that are required for the task at hand. This will ultimately save you time and stress. If you are using Anaconda, it is very easy and efficient to separate these into isolated containers such that they can all run without causing problems. These containers are called Conda environments. Conda is a package manager for Python You can think of these environments as different computers that do not know about the existence of each other. When I create a new environment, I start with a blank slate and need to install packages again. The great part about this is that you do not actually download the packages twice when doing this, a pointer is created which points to the specific version of the package you want to install that is already downloaded on your computer. This may seem pointless unless you have had dependency issues before on your computer, but I can promise you it is worth knowing about this. Another useful feature is that you can install all the packages you like in just one line by using a YAML (.yml) file. This is a file that tells the environment what packages you want to install and what dependencies are required to be downloaded. You do not need to write this file, it can be exported with one line of code from an environment where you already have all the required packages pretty neat right? All of the required commands are shown in the Gist below. Here is an example of what the YAML file looks like when the conda env export > environment.yml command is run. There is another good reason for separating things into environments like this. If I want to get reproducible results for data analysis that I am doing, it can widely depend on the versions of different packages and also the operating system that you are working on. By creating environment.yml files that contain all of the dependencies, it is easier for someone to reproduce your results. So what did we do when we created our Conda environment? We essentially isolated it from the rest of our system. However, what if we have additional things that we want to work with the environments that are not just Python packages. In this case, we use Docker to create containers. If your application: you can containerize the whole thing  using Docker . In this case, all these components will be encapsulated in a Docker container : I admit the concept behind Docker and containers is a bit confusing. Building a docker image is not a trivial task. Fortunately, however, the Jupyter folks created repo2docker for this. repo2docker takes a GitHub repository and automatically makes a docker image and uploads it to the docker image repository for you. This can be done using one line of code. After running the above code, you should have some code pop up in the terminal that looks like the following: Simply copy and paste the URL in your browser and you then have access to your docker image and can get going! You can read more about using repo2docker here . Another really useful thing to use is binder. Binder builds on repo2docker to provide a service where you provide a GitHub repository, and it gives you a working JupyterHub where you can publish your project, demo, etc. The GitHub repository associated with this tutorial can be run on binder by clicking on the link in the ReadMe section. You can read more about using Binder here . Parallelization with Dask and Kubernetes It has taken us quite a while to get to the parallelization part of the tutorial, but the previous steps were necessary to get here. Lets now dive into using Dask and Kubernetes. Dask has two parts associated with it: [1] Dynamic task scheduling optimized for computation like Airflow. [2] Big Data collections like parallel (Numpy) arrays, (Pandas) dataframes, and lists. Dask has only been around for a couple of years but is gradually growing momentum due to the popularity of Python for machine learning applications. Dask allows scaling up (1000 core cluster) of Python applications so that they can be processed much faster than on a regular laptop. I would refer anyone who is interested in working with Dask to the GitHub repository by Tom Augspurger (one of the main creators of Dask), which can be found here . So we have talked about Dask, where does Kubernetes come in here? If we run Dask on our laptop, it allows us to distribute our code to multiple cores at once, but it does not help us run the code on multiple systems at the same time. We have run it locally. Ideally, we want to run on a cloud provisioned cluster, and wed like this cluster to be self-repairing that is, wed like our code to respond to failures and expand onto more machines if we need them. We need a cluster manager. Kubernetes is a cluster manager. We can think of it like being an operating system for the cluster. It provides service discovery, scaling, load-balancing, and is self-healing. Kubernetes think of applications as stateless, and movable from one machine to another to enable better resource utilization. There is a controlling master node on which the cluster operating system runs, and worker nodes which perform the bulk of the work. If a node (computer associated with the cluster) loses connection or breaks, the master node will assign the work to someone new, just like your boss would if you stopped working. The master and worker nodes consist of several pieces of software which allow it to perform its task. It gets pretty complicated so I will quickly give a high-level overview. Master Node : Worker Node : Doing all of this is great, but it isnt particularly helpful unless we have 100 computers at our disposal to make use of the power that Kubernetes and Dask afford us. Enter the cloud. Dask Cloud Deployment If you want to run Dask to speed up your machine learning code in Python, Kubernetes is the recommended cluster manager. This can be done on your local machine using Minikube or on any of the 3 major cloud providers, Microsoft Azure, Google Compute Cloud, or Amazon Web Services. You are probably familiar with cloud computing since it is pretty much everywhere these days. It is now very common for companies to have all of their computing infrastructure on the cloud, since this reduces their capital expenditure on computing equipment and moves it to operational expenditure, requires less maintenance and also significantly reduces the running cost. Unless you are working with classified information or have very strict regulatory requirements, you can probably get away with running things on the cloud. Using the cloud allows you to leverage the collective performance of several machines to perform the same task. For example, if you are performing hyperparameter optimization on a neural network and it will need to rerun the model 10,000 times to get the best parameter selection (a fairly common problem) then it would be nonsensical to run it on one computer if it will take 2 weeks. If you can run this same model on 100 computers you will likely finish the task in a few hours. I hope I have made a good case for why you should make use of the cloud, but be aware that it can get quite expensive if you use very powerful machines (especially if you do not turn them off after using them!) To set up the environment on the cloud, you must do the following: First run the following and then See https://docs.dask.org/en/latest/setup/kubernetes-helm.html for all the details. Deep Learning on the Cloud There are several useful tools which are available for building deep learning algorithms with Kubernetes and Dask. For example, TensorFlow can be put on the cloud using tf.distributed of kubeflow . The parallelism can be trivially used during grid optimization since different models can be run on each worker node. Examples can be found on the GitHub repository here . What do you use? For my own research (I am an environmental scientist) and in my consulting work (machine learning consultant) I regularly use either JupyterHub, a Kubernetes cluster with Dask on Harvards supercomputer Odyssey, or I will run the same infrastructure on AWS (no real prejudice against Azure or the Google Cloud, I was just taught how to use AWS first). Example Cloud Deployment on AWS In this section, I will run through the setup of a Kubernetes Cluster running Dask on AWS. The first thing you need to do is set up an account on AWS, you will not be able to run the following lines of code unless you already have an account. First, we download the AWS command line interface and configure it with our private key supplied by AWS. We then install Amazons Elastic Container Service (EKS) for Kubernetes using the brew commands. Creating a Kubernetes cluster is now ludicrously simple, we only need to run one command, but you should specify the cluster name, the number of nodes, and the region you are in (in this case I am in Boston so I choose us-east-1 ) and then run the command. Now we must configure the cluster with the following commands: Now we set up Helm and Dask on the cluster Wait two minutes for this to complete and then we can install Dask. A few more Kubernetes commands. For more details and a shell, you will need a command like this. Your exact pod names will be different. Once you are in the cluster, you can clone the GitHub repository and watch Dask go! Kaggle Rossman Competition I recommend that once you have got the Dask cloud deployment up and running you try running the rossman_kaggle.ipynb . This is example code from the Kaggle Rossman competition, which allowed users to use any data they wanted to try and predict pharmacy sales in Europe. The competition was run in 2015. The steps in this notebook run you through how to set up your coding environment for a multilayer perceptron in order to apply it to a parallel cluster and then perform hyperparameter optimization. All of the steps in this code are split into functions which are then run in an sklearn pipeline (this is the recommended way to run large machine learning programs). There are several other examples on the repository that you can run on the parallel cluster and play with. Also, feel free to clone the repository and tinker with it as much as you like. Where can I learn more? To learn more about Dask, check out the following links: dask/dask-tutorial Dask tutorial. Contribute to dask/dask-tutorial development by creating an account on GitHub. github.com Dask - Dask 1.1.4 documentation Internally, Dask encodes algorithms in a simple format involving Python dicts, tuples, and functions. This graph format docs.dask.org To learn more about Dask with Kubernetes: dask/dask-kubernetes Native Kubernetes integration for dask. Contribute to dask/dask-kubernetes development by creating an account on github.com Dask Kubernetes - Dask Kubernetes 0.7.0 documentation Currently, it is designed to be run from a pod on a Kubernetes cluster that has permissions to launch other pods kubernetes.dask.org To learn more about Helm: Kubernetes and Helm - Dask 1.1.4 documentation If this is not the case, then you might consider setting up a Kubernetes cluster on one of the common cloud providers docs.dask.org If you are struggling to work through any of the above steps, there are multiple other walkthroughs that go through the specifics in more detail: Adding Dask and Jupyter to a Kubernetes Cluster In this post, we're going to set up Dask and Jupyter on a Kubernetes cluster running on AWS. If you don't have a ramhiser.com Setup private dask clusters in Kubernetes alongside JupyterHub on Jetstream | Andrea Zonca's blog In this post we will leverage software made available by the Pangeo community to allow each user of a Jupyterhub zonca.github.io For setting up the cluster on the Google Cloud (sadly could not find one for Microsoft Azure) check these links out: ogrisel/docker-distributed Experimental docker-compose setup to bootstrap distributed on a docker-swarm cluster. - ogrisel/docker-distributed github.com hammerlab/dask-distributed-on-kubernetes Deploy dask-distributed on google container engine using kubernetes - hammerlab/dask-distributed-on-kubernetes github.com Now you should have a working parallel cluster on which to perform machine learning on big data or for big compute tasks! Thanks for reading! Newsletter For updates on new blog posts and extra content, sign up for my newsletter. Newsletter Subscription Enrich your academic journey by joining a community of scientists, researchers, and industry professionals to obtain mailchi.mp 585 3 585 585 3 Get an email whenever Matthew Stewart publishes. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Aidan Morrison Mar 11, 2019 Why data-scientists need more direction, but hate the directions theyre given Everyone knows that beneath the hype, data-scientists struggle in their jobs. Hang around for a few beers after some kind of data-science meetup, and youll hear no end to the rants: Data-scientists are frequently hired to be pets and playthings for aspirational executives with little need for their services, or 11 min read 11 min read Share your ideas with millions of readers. Mohamed Chrif Haidara Mar 11, 2019 Dimensionality Reduction toolbox in python This article is derived from my work with my friend Herv Trinh. In recent years, the volume of data has exploded by more than 80%. This has led to the emergence of many models of Machine Learning, since it is easier to train these models with an important dataset. However 5 min read 5 min read David Comfort Mar 11, 2019 Member-only How to Build a Reporting Dashboard using Dash and Plotly In this blog post, I will provide a step-by-step tutorial on how to build a reporting dashboard using Dash, a Python framework for building analytical web applications. Rather than go over the basics of building a Dash app, I provide a detailed guide to building a multi-page dashboard with data 24 min read 24 min read Daniel Shenfeld Mar 11, 2019 Member-only How to Build an AI Moat Forging the link between better models and better products Jerry Chen introduced the concept of systems of intelligence to explain how products powered by AI and data can help companies build deep moats to protect their profits and market share from competitors: Successful companies here can build a virtuous cycle of data because the more data you generate and 7 min read 7 min read Yitong Ren Mar 11, 2019 Member-only Get Started with Using CNN+LSTM for Forecasting A method you should consider when you have data of low granularity with recurring local pattern Predicting the trend has been an ancient discipline yet its never fallen from popularity. Whether it is stock price in financial market, power or energy consumption, or sales projection for corporate planning, a series of time-based data points can be the representation of how the world is thinking in any 4 min read 4 min read Matthew Stewart Environmental + Data Science PhD @Harvard | ML consultant @Critical Future | Blogger @TDS | Content Creator @EdX. https://mpstewart.net More from Medium Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Naga Sanjay Continuous Training of ML models. Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 767,\n",
       "  'url': 'https://medium.com/swlh/flutter-custom-tab-indicator-for-tabbar-d72bbc6c9d0c',\n",
       "  'title': 'Flutter: Custom tab indicator for\\xa0TabBar',\n",
       "  'subtitle': '-',\n",
       "  'claps': 215,\n",
       "  'responses': 3.0,\n",
       "  'reading_time': 3,\n",
       "  'publication': 'The Startup',\n",
       "  'date': '2019-12-30',\n",
       "  'clap_prop': 0.00010619995327202056,\n",
       "  'text': \"Startup Dec 30 2019 Member-only Listen Save Flutter Custom tab indicator TabBar Welcome tutorial create custom tab indicator TabBar Flutter connect Instagram Lets start seeing end goal- Customization Options implement Customize tab bar three way 3 Implementing custom decoration TabBar indicator setting indicator property TabBar SPOILER Third option best one dont re-invent wheel 2nd option limited Lets Start 1 Add TabBar talk implementing TabBar Flutter need help implementing TabBar flutter check documentation flutter team Work tab Working tab common pattern apps follow Material Designguidelines Flutter includes convenient flutter.dev TabBar class API doc TabBar class material library Dart programming language api.flutter.dev starter code- 2 Implementing custom tab indicator Lets see done code snippet- 4 Overriding createBoxPainter onChanged method return BoxPainter paint decoration case _painter parameter omitted chance painter change 3 Implementing custom box painter create custom painter type BoxPainter Lets see added code snippet- 4 Overriding paint method draw circle based radius paint position 4 Updating indicator let update TabBar indicator Lets check output- Check project gist Thank staying till end flutter spinner blogs- Custom Spinner Tooltip Flutter Welcome tutorial building Custom Spinner tool tip medium.com Flutter Custom Spinner Thumb Welcome tutorial Custom Spinner Thumb medium.com Flutter Custom Spinner Thumb -Part 2 Welcome tutorial building custom spinner thumb medium.com posting flutter stay tuned Gursheesh Singh Chandigarh Chandigarh India Professional Profile LinkedIn View Gursheesh Singh 's professional profile LinkedIn LinkedIn world 's largest business network helping www.linkedin.com Gursheesh Singh Welcome back Instagram Sign check friend family interest capturing sharing www.instagram.com Gursheesh singh latest Tweets Gursheesh singh GursheeshChawla Business Solution developer Flutter developer Android twitter.com 486 5 486 486 5 Sign Top 5 Stories Startup Get smarter building thing Join 176,621+ others receive Startup 's top 5 story tool idea book delivered straight inbox week Take look Emails sent shadow_kelvin777 ymail.com Startup Get smarter building thing Follow join Startups +8 million monthly reader +760K follower Nick Wolny Dec 30 2019 Member-only Learned 3-Step System Organizing Creativity Top Music Conservatory Apply framework career skill want master college education get shaft day Student loan created financial crisis underemployment working job unrelated field study snarl young people year graduation rise hustle culture made u grind day night spawn burnout epidemic 5 min read 5 min read Share idea million reader Josh White Dec 30 2019 Member-only Best Writers Better Everyone Else give possession story reader Theres saying work art thats appropriate idea Heres saying paraphrased take writer reader write story finish art writing isnt complete clear comprehensive reader two may ask Heres 5 min read 5 min read Andrew Scott Dec 30 2019 Member-only Welcome Python Meet Dunders quick introduction several __magic__ method python Dunder Dunder method name common pronunciation python built-in method name start end double underscore Since Dunder easier say double underscore name stuck method also sometimes referred Special Methods Magic Methods However thing magic 4 min read 4 min read R. Shawn McBride Dec 30 2019 Member-only Keep Right Leaders Ideas Private Businesses Leadership valuable explored valuable leadership recent article keeping right leader tricky Good leader case option go elsewhere since survival enterprise could tied keeping right people 7 min read 7 min read Ajanthan Mani Dec 30 2019 Member-only Agile Personal Growth reading lot lot article personal life improvement year gave different useful way personal growth practicing way article suggested quite frankly gave day 4 min read 4 min read TheBoringDeveloper Flutter enthusiast Medium Maneesha Erandi Flutter Bottom Navigation Bar Animation Jackie Moraa Lottie Animations Flutter Part 1 Nabin Dhakal CodeX Routes Navigation Flutter Kaushiki Kumari Flutter Popular Packages Help Status Writers Blog Careers Privacy Terms Text speech\",\n",
       "  'full_text': \"The Startup Dec 30, 2019 Member-only Listen Save Flutter: Custom tab indicator for TabBar Welcome to this tutorial to create a custom tab indicator for TabBar in Flutter. You can connect with me on Instagram Lets start by seeing our end goal- Customization Options We can implement Customize tab bar in three ways: 3. Implementing a custom decoration for TabBar indicator and then setting it to indicator property of TabBar SPOILER Third option is the best one as we dont have to re-invent the wheel and the 2nd option is limited. Lets Start 1. Add TabBar: Here we will not talk about implementing TabBar in Flutter. If you need help implementing TabBar in flutter, check out this documentation from flutter team: Work with tabs Working with tabs is a common pattern in apps that follow the Material Designguidelines. Flutter includes a convenient flutter.dev TabBar class API docs for the TabBar class from the material library, for the Dart programming language. api.flutter.dev Here is the starter code- 2. Implementing custom tab indicator: Lets see what we have done in the above code snippet- 4. Overriding the  createBoxPainter([onChanged])  method which returns a BoxPainter that will paint this decoration (In our case _painter ). The parameter can be omitted if there is no chance that the painter will change. 3. Implementing custom box painter: Now, we will create our custom painter which will be of type BoxPainter. Lets see what we have added in the above code snippet- 4. Overriding the paint() method which draws the circle based on radius, paint and position 4. Updating the indicator: Now lets update the TabBar indicator Lets check the output- Check the project here or gist here . Thank you for staying till the end More flutter spinner blogs- Custom Spinner Tooltip Flutter Welcome to this tutorial for building a Custom Spinner tool tip medium.com Flutter Custom Spinner Thumb Welcome to this tutorial for Custom Spinner Thumb. medium.com Flutter Custom Spinner Thumb -Part 2 Welcome to this tutorial for building a custom spinner thumb medium.com I will be posting more about flutter, so stay tuned :) Gursheesh Singh - Chandigarh, Chandigarh, India | Professional Profile | LinkedIn View Gursheesh Singh's professional profile on LinkedIn. LinkedIn is the world's largest business network, helping www.linkedin.com Gursheesh Singh Welcome back to Instagram. Sign in to check out what your friends, family & interests have been capturing & sharing www.instagram.com Gursheesh singh The latest Tweets from Gursheesh singh (@GursheeshChawla). Business Solution developer | Flutter developer | Android twitter.com 486 5 486 486 5 Sign up for Top 5 Stories By The Startup Get smarter at building your thing. Join 176,621+ others who receive The Startup's top 5 stories, tools, ideas, books delivered straight into your inbox, once a week. Take a look. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from The Startup Get smarter at building your thing. Follow to join The Startups +8 million monthly readers & +760K followers. Nick Wolny Dec 30, 2019 Member-only I Learned This 3-Step System For Organizing Creativity From A Top Music Conservatory Apply its framework to any career skill you want to master A college education gets the shaft these days. Student loans have created a financial crisis, underemployment (working a job unrelated to your field of study) snarls young people for years after graduation, and the rise of hustle culture made us grind day and night, only to spawn a burnout epidemic 5 min read 5 min read Share your ideas with millions of readers. Josh White Dec 30, 2019 Member-only What the Best Writers Do Better Than Everyone Else They give possession of their stories to the reader. Theres a saying about works of art thats appropriate for this idea. Heres the saying, paraphrased: It takes a writer and a reader to write a story to a finish. The art of writing isnt complete until its clear and comprehensive for the reader. Why two? You may ask. Heres 5 min read 5 min read Andrew Scott Dec 30, 2019 Member-only Welcome to Python, Meet the Dunders A quick introduction to several of the __magic__ methods in python What is a Dunder? Dunder method name is the common pronunciation for pythons built-in method names that start and end with double underscores. Since Dunder is easier to say than double underscore, the name stuck. These methods are also sometimes referred to as Special Methods or Magic Methods. However, the only thing magic or 4 min read 4 min read R. Shawn McBride Dec 30, 2019 Member-only How Can We Keep The Right Leaders? Ideas For Private Businesses Leadership can be valuable. I explored just how valuable leadership can be in a recent article. But keeping the right leaders can be tricky. Good leaders, in most cases, have options to go elsewhere. And since the very survival of our enterprises could be tied to keeping the right people 7 min read 7 min read Ajanthan Mani Dec 30, 2019 Member-only Agile for Personal Growth I have been reading lots and lots of articles about personal life improvements over this year, and each of them gave me different useful ways for personal growth. I have been practicing some of the ways the articles suggested, and quite frankly, I gave up after a few days. 4 min read 4 min read TheBoringDeveloper Flutter enthusiast More from Medium Maneesha Erandi Flutter Bottom Navigation Bar Animation Jackie Moraa Lottie Animations with Flutter | Part 1 Nabin Dhakal in CodeX Routes and Navigation in Flutter Kaushiki Kumari Flutter Popular Packages Help Status Writers Blog Careers Privacy Terms About Text to speech\"},\n",
       " {'id': 5453,\n",
       "  'url': 'https://towardsdatascience.com/forget-apis-do-python-scraping-using-beautiful-soup-import-data-file-from-the-web-part-2-27af5d666246',\n",
       "  'title': 'Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part\\xa02',\n",
       "  'subtitle': '-',\n",
       "  'claps': 459,\n",
       "  'responses': 2.0,\n",
       "  'reading_time': 8,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-01',\n",
       "  'clap_prop': 0.00022672455140398807,\n",
       "  'text': 'Towards Data Science Mar 31 2019 Member-only Listen Save Forget APIs Python Scraping Using Beautiful Soup Import Data File web Part 2 APIs always every website Beautiful Soup going stay forever collect kind data website oday Data play critical role every industry data coming internet company invest million dollar one tech gain user without profited invested product return Internet vast contains information one topic nerdy professor need importance extracting information web becoming increasingly loud clear time adding information facebook twitter LinkedIn giving feedback Yelp information treated Data kind data internet come many different way example comment restaurant feedback Yelp Twitter discussion Reddit user discussion Stock price etc collect data organize analyze going talk tutorial several way extracting importing data Internet use APIs retrieve information major website Thats everybody using day import data internet primary site like Twitter Twitch Instagram Facebook provides APIs access website dataset data available structured form website doesnt provide API think dont want u use user data dont offer lack knowledge topic going import data web without using APIs processed please look part 1 series everything connected like dot Something dont know data File Starter Data Science Import data File new Data Science field must working hard learn Data Science concept fast towardsdatascience.com Beautiful Soup Beautiful Soup best Library scrap data particular website Internet comfortable work also par extract structured data HTML Beautiful Soup automatically transforms incoming text Unicode outgoing version UTF-8 dont remember encoding except document doesnt define encoding Beautiful Soup cant catch one mention original encoding Rules run program please use Jupyter python environment run program Instead running whole program taking precaution program doesnt break website Please check website term condition start pulling data sure read statement legal use data Basic-Getting Familiar HTML HTML code play essential role extracting data website processed let u jump basic HTML tag got tiny bit knowledge HTML tag move ahead next level basic syntax HTML webpage Every tag serf block inside webpage 1 DOCTYPE html HTML document must start type declaration 2 HTML document contained html /html 3 meta script declaration HTML document head /head 4 visible portion HTML document body /body tag 5 Title heading defined h1 h6 tag 6 Paragraphs defined p tag useful tag include hyperlink table table tr table row td table column Lets Check HTML page List Asian country area Wikipedia need additional citation verification .improve article adding citation reliable source Unsourced en.wikipedia.org Let u take Wikipedia page scrapping google chrome go page first right-click open browser inspector inspect webpage result see table inside wiki table sortable inspect find table information fantastic yeah going amazing see beautiful soup Lets Start DIY project know data located going start scrapping data process need install import library face trouble installation use sudo front every line Requests meant used human communicate language suggests dont manually join query string URLs form-encode POST data Requests enable send HTTP/1.1 request utilizing Python combine content like header form data multipart file parameter simple Python library also enables obtain response data Python way BS4 BeautifulSoup Beautiful Soup Python library extracting data HTML XML file work favourite parser produce natural way operating examining transforming parse tree usually save programmer hour day work begin studying source code given web page building BeautifulSoup soup object BeautifulSoup function.Now need use Beautiful Soap function help u parse apply HTML fetched Wikipedia page going use Beautiful Soup parse HTML data collected URL variable assign different variable store data Beautiful Soup format called Soup get concept structure underlying HTML web page use Beautiful Soups prettify function check get prettify function visit link look Wikipedia page Asian country see little bit information country area wikipedia table already setup-which make work easy Lets look prettify HTML Beginning HTML table tag class identifier wikitable sortable remember class future use go program see table made look row begin finish tr /tr tag first row header ha th tag data row underneath every club ha td tag Using td tag going tell Python secure data go ahead let work Beautiful Soup function demonstrate capture deliver data u HTML website title function Beautiful Soup return HTML tag heading content within use information start preparing attack HTML know data remains within HTML table firstly give Beautiful Soup retrieve occurrence table tag within page add array called all_tables table class wikitable sortable link country name title table class wikitable sortable connection country name title going extract link within used find_all URL extract title name country create list Countries extract name country link add list country convert list country Pandas DataFrame work python interested scrapping data high volume consider using Scrapy powerful python scraping framework also try integrate code public APIs performance data retrieval significantly higher scraping webpage example take look Facebook Graph API help get hidden data shown Facebook webpage Consider using database backend like MySQL collect information get big carry u end Beautiful Soup tutorial Confidently provides quite get working examine scraping next project Weve introduced request fetch URL HTML data Beautiful Soup parse HTML Pandas convert data data frame proper presentation find tutorial notebook question please feel free ask next tutorial going talk APIs Feel Free contact LinkedIn References 1. http //www.gregreda.com/2013/03/03/web-scraping-101-with-python/ 2. http //www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/ 3. http //github.com/stewync/Web-Scraping-Wiki-tables-using-BeautifulSoup-and-Python/blob/master/Scraping 2BWiki 2Btable 2Busing 2BPython 2Band 2BBeautifulSoup.ipynb 4. http //en.wikipedia.org/wiki/List_of_Asian_countries_by_area 5. http //www.crummy.com/software/BeautifulSoup/ 450 3 450 450 3 Towards Data Science home data science Medium publication sharing concept idea code Andrew Donaldson Mar 31 2019 Member-only Dont Fear Robots Impressive tech important remember Boston Dynamics video trailer movie isnt yet made Every time one Boston Dynamics video hit met mix amazement prediction doom hand new robot overlord impressive also well choreographed staged Impressive tech important 4 min read 4 min read Share idea million reader Jasmine Vasandani Mar 31 2019 Member-only 10 Steps Teaching Data Science Well resource data science instructor increase demand data scientist ha parallel growth training program within company educational institution teach people data science addition focusing meeting job demand need resource instructor facilitating data science classroom Heres 4 min read 4 min read Karan Bhanot Mar 31 2019 Member-only income 50K/yr Machine Learning tell Machine learning breaking ground numerous field including Finance could use Machine Learning model identify income individual found right dataset called Census Income Dataset used information dataset predict someone would earn income 7 min read 7 min read Ane Berasategi Mar 31 2019 Member-only Semantic search brief post semantics search semantic search READ ORIGINAL POST BLOG Semantics branch linguistics studying meaning word symbolic use also including multiple meaning One morning shot elephant pajama got pajama Ill never know Groucho Marx sentence semantically ambiguous clear author 5 min read 5 min read Sik-Ho Tsang Mar 31 2019 Review DPN Dual Path Networks Image Classification Better ResNet DenseNet PolyNet ResNeXt Winner ILSVRC 2017 Object Localization Challenge story DPN Dual Path Network briefly reviewed work National University Singapore Beijing Institute Technology National University Defense Technology Qihoo 360 AI Institute ResNet enables feature re-usage DenseNet enables new feature exploration DPN pick advantage ResNet 6 min read 6 min read Sahil Dhankhad Data Science Machine Learning Researcher LakeheadU Learning something everyday Data www.linkedin.com/in/sahil-dhankhad-303350135 Medium Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Antonio Blago ILLUMINATION Become Data Analyst 2023 Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Mar 31, 2019 Member-only Listen Save Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2 APIs are not always there for you for every website, but Beautiful Soup is going to stay with you forever to collect any kind of data from any website. T oday, Data play a critical role in every industry. And most of this data is coming from the internet. Most company, invest millions of dollars in one tech to gain users without having profited from that invested product return. The Internet is so vast that it contains more information about one topic than your nerdy professor. And the need & importance of extracting information from the web is becoming increasingly loud and clear. Most time, when we are adding any information in your facebook, twitter, LinkedIn and giving feedback on Yelp, this information is treated as Data. And this kind of data from the internet comes in many different ways example comment, restaurant feedback on Yelp, Twitter discussion, Reddit user discussion and Stock price etc. You can collect all this data, organize it and analyze it. That what we are going to talk about in this tutorial. There are several ways of extracting or importing data from the Internet. You can use APIs, to retrieve information from any major website. Thats what everybody is using these days to import data from the internet all primary site like Twitter, Twitch, Instagram, Facebook provides APIs to access their website dataset. And all this data available in a structured form. But most of the website doesnt provide an API. I think they dont want us to use their users data or they dont offer it because of lack of knowledge. So, In this topic, we are going to import data from the web without using any APIs. But before we processed, please have a look at our part 1 of this series because everything connected like dots. Something You dont know about data File if you just a Starter in Data Science, Import data File If you are new into Data Science field, then you must be working so hard to learn Data Science concept so fast. Now towardsdatascience.com What is Beautiful Soup Beautiful Soup is the best Library to scrap the data from a particular website or the Internet. And it is most comfortable to work on also. It parses and extracts structured data from HTML . Beautiful Soup automatically transforms incoming texts to Unicode and outgoing versions to UTF-8. You dont have to remember about encodings except the document doesnt define an encoding, and Beautiful Soup cant catch one. Then you have to mention the original encoding. Rules: To run your program, please use Jupyter python environment to run your program. Instead of running the whole program at once. We are just taking precaution, so your program doesnt break the website. Please check out the website term and conditions before you start pulling out data from there. Be sure you read the statement about the legal use of data. Basic-Getting Familiar with HTML HTML code plays an essential role in extracting data from the website. So, before we processed, let us jump to the basic of the HTML tags. If you have got a tiny bit of knowledge of HTML tags, you can move ahead to the next level. This is the basic syntax of an HTML webpage. Every <tag> serves a block inside the webpage: 1. <!DOCTYPE html> : HTML documents must start with a type declaration. 2. The HTML document is contained between <html> and </html> . 3. The meta and script declaration of the HTML document is between <head> and </head> . 4. The visible portion of the HTML document is between <body> and </body> tags. 5. Title headings are defined with the <h1> through <h6> tags. 6. Paragraphs are defined with the <p> tag. Other useful tags include <a> for hyperlinks, <table> for tables, <tr> for table rows, and <td> for table columns. Lets Check your HTML page List of Asian countries by area - Wikipedia needs additional citations for verification .improve this article by adding citations to reliable sources. Unsourced en.wikipedia.org Let us take a Wikipedia page to do the scrapping. If you have google chrome, then go to the page, first right-click on it and open your browser inspector to inspect the webpage. From the result you can see the table is inside the wiki table sortable and if you inspect it more you can find all of your table information there, its fantastic yeah!!!. Its going to be more amazing to see what you can do with beautiful soup. Lets Start Your DIY project Now we know about our data and where it is located. So, we are going to start scrapping our data. Before we process, You need to install or import some libraries. if you face any trouble in your installation, you can use sudo in front of every line. Requests  It is meant to be used by humans to communicate with the language. This suggests you dont have to manually join query strings to URLs, or form-encode your POST data. Requests will enable you to send HTTP/1.1 requests utilizing Python. In it, you can combine content like headers, form data, multipart files, and parameters by through simple Python libraries. It also enables you to obtain the response data of Python in the same way. BS4 BeautifulSoup  Beautiful Soup is a Python library for extracting data out of HTML and XML files. It works with your favourite parser to produce natural ways of operating, examining and transforming the parse tree. It usually saves programmers hours or days of work. We begin by studying the source code for a given web page and building a BeautifulSoup (soup)object with the BeautifulSoup function.Now, we need to use Beautiful Soap function which will help us parse and apply with the HTML we fetched from our Wikipedia page: Then we are going to use Beautiful Soup to parse the HTML data that we have collected in our URL variable, and we assign a different variable to store the data in Beautiful Soup format called Soup. To get a concept of the structure of the underlying HTML in our web page, use Beautiful Soups prettify function and check it. This is what we get from the prettify() function : If you visit this   link  and have a look to our Wikipedia page for the Asian countries, we can see there is little bit more information about the country areas. The wikipedia table already setup-which make our work more easy. Lets have a look for it in our prettify HTML: And there it is,Beginning with an HTML <table> tag with a class identifier of wikitable sortable. We will remember this class for future use. If you go down in your program, you will see how the table is made up, and you will have a look at the rows begin and finish with <tr> and </tr> tags. The first row of headers has <th> tags while the data rows underneath for every club has <td> tags. Using the <td> tags that we are going to tell Python to secure our data from. Before we go ahead, lets work out some Beautiful Soup functions to demonstrate how it captures and can deliver data to us from the HTML website. If we do the title function, Beautiful Soup will return the HTML tags for the heading and the content within them. We can use this information to start preparing our attack on the HTML . We know the data remains within an HTML table so firstly, we give Beautiful Soup off to retrieve all occurrences of the <table> tag within the page and add them to an array called all_tables. Under table class wikitable sortable we have links with country name as the title. Under table class wikitable sortable we have connections with country name as the title. Now, we are going to extract all the links within <a> , we used find_all() . From the URL, we have to extract the title which is the name of countries. To do that, we have to create a list Countries so that we can extract the name of countries from the link and add it to the list countries. Now, we have to convert the list countries into Pandas DataFrame to work in python. If you are interested in scrapping the data in high volume, you should consider using Scrapy , a powerful python scraping framework and also try to integrate your code with some publics APIs. The performance of data retrieval is significantly higher than scraping webpages. For example, take a look at Facebook Graph API , which can help you get hidden data which is not shown on Facebook webpages. Consider using a database backend like MySQL to collect your information when it gets too big. And that carries us to the end of our Beautiful Soup tutorial. Confidently, it provides you quite to get working on to examine some scraping out for your next project. Weve introduced request to fetch the URL and HTML data, Beautiful Soup to parse the HTML and Pandas to convert the data into a data frame for proper presentation. You can find this tutorial notebook here. If you have question, please feel free to ask. In next tutorial we are going to talk about the APIs. Feel Free to contact me on LinkedIn . References: 1. http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/ . 2. http://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/ . 3. https://github.com/stewync/Web-Scraping-Wiki-tables-using-BeautifulSoup-and-Python/blob/master/Scraping%2BWiki%2Btable%2Busing%2BPython%2Band%2BBeautifulSoup.ipynb 4. https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area 5. https://www.crummy.com/software/BeautifulSoup/ 450 3 450 450 3 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Andrew Donaldson Mar 31, 2019 Member-only Dont Fear the Robots Impressive as the tech here is, it is important to remember that the Boston Dynamics videos are trailers for a movie that isnt yet made. Every time one of these Boston Dynamics videos hit, they are met with a mix of amazement and predictions of doom at the hands of our new robot overlords. It is impressive. It is also very well choreographed and staged. Impressive as the tech here is, it is important 4 min read 4 min read Share your ideas with millions of readers. Jasmine Vasandani Mar 31, 2019 Member-only 10 Steps to Teaching Data Science Well A resource for data science instructors. With the increase in demand for data scientists, there has been a parallel growth in training programs within companies and educational institutions that teach people data science. In addition to focusing on meeting the job demand, there need to be more resources for instructors facilitating data science classrooms. Heres a 4 min read 4 min read Karan Bhanot Mar 31, 2019 Member-only Will your income be more than $50K/yr? Machine Learning can tell Machine learning is breaking grounds in numerous fields including Finance. What if we could use Machine Learning models to identify incomes of individuals? I found just the right dataset for this, called Census Income Dataset. I used the information in the dataset to predict if someone would earn an income 7 min read 7 min read Ane Berasategi Mar 31, 2019 Member-only Semantic search A brief post on semantics, search, and semantic search READ THE ORIGINAL POST IN MY BLOG Semantics is a branch of linguistics studying the meanings of words, their symbolic use, also including their multiple meanings. One morning I shot an elephant in my pajamas. How he got into my pajamas Ill never know. Groucho Marx This sentence is semantically ambiguous: its not clear if the author 5 min read 5 min read Sik-Ho Tsang Mar 31, 2019 Review: DPN Dual Path Networks (Image Classification) Better Than ResNet, DenseNet, PolyNet, ResNeXt, Winner of ILSVRC 2017 Object Localization Challenge In this story, DPN (Dual Path Network) is briefly reviewed. This is a work by National University of Singapore, Beijing Institute of Technology, National University of Defense Technology, and Qihoo 360 AI Institute. ResNet enables feature re-usage while DenseNet enables new features exploration. DPN picks the advantages from both ResNet 6 min read 6 min read Sahil Dhankhad Data Science and Machine Learning Researcher. LakeheadU. Learning something everyday about Data. ( www.linkedin.com/in/sahil-dhankhad-303350135 ) More from Medium Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Antonio Blago in ILLUMINATION Why You Should (not) Become a Data Analyst in 2023! Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Help Status Writers Blog Careers Privacy Terms About Text to speech'},\n",
       " {'id': 711,\n",
       "  'url': 'https://towardsdatascience.com/4-steps-to-break-into-data-science-in-2020-4750418c726c',\n",
       "  'title': '4 Steps to Break Into Data Science in\\xa02020',\n",
       "  'subtitle': 'Go from 0 to Junior Data Scientist in 1\\xa0year',\n",
       "  'claps': 398,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 5,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-12-30',\n",
       "  'clap_prop': 0.0001965934018709962,\n",
       "  'text': 'Towards Data Science Dec 30 2019 Member-only Listen Save 4 Steps Break Data Science 2020 2020 almost mean time year take piece paper make list goal want accomplish next year Nothing wrong probably know easy make exhaustive list near-impossible time-consuming goal make feel overwhelmed likely motivated much youre planning enroll data science next year Id say youve made great decision field widely accepted job everywhere salary great even management slowly figuring data science needed start let slightly demotivate yes necessary one year isnt enough learn entire field Dont get wrong 1 year enough land first job chance wont go 0 data science team lead year manage please share story comment section said let explore skill youll need learn enough get started 1 Brushing Math skill Youve likely heard harsh math prerequisite data science amount math youll need know vary much depending job role general answer much math need get started would say le think reasoning follow tempting dive deep every somewhat related field like calculus linear algebra probability statistic need know time stop Dont get wrong time world guest become expert above-mentioned field otherwise make sure youre wasting time break field junior-level data scientist youll need know math intuitive level Youll need know certain situation thats intuition part come wouldnt spend much time solving complex math task hand youre good intuitive level know code thats enough Theres plenty time get deeper math get job need learn everything beforehand dont advanced math degree already wouldnt suggest spend 23 month brushing math skill 2 Programming Yes coding skill essential data science get job industry coding skill lacking likely know need wont know also likely youll suffer SOCPS Stack Overflow Copy Paste Syndrome possibly even without reading question answer Theres nothing wrong looking elegant solution online know write basic solution youve never written line code start small read book Python R role data science get complete picture dive deeper syntax Dont worry memorizing everything make sure know look get stuck youve already read book finished course programming know syntax dont know approach problem spend time learning algorithm data structure Also go common coding interview question get creative juice flowing piss Youre satisfied programming skill Thats awesome spend time analysis library like Numpy Pandas much time youll spend coding also vary much wont complete beginner one need library knowledge Id say 34 month enough complete beginner around 1 month youre learning analysis library 3 Databases highly likely data youre analyzing come sort database Thats typical work environment get different book online course wont get nicely formatted CSV file often youll need domain knowledge someone domain knowledge also good amount SQL knowledge youll analysis programming language like Python R dont spend much time learning SQL analytic function PLSQL/T-SQL advanced stuff SQL work case rely mostly joining couple table perform analysis much time youll spend depends way youll use prior knowledge starter dont spend month 4 let learn Data Science youve followed step dont prior knowledge probably August September 2020 lot time ha passed prerequisite needed land first job Well precise Youre looking job data science weve covering prerequisite far would suggest next 2 month get comfortable basic data analysis visualization library like havent already probably since learning prerequisite without clear indication need boring Dont go tutorial download datasets web perform solid analysis go online see others done dataset see improve 2 month period also get acquainted machine learning algorithm like Maybe wont use practice provide base learning advanced algorithm like XGBoost Neural network later Like analysis library make sure follow tutorial tutorial good quality work feel like try implementing algorithm scratch Numpy thats mandatory Whats next couple month left 2020 create GitHub account upload 35 finest analysis/ML piece potential employer see Also make nice looking resume cover letter really feel like document learning journey form online blog Online presence help career development dont publish nonsense content daily basis trust judgment thats start sending resume company want work nothing else sincerely hope 2020 year Go crush Loved article Become Medium member continue learning without limit Ill receive portion membership fee use following link extra cost Join Medium referral link Dario Radei Medium member portion membership fee go writer read get full access every story medium.com 413 1 413 413 1 Subscribe Never miss thing Data Science News pocket Emails sent shadow_kelvin777 ymail.com Towards Data Science home data science Medium publication sharing concept idea code Boris Shabash Dec 30 2019 Member-only Reasoning Probability Pyro Model Good Enough core probabilistic programming designed answer question uncertainty popular example online discus use probabilistic programming aid neural network making capable dealing novel example havent trained However tutorial going 5 min read 5 min read Share idea million reader John Daniel Dec 30 2019 Support Team SVM Topic Overview Understanding Support Vector Machine 2020 Support Vector Machine SVM powerful versatile Machine Learning model SVM used classification regression problem outlier detection one popular model Machine Learning Data Scientist interested Machine Learning toolbox 6 min read 6 min read Rebecca Weng Dec 30 2019 Member-only Cryptography Crash Course Intimidated Conceptual overview suggestion reading/watching last post talked importance mindful handle data ha access protect information law changing isnt covered academia vs. industry post decided give quick crash 7 min read 7 min read Jackson Gilkey Dec 30 2019 Member-only Creating Graphs Python using Networkx intro building first Graph Python youre interested Graph Theory analysis Python wondering get started blog Well start presenting key concept implementing Python using handy Networkx Package Graph Theory Terminology Graph G V E data structure 4 min read 4 min read Laura Uzcategui Dec 30 2019 Journey Machine Learning AI Lets walk together journey transition Software Engineering Machine Learning first blog Machine Learning ML journey Ive quite interested thing data beginning career Software Engineer interested following baby step journey ML fundamental 5 min read 5 min read Dario Radei Data Scientist Tech Writer betterdatascience.com Medium Zach Quinn Pipeline Data Engineering Resource 3 Data Science Projects Got 12 Interviews 1 Got Trouble Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Anna Wu Google Data Scientist Interview Questions Step-by-Step Solutions Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Dec 30, 2019 Member-only Listen Save 4 Steps to Break Into Data Science in 2020 2020 is almost here, which means its that time of the year when you take a piece of paper and make a list of goals you want to accomplish in the next year. Nothing wrong with that, but you probably know that its very easy to make an exhaustive list of near-impossible, time-consuming goals that will only make you feel overwhelmed, and very likely not motivated because theres so much to do. If youre planning to enroll in data science in the next year, Id say youve made a great decision. The field is widely accepted, there are jobs everywhere, salaries are great, and even the management is slowly figuring out why data science is needed. But before we start, let me to slightly demotivate you ( yes, its necessary ) one year isnt enough to learn the entire field. Dont get me wrong, 1 year is enough for you to land your first job, but the chances are you wont go from 0 to data science team lead in a year ( if you manage to do so please share your story in the comment section ). With that being said, lets explore all the skills youll need and how to learn just enough of them to get you started. 1. Brushing up the Math skills Youve most likely heard of harsh math prerequisites of data science. The amount of math youll need to know will vary much depending on the job role, but as a general answer to how much math you will need to get started, I would say: less than you think . The reasoning will follow. Its tempting to dive deep into every somewhat related field like calculus, linear algebra, probability, or statistics but you need to know when its time to stop . Dont get me wrong, if you have all of the time in the world be my guest, become an expert in the above-mentioned fields, but otherwise, make sure youre not wasting your time. To break into the field as a junior-level data scientist youll need to know math, but more on the intuitive level . Youll need to know what to do in certain situations thats where the intuition part comes in but I wouldnt spend much time solving complex math tasks by hand. If youre good on the intuitive level and know how to code thats enough. Theres plenty of time to get deeper into math after you get a job no need to learn everything beforehand . If you dont have an advanced math degree already I wouldnt suggest you spend more than 23 months brushing up the math skills. 2. What about Programming? Yes, coding skills are essential to data science. If you get a job in the industry and your coding skills are lacking, most likely you will know what you need to do, but you wont know how to do it . Its also likely youll suffer from SOCPS ( Stack Overflow Copy Paste Syndrome ), possibly even without reading the questions and answers. Theres nothing wrong with looking for more elegant solutions online, but you should know how to write a basic solution by yourself. If youve never written a line of code before, start small, read a book on Python or R and their role in data science ( to get a complete picture ). Then dive deeper into syntax. Dont worry about memorizing everything, just make sure to know where to look when you get stuck. If youve already read a book or finished a course on programming and you know the syntax, but dont know how to approach the problem, spend some time learning algorithms and data structures . Also go through most common coding interview questions, as those will get your creative juices flowing ( or piss you off ). Youre satisfied with your programming skills? Thats awesome! Now spend some time with analysis libraries like Numpy and Pandas. How much time youll spend on coding will also vary much. It wont be the same for complete beginners, or the ones who just need library knowledge. Id say 34 months will be enough for complete beginners, and around 1 month if youre learning the analysis libraries only. 3. Databases? Its highly likely that data youre analyzing will come from some sort of a database. Thats where a typical work environment gets different from books or online courses you wont get a nicely formatted CSV file . More often than not youll need a domain knowledge ( or someone with domain knowledge ), and also a good amount of SQL knowledge. If youll be doing analysis in programming languages like Python or R, then dont spend too much time learning SQL analytic functions, PLSQL/T-SQL and all of that more advanced stuff. Your SQL work, in this case, will rely mostly on joining a couple of tables on which you can perform the analysis. How much time youll spend here depends on the way youll use them and on the prior knowledge, but for starters dont spend more than a month here. 4. Now lets learn some Data Science If youve followed each step from above and you dont have some prior knowledge than its probably August or September of 2020. A lot of time has passed by, but you have all the prerequisites needed to land your first job. Well, not all to be precise. Youre looking for a job in data science, and weve only been covering prerequisites so far. I would suggest that for the next 2 months you get comfortable with the basic data analysis and visualization libraries, like: That is if you havent already ( you probably have since learning the prerequisites without a clear indication of why you need them can be boring ). Dont just go over tutorials, download some datasets from the web and perform a solid analysis . Then go online and see what others have done on the same dataset to see where you can improve. In the same 2 month period you should also get acquainted with some of machine learning algorithms, like: Maybe you wont use some of them in practice, but they will provide you with a base for learning more advanced algorithms like XGBoost and Neural networks later on. Like with the analysis libraries, make sure to not follow tutorial by tutorial, but to do good quality work by yourself. If you feel like it, try implementing the algorithms from scratch in Numpy but thats not mandatory. Whats next? With only a couple of months left in 2020, create a GitHub account a there upload 35 of your finest analysis/ML pieces for potential employers to see. Also, make a nice looking resume and cover letter. If you really feel like it, document your learning journey in the form of an online blog. Online presence can only help you in career development, that is if you dont publish nonsense content on a daily basis but I trust your judgment. And thats it, start sending your resume to the companies you want to work for theres nothing else you can do . I sincerely hope 2020 will be your year. Go crush it. Loved the article? Become a  Medium member  to continue learning without limits. Ill receive a portion of your membership fee if you use the following link, with no extra cost to you. Join Medium with my referral link - Dario Radei As a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story medium.com 413 1 413 413 1 Subscribe - Never miss a thing. Data Science News in your pocket. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Boris Shabash Dec 30, 2019 Member-only Reasoning With Probability and Pyro Is My Model Good Enough? At its core, probabilistic programming is designed to answer questions about uncertainty. Some very popular examples online discuss the use of probabilistic programming to aid with neural networks and making them more capable of dealing with novel examples they havent been trained on. However, in this tutorial I am going 5 min read 5 min read Share your ideas with millions of readers. John Daniel Dec 30, 2019 The Support Team SVM Topic Overview: Understanding Support Vector Machine 2020 A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model. SVM can be used for classification or regression problem and outlier detection. It is one of the most popular models in Machine Learning that any Data Scientist interested in Machine Learning should have in their toolbox 6 min read 6 min read Rebecca Weng Dec 30, 2019 Member-only Cryptography Crash Course for the Intimidated Conceptual overview and suggestions for reading/watching In my last post, I talked about the importance of being mindful when you handle data. Who has access? How can you protect the information? How are some laws changing? What is and isnt covered in academia vs. industry. For this post, I decided to give myself a quick crash 7 min read 7 min read Jackson Gilkey Dec 30, 2019 Member-only Creating Graphs in Python using Networkx An intro to building your first Graph in Python If youre interested in doing Graph Theory analysis in Python and wondering where to get started then this is the blog for you. Well start by presenting a few key concepts and then implementing them in Python using the handy Networkx Package. Some Graph Theory Terminology A Graph G(V, E) is a data structure 4 min read 4 min read Laura Uzcategui Dec 30, 2019 On the Journey to Machine Learning / AI. Lets walk together through the journey to transition between Software Engineering and Machine Learning This is my first blog on Machine Learning (ML) and my journey through it. Ive been quite interested in all things data from the very beginning of my career as a Software Engineer. If you are interested in following baby steps on the journey to ML, its fundamentals and how 5 min read 5 min read Dario Radei Data Scientist & Tech Writer | betterdatascience.com More from Medium Zach Quinn in Pipeline: A Data Engineering Resource 3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble. Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Anna Wu Google Data Scientist Interview Questions (Step-by-Step Solutions!) Help Status Writers Blog Careers Privacy Terms About Text to speech'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "11e4e93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02819796, 0.02673027, 0.02332259, 0.02420798, 0.02496844,\n",
       "       0.02492454, 0.02224086, 0.0233674 , 0.02589667, 0.021964  ,\n",
       "       0.0218557 , 0.02548448, 0.02110733, 0.02528046, 0.02149188,\n",
       "       0.01932632, 0.02058601, 0.01934524, 0.0217936 , 0.01888271,\n",
       "       0.02080023, 0.01809744, 0.01987521, 0.01808742, 0.02241193,\n",
       "       0.02432855, 0.01692712, 0.01763574, 0.02291824, 0.01779547,\n",
       "       0.01719704, 0.01629414, 0.01883609, 0.01671677, 0.02291868,\n",
       "       0.01645533, 0.01547616, 0.01763472, 0.0155591 , 0.01530193,\n",
       "       0.01886233, 0.01554922, 0.01591717, 0.01503476, 0.01498796,\n",
       "       0.01556717, 0.01909892, 0.01644144, 0.01843815, 0.01786114])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_prop = 0.90\n",
    "(score_prop * scores) + ((1 - score_prop) * claps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d33c8712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.81833209e-03, 2.78535416e-02, 1.65302918e-04, 1.02487809e-02,\n",
       "       2.15720307e-02, 2.20679395e-02, 8.26514588e-05, 1.19018101e-02,\n",
       "       4.09951236e-02, 6.77741962e-03, 8.84370609e-03, 4.85164063e-02,\n",
       "       5.86825357e-03, 5.38887511e-02, 2.40515745e-02, 3.30605835e-03,\n",
       "       1.91751384e-02, 9.91817506e-03, 3.76064138e-02, 8.84370609e-03,\n",
       "       2.80188445e-02, 1.57037772e-03, 2.00016530e-02, 6.61211670e-03,\n",
       "       4.99214811e-02, 7.47169188e-02, 9.91817506e-04, 8.09984296e-03,\n",
       "       6.09967766e-02, 1.45466567e-02, 1.08273411e-02, 2.56219522e-03,\n",
       "       2.95065708e-02, 9.67022068e-03, 7.49648731e-02, 1.03314323e-02,\n",
       "       1.40507480e-03, 2.50433920e-02, 4.62848169e-03, 2.31424085e-03,\n",
       "       3.81023225e-02, 5.53764774e-03, 1.04967353e-02, 1.98363501e-03,\n",
       "       2.31424085e-03, 8.67840317e-03, 4.40532275e-02, 1.77700636e-02,\n",
       "       3.79370196e-02, 3.28952806e-02])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be0e0612",
   "metadata": {},
   "outputs": [],
   "source": [
    "claps = np.array([resul[\"_source\"][\"claps\"] for resul in res[\"hits\"][\"hits\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "821f0ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22, 337,   2, 124, 261, 267,   1, 144, 496,  82, 107, 587,  71,\n",
       "       652, 291,  40, 232, 120, 455, 107, 339,  19, 242,  80, 604, 904,\n",
       "        12,  98, 738, 176, 131,  31, 357, 117, 907, 125,  17, 303,  56,\n",
       "        28, 461,  67, 127,  24,  28, 105, 533, 215, 459, 398])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claps = np.array(claps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f15ba8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'medium_index',\n",
       " '_id': '1qv6-IQBuuM1GJakvjpI',\n",
       " '_score': 11.030249,\n",
       " '_ignored': ['full_text.keyword', 'text.keyword'],\n",
       " '_source': {'id': 3187,\n",
       "  'url': 'https://towardsdatascience.com/how-to-use-python-features-in-your-data-analytics-project-e8032374d6fc',\n",
       "  'title': 'How to use Python features in your data analytics project',\n",
       "  'subtitle': 'Python tutorial in Azure using OO,\\xa0NumPy…',\n",
       "  'claps': 22,\n",
       "  'responses': 1.0,\n",
       "  'reading_time': 9,\n",
       "  'publication': 'Towards Data Science',\n",
       "  'date': '2019-04-25',\n",
       "  'clap_prop': 1.0866971962718383e-05,\n",
       "  'text': 'Towards Data Science Apr 25 2019 Listen Save use Python feature data analytics project Python tutorial Azure using OO NumPy panda SQL PySpark 1 Introduction lot company moving cloud consider tooling shall used data analytics On-premises company mostly use propriety software advanced analytics BI reporting However tooling may logical choice cloud environment Reasons 1 lack integration cloud provider 2 lack big data support 3 lack support new use case machine learning deep learning Python general-purpose programming language widely used data analytics Almost cloud data platform offer Python support often new feature become available Python first Python seen Swiss Army knife data analytics 2 Objective tutorial two project created take account important feature Python Projects described follows Therefore following step executed standalone tutorial focus learn different aspect Python focus le deep dive separate aspect case interested deep learning see devops AI refer previous blog focus security see 3 Prerequisites following resource need created 4 OO NumPy panda sql Jupyter DSVM part sample Python project three class Using class football player data registered following step executed 4a Get started Log Windows Data Science Virtual Machine DSVM desktop overview icon preinstalled component found Click Jupyter short cut start Jupyter session Subsequently open Jupyter command line session found taskbar Copy URL open Firefox session download following notebook desktop DSVM Finally select upload notebook jupyter session click run button menu run code cell important part notebook also discussed remaining chapter 4b Object-oriented OO programming part tutorial inspired following tutorial Theophano Mitsa part three class created keep track football player data snippet first class Player found following seen class Subsequently class FirstTeamPlayer found following seen class Finally snippet training class found following seen class example three class instantiated used found final snippet 4c Matrix analytics using NumPy NumPy fundamental package scientific computing Python tutorial used matrix analytics Notice attribute _rawData already encapsulated Player class NumPy array NumPy often used Matplotlib visualize data snippet data taken player class matrix operation done basic advanced Full example found github project 4d Statistical analytics using panda Pandas package high-performance easy-to-use data structure data analysis tool Python hood panda us NumPy array structure tutorial used calculate basic statistic snippet data taken player class statistic operation done Full example found github project 4e Read/write database Finally data written SQL database tutorial MSSQL database used part DSVM Look Microsoft SQL Server Management Studio SSMS icon found taskbar start new session Log using Windows Authentication see also Look New Query menu start new query session execute following script Finally data written read database Pandas dataframes used see also snippet 5 PySpark Azure Databricks Spark cluster previous chapter code wa run single machine case data generated advanced calculation need done e.g deep learning possibility take heavier machine thus scale execute code compute distributed VMs Spark analytics framework distribute compute VMs thus scale adding VMs work time efficient supercomputer work Python used Spark often referred PySpark tutorial Azure Databricks used Apache Spark-based analytics platform optimized Azure following step executed 5a Get started Start Azure Databricks workspace go Cluster Create new cluster following setting Subsequenly select Workspace right-click select import radio button select import following notebook using URL See also picture Select notebook imported 4b attach notebook cluster created 4a Make sure cluster running otherwise start Walk notebook cell cell using shortcut SHIFT+ENTER Finally want keep track model create HTTP endpoint model and/or create DevOps pipeline project see advanced DevOps AI tutorial focus security see 5b Setup project project machine learning model created predicts income class person using feature age hour week working education following step executed Notice pyspark.ml library used build model also possible run scikit-learn library Azure Databricks however work would done driver master node compute distributed See snippet pyspark package used 6 Conclusion tutorial two Python project created follows lot company consider tooling use cloud data analytics almost cloud data analytics platform Python support therefore Python seen Swiss Army knife data analytics tutorial may helped explore possibilites Python 23 1 23 23 1 Towards Data Science home data science Medium publication sharing concept idea code Ryan Burge Apr 25 2019 Member-only Religious Composition Two Major Parties make lot data visualization time pretty good idea result calculation going look like write code fact result obvious dont even post anywhere 4 min read 4 min read Share idea million reader Ashutosh Singh Apr 25 2019 Member-only Classifying Products Banned Approved using Text Mining- Part II part explain optimize existing Machine Learning model Part deployment ML model using Flask previous article series discussed business problem shown train model using fastText classification Banned Approved product based information like Product Name product description specification 5 min read 5 min read Favio Vzquez Apr 25 2019 Jungle Koalas Pandas Optimus Spark expect newest library Databricks Koalas Optimus framework Apache Spark 3.x excited data science probably know Spark+AI latest summit started yesterday April 24th 2019 great thing talk spin-off youve following co-created 8 min read 8 min read Rinu Gour Apr 25 2019 Member-only Complete Guide Learn R R Programming Technology open source programming language Also R programming language latest cutting-edge tool R Basics hottest trend Moreover R command line interface C.L.I consists prompt usually character History R John Chambers colleague developed R Bell Laboratories Basically 8 min read 8 min read Sriram Parthasarathy Apr 25 2019 Member-only forecast sale revenue Compare various forecasting approach 100 method forecast sale question becomes one choose article briefly cover popular way forecast sale compare method key metric Depending use case customer may ok simple 5 min read 5 min read Ren Bremer Data Solution Architect Microsoft working Azure service ADFv2 ADLSgen2 Azure DevOps Databricks Function Apps SQL Opinions mine Medium Frank Andrade Towards Data Science Predicting FIFA World Cup 2022 Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup Machine Learning Adejumo Ridwan Suleiman Python Plain English 9 Things Shows Professional Python Developer Sunil Kumar JavaScript Plain English Salary Increased 13 Times 5 YearsHere Help Status Writers Blog Careers Privacy Terms Text speech',\n",
       "  'full_text': 'Towards Data Science Apr 25, 2019 Listen Save How to use Python features in your data analytics project Python tutorial in Azure using OO, NumPy, pandas, SQL, PySpark 1. Introduction A lot of companies are moving to cloud and consider what tooling shall be used for data analytics. On-premises, companies mostly use propriety software for advanced analytics, BI and reporting. However, this tooling may not be the most logical choice in a cloud environment. Reasons can be 1) lack of integration with cloud provider, 2) lack of big data support or 3) lack of support for new use cases such as machine learning and deep learning. Python is a general-purpose programming language and is widely used for data analytics. Almost all cloud data platforms offer Python support and often new features become available in Python first. In this, Python can be seen as the Swiss Army knife of data analytics. 2. Objective In this tutorial, two projects are created that take into account important features of Python. Projects are described as follows: Therefore, the following steps are executed: It is a standalone tutorial in which the focus is to learn the different aspects of Python. The focus is less to deep dive in the separate aspects. In case you more interested in deep learning, see here or in devops for AI, refer to my previous blogs, here and with focus on security, see here . 3. Prerequisites The following resources need to be created: 4. OO, NumPy, pandas and sql with Jupyter on DSVM In this part, a sample Python project with three classes. Using these classes, football players data is registered. The following steps are executed: 4a. Get started Log in to your Windows Data Science Virtual Machine (DSVM). On the desktop, an overview of icons of preinstalled components can be found. Click on Jupyter short cut to start a Jupyter session. Subsequently, open the Jupyter command line session that can be found in the taskbar. Copy the URL and open this in a Firefox session. Then download the following notebook to the desktop of your DSVM: Finally, select to upload the notebook in your jupyter session. Then click on the run button in the menu to run code in a cell. The most important parts of the notebook are also discussed in the remaining of the chapter. 4b. Object-oriented (OO) programming This part of the tutorial is inspired by the following tutorial by Theophano Mitsa. In this part, three classes are created to keep track of football players data. A snippet of the first class Player can be found below: The following can be seen in this class: Subsequently, class FirstTeamPlayer can be found below: The following can be seen in this class: Finally, a snippet of the training class can be found below: The following can be seen in this class: An example how the three classes are instantiated and are used can be found in the final snippet below: 4c. Matrix analytics using NumPy NumPy is the fundamental package for scientific computing with Python. In this tutorial it will be used to do matrix analytics. Notice that attribute _rawData is already encapsulated in the Player class as a NumPy array. NumPy is often used with Matplotlib to visualize data. In the snippet below, the data is taken from player class and then some matrix operations are done, from basic to more advanced. Full example can be found in the github project. 4d. Statistical analytics using pandas Pandas is the package for high-performance, easy-to-use data structures and data analysis tools in Python. Under the hood, pandas uses NumPy for its array structure. In this tutorial it will be used to calculate some basic statistics. In the snippet below, the data is taken from player class and then some statistics operations are done. Full example can be found in the github project. 4e. Read/write to database Finally, the data will be written to a SQL database. In this tutorial, the MSSQL database is used that is part of the DSVM. Look for the Microsoft SQL Server Management Studio (SSMS) icon that can be found in taskbar and start a new session. Log in using Windows Authentication, see also below. Look for New Query in the menu and start a new query session. Then execute the following script: Finally, the data can be written to and read from the database. Pandas dataframes will be used for this, see also the snippet below. 5. PySpark with Azure Databricks on Spark cluster In the previous chapter, all code was run on a single machine. In case more data is generated or more advanced calculations need to be done (e.g. deep learning), the only possibility is to take a heavier machine an thus to scale up to execute the code. That is, compute cannot be distributed to other VMs. Spark is an analytics framework that can distribute compute to other VMs and thus can scale out by adding more VMs to do work. This is most of times more efficient than having a supercomputer doing all the work. Python can be used in Spark and is often referred to as PySpark. In this tutorial, Azure Databricks will be used that is an Apache Spark-based analytics platform optimized for the Azure. In this, the following steps are executed. 5a. Get started Start your Azure Databricks workspace and go to Cluster. Create a new cluster with the following settings: Subsequenly, select Workspace, right-click and then select import. In the radio button, select to import the following notebook using URL: See also picture below: Select the notebook you imported in 4b and attach the notebook to the cluster you created in 4a. Make sure that the cluster is running and otherwise start it. Walk through the notebook cell by cell by using shortcut SHIFT+ENTER. Finally, if you want to keep track of the model, create an HTTP endpoint of the model and/or create a DevOps pipeline of the project, see my advanced DevOps for AI tutorial here , and with focus on security, see here . 5b. Setup of project In this project, a machine learning model is created that predicts the income class of a person using features as age, hours of week working, education. In this, the following steps are executed: Notice that pyspark.ml libraries are used to build the model. It is also possible to run scikit-learn libraries in Azure Databricks, however, then work would only be done be the driver (master) node and the compute is not distributed. See below a snippet of what pyspark packages are used. 6. Conclusion In this tutorial, two Python projects were created as follows: A lot of companies consider what tooling to use in the cloud for data analytics. In this, almost all cloud data analytics platforms have Python support and therefore, Python can be seen as the Swiss Army knife of data analytics. This tutorial may have helped you to explore the possibilites of Python. 23 1 23 23 1 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Ryan Burge Apr 25, 2019 Member-only The Religious Composition of the Two Major Parties I make a lot of data visualizations. Most of the time I have a pretty good idea of what the results of the calculations are going to look like before I write the code. In fact, some of the results are so obvious that I dont even post them anywhere 4 min read 4 min read Share your ideas with millions of readers. Ashutosh Singh Apr 25, 2019 Member-only Classifying Products as Banned Or Approved using Text Mining- Part II In this part, we will explain how to optimize the existing Machine Learning model in Part I and the deployment of this ML model using Flask. In the previous article of this series, We have discussed the business problem, shown how to train the model using fastText and classification of Banned or Approved products based on information like(Product Name, product description, and specifications). 5 min read 5 min read Favio Vzquez Apr 25, 2019 The Jungle of Koalas, Pandas, Optimus and Spark What to expect from the newest library from Databricks (Koalas), the Optimus framework and Apache Spark 3.x If you are as excited about data science as me, you probably know that the Spark+AI latest summit started yesterday (April 24th 2019). And there are great things to talk about. But I will do it with a spin-off. If youve been following me you now that I co-created a 8 min read 8 min read Rinu Gour Apr 25, 2019 Member-only A Complete Guide to Learn R R Programming Technology is an open source programming language. Also, the R programming language is the latest cutting-edge tool. R Basics is the hottest trend. Moreover, the R command line interface (C.L.I) consists of a prompt, usually the > character. History of R John Chambers and colleagues developed R at Bell Laboratories. Basically 8 min read 8 min read Sriram Parthasarathy Apr 25, 2019 Member-only How to forecast sales revenue: Compare various forecasting approaches There are 100s of methods to forecast sales. The question becomes which ones to choose. In this article, I will briefly cover the popular ways to forecast sales and how to compare the methods with key metrics. Depending on the use case, a customer may be ok with a simple 5 min read 5 min read Ren Bremer Data Solution Architect @ Microsoft, working with Azure services as ADFv2, ADLSgen2, Azure DevOps, Databricks, Function Apps and SQL. Opinions here are mine. More from Medium Frank Andrade in Towards Data Science Predicting The FIFA World Cup 2022 With a Simple Model using Python Sergio Pessoa Predicting FIFA 2022 World Cup with Machine Learning Adejumo Ridwan Suleiman in Python in Plain English 9 Things You Do That Shows You Are Not A Professional Python Developer Sunil Kumar in JavaScript in Plain English My Salary Increased 13 Times in 5 YearsHere Is How I Did It Help Status Writers Blog Careers Privacy Terms About Text to speech'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"hits\"][\"hits\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bfbd7b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/09/2022 15:10:07 - WARNING - haystack.finder -   DEPRECATION WARNINGS: \n",
      "            1. The 'Finder' class will be deprecated in the next Haystack release in \n",
      "            favour of a new `Pipeline` class that supports building custom search pipelines using Haystack components\n",
      "            including Retriever, Readers, and Generators.\n",
      "            For more details, please refer to the issue: https://github.com/deepset-ai/haystack/issues/544\n",
      "            2. The `question` parameter in search requests & results is renamed to `query`.\n",
      "12/09/2022 15:10:07 - INFO - elasticsearch -   POST https://localhost:9200/medium_index/_search [status:200 request:0.075s]\n",
      "12/09/2022 15:10:07 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/09/2022 15:10:07 - INFO - haystack.finder -   Reader is looking for detailed answer in 52378 chars ...\n",
      "/Users/kelvinchristian/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Loading faiss.\n",
      "/Users/kelvinchristian/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Loading faiss.\n",
      "/Users/kelvinchristian/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Loading faiss.\n",
      "/Users/kelvinchristian/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Loading faiss.\n",
      "/Users/kelvinchristian/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Loading faiss.\n",
      "/Users/kelvinchristian/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Loading faiss.\n",
      "/Users/kelvinchristian/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Loading faiss.\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Successfully loaded faiss.\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Successfully loaded faiss.\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Successfully loaded faiss.\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Successfully loaded faiss.\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Successfully loaded faiss.\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Successfully loaded faiss.\n",
      "12/09/2022 15:10:10 - INFO - faiss.loader -   Successfully loaded faiss.\n",
      "12/09/2022 15:10:12 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "12/09/2022 15:10:12 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "12/09/2022 15:10:12 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "12/09/2022 15:10:12 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "12/09/2022 15:10:12 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "12/09/2022 15:10:12 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "12/09/2022 15:10:12 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "12/09/2022 15:10:13 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "12/09/2022 15:10:13 - INFO - farm.data_handler.processor -   *** Show 1 random examples ***\n",
      "12/09/2022 15:10:13 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0-0\n",
      "Clear Text: \n",
      " \tpassage_text: Towards Data Science Nov 1 2019 Listen Save Pandas basic advanced Data Scientists Pandas make life easier data scientist data scientist someone obtain scrub explore model interpret data blending hacking statistic machine learning Hilary Mason smoke fire similar dealing data panda never take holiday help explore interpret data fast Pandas commonly used python library used data manipulation data analysis article try address panda concept trick make life easier Let u start basic advanced level walk panda help weather data Import Pandas create data frame many way creating data frame using file list dictionary etc created data frame reading data CSV file Output Select specific column Sometimes need operate manipulate specific column Let u assume would like analyze temperature changing daily case select temperature day Rename column Pandas provide simple function rename change name column set column make job easy Filtering data frame Suppose would like see city hotter along date Output far seen basic let dive deep real panda start said panda never take holiday even would like complex query Grouping Suppose want Manipulate particular group data case let u get row belong new york group object get summary sum mean median group time Group City group object wa created want see specific group data need get group Output Aggregations section grouped data city would like see average temperature average wind speed use aggregation Group aggregate Output Merging\n",
      " \tquestion_text: pandas join\n",
      " \tpassage_id: 0\n",
      "Tokenized: \n",
      " \tpassage_start_t: 0\n",
      " \tpassage_start_c: 0\n",
      " \tpassage_tokens: [565, 1722, 5954, 5423, 4662, 1442, 112, 954, 13041, 11873, 13163, 281, 3280, 3319, 5423, 14008, 13163, 281, 146, 301, 3013, 414, 9744, 414, 9744, 951, 6925, 24168, 5393, 1421, 18107, 414, 29354, 11597, 23725, 3563, 2239, 14909, 1766, 7332, 4603, 668, 1122, 4098, 414, 181, 5219, 393, 185, 2317, 244, 5393, 18107, 414, 1769, 13163, 281, 10266, 341, 39825, 5560, 341, 414, 18110, 414, 1966, 1566, 860, 1100, 181, 5219, 4286, 7610, 146, 301, 3013, 2780, 1717, 386, 3280, 3319, 672, 1656, 181, 5219, 244, 1650, 414, 20891, 13163, 281, 1045, 414, 5120, 171, 169, 2351, 414, 5120, 634, 2870, 889, 36451, 4753, 1412, 414, 5120, 2600, 414, 47896, 2870, 38252, 10908, 2167, 6730, 7411, 240, 4303, 21922, 2167, 6730, 2780, 1717, 6876, 74, 101, 11526, 5181, 2992, 1230, 403, 5163, 5181, 183, 6340, 4344, 6730, 13163, 281, 694, 2007, 5043, 38453, 464, 766, 6730, 278, 6730, 146, 633, 1365, 13663, 14322, 414, 5120, 45532, 74, 101, 192, 343, 29448, 552, 1248, 38252, 444, 450, 3280, 905, 12175, 1844, 588, 181, 5219, 386, 26, 181, 5219, 393, 185, 2317, 190, 74, 101, 2632, 25860, 826, 154, 45532, 236, 27420, 10246, 1989, 333, 414, 403, 905, 1717, 120, 3236, 9943, 92, 1423, 9657, 333, 7626, 120, 4819, 6797, 1266, 9640, 333, 86, 826, 412, 333, 7626, 13332, 1412, 236, 192, 2167, 333, 414, 240, 120, 333, 38252, 14644, 4950, 1635, 2810, 38015, 414, 343, 74, 101, 192, 674, 5181, 674, 2508, 2078, 304, 40796, 826, 13884, 38252, 4213]\n",
      " \tpassage_start_of_word: [1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tquestion_tokens: [642, 463, 281, 1962]\n",
      " \tquestion_offsets: [0, 1, 4, 7]\n",
      " \tquestion_start_of_word: [1, 0, 0, 1]\n",
      "Features: \n",
      " \tinput_ids: [0, 642, 463, 281, 1962, 2, 2, 565, 1722, 5954, 5423, 4662, 1442, 112, 954, 13041, 11873, 13163, 281, 3280, 3319, 5423, 14008, 13163, 281, 146, 301, 3013, 414, 9744, 414, 9744, 951, 6925, 24168, 5393, 1421, 18107, 414, 29354, 11597, 23725, 3563, 2239, 14909, 1766, 7332, 4603, 668, 1122, 4098, 414, 181, 5219, 393, 185, 2317, 244, 5393, 18107, 414, 1769, 13163, 281, 10266, 341, 39825, 5560, 341, 414, 18110, 414, 1966, 1566, 860, 1100, 181, 5219, 4286, 7610, 146, 301, 3013, 2780, 1717, 386, 3280, 3319, 672, 1656, 181, 5219, 244, 1650, 414, 20891, 13163, 281, 1045, 414, 5120, 171, 169, 2351, 414, 5120, 634, 2870, 889, 36451, 4753, 1412, 414, 5120, 2600, 414, 47896, 2870, 38252, 10908, 2167, 6730, 7411, 240, 4303, 21922, 2167, 6730, 2780, 1717, 6876, 74, 101, 11526, 5181, 2992, 1230, 403, 5163, 5181, 183, 6340, 4344, 6730, 13163, 281, 694, 2007, 5043, 38453, 464, 766, 6730, 278, 6730, 146, 633, 1365, 13663, 14322, 414, 5120, 45532, 74, 101, 192, 343, 29448, 552, 1248, 38252, 444, 450, 3280, 905, 12175, 1844, 588, 181, 5219, 386, 26, 181, 5219, 393, 185, 2317, 190, 74, 101, 2632, 25860, 826, 154, 45532, 236, 27420, 10246, 1989, 333, 414, 403, 905, 1717, 120, 3236, 9943, 92, 1423, 9657, 333, 7626, 120, 4819, 6797, 1266, 9640, 333, 86, 826, 412, 333, 7626, 13332, 1412, 236, 192, 2167, 333, 414, 240, 120, 333, 38252, 14644, 4950, 1635, 2810, 38015, 414, 343, 74, 101, 192, 674, 5181, 674, 2508, 2078, 304, 40796, 826, 13884, 38252, 4213, 2]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpassage_start_t: 0\n",
      " \tstart_of_word: [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      " \tlabels: []\n",
      " \tid: [0, 0, 0]\n",
      " \tseq_2_start_t: 7\n",
      " \tspan_mask: [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "_____________________________________________________\n",
      "12/09/2022 15:10:13 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "12/09/2022 15:10:13 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "12/09/2022 15:10:13 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "12/09/2022 15:10:13 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "12/09/2022 15:10:13 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "12/09/2022 15:10:13 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples:   0%|                          | 0/1 [00:00<?, ? Batches/s]12/09/2022 15:10:13 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "12/09/2022 15:10:13 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "12/09/2022 15:10:13 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "Inferencing Samples: 100%|██████████████████| 1/1 [00:05<00:00,  5.08s/ Batches]\n",
      "Inferencing Samples: 100%|██████████████████| 1/1 [00:13<00:00, 13.23s/ Batches]\n",
      "Inferencing Samples:   0%|                          | 0/1 [00:00<?, ? Batches/s]Process SpawnPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 356, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/kelvinchristian/opt/anaconda3/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Inferencing Samples:   0%|                          | 0/1 [00:06<?, ? Batches/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/29/5gqsxp7n0pq0_m7j85mtnl3c0000gn/T/ipykernel_11467/3799101032.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_retriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_reader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/haystack/finder.py\u001b[0m in \u001b[0;36mget_answers\u001b[0;34m(self, question, top_k_reader, top_k_retriever, filters, index)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reader is looking for detailed answer in {len_chars} chars ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         results = self.reader.predict(query=question,\n\u001b[0m\u001b[1;32m     83\u001b[0m                                       \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                                       top_k=top_k_reader)  # type: Dict[str, Any]\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/haystack/reader/farm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, query, documents, top_k)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# get answers from QA model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# TODO: Need fix in FARM's `to_dict` function of `QAInput` class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         predictions = self.inferencer.inference_from_objects(\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0mobjects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiprocessing_chunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/farm/infer.py\u001b[0m in \u001b[0;36minference_from_objects\u001b[0;34m(self, objects, return_json, multiprocessing_chunksize, streaming)\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# TODO investigate this deprecation warning. Timo: I thought we were about to implement Input Objects, then we can and should use inference from (input) objects!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;31m#logger.warning(\"QAInferencer.inference_from_objects() will soon be deprecated. Use QAInferencer.inference_from_dicts() instead\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m         return self.inference_from_dicts(dicts, return_json=return_json,\n\u001b[0m\u001b[1;32m    708\u001b[0m                                          multiprocessing_chunksize=multiprocessing_chunksize, streaming=streaming)\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/farm/infer.py\u001b[0m in \u001b[0;36minference_from_dicts\u001b[0;34m(self, dicts, return_json, multiprocessing_chunksize, streaming)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mquestions_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestions_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'More than one question for document. NaturalQuestions inference will return just the answer to the first question.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         return Inferencer.inference_from_dicts(self, dicts, return_json=return_json,\n\u001b[0m\u001b[1;32m    689\u001b[0m                                                multiprocessing_chunksize=multiprocessing_chunksize, streaming=streaming)\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/farm/infer.py\u001b[0m in \u001b[0;36minference_from_dicts\u001b[0;34m(self, dicts, return_json, multiprocessing_chunksize, streaming)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;31m# return a generator object if streaming is enabled, else, cast the generator to a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstreaming\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/farm/infer.py\u001b[0m in \u001b[0;36m_inference_with_multiprocessing\u001b[0;34m(self, dicts, return_json, aggregate_preds, multiprocessing_chunksize)\u001b[0m\n\u001b[1;32m    534\u001b[0m                 \u001b[0;31m# TODO change format of formatted_preds in QA (list of dicts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0maggregate_preds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                     predictions = self._get_predictions_and_aggregate(\n\u001b[0m\u001b[1;32m    537\u001b[0m                         \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaskets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m                     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/farm/infer.py\u001b[0m in \u001b[0;36m_get_predictions_and_aggregate\u001b[0;34m(self, dataset, tensor_names, baskets)\u001b[0m\n\u001b[1;32m    622\u001b[0m                 \u001b[0;31m# Aggregation works on preds, not logits. We want as much processing happening in one batch + on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;31m# So we transform logits to preds here as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m                 \u001b[0;31m# preds = self.model.logits_to_preds(logits, **batch)[0] (This must somehow be useful for SQuAD)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_to_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/farm/modeling/adaptive_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;31m# Run forward pass of language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# Run forward pass of (multiple) prediction heads using the output from above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/farm/modeling/adaptive_model.py\u001b[0m in \u001b[0;36mforward_lm\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;31m# Run forward pass of language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextraction_layer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# get output from an earlier layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/farm/modeling/language_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, segment_ids, padding_mask, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \"\"\"\n\u001b[0;32m--> 670\u001b[0;31m         output_tuple = self.model(\n\u001b[0m\u001b[1;32m    671\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         )\n\u001b[0;32m--> 709\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    710\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 )\n\u001b[1;32m    443\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    445\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add cross attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=question, top_k_retriever=10, top_k_reader=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13353bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'elasticsearch tutorial',\n",
       " 'no_ans_gap': 1.0786566734313965,\n",
       " 'answers': [{'answer': 'port check failed',\n",
       "   'score': 5.505100727081299,\n",
       "   'probability': 0.6679161787033081,\n",
       "   'context': 'ked RDS instance troubleshooting technique used connectivity issue application stack component Consul Elasticsearch user able access web application port check used see connectivity working port check successful following could common reason port check failed could common reason post covered one commonly used port check technique used combination tool technique effectively troubleshoot identify root cause Troubleshooting connectivity issue become challenging task Hence important spend time famil',\n",
       "   'offset_start': 242,\n",
       "   'offset_end': 259,\n",
       "   'offset_start_in_doc': 2545,\n",
       "   'offset_end_in_doc': 2562,\n",
       "   'document_id': 'MqsB-YQBuuM1GJakRE_e',\n",
       "   'meta': {'id': 1032,\n",
       "    'url': 'https://medium.com/swlh/how-to-troubleshoot-connectivity-issues-in-aws-deployments-343f90cb8609',\n",
       "    'title': 'How To Troubleshoot Connectivity Issues In AWS Deployments?',\n",
       "    'subtitle': '-',\n",
       "    'claps': 132,\n",
       "    'responses': 1.0,\n",
       "    'reading_time': 8,\n",
       "    'publication': 'The Startup',\n",
       "    'date': '2019-10-14',\n",
       "    'clap_prop': 6.520183177631029e-05,\n",
       "    'full_text': \"The Startup Oct 14, 2019 Member-only Listen Save How To Troubleshoot Connectivity Issues In AWS Deployments? How To Troubleshoot Connectivity Issues In AWS Deployments? Whether you are just learning AWS or have been using the amazon web services for some time, you will invariably run into connectivity issues in your deployments. For example, not able to SSH into the EC2 instance, the application tier is not able to talk to the database, and so on. In fact, there may be times when the connectivity was working fine when your stack was deployed, but it broke after some time. I am sure you can relate to at least some of these experiences. In this post, I will talk about how to troubleshoot and resolve some of the commonly encountered connectivity issues in AWS deployments. Using Telnet For Port Check Before we start talking about the issues, lets first learn a simple technique named port check that can be used for troubleshooting. In networking, a port check refers to testing whether a port on a given node is listening or not. For example, if you want to check for the standard SSH port on a machine, you would check port 22 . Why is a port check important? It is important because it is one of the most fundamental checks you can do for testing connectivity between two components without even knowing much about the components themselves. To explain this further, if an application running on an EC2 instance is failing to connect to the RDS instance and the port check for the database port fails from the EC2 instance, you can easily confirm that there is some connectivity issue between these two. You can use the telnet utility for a port check. It is available on most platforms and is often pre-installed or can be easily installed at a later point. In order to do the port check, you will specify a command like the one shown below. telnet <target-ip-address-or-dns-name> <port> If you are able to telnet successfully, the port check is successful. Otherwise, it has failed. Simple! The following screenshot shows a successful port check using telnet on an EC2 instance with the IP address 10.1.0.195 on the SSH port 22 . Here is another screenshot that shows a failed port check. In this case, the telnet connection has simply hung (that is, it is not able to connect successfully). But, you may see other variants like not able to connect, etc. Common Connectivity Issues In AWS Deployments Lets talk about some common connectivity issues in AWS deployments now. Here is a list of such issues. These are just some of the commonly encountered issues. But, these represent some commonly observed patterns and you may find other issues that follow the same pattern. So, lets discuss how to troubleshoot and resolve these. Not able to connect to an EC2 instance via SSH If you are not able to SSH into an EC2 instance, you can do a port check using the instance IP or DNS name on the SSH port to see if the connectivity is working at least. If the port check was successful (that is, you were able to telnet to the SSH port), but you are still not able to connect via SSH, check for the following. However, if the port check failed, the following could be some common reasons. An application running on an EC2 instance is not able to connect to the RDS instance To troubleshoot this further, you can do a port check from the EC2 instance to the RDS instance port and see if that works. The following screenshot shows a successful telnet port check to an RDS instance. In that case, check for the following. If the port check failed, these could be some common reasons. Although we talked about RDS instance here, these troubleshooting techniques can be used for connectivity issues for other application stack components, such as Consul, Elasticsearch, and so on. The users are not able to access the web application Again, a port check can be used here to see if the connectivity is working. If the port check is successful, the following could be some common reasons. If the port check failed, these could be some common reasons. In this post, we covered one of the most commonly used port check technique. It can be used in combination with other tools and techniques to effectively troubleshoot and identify the root cause. Troubleshooting connectivity issues can become a challenging task. Hence, it is important to spend some time and familiarize yourself with these tools and techniques in advance so that you are better prepared when issues arise. Other Readings How To Troubleshoot AWS CloudFormation Errors? medium.com AWS CloudFormation An Architects Best Friend AWS CloudFormation An Architects Best Friend medium.com Not Just Another AWS RDS Tutorial Not Just Another AWS RDS Tutorial medium.com Not Just Another AWS EC2 Tutorial Not Just Another AWS EC2 Tutorial medium.com Be a smart troubleshooter! Nitin If you liked this post, you will find my  AWS Advanced For Developers  course and other  AWS Courses  helpful that focus on many such best practices and techniques to design and deploy real-world applications in AWS. Originally published at  Cloud Nine Apps  . 142 1 142 142 1 Sign up for Top 5 Stories By The Startup Get smarter at building your thing. Join 176,621+ others who receive The Startup's top 5 stories, tools, ideas, books delivered straight into your inbox, once a week. Take a look. Emails will be sent to shadow_kelvin777@ymail.com. Not you? More from The Startup Get smarter at building your thing. Follow to join The Startups +8 million monthly readers & +760K followers. Matt Slutzkin Oct 14, 2019 Are you a genius, or just very talented? And why all successful companies need both types of people. A work colleague of mine recently told me a quote by philosopher Arthur Schopenhauer: Talent hits a target no one else can hit; Genius hits a target no one else can see. This quote has been bouncing around the internet for a while now, in fact, even US President Donald 4 min read 4 min read Share your ideas with millions of readers. Ryan Gross Oct 14, 2019 Member-only A practical guide to get started with DataOps How your organization can adopt the (non-technical) practices in DataOps to improve Data Governance outcomes A few months ago I wrote a post on the coming rise of DataOps, in which I predicted that the world of Data Governance will see some of the same shakeups that IT Operations experienced during the rise of DevOps. In this post, Ill share some practical tips for how 6 min read 6 min read William Anderson Oct 14, 2019 Member-only How To Create A Work Environment Where Problem-Solving Thrives Create A Workplace That Allows For Thinking Andys head was kicked back as she stared at the ceiling. With a quick swing, her eyes refocused on the computer monitor in front of her as I walked by. Is everything alright? Totally, I just needed to sort out how I wanted to connect these services together. We 3 min read 3 min read Byrne Hobart Oct 14, 2019 Member-only Basis Risk, Alpha Factories, and Being too Clever for your own Good The beauty of a robust financial market is that you can make exactly the bet you want to make. Suppose youre an American investor and youve found a really great company in, say, Argentina. You buy the stock, it rises 50% but its priced in Argentine Pesos, which are 6 min read 6 min read Tamara Naidoo Oct 14, 2019 Member-only Reasons why you and your Mcdonalds Burger are complicit in international tax avoidance In the millenial experience where individualistic expression is so highly valued, we all seem to agree on one thing - McDonalds. Between the memories of kids' birthday parties, the sobering smell of fries at 03:00am or the weary slurp on a McFlurry after work - McDonalds has built a reputation 6 min read 6 min read Nitin Patil Nitin Patil is a filmmaker at TwilightGlowFilms.com and technopreneur at CloudNineApps.com . He is also a writer and motivational blogger at NitinPatil.net . More from Medium Tiexin Guo in 4th Coffee 10 New DevOps Tools to Watch in 2023 MyHistoryFeed Best Practices for Kubernetes in 2023 John David Luther in AWS Tip 12 AWS Essential Bookmarks Dmitry Kruglov in Better Programming The Architecture of a Modern Startup Help Status Writers Blog Careers Privacy Terms About Text to speech\"}},\n",
       "  {'answer': 'Elastic',\n",
       "   'score': -4.1459269523620605,\n",
       "   'probability': 0.004034311976283789,\n",
       "   'context': \" circumstance particularly confusing exactly slot-filling end QA begin former usually mean searching extracting answer textual data rather figuring slot fill query database context Game Thrones bot mean taking user question searching proper index ElasticSearch extracting correct answer returned result going exactly let look different type question user might ask Essentially three category question 1 Questions answered querying knowledge graph ha Hound killed Jon Snow 's father motto house Glover\",\n",
       "   'offset_start': 247,\n",
       "   'offset_end': 254,\n",
       "   'offset_start_in_doc': 6879,\n",
       "   'offset_end_in_doc': 6886,\n",
       "   'document_id': 'Cqv8-IQBuuM1GJakaEAG',\n",
       "   'meta': {'id': 3271,\n",
       "    'url': 'https://towardsdatascience.com/building-a-chatbot-for-slack-from-scratch-part-1-understanding-language-1f085b2eda6c',\n",
       "    'title': 'Building a Game of Thrones chatbot for Slack from scratch: Part 1 Understanding Language',\n",
       "    'subtitle': '-',\n",
       "    'claps': 72,\n",
       "    'responses': 0.0,\n",
       "    'reading_time': 13,\n",
       "    'publication': 'Towards Data Science',\n",
       "    'date': '2019-03-25',\n",
       "    'clap_prop': 3.556463551435107e-05,\n",
       "    'full_text': \"Towards Data Science Mar 25, 2019 Listen Save Building a Game of Throne chatbot for Slack: Part 1 Understanding Language Lessons learned applying deep learning to natural language understanding and incorporating question and answering This past summer I decided to test my NLP skills and undertake building a chatbot. As a big Game of Thrones fan I settled on creating a chatbot for Game of Thrones. Initially, my goal was just to provide a way to get various types of GOT news easily from places like Reddit, Watchers on the Wall, Los Siete Reinos, and Twitter. However, I quickly decided to expand into other tasks and I became particularly intrigued at how to integrate modern NLP approaches to enrich my chatbot. This first article will cover the natural language understanding and question answering components; and the second article will talk more about the platform and architecture. If want to use the chatbot then you can join the Citadel community on Slack (though, as I will describe, I havent yet added all the features described into the actual production version). In the future I plan on adding support to install it on your workspace. Also be warned that some of the examples in this article and the content in the bot itself contain information through season 7. Natural Language Understanding (NLU) Deep learning for NLU: An Introduction Where and how to integrate deep learning into a chatbot is actually a somewhat tricky question. On a fundamental level with chatbots you can use rule based methods, machine learning, or some combination of the two. With rule based methods you generally have a guarantee that the chatbot will respond properly to user queries as a long as users write in a formulaic and limited way. With machine learning (or even statistical NLP methods) you can break out of the rigid formulas and allow users to type more naturally. However, by doing this you also introduce uncertainty (even with the best models). Also even the SOTA models generally only work for limited types of dialogue. For instance, a model trained for goal oriented dialogue will typically break down if the user begins to engage in chit-chat. Deep learning of course also requires data and in this situation we often have a cold-start-problem. Therefore, you often need to write sample data of what you think users will ask. This process is both time consuming and often inaccurate. The formualic way Before looking at why we might need machine learning based models, lets look at some of the limitations of using rule based methods. With the formulaic way for the chatbot we might write the code something like this: Now users have to write in a very formulaic way to use the chatbot. They have to write exactly Quote Jon Snow or News Reddit This is fine if you only want to include simple queries. But what if what want to support functionality for phrases like Quote about Jon Snow or even Quote from Jon Snow when he is talking about Stannis? Yes we could force users to do things in a formulaic way too but that quickly becomes complicated and burdensome for users. Similarly with news, supporting complex queries like Get news from the past 24 Hours on Reddit or even News about Jon Snow in Season 8 becomes arduous at best and impossible at worst. Slot filling and intent detection This brings us to machine learning for slot filling and intent detection. A key area for using deep learning in chatbots is to automatically take a users inputted string and map the relevant tokens to slots for an API (this is known as slot filling or SLU). A related area is intent detection which focuses on mapping the utterance to an intent. Here is an example of a SLU annotated Slot filling in a sense is a more fine grained version of named entity recognition (NER). For instance, in a pure NER setting Jon Snow might always have the label character whereas for slot filling the label will change based on the slot he should occupy. The format of annotation is called IOB , this stands for inside-outside-begining. It is meant to show chunks of the tokens together. As the bots response will depend on both the slots and the users goal many papers focus on joint slot filling and intent detection. Additionally many NLU libraries such as the Rasa-NLU framework provide joint SLU intent detection. Once we have the slots filled we still need to construct the actual query. The query construction will depend on how your database is set up. As such, in most cases this code you write manually yourself. However, there are some models that learn a direct mapping of the utterance to a SQL query. The vast majority of the time though, you will have an existing API or want to construct one. So lets look at how you might turn this into a simple API request : Now we could use this simple Flask API to handle these requests. Returning to our previous news example we would label data in the following format to work with the API: As you can see this format allows us to construct API calls and SQL queries much easier. Now we could define a function (assuming we had already run a NER and tagged the news stories). Limited data scenarios The problem of this approach, and of course deep learning in general, is the need for large amounts of labeled training data. One approach that Im currently researching is the use of meta-learning on many annotated dialogue datasets in order to enable the model to rapidly adapt to just a few examples. Slot alignment is another interesting (although somewhat limited) approach. Towards Zero-Shot Frame Semantic Parsing for Domain Scaling , a 2017 article by Google Researchers, described using the names of the slots and/or documentation of the slots in the API to effectively perform zero shot filling. The idea is that if the model was already trained on booking an airline then it should also be able to book a bus as the slots should generally overlap (i.e., both would have a start_city , destination_city ). Taking this idea a step further a restaurant based dialogue system might have a restaurant_city (i.e., book me a restaurant in Chicago) and a hotel might have hotel_city . By exploiting similar semantics between phrases a model could learn to fill restaurant_city effectively, even though it was only trained on airline booking data. Of course this approach also has limitations: (1) it cannot work on drastically different domains with little to no overlap; (2) in some cases there actually could be negative transfer (e.g., it performed worse on taxi booking; it confused drop_off and pickup_spot because these are context dependent; even though these could align with start_city and destination_city their representation are not similar). For my use case this approach would likely not work as there are few overlapping semantic slots between the large public slot filling datasets and my GOT chatbot. Beyond joint slot filling and intent detection models But even joint NLU models have their limitations as they do not use context. For instance, suppose the user stated Quote from Robert Baratheon and then said Get me another quote from him. In this scenario one of the NLU models previously described wont know what to do as it does not use conversation history. Similarly, a user might ask the question Who is Jon Snow's mother? the bot would (hopefully) return Lyanna Stark then if the user asked When did she run off with Rhaegar? it would likely not even cast her to a slot. There might also be times when we need to update or ask for additional information about certain slots. For instance, if the user asked for News from the past 24 hours about Season 8? but the API required the news source to be specified the bot might reply From what source? Or alternatively if the user stated Get the scene from episode 2?, the bot might then reply from what season? End-to-end dialogue models should be able to handle these tasks. One challenge that was created to measure the progress at this task was the Dialogue State Tracking. In particular DSTC2 , the second version of the challenge, measured how well models could issue and update API calls and request additional information from the user when needed. One of the first models to do well on this challenge was Memory Networks adopted for goal oriented dialogue. This was done by researchers from Facebook in the paper Learning End-to-End Goal-Oriented Dialog. They showed that Memory Networks outperformed other machine learning methods by large margins. More recently there have been papers like  Mem2Seq  that actively incorporate dialogue history with the knowledge base and use them both in response generation. Specifically, Mem2Seq has two parts, a memory encoder which encodes the dialog history and a decoder that uses the encoded dialogue/KB to generate a user response. Mem2Seq acheived SOTA results on the DSTC2 challenge, BABI, and the in-car stanford dataset. To actually train Mem2Seq for GOT-Bot requires three things: a knowledge base, annotated intents, and slot annotated dialogue histories. This makes it harder to adapt to GOT-Bot as the KB needs to be converted into triplets such as (person, person2, relation). Question Answering The line between where question answering begins and slot filling ends is often quite blurry. In research terms we usually see QA as referring to the answering of a question based on unstructured textual data. (It can also be based on a structured knowledge base, but in this circumstance it is particularly confusing where exactly slot-filling ends and QA begins). In the former, this usually means searching and extracting the answer from textual data rather than figuring what slots to fill to query a database. In the context of the Game of Thrones bot it means taking a user question, searching the proper indices on ElasticSearch, and then extracting the correct answer from the returned results. Before going into how exactly lets look at different types of questions a user might ask: Essentially there are three categories of questions: (1) Questions that can be answered by querying the knowledge graph. Who has the Hound killed? Who is Jon Snow's father? What is the motto of house Glover? Who was Margeary married to? What region is Harrenhall in? These questions all have known answers that can be found in a structured knowledge graph. The problem is that we need to turn the user query into SQL or an API request. This is similar to what we need to do with slot-filling. In many cases we can actually cast this as a slot filling problem via phrasing questions as another intent. For instance, Or in the case of the following question: We could then construct an API request in a similiar fashion. However, there is an abundance of datasets with questions to SQL, so in this instance it might make sense to use one of those datasets. (2) Questions not in the knowledge graph but that still have known answers and can be extracted from the MediaWiki pages or other GOT sites. How did the war of the five king's start? What happened during Harrenhal tourney? What was the war of the five kings? How did Robert's rebellion end? Who got the Tyrell's to support the Lannisters? The most relevant datasets/models for this task are datasets like MS MARCO and TriviaQA. Although many researchers evaluate on SQUAD, in reality you are going to almost never have the exact context paragraph given to you. This makes models that perform well on MS MARCO ideal as they are given a whole list of ranked results and have to extract the correct answer from them. The QuAC dataset or Question and Answering in Context is similar to the previously mentioned end-to-end dialogue models for question and answering. It contains questions and follow up questions that involve multiple dialogue turns. Models like FlowQA can work well at this conversational QA task as they add dialogue history to the base model. (3) Questions where the answer is subjective or speculative and that require finding similar questions or alternatively performing multi-hop inference. Why did Sansa trust Joffery? Who will survive season 8 and why? If Robb Stark hadn't broken his marriage pack would've the Freys betrayed him? Who will kill Cersei? Is Jon the prince that was promised? These questions have no definitive answers and require either analysis or speculation. Therefore the best solution is to find similiar questions that were already answered. This can be done through the scraped Quora index. However, here we will not use a QA model but a question similarity model. The question similarity can be done using a variety of methods. My current model in production uses a basic ElasticSearch and then reranks the results using the Universal Sentence Encoder + cosine similarity. In order to gather more data to improve ranking the bot currently shows the user all of the top ten results. We can then retrain the model based on the users choices. However, there are several problems with this approach. First, in many cases the initial ElasticSearch often does not return good questions. Second, users might return another interesting answer that does not directly answer their question. Still this weak supervision means that one can manually annotate the examples much quicker later. Creating a good on-boarding process Creating a good on-boarding process is also essential to getting users. Your bot needs to make an immediate positive impression or else people will move-on. For this reason in order to make a good on-boarding process I decided to write a rule based conversation. The bot first introduces itself with a Direct Message welcoming them to the Citadel. Throughout the on-boarding process the users state is tracked in Redis. At the end of each response the users state is updated in the Redis. Here I decided to use simple dictionaries to map the users state to actions in order to avoid lengthy if statements. The on-boarding process aims to get users acquainted with the basic features of the bot in a fun and friendly manner. One problem with manually defined rules is if the user says something unexpected or even slightly different than what you hard-code, the bot will fail. I found this out the hard way when I accidentally let a bug slip past my unit tests and my manual tests. I was expecting users to respond yes to the question Would you like me to show you around the Citadel However users often responded with things like yes thanks or yes please a really simple error that I did not catch. That is why I recommend having a variety of people beta-test your chatbot because you may inadvertantly miss some things. Response generation I didnt talk too much about the actual response generation in this article. For the most part this is done by recombining the responses of the previously described elements with basic phrases. There are of course many more sophisticated methods to generate responses that are unique and change over time. However, right now Im still using simple phrases to combine the results of NLU calls from the API. What about chit-chat and non-goal oriented interactions? This is an area I have not researched that much but I hopefully will be able to dive into subsequent additions. Essentially, this is when the user doesnt want to accomplish a specific tasks but just wants to chat about elements of Game of Thrones in general and hear witty/interesting response from the bot. The Current State of the Bot and future improvements Currently, the chatbot is still in the formulaic state. I havent been able to annotate enough training data or incorporate meta-/unsupervised learning effectively enough to make slot filling perform consistently. However, my trained models are getting better and Im hoping to roll out an update soon which incorporates them. Im also looking at training Mem2Seq to handle the whole dialogue process via meta-learning, however this is in the more distant future. In terms of question and answering the searching of the Quora index is still very poor and there is no support for querying the knowledge base. Im hoping to improve QA question ranking of the Quora index using the BERT Reranker that was pre-trained on MS MARCO. Im hoping to rewrite the news system so you can ask for things like latest about Season 8 or new Jon Snow memes from Reddit. Finally, Im adding some rule based dialog flows for more realistic chat sequences. In part two of this series I will go into more practical aspects of the chatbot such as the platforms and tools used. 74 74 74 More from Towards Data Science Your home for data science. A Medium publication sharing concepts, ideas and codes. Eben du Toit Mar 25, 2019 Member-only Transforming BigQuery JSON API responses recursively Building key-value pairs from field/value row nests Say it with me: Nested JSON is hard to work with!. Am I right? Most certainly! Now that we got that out of the way, let me just say that I believe in JSON through-and-through. It is logical, it is universal and most languages use it to create fast-access hash-map 2 min read 2 min read Share your ideas with millions of readers. Maxim Scherbak Mar 25, 2019 Member-only Data Science vs Business Intelligence: same but completely different What to expect from your first Data Science project? A guide for businesses. Part 2 This is part 2 of this series. See part 1 here Unlike big tech companies, businesses, in general, are only dipping their toes into Data Science and AI. As always with early adoption, it doesnt go easy most of the projects do not advance beyond Proof of Concept phase 8 min read 8 min read Susmith Reddy Mar 25, 2019 Pre-Processing in OCR!!! A basic explanation of the most widely used preprocessing techniques by the OCR system. Welcome to part II, in the series about working of an OCR system. In the previous post, we briefly discussed the different phases of an OCR system. Among all the phases of OCR, Preprocessing and Segmentation are the most important phases, as the accuracy of the OCR system highly depends 7 min read 7 min read Zeming Yu Mar 25, 2019 Member-only My top 10 Python packages for data science After making a leap from SAS to Python over 4 years ago, I never looked back. Over the last 4 years, I have transitioned from using SAS exclusively for all data processing and statistical modeling tasks to using Python for these tasks. One barrier I had to overcome was to keep discovering and learning to use all the great packages put together by the open-source community. 4 min read 4 min read Cassie Kozyrkov Mar 25, 2019 Data science effectiveness as a UX problem We data scientists spend so much of our effort helping you understand your users that you forget that we are users too. Data scientists are users too. There are many instances where it feels like someone attempted to make a data science tool for data scientists without ever having met 6 min read 6 min read Isaac Godfried Data Scientist, ex-Data Engineer, Maintainer of Flow Forecast More from Medium Anthony Cavin in Towards Data Science GPT-3 Parameters and Prompt Design Vinithavn in Geek Culture A Chatbot Application by finetuning GPT-3 Vasily Konovalov in DeepPavlov Introducing DeepPavlov Library 1.0.0an Open-Source Natural Language Processing Library Shreyz-max Fine-Tuning GPT3 for free Help Status Writers Blog Careers Privacy Terms About Text to speech\"}}],\n",
       " 'question': 'elasticsearch tutorial'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b101113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.custom_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6661719",
   "metadata": {},
   "outputs": [],
   "source": [
    "[da[\"_score\"] for da in res[\"hits\"][\"hits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be882288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26532bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_input = {\n",
    "    'question': 'Python tutorial',\n",
    "    'context': res[\"hits\"][\"hits\"][0][\"_source\"][\"text\"]\n",
    "}\n",
    "result = nlp(QA_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa71dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839de3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res[\"hits\"][\"hits\"][0][\"_source\"][\"text\"][4400:4500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b5aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
